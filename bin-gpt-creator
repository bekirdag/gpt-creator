#!/usr/bin/env bash
# gpt-creator — scaffolding & orchestration CLI
# Aligns with Product Definition & Requirements (PDR v0.2)
# Usage: gpt-creator <command> [args]

set -Eeuo pipefail

mkdir -p "${PWD}/.gpt-creator/shims" 2>/dev/null || true
export BASH_ENV="${BASH_ENV:-$PWD/.gpt-creator/shims/bash_env.sh}"

if [[ -t 1 && $# -eq 0 && -z "${GC_SKIP_TUI_AUTO:-}" ]]; then
  export GC_SKIP_TUI_AUTO=1
  exec "$0" tui
fi

# Crash logging globals
GC_CRASH_LOGGED=0
GC_LAST_ERROR_CMD=""
GC_LAST_ERROR_STATUS=0
GC_FAIL_LOG_DIR=""
GC_INVOCATION=""

GC_LAST_CRASH_LOG=""
GC_MAIN_PID="$$"

if [[ -n "${GC_REPORTS_ON:-}" && "${GC_REPORTS_ON}" != "0" ]]; then
  GC_REPORTS_ON=1
else
  GC_REPORTS_ON=0
fi
GC_REPORTS_IDLE_TIMEOUT="${GC_REPORTS_IDLE_TIMEOUT:-1800}"
GC_REPORTS_CHECK_INTERVAL="${GC_REPORTS_CHECK_INTERVAL:-60}"
GC_REPORTS_INITIALIZED=0
GC_REPORTS_STORE_DIR=""
GC_REPORTS_ACTIVITY_FILE=""
GC_REPORTS_IDLE_SENTINEL=""
GC_REPORTS_WATCHDOG_PID=""
# shellcheck disable=SC2034
GC_FILTERED_ARGS=()
# shellcheck disable=SC2034
GC_LAST_AUTO_COMMIT_HASH=""
# shellcheck disable=SC2034
GC_LAST_AUTO_COMMIT_STATUS=""
# Ensure docker compose commands have adequate timeouts unless caller overrides
if [[ -n "${GC_DOCKER_COMPOSE_TIMEOUT:-}" ]]; then
  COMPOSE_HTTP_TIMEOUT="$GC_DOCKER_COMPOSE_TIMEOUT"
  DOCKER_CLIENT_TIMEOUT="$GC_DOCKER_COMPOSE_TIMEOUT"
fi
: "${COMPOSE_HTTP_TIMEOUT:=600}"
: "${DOCKER_CLIENT_TIMEOUT:=$COMPOSE_HTTP_TIMEOUT}"
export COMPOSE_HTTP_TIMEOUT DOCKER_CLIENT_TIMEOUT

: "${GC_DOCKER_HEALTH_TIMEOUT:=10}"
: "${GC_DOCKER_HEALTH_INTERVAL:=1}"
: "${GC_PNPM_VERSION:=10.17.1}"
# shellcheck disable=SC2034
GC_CODEX_EXEC_TIMEOUT_INITIAL="${GC_CODEX_EXEC_TIMEOUT-}"
: "${GC_CODEX_EXEC_TIMEOUT:=0}"
: "${GC_CODEX_EXEC_MAX_DURATION:=900}"
: "${GC_CODEX_MAX_TURNS:=180}"
: "${GC_CODEX_MAX_TOKENS_PER_TASK:=0}"
export GC_CODEX_EXEC_MAX_DURATION
export GC_CODEX_MAX_TURNS

resolve_cli_root() {
  local source="${BASH_SOURCE[0]}"
  while [[ -L "$source" ]]; do
    local dir
    dir="$(cd "$(dirname "$source")" && pwd)"
    source="$(readlink "$source")"
    [[ "$source" != /* ]] && source="$dir/$source"
  done
  local abs_dir
  abs_dir="$(cd "$(dirname "$source")" && pwd)"
  GC_SELF_PATH="${abs_dir}/$(basename "$source")"
  cd "${abs_dir}/.." && pwd
}

cmd_task_convert() {
  warn "'task-convert' is deprecated; use 'create-tasks' instead. Running create-tasks now."
  cmd_create_tasks "$@"
}

CLI_ROOT="$(resolve_cli_root)"
unset -f resolve_cli_root

gc_configure_tmpdir() {
  local base="${1:-${PROJECT_ROOT:-$PWD}}"
  local dir="${base}/.gpt-creator/tmp"
  if mkdir -p "$dir" 2>/dev/null; then
    GC_TMP_DIR="$dir"
    TMPDIR="$dir"
    export GC_TMP_DIR TMPDIR
    mkdir -p "${base}/tmp" 2>/dev/null || true
  else
    if [[ -n "${GC_TMP_DIR:-}" ]]; then
      :
    elif [[ -n "${TMPDIR:-}" && -d "${TMPDIR}" ]]; then
      GC_TMP_DIR="$TMPDIR"
      export GC_TMP_DIR
    fi
  fi
}

gc_configure_tmpdir

if [[ -z "${GC_COMMAND_FAILURE_CACHE:-}" ]]; then
  if [[ -n "${GC_TMP_DIR:-}" ]]; then
    GC_COMMAND_FAILURE_CACHE="${GC_TMP_DIR}/command-failures.json"
  else
    GC_COMMAND_FAILURE_CACHE="${TMPDIR:-/tmp}/command-failures.json"
  fi
fi
export GC_COMMAND_FAILURE_CACHE
GC_COMMAND_FAILURE_WARN_DIGESTS=""

if [[ -z "${GC_COMMAND_STREAM_CACHE:-}" ]]; then
  if [[ -n "${GC_TMP_DIR:-}" ]]; then
    GC_COMMAND_STREAM_CACHE="${GC_TMP_DIR}/command-stream.json"
  else
    GC_COMMAND_STREAM_CACHE="${TMPDIR:-/tmp}/command-stream.json"
  fi
fi
export GC_COMMAND_STREAM_CACHE
GC_COMMAND_STREAM_WARN_DIGESTS=""

if [[ -z "${GC_COMMAND_FILE_CACHE:-}" ]]; then
  if [[ -n "${GC_TMP_DIR:-}" ]]; then
    GC_COMMAND_FILE_CACHE="${GC_TMP_DIR}/command-file-cache.json"
  else
    GC_COMMAND_FILE_CACHE="${TMPDIR:-/tmp}/command-file-cache.json"
  fi
fi
export GC_COMMAND_FILE_CACHE
GC_COMMAND_FILE_WARN_DIGESTS=""

if [[ -z "${GC_COMMAND_SCAN_CACHE:-}" ]]; then
  if [[ -n "${GC_TMP_DIR:-}" ]]; then
    GC_COMMAND_SCAN_CACHE="${GC_TMP_DIR}/command-scan.json"
  else
    GC_COMMAND_SCAN_CACHE="${TMPDIR:-/tmp}/command-scan.json"
  fi
fi
export GC_COMMAND_SCAN_CACHE
GC_COMMAND_SCAN_WARN_DIGESTS=""
GC_COMMAND_GUARD_WARN_DIGESTS=""
GC_CODEX_USAGE_LIMIT_CONFIRMED=0

if [[ -n "${GC_SCAN_DEDUP_EPOCH:-}" && "${GC_SCAN_DEDUP_EPOCH}" =~ ^[0-9]+$ ]]; then
  :
else
  GC_SCAN_DEDUP_EPOCH=0
fi
export GC_SCAN_DEDUP_EPOCH

gc_note_mutation() {
  if ! [[ "${GC_SCAN_DEDUP_EPOCH:-}" =~ ^[0-9]+$ ]]; then
    GC_SCAN_DEDUP_EPOCH=0
  fi
  GC_SCAN_DEDUP_EPOCH=$((GC_SCAN_DEDUP_EPOCH + 1))
  export GC_SCAN_DEDUP_EPOCH
}

gc_env_file() { echo "${PROJECT_ROOT:-$PWD}/.env"; }

gc_random_string() {
  local helper_path
  helper_path="$(gc_clone_python_tool "random_string.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path"
}

gc_usage_limit_is_provider_signal() {
  local message="${1:-}"
  local lower="${message,,}"
  if [[ "$lower" == *"429"* ]]; then
    return 0
  fi
  if [[ "$lower" == *"insufficient_quota"* || "$lower" == *"insufficient quota"* ]]; then
    return 0
  fi
  if [[ "$lower" == *"rate_limit_exceeded"* || "$lower" == *"rate limit exceeded"* ]]; then
    return 0
  fi
  return 1
}

gc_bootstrap_docs_registry() {
  local db_path="${1:-}"
  [[ -n "$db_path" ]] || return 0

  local repo_root="${PROJECT_ROOT:-$PWD}"
  local candidate_config="${repo_root}/config/bootstrap_docs_catalog.sql"
  local candidate_legacy="${repo_root}/.gpt-creator/staging/plan/tasks/bootstrap_docs_catalog.sql"
  local sql_file_default=""
  if [[ -f "$candidate_config" ]]; then
    sql_file_default="$candidate_config"
  elif [[ -f "$candidate_legacy" ]]; then
    sql_file_default="$candidate_legacy"
  else
    sql_file_default="$candidate_config"
  fi
  local sql_file="${GC_DOCUMENTATION_BOOTSTRAP_SQL:-$sql_file_default}"
  [[ -f "$sql_file" ]] || return 0

  local sqlite_bin="${SQLITE_BIN:-sqlite3}"
  if ! command -v "$sqlite_bin" >/dev/null 2>&1; then
    return 0
  fi

  local db_dir
  db_dir="$(dirname "$db_path")"
  mkdir -p "$db_dir"

  "$sqlite_bin" "$db_path" ".read $sql_file" >/dev/null 2>&1 || true
}

gc_doc_indexer_available() {
  local python_bin="${PYTHON_BIN:-python3}"
  local pkg_root="${CLI_ROOT}/src"
  if ! command -v "$python_bin" >/dev/null 2>&1; then
    return 1
  fi
  if [[ ! -d "$pkg_root" ]]; then
    return 1
  fi
  local helper_path
  helper_path="$(gc_clone_python_tool "doc_indexer_available.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  if PYTHONPATH="${pkg_root}${PYTHONPATH:+:$PYTHONPATH}" \
    "$python_bin" "$helper_path" "$pkg_root"; then
    return 0
  fi
  return 1
}

gc_doc_catalog_ready() {
  local root="${1:-${PROJECT_ROOT:-$PWD}}"
  local runtime_dir="${root}/.gpt-creator"
  local staging_dir="${runtime_dir}/staging"
  local doc_library="${staging_dir}/doc-library.md"
  local doc_index="${staging_dir}/doc-index.md"
  local doc_catalog="${staging_dir}/doc-catalog.json"
  local tasks_db="${staging_dir}/plan/tasks/tasks.db"
  local vector_index="${staging_dir}/plan/tasks/documentation-vector-index.sqlite"

  if [[ ! -s "$doc_library" || ! -s "$doc_index" || ! -s "$doc_catalog" ]]; then
    return 1
  fi
  if [[ ! -f "$tasks_db" ]]; then
    return 1
  fi
  local vector_required=0
  if [[ "${GC_REQUIRE_VECTOR_INDEX:-0}" == "1" ]]; then
    vector_required=1
  elif gc_doc_indexer_available; then
    vector_required=1
  fi
  if (( vector_required )) && [[ ! -s "$vector_index" ]]; then
    return 1
  fi

  if command -v python3 >/dev/null 2>&1; then
    local helper_path
    helper_path="$(gc_clone_python_tool "doc_catalog_ready.py" "${PROJECT_ROOT:-$PWD}")" || return 1
    if ! python3 "$helper_path" "$tasks_db"; then
      return 1
    fi
  else
    return 1
  fi

  return 0
}

gc_populate_doc_catalog_shims() {
  local root="${1:-${PROJECT_ROOT:-$PWD}}"
  local runtime_dir="${root}/.gpt-creator"
  local staging_dir="${runtime_dir}/staging"
  local doc_library="${staging_dir}/doc-library.md"
  local doc_index="${staging_dir}/doc-index.md"
  local fallback_library="${root}/docs/doc-library.md"
  local fallback_index="${root}/docs/doc-index.md"

  if [[ -z "$root" ]]; then
    return 0
  fi

  mkdir -p "$(dirname "$doc_library")" "$(dirname "$doc_index")"

  if [[ ! -s "$doc_library" && -s "$fallback_library" ]]; then
    cp -f "$fallback_library" "$doc_library" 2>/dev/null || true
    info "Seeded doc-library.md from docs directory."
  fi
  if [[ ! -s "$doc_index" && -s "$fallback_index" ]]; then
    cp -f "$fallback_index" "$doc_index" 2>/dev/null || true
    info "Seeded doc-index.md from docs directory."
  fi
}

gc_clone_python_tool() {
  local script_name="${1:?python script name required}"
  local root_param="${2:-}"
  local root="${root_param:-${PROJECT_ROOT:-$PWD}}"
  if [[ -z "$root" ]]; then
    die "Unable to determine project root while preparing ${script_name}"
  fi
  local source_path="${CLI_ROOT}/scripts/python/${script_name}"
  if [[ ! -f "$source_path" ]]; then
    die "Python helper missing at ${source_path}"
  fi
  local target_dir="${root}/.gpt-creator/shims/python"
  local target_path="${target_dir}/${script_name}"
  if [[ ! -d "$target_dir" ]]; then
    mkdir -p "$target_dir" || die "Failed to create ${target_dir}"
  fi
  if [[ ! -f "$target_path" || "$source_path" -nt "$target_path" ]]; then
    cp "$source_path" "$target_path" || die "Failed to copy ${script_name} helper"
  fi
  echo "$target_path"
}

gc_require_documentation_catalog() {
  local root="${1:-${PROJECT_ROOT:-$PWD}}"
  ensure_ctx "$root"

  if gc_doc_catalog_ready "$root"; then
    gc_populate_doc_catalog_shims "$root"
    return 0
  fi

  info "Documentation catalog missing or stale; running 'gpt-creator scan'."
  if ! cmd_scan --project "$root"; then
    warn "Documentation scan failed."
    return 1
  fi

  gc_populate_doc_catalog_shims "$root"

  if ! gc_doc_catalog_ready "$root"; then
    warn "Documentation catalog still incomplete after scan; inspect ${root}/.gpt-creator/staging."
    return 1
  fi

  if ! cmd_sweep_artifacts --project "$root"; then
    warn "Artifact sweep encountered issues; run 'gpt-creator sweep-artifacts --project \"$root\"' manually."
  fi

  return 0
}

gc_refresh_documentation_if_needed() {
  local root="${1:-${PROJECT_ROOT:-$PWD}}"
  if [[ "${GC_SKIP_AUTO_DOC_REFRESH:-0}" == "1" ]]; then
    info "Skipping automatic documentation refresh (GC_SKIP_AUTO_DOC_REFRESH=1)."
    return 0
  fi

  if gc_require_documentation_catalog "$root"; then
    ok "Documentation catalog refreshed."
    return 0
  fi

  warn "Automatic scan failed; rerun 'gpt-creator scan' manually."
  return 1
}

gc_parse_int() {
  local value="$1"
  local fallback="$2"
  if [[ "$value" =~ ^[0-9]+$ ]]; then
    printf '%s\n' "$value"
  else
    printf '%s\n' "$fallback"
  fi
}

gc_trim_prompt_file() {
  local prompt_file="$1"
  local max_tokens_raw="${GC_MAX_PROMPT_TOKENS:-8000}"
  [[ -f "$prompt_file" ]] || return 0
  if ! [[ "$max_tokens_raw" =~ ^[0-9]+$ ]]; then
    return 0
  fi
  local max_tokens=$((max_tokens_raw))
  if (( max_tokens <= 0 )); then
    return 0
  fi
  local helper_path
  helper_path="$(gc_clone_python_tool "trim_prompt_file.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$prompt_file" "$max_tokens"
}

gc_trim_prompt_file_lean() {
  local prompt_file="$1"
  [[ -f "$prompt_file" ]] || return 0
  local helper_path
  helper_path="$(gc_clone_python_tool "trim_prompt_file_lean.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$prompt_file"
}

gc_write_env_var() {
  local target="$1" key="$2" value="$3"
  local helper_path
  helper_path="$(gc_clone_python_tool "write_env_var.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$target" "$key" "$value"
}

gc_set_env_var() {
  local key="$1" value="$2"
  local env_file
  env_file="$(gc_env_file)"
  gc_write_env_var "$env_file" "$key" "$value"
}

gc_remove_env_var() {
  local target="$1" key="$2"
  [[ -f "$target" ]] || return 0
  local helper_path
  helper_path="$(gc_clone_python_tool "remove_env_var.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$target" "$key"
}

gc_decode_base64() {
  local value="${1:-}"
  local helper_path
  helper_path="$(gc_clone_python_tool "decode_base64.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$value"
}

cmd_show_file() {
  local project="${PROJECT_ROOT:-$PWD}"
  local target_path=""
  local range_spec="" head_lines="" tail_lines="" refresh=0 diff_mode=0
  local max_lines="${GC_SHOW_FILE_MAX_LINES:-400}"

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project)
        project="$(abs_path "$2")"
        shift 2
        ;;
      --range)
        if [[ $# -lt 2 ]]; then
          die "--range requires an argument in the form START:END (e.g. 120:160)"
        fi
        local next_range="$2"
        if [[ -z "$next_range" || "$next_range" == --* ]]; then
          die "--range requires an argument in the form START:END (e.g. 120:160)"
        fi
        range_spec="$next_range"
        shift 2
        ;;
      --head)
        if [[ $# -lt 2 ]]; then
          die "--head requires a positive line count"
        fi
        local next_head="$2"
        if [[ -z "$next_head" || "$next_head" == --* ]]; then
          die "--head requires a positive line count"
        fi
        warn "Tip: prefer --range start:end or --tail N for targeted snippets; --head is retained for compatibility."
        head_lines="$next_head"
        shift 2
        ;;
      --tail)
        if [[ $# -lt 2 ]]; then
          die "--tail requires a positive line count"
        fi
        local next_tail="$2"
        if [[ -z "$next_tail" || "$next_tail" == --* ]]; then
          die "--tail requires a positive line count"
        fi
        tail_lines="$next_tail"
        shift 2
        ;;
      --max-lines)
        if [[ $# -lt 2 ]]; then
          die "--max-lines requires a positive line count"
        fi
        local next_max="$2"
        if [[ -z "$next_max" || "$next_max" == --* ]]; then
          die "--max-lines requires a positive line count"
        fi
        max_lines="$next_max"
        shift 2
        ;;
      --refresh|--force)
        refresh=1
        shift
        ;;
      --diff)
        diff_mode=1
        shift
        ;;
      -h|--help)
        cat <<'EOHELP'
Usage: gpt-creator show-file [options] PATH

Display a cached snippet of PATH without repeatedly streaming large files.

Options:
  --project DIR       Project root (defaults to current PROJECT_ROOT)
  --range A:B         Show inclusive line range A..B (1-based)
  --head N            Show first N lines (legacy; prefer --range/--tail for precise snippets)
  --tail N            Show last N lines
  --max-lines N       Default line count when no range/head/tail is provided (default: 400)
  --diff              Show a unified diff against the cached snapshot (if any)
  --refresh           Re-read the file and update the cache even if unchanged
EOHELP
        return 0
        ;;
      --)
        shift
        break
        ;;
      -*)
        die "Unknown show-file option: ${1}"
        ;;
      *)
        if [[ -z "$target_path" ]]; then
          target_path="$1"
          shift
        else
          break
        fi
        ;;
    esac
  done

  [[ -n "$target_path" ]] || die "show-file requires a path argument"
  local project_abs
  project_abs="$(abs_path "$project")"
  local resolved_path
  resolved_path="$(abs_path "$target_path")"

  if [[ ! -f "$resolved_path" && "$target_path" != /* ]]; then
    local staging_root
    staging_root="$(abs_path "${project_abs}/.gpt-creator/staging")"
    local staging_candidate="${staging_root}/${target_path#./}"
    staging_candidate="$(abs_path "$staging_candidate")"
    if [[ "$staging_candidate" == "${staging_root}"/* && -f "$staging_candidate" ]]; then
      resolved_path="$staging_candidate"
    fi
  fi

  [[ -f "$resolved_path" ]] || die "File not found: ${target_path}"

  [[ -z "$max_lines" || "$max_lines" =~ ^[0-9]+$ ]] || die "--max-lines must be numeric"
  [[ -z "$head_lines" || "$head_lines" =~ ^[0-9]+$ ]] || die "--head value must be numeric"
  [[ -z "$tail_lines" || "$tail_lines" =~ ^[0-9]+$ ]] || die "--tail value must be numeric"

  local cache_dir="${GC_TMP_DIR:-${project_abs}/.gpt-creator/tmp}/view-cache"
  mkdir -p "$cache_dir"

  local rel_path="$resolved_path"
  if [[ "$resolved_path" == "$project_abs"* ]]; then
    rel_path="${resolved_path#$project_abs/}"
  fi

  local helper_path
  helper_path="$(gc_clone_python_tool "show_file.py" "${PROJECT_ROOT:-$PWD}")" || return 1

  GC_SHOW_FILE_PROJECT="$project_abs" \
  GC_SHOW_FILE_PATH="$resolved_path" \
  GC_SHOW_FILE_REL="$rel_path" \
  GC_SHOW_FILE_RANGE="$range_spec" \
  GC_SHOW_FILE_HEAD="$head_lines" \
  GC_SHOW_FILE_TAIL="$tail_lines" \
  GC_SHOW_FILE_MAX_LINES="$max_lines" \
  GC_SHOW_FILE_REFRESH="$refresh" \
  GC_SHOW_FILE_DIFF="$diff_mode" \
  GC_SHOW_FILE_CACHE_DIR="$cache_dir" \
  python3 "$helper_path"
}

gc_env_sync_ports() {
  GC_PORT_RESERVATIONS=""
  GC_DB_HOST_PORT="${GC_DB_HOST_PORT:-${DB_HOST_PORT:-${DB_PORT:-3306}}}"
  DB_NAME="${DB_NAME:-$GC_DB_NAME}"
  DB_USER="${DB_USER:-$GC_DB_USER}"
  DB_PASSWORD="${DB_PASSWORD:-$GC_DB_PASSWORD}"
  DB_ROOT_PASSWORD="${DB_ROOT_PASSWORD:-$GC_DB_ROOT_PASSWORD}"
  DB_HOST_PORT="${DB_HOST_PORT:-$GC_DB_HOST_PORT}"
  GC_API_HOST_PORT="${GC_API_HOST_PORT:-${API_HOST_PORT:-3000}}"
  GC_WEB_HOST_PORT="${GC_WEB_HOST_PORT:-${WEB_HOST_PORT:-5173}}"
  GC_ADMIN_HOST_PORT="${GC_ADMIN_HOST_PORT:-${ADMIN_HOST_PORT:-5174}}"
  GC_PROXY_HOST_PORT="${GC_PROXY_HOST_PORT:-${PROXY_HOST_PORT:-8080}}"
  API_HOST_PORT="${API_HOST_PORT:-$GC_API_HOST_PORT}"
  WEB_HOST_PORT="${WEB_HOST_PORT:-$GC_WEB_HOST_PORT}"
  ADMIN_HOST_PORT="${ADMIN_HOST_PORT:-$GC_ADMIN_HOST_PORT}"
  PROXY_HOST_PORT="${PROXY_HOST_PORT:-$GC_PROXY_HOST_PORT}"
  local api_base_default="http://localhost:${GC_API_HOST_PORT}/api/v1"
  GC_API_BASE_URL="${GC_API_BASE_URL:-$api_base_default}"
  VITE_API_BASE="${VITE_API_BASE:-$GC_API_BASE_URL}"
  local expected_health="${GC_API_BASE_URL%/}/health"
  if [[ -z "${GC_API_HEALTH_URL:-}" ]]; then
    GC_API_HEALTH_URL="$expected_health"
    gc_set_env_var GC_API_HEALTH_URL "$GC_API_HEALTH_URL"
  elif [[ "$GC_API_HEALTH_URL" == "http://localhost:${GC_API_HOST_PORT}/health" && "$expected_health" != "$GC_API_HEALTH_URL" ]]; then
    GC_API_HEALTH_URL="$expected_health"
    gc_set_env_var GC_API_HEALTH_URL "$GC_API_HEALTH_URL"
  fi
  local proxy_origin="http://localhost:${GC_PROXY_HOST_PORT}"
  GC_WEB_URL="${GC_WEB_URL:-${proxy_origin}/}"
  GC_ADMIN_URL="${GC_ADMIN_URL:-${proxy_origin}/admin/}"
  gc_reserve_port db "$GC_DB_HOST_PORT"
  gc_reserve_port api "$GC_API_HOST_PORT"
  gc_reserve_port web "$GC_WEB_HOST_PORT"
  gc_reserve_port admin "$GC_ADMIN_HOST_PORT"
  gc_reserve_port proxy "$GC_PROXY_HOST_PORT"

  if gc_reports_enabled; then
    gc_reports_initialize
    gc_reports_touch_activity "$(date +%s)"
  fi
}

gc_sanitize_env_file() {
  local env_file="$1"
  local helper_path
  helper_path="$(gc_clone_python_tool "sanitize_env_file.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$env_file"
}

gc_load_env() {
  local env_file
  env_file="$(gc_env_file)"
  if [[ -f "$env_file" ]]; then
    gc_sanitize_env_file "$env_file"
    set -a
    # shellcheck disable=SC1090
    source "$env_file"
    set +a
  fi
  GC_DB_NAME="${GC_DB_NAME:-${DB_NAME:-app}}"
  GC_DB_USER="${GC_DB_USER:-${DB_USER:-app}}"
  GC_DB_PASSWORD="${GC_DB_PASSWORD:-${DB_PASSWORD:-app_pass}}"
  GC_DB_ROOT_PASSWORD="${GC_DB_ROOT_PASSWORD:-${DB_ROOT_PASSWORD:-root}}"
  gc_env_sync_ports
}

GC_API_KEYS_METADATA=(
  "openai|OpenAI Codex|OPENAI_API_KEY|AI automation commands (plan, generate, work-on-tasks)|GC_OPENAI_API_KEY,GC_OPENAI_KEY"
  "jira|Jira Automation|JIRA_API_TOKEN|Backlog integrations that sync with Jira (create-jira-tasks)|GC_JIRA_API_TOKEN"
  "github|GitHub Auto Reports|GC_GITHUB_TOKEN|Crash/stall reports published as GitHub issues|"
)

gc_find_api_key_entry() {
  local query input lower entry key_id label primary desc alias_csv
  query="${1:-}"
  [[ -n "$query" ]] || return 1
  lower="$(to_lower "$query")"
  for entry in "${GC_API_KEYS_METADATA[@]}"; do
    IFS='|' read -r key_id label primary desc alias_csv <<<"$entry"
    if [[ "$lower" == "$(to_lower "$key_id")" || "$lower" == "$(to_lower "$primary")" ]]; then
      printf '%s\n' "$entry"
      return 0
    fi
    if [[ -n "$alias_csv" ]]; then
      IFS=',' read -ra input <<<"$alias_csv"
      for alias in "${input[@]}"; do
        alias="${alias//[[:space:]]/}"
        [[ -n "$alias" ]] || continue
        if [[ "$lower" == "$(to_lower "$alias")" ]]; then
          printf '%s\n' "$entry"
          return 0
        fi
      done
    fi
  done
  return 1
}

gc_apply_api_key_aliases() {
  local entry key_id label primary desc alias_csv value alias
  local alias_arr
  for entry in "${GC_API_KEYS_METADATA[@]}"; do
    IFS='|' read -r key_id label primary desc alias_csv <<<"$entry"
    value="${!primary:-}"
    if [[ -z "$value" && -n "$alias_csv" ]]; then
      IFS=',' read -ra alias_arr <<<"$alias_csv"
      for alias in "${alias_arr[@]}"; do
        alias="${alias//[[:space:]]/}"
        [[ -n "$alias" ]] || continue
        if [[ -n "${!alias:-}" ]]; then
          value="${!alias}"
          break
        fi
      done
    fi
    [[ -n "$value" ]] || continue
    export "$primary"="$value"
    if [[ -n "$alias_csv" ]]; then
      IFS=',' read -ra alias_arr <<<"$alias_csv"
      for alias in "${alias_arr[@]}"; do
        alias="${alias//[[:space:]]/}"
        [[ -n "$alias" && "$alias" != "$primary" ]] || continue
        if [[ -z "${!alias:-}" ]]; then
          export "$alias"="$value"
        fi
      done
    fi
  done
}

gc_api_keys_loaded=0

gc_load_api_keys() {
  if (( gc_api_keys_loaded )); then
    gc_apply_api_key_aliases
    return
  fi
  gc_ensure_config_dir
  local keys_file
  keys_file="$(gc_keys_file)"
  if [[ -f "$keys_file" ]]; then
    gc_sanitize_env_file "$keys_file"
    set -a
    # shellcheck disable=SC1090
    source "$keys_file"
    set +a
  fi
  gc_apply_api_key_aliases
  gc_api_keys_loaded=1
}

gc_read_env_file_var() {
  local file="$1" key="$2"
  [[ -f "$file" ]] || return 0
  local helper_path
  helper_path="$(gc_clone_python_tool "read_env_file_var.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$file" "$key"
}

gc_api_keys_list() {
  gc_load_api_keys
  gc_ensure_config_dir
  local keys_file status_header="Status"
  keys_file="$(gc_keys_file)"
  printf "API keys status (storage file: %s)\n\n" "$keys_file"
  printf "%-22s %-20s %-24s %s\n" "Service" "Environment" "$status_header" "Used for"
  printf "%-22s %-20s %-24s %s\n" "-------" "-----------" "------" "-------"
  local entry key_id label primary desc alias_csv value stored_value status alias
  local alias_arr
  for entry in "${GC_API_KEYS_METADATA[@]}"; do
    IFS='|' read -r key_id label primary desc alias_csv <<<"$entry"
    value="${!primary:-}"
    if [[ -z "$value" && -n "$alias_csv" ]]; then
      IFS=',' read -ra alias_arr <<<"$alias_csv"
      for alias in "${alias_arr[@]}"; do
        alias="${alias//[[:space:]]/}"
        [[ -n "$alias" ]] || continue
        if [[ -n "${!alias:-}" ]]; then
          value="${!alias}"
          break
        fi
      done
    fi
    stored_value="$(gc_read_env_file_var "$keys_file" "$primary")"
    if [[ -n "$value" ]]; then
      if [[ -n "$stored_value" ]]; then
        status="configured (stored)"
      else
        status="configured (env)"
      fi
    else
      status="missing"
    fi
    printf "%-22s %-20s %-24s %s\n" "$label" "$primary" "$status" "$desc"
  done
  if [[ -n "${GC_GITHUB_TOKEN:-}" && -z "${GC_GITHUB_REPO:-}" ]]; then
    printf "\nHint: set GC_GITHUB_REPO (owner/name) so GitHub Auto Reports knows where to file issues.\n"
  fi
  printf "\nSet a value with: gpt-creator keys set <service>\n"
}

gc_api_keys_set() {
  local query="${1:-}"
  [[ -n "$query" ]] || die "keys set requires a service name or environment variable"
  local entry
  if ! entry="$(gc_find_api_key_entry "$query")"; then
    die "Unknown API key: ${query}"
  fi
  IFS='|' read -r key_id label primary desc alias_csv <<<"$entry"
  gc_load_api_keys
  gc_ensure_config_dir
  local keys_file value alias
  local alias_arr
  keys_file="$(gc_keys_file)"
  printf "Updating %s (%s)\n" "$label" "$primary"
  printf "Press ENTER without a value to remove the stored credential.\n"
  if [[ -n "$alias_csv" ]]; then
    printf "Aliases: %s\n" "${alias_csv//,/ }"
  fi
  if [[ -t 0 ]]; then
    read -rsp "Enter value: " value
    printf '\n'
  else
    info "Reading ${primary} from stdin (input will be visible)."
    if ! read -r value; then
      die "Failed to read value from stdin"
    fi
  fi
  value="${value//$'\r'/}"
  value="$(printf '%s' "$value" | sed -e 's/^[[:space:]]*//' -e 's/[[:space:]]*$//')"
  if [[ -z "$value" ]]; then
    gc_remove_env_var "$keys_file" "$primary"
    unset "$primary"
    if [[ -n "$alias_csv" ]]; then
      IFS=',' read -ra alias_arr <<<"$alias_csv"
      for alias in "${alias_arr[@]}"; do
        alias="${alias//[[:space:]]/}"
        [[ -n "$alias" ]] || continue
        unset "$alias"
      done
    fi
    ok "Removed stored value for ${label}"
    return 0
  fi
  gc_write_env_var "$keys_file" "$primary" "$value"
  chmod 600 "$keys_file" 2>/dev/null || true
  export "$primary"="$value"
  if [[ -n "$alias_csv" ]]; then
    IFS=',' read -ra alias_arr <<<"$alias_csv"
    for alias in "${alias_arr[@]}"; do
      alias="${alias//[[:space:]]/}"
      [[ -n "$alias" && "$alias" != "$primary" ]] || continue
      export "$alias"="$value"
    done
  fi
  gc_apply_api_key_aliases
  ok "Stored credentials for ${label}"
  printf "Saved to %s\n" "$keys_file"
}

gc_create_env_if_missing() {
  local env_file
  env_file="$(gc_env_file)"
  if [[ -f "$env_file" ]]; then
    return
  fi
  local slug
  slug="$(basename "${PROJECT_ROOT:-$PWD}")"
  slug=$(printf '%s' "$slug" | tr -c '[:alnum:]' '_')
  slug=$(printf '%s' "$slug" | tr '[:upper:]' '[:lower:]')
  slug=$(printf '%.12s' "$slug")
  [[ -n "$slug" ]] || slug="app"
  local db_name="${slug}_db"
  local db_user="gc_${slug}_user"
  local db_password
  db_password="$(gc_random_string)"
  local db_root_password
  db_root_password="$(gc_random_string)"
  cat > "$env_file" <<EOF
# gpt-creator environment
DB_NAME=${db_name}
DB_USER=${db_user}
DB_PASSWORD=${db_password}
DB_ROOT_USER=root
DB_ROOT_PASSWORD=${db_root_password}
DB_HOST=127.0.0.1
DB_PORT=3306
DB_HOST_PORT=3306
API_HOST_PORT=3000
WEB_HOST_PORT=5173
ADMIN_HOST_PORT=5174
PROXY_HOST_PORT=8080
DATABASE_URL=mysql://${db_user}:${db_password}@127.0.0.1:3306/${db_name}
VITE_API_BASE=http://localhost:3000/api/v1
# Optional: GitHub issue reporting
GC_GITHUB_REPO=bekirdag/gpt-creator
GC_GITHUB_TOKEN=
# GC_REPORTER=
# GC_REPORT_ASSIGNEE=
EOF
  chmod 600 "$env_file" || true
}

VERSION="0.2.0"
APP_NAME="gpt-creator"

# Defaults (override via env)
CODEX_BIN="${CODEX_BIN:-codex}"
CODEX_MODEL="${CODEX_MODEL:-gpt-5-codex}"
CODEX_FALLBACK_MODEL="${CODEX_FALLBACK_MODEL:-gpt-5-codex}"
CODEX_REASONING_EFFORT="${CODEX_REASONING_EFFORT:-high}"
EDITOR_CMD="${EDITOR_CMD:-code}"
DOCKER_BIN="${DOCKER_BIN:-docker}"
MYSQL_BIN="${MYSQL_BIN:-mysql}"

# Colors (TTY-only)
if [[ -t 1 ]]; then
  c_reset=$'\033[0m'
  c_red=$'\033[31m'; c_yellow=$'\033[33m'; c_cyan=$'\033[36m'; c_green=$'\033[32m'
else
  c_reset=; c_red=; c_yellow=; c_cyan=; c_green=
fi

ts() { date +"%Y-%m-%dT%H:%M:%S"; }
die() { echo "${c_red}✖${c_reset} $*" >&2; exit 1; }
info(){ echo "${c_cyan}➜${c_reset} $*"; }
ok()  { echo "${c_green}✔${c_reset} $*"; }
warn(){ echo "${c_yellow}!${c_reset} $*"; }

gc_format_duration_compact() {
  local total="${1:-0}"
  if [[ ! "$total" =~ ^[0-9]+$ ]]; then
    total=0
  fi
  local hours=$(( total / 3600 ))
  local minutes=$(( (total % 3600) / 60 ))
  local seconds=$(( total % 60 ))
  local parts=()
  if (( hours > 0 )); then
    parts+=("${hours}H")
  fi
  if (( minutes > 0 || hours > 0 )); then
    parts+=("${minutes}M")
  fi
  parts+=("${seconds}S")
  printf '%s' "${parts[*]}"
}

gc_format_tokens_compact() {
  local raw="${1:-0}"
  if [[ ! "$raw" =~ ^[0-9]+$ ]]; then
    raw=0
  fi
  if (( raw >= 1000000000 )); then
    local rounded=$(( (raw + 500000000) / 1000000000 ))
    printf '%dB' "$rounded"
  elif (( raw >= 1000000 )); then
    local rounded=$(( (raw + 500000) / 1000000 ))
    printf '%dM' "$rounded"
  elif (( raw >= 1000 )); then
    local rounded=$(( (raw + 500) / 1000 ))
    printf '%dK' "$rounded"
  else
    printf '%d' "$raw"
  fi
}

gc_render_task_banner() {
  local header_position="top"
  case "$1" in
    --header-bottom)
      header_position="bottom"
      shift
      ;;
    --header-top)
      shift
      ;;
  esac

  local header="${1:?header text required}"
  shift
  local -a lines=("$@")

  local min_width=23
  local banner_margin=4
  local header_padding_extra=8
  local inner_width="$min_width"

  local line
  for line in "${lines[@]}"; do
    local length=${#line}
    local candidate=$(( length + banner_margin ))
    if (( candidate > inner_width )); then
      inner_width=$candidate
    fi
  done

  local header_width=$(( inner_width + header_padding_extra ))
  if (( header_width < ${#header} + 2 )); then
    header_width=$(( ${#header} + 2 ))
  fi

  local shade_line
  printf -v shade_line '%*s' "$header_width" ''
  shade_line="${shade_line// /░}"

  local header_pad_left=$(( (header_width - ${#header}) / 2 ))
  local header_pad_right=$(( header_width - header_pad_left - ${#header} ))
  (( header_pad_left < 0 )) && header_pad_left=0
  (( header_pad_right < 0 )) && header_pad_right=0
  local header_line_left header_line_right
  printf -v header_line_left '%*s' "$header_pad_left" ''
  printf -v header_line_right '%*s' "$header_pad_right" ''
  local header_line="${header_line_left// /░}${header}${header_line_right// /░}"

  local border_inner
  printf -v border_inner '%*s' "$inner_width" ''
  local border_line="|${border_inner// /-}|"

  if [[ "$header_position" == "top" ]]; then
    info "$shade_line"
    info "$header_line"
    info "$shade_line"
  fi

  info "$border_line"
  for line in "${lines[@]}"; do
    local line_length=${#line}
    local pad_total=$(( inner_width - line_length ))
    (( pad_total < 0 )) && pad_total=0
    local pad_left=$(( pad_total / 2 ))
    local pad_right=$(( pad_total - pad_left ))
    local padded_line
    printf -v padded_line '|%*s%s%*s|' "$pad_left" '' "$line" "$pad_right" ''
    info "$padded_line"
    info "$border_line"
  done

  if [[ "$header_position" == "bottom" ]]; then
    printf '\n'
    info "$shade_line"
    info "$header_line"
    info "$shade_line"
  fi
}

gc_user_config_root() {
  local helper_path
  helper_path="$(gc_clone_python_tool "user_config_root.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path"
}

gc_config_dir() {
  local root
  root="$(gc_user_config_root)"
  [[ -n "$root" ]] || root="."
  printf '%s\n' "${root%/}/gpt-creator"
}

gc_keys_file() {
  printf '%s/api-keys.env\n' "$(gc_config_dir)"
}

gc_ensure_config_dir() {
  local dir
  dir="$(gc_config_dir)"
  mkdir -p "$dir"
}

abs_path() {
  local target="${1:-}"
  local helper_path
  helper_path="$(gc_clone_python_tool "abs_path.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  local resolved=""
  if resolved="$(python3 "$helper_path" "$target" 2>/dev/null)"; then
    printf '%s\n' "${resolved:-$target}"
    return 0
  fi
  perl -MCwd=abs_path -e 'print abs_path(shift)."\n"' "$target" || echo "$target"
}

to_lower() {
  printf '%s' "$1" | tr '[:upper:]' '[:lower:]'
}

slugify_name() {
  local s="${1:-}"
  s="$(to_lower "$s")"
  s="$(printf '%s' "$s" | tr -cs 'a-z0-9' '-')"
  s="$(printf '%s' "$s" | sed -E 's/-+/-/g; s/^-+//; s/-+$//')"
  printf '%s\n' "${s:-gptcreator}"
}

GC_PORT_RESERVATIONS=""

gc_port_for_service() {
  local service="$1"
  local entry
  for entry in $GC_PORT_RESERVATIONS; do
    local svc="${entry%%:*}"
    if [[ "$svc" == "$service" ]]; then
      printf '%s\n' "${entry#*:}"
      return 0
    fi
  done
  return 1
}

gc_unreserve_port() {
  local service="$1"
  [[ -n "$service" ]] || return 0
  local entry new_list=""
  for entry in $GC_PORT_RESERVATIONS; do
    local svc="${entry%%:*}"
    if [[ "$svc" == "$service" ]]; then
      continue
    fi
    if [[ -z "$new_list" ]]; then
      new_list="$entry"
    else
      new_list+=" $entry"
    fi
  done
  GC_PORT_RESERVATIONS="$new_list"
}

gc_reserve_port() {
  local service="$1" port="$2"
  [[ -n "$service" && -n "$port" ]] || return 0
  gc_unreserve_port "$service"
  if [[ -z "${GC_PORT_RESERVATIONS:-}" ]]; then
    GC_PORT_RESERVATIONS="${service}:${port}"
  else
    GC_PORT_RESERVATIONS+=" ${service}:${port}"
  fi
}

gc_port_is_reserved() {
  local port="$1"
  local entry
  for entry in $GC_PORT_RESERVATIONS; do
    if [[ "${entry#*:}" == "$port" ]]; then
      return 0
    fi
  done
  return 1
}

gc_port_reserved_by_other() {
  local port="$1" service="$2"
  local entry
  for entry in $GC_PORT_RESERVATIONS; do
    local svc="${entry%%:*}"
    local val="${entry#*:}"
    if [[ "$val" == "$port" && "$svc" != "$service" ]]; then
      return 0
    fi
  done
  return 1
}

GC_PROGRESS_DIR_MIGRATIONS=(
  "docs/delivery::staging/docs"
  "Design::staging/docs"
  "legal_editor_tmp::artifacts"
)

GC_PROGRESS_FILE_MIGRATIONS=(
  "tmp_*::artifacts/tmp"
  "final_*::artifacts/final"
  "dump_*::artifacts/dump"
  "diff*::artifacts/diff"
  "change_*::artifacts/change"
  "changes_*::artifacts/change"
  "codex_*::artifacts/codex"
  "delivery_plan*::artifacts/delivery"
  "qaDoc.json::artifacts/delivery"
  "backlog.md::artifacts/planning"
  "plan.md::artifacts/planning"
  "tasks.json::staging/plan/legacy"
  "encoded*::artifacts/encoded"
  "session_lifecycle*::artifacts/session"
  "breadcrumbs*.json::artifacts/ui"
  "cli_content.json::artifacts/context"
  "focusTrap.json::artifacts/ui"
  "navStore.json::artifacts/ui"
  "service_content.json::artifacts/context"
  "login_diff.txt::artifacts/diff"
  "login_json.txt::artifacts/context"
  "output_payload.json::artifacts/output"
  "changes_output.json::artifacts/change"
  "changes_strings.txt::artifacts/change"
  "*.patch::artifacts/patches"
  "*_patch.jsonstr::artifacts/patches"
  "*.rej::artifacts/patches"
  "*.rej.orig::artifacts/patches"
  "*.orig::artifacts/patches"
)

GC_PROGRESS_MIGRATION_LAST_COUNT=0

gc_move_progress_artifact() {
  local source_path="$1"
  local dest_dir="$2"
  [[ -e "$source_path" ]] || return 1
  mkdir -p "$dest_dir"
  local base name ext target candidate idx
  base="$(basename "$source_path")"
  name="$base"
  ext=""
  if [[ -f "$source_path" ]]; then
    if [[ "$base" == .* ]]; then
      if [[ "$base" == *.* ]]; then
        ext=".${base##*.}"
        name="${base%$ext}"
        [[ -z "$name" ]] && name="$base"
      fi
    else
      if [[ "$base" == *.* ]]; then
        ext=".${base##*.}"
        name="${base%$ext}"
      fi
    fi
  fi
  target="${dest_dir}/${base}"
  if [[ -e "$target" ]]; then
    idx=2
    while :; do
      if [[ -n "$ext" && "$name" != "$base" ]]; then
        candidate="${dest_dir}/${name}-${idx}${ext}"
      else
        candidate="${dest_dir}/${base}-${idx}"
      fi
      if [[ ! -e "$candidate" ]]; then
        target="$candidate"
        break
      fi
      idx=$((idx + 1))
    done
  fi
  if mv -- "$source_path" "$target"; then
    printf '%s\t%s\n' "$source_path" "$target"
    return 0
  fi
  return 1
}

gc_migrate_progress_artifacts() {
  local project_root="$1"
  local work_dir_name="${GC_WORK_DIR_NAME:-.gpt-creator}"
  local gc_root="${project_root}/${work_dir_name}"
  local skip="${GC_SKIP_PROGRESS_MIGRATION:-0}"
  [[ -d "$project_root" ]] || return 0
  [[ "$skip" == "1" ]] && return 0
  if [[ -n "${GC_ROOT:-}" && "$project_root" == "$GC_ROOT" ]]; then
    return 0
  fi
  GC_PROGRESS_MIGRATION_LAST_COUNT=0
  local -a moved=()
  local entry src_rel dest_rel src_path dest_dir record

  for entry in "${GC_PROGRESS_DIR_MIGRATIONS[@]}"; do
    src_rel="${entry%%::*}"
    dest_rel="${entry#*::}"
    src_path="${project_root}/${src_rel}"
    [[ -d "$src_path" ]] || continue
    dest_dir="${gc_root}/${dest_rel}"
    if record="$(gc_move_progress_artifact "$src_path" "$dest_dir")"; then
      moved+=("$record")
    fi
  done

  shopt -s nullglob dotglob
  for entry in "${GC_PROGRESS_FILE_MIGRATIONS[@]}"; do
    src_rel="${entry%%::*}"
    dest_rel="${entry#*::}"
    local path
    for path in "$project_root"/$src_rel; do
      [[ -e "$path" ]] || continue
      [[ "$path" == "$gc_root"* ]] && continue
      dest_dir="${gc_root}/${dest_rel}"
      if record="$(gc_move_progress_artifact "$path" "$dest_dir")"; then
        moved+=("$record")
      fi
    done
  done
  shopt -u nullglob dotglob

  if ((${#moved[@]} > 0)); then
    local log_dir="${gc_root}/logs"
    mkdir -p "$log_dir"
    local log_file="${log_dir}/progress-migration.log"
    {
      printf -- '--- %s ---\n' "$(date '+%Y-%m-%d %H:%M:%S')"
      local row src_abs dst_abs src_relpath dst_relpath
      for row in "${moved[@]}"; do
        src_abs="${row%%$'\t'*}"
        dst_abs="${row#*$'\t'}"
        src_relpath="${src_abs#$project_root/}"
        dst_relpath="${dst_abs#$project_root/}"
        printf -- '%s -> %s\n' "$src_relpath" "$dst_relpath"
      done
    } >>"$log_file"
    info "Migrated ${#moved[@]} work artifacts into .gpt-creator (see ${log_file#$project_root/})."
  fi
  GC_PROGRESS_MIGRATION_LAST_COUNT=${#moved[@]}
}

# Manual command helper to sweep legacy artifacts
cmd_sweep_artifacts() {
  local -a projects=()
  local arg project_path
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project|-p)
        [[ -n "${2:-}" ]] || die "--project requires a directory path"
        project_path="$(abs_path "$2")"
        projects+=("$project_path")
        shift 2
        ;;
      -h|--help)
        cat <<'EOF'
Usage: gpt-creator sweep-artifacts [--project PATH] [PATH...]

Sweep legacy Codex work artifacts (tmp/final/diff/etc.) into the standard
.gpt-creator folder structure. Defaults to the current directory when no path
is supplied. You can pass multiple project directories to tidy them in batch.
EOF
        return 0
        ;;
      --)
        shift
        while [[ $# -gt 0 ]]; do
          projects+=("$(abs_path "$1")")
          shift
        done
        ;;
      -*)
        die "Unknown flag for sweep-artifacts: $1"
        ;;
      *)
        projects+=("$(abs_path "$1")")
        shift
        ;;
    esac
  done

  if ((${#projects[@]} == 0)); then
    projects+=("$(abs_path "${PROJECT_ROOT:-$PWD}")")
  fi

  local root count prev_skip_set prev_skip_val
  for root in "${projects[@]}"; do
    if [[ ! -d "$root" ]]; then
      warn "Skipping missing directory: ${root}"
      continue
    fi
    info "Tidying progress artifacts under ${root}"
    prev_skip_set=0
    prev_skip_val=""
    if [[ ${GC_SKIP_PROGRESS_MIGRATION+x} ]]; then
      prev_skip_set=1
      prev_skip_val="$GC_SKIP_PROGRESS_MIGRATION"
    else
      prev_skip_set=0
      prev_skip_val=""
    fi
    GC_SKIP_PROGRESS_MIGRATION=0
    gc_migrate_progress_artifacts "$root"
    count=${GC_PROGRESS_MIGRATION_LAST_COUNT:-0}
    if (( prev_skip_set )); then
      GC_SKIP_PROGRESS_MIGRATION="$prev_skip_val"
    else
      unset GC_SKIP_PROGRESS_MIGRATION
    fi
    if (( count > 0 )); then
      ok "Relocated ${count} artifact(s) into ${root}/.gpt-creator."
    else
      info "No legacy artifacts found outside .gpt-creator."
    fi
  done

  return 0
}

cmd_tidy_progress() {
  warn "'tidy-progress' has been renamed to 'sweep-artifacts'. Running the renamed command."
  cmd_sweep_artifacts "$@"
}

# Context directories inside project
ensure_ctx() {
  local root="${1:-}" TMP_DIR=""
  if [[ -z "${root}" ]]; then root="${PROJECT_ROOT:-$PWD}"; fi
  PROJECT_ROOT="$(abs_path "$root")"
  GC_DIR="${PROJECT_ROOT}/.gpt-creator"
  STAGING_DIR="${GC_DIR}/staging"
  INPUT_DIR="${STAGING_DIR}/inputs"
  PLAN_DIR="${STAGING_DIR}/plan"
  LOG_DIR="${GC_DIR}/logs"
  ART_DIR="${GC_DIR}/artifacts"
  TMP_DIR="${GC_DIR}/tmp"
  mkdir -p "$GC_DIR" "$STAGING_DIR" "$INPUT_DIR" "$PLAN_DIR" "$LOG_DIR" "$ART_DIR" "$TMP_DIR"
  export INPUT_DIR
  gc_migrate_progress_artifacts "$PROJECT_ROOT"
  gc_configure_tmpdir "$PROJECT_ROOT"
  gc_create_env_if_missing
  gc_load_env
  local base_name
  base_name="$(basename "$PROJECT_ROOT")"
  PROJECT_SLUG="$(slugify_name "${GC_DOCKER_PROJECT_NAME:-$base_name}")"
  GC_DOCKER_PROJECT_NAME="${GC_DOCKER_PROJECT_NAME:-$PROJECT_SLUG}"
  COMPOSE_PROJECT_NAME="${COMPOSE_PROJECT_NAME:-$GC_DOCKER_PROJECT_NAME}"
  GC_FAIL_LOG_DIR="$LOG_DIR"
  export GC_DOCKER_PROJECT_NAME COMPOSE_PROJECT_NAME PROJECT_SLUG

  if gc_reports_enabled; then
    gc_reports_initialize
    gc_reports_touch_activity "$(date +%s)"
  fi
}

gc_project_templates_root() {
  local root="${CLI_ROOT}/project_templates"
  mkdir -p "$root"
  printf '%s\n' "$root"
}

gc_find_primary_rfp() {
  local search_root="${1:-.}"
  find "$search_root" -maxdepth 3 -type f \
    \( -iname 'rfp.md' -o -iname '*rfp*.md' -o -iname '*request*for*proposal*.md' \) \
    | sort | head -n 1
}

gc_bootstrap_state_dir() {
  printf '%s\n' "${PLAN_DIR}/bootstrap"
}

gc_bootstrap_state_file() {
  printf '%s\n' "$(gc_bootstrap_state_dir)/state.json"
}

gc_capture_error_context() {
  local status="${1:-0}"
  local command="${2:-}"
  (( status == 0 )) && return
  GC_LAST_ERROR_STATUS="$status"
  GC_LAST_ERROR_CMD="$command"
}

gc_logs_dir() {
  local dir="${GC_FAIL_LOG_DIR:-}"
  if [[ -z "$dir" ]]; then
    if [[ -n "${PROJECT_ROOT:-}" ]]; then
      dir="${PROJECT_ROOT}/.gpt-creator/logs"
    else
      dir="${PWD}/.gpt-creator/logs"
    fi
  fi
  if ! mkdir -p "$dir" 2>/dev/null; then
    return 1
  fi
  printf '%s\n' "$dir"
}

gc_reports_enabled() {
  (( GC_REPORTS_ON != 0 ))
}

gc_reports_current_user() {
  if [[ -n "${GC_REPORTER:-}" ]]; then
    printf '%s\n' "$GC_REPORTER"
    return 0
  fi
  local name
  name="$(git config user.name 2>/dev/null || true)"
  if [[ -z "$name" ]]; then
    name="${USER:-}"
  fi
  if [[ -z "$name" ]]; then
    name="$(whoami 2>/dev/null || true)"
  fi
  printf '%s\n' "${name:-maintainer}"
}

gc_reports_escape() {
  local s="${1:-}"
  s="${s//\\/\\\\}"
  s="${s//$'\n'/\\n}"
  s="${s//\"/\\\"}"
  printf '%s' "$s"
}

gc_reports_issue_file() {
  local kind="${1:-generic}"
  local dir="${GC_REPORTS_STORE_DIR:-}"
  if [[ -z "$dir" ]]; then
    local base
    base="$(gc_logs_dir)" || return 1
    dir="${base}/issue-reports"
  fi
  if ! mkdir -p "$dir" 2>/dev/null; then
    return 1
  fi
  local stamp random file
  stamp="$(date -u +"%Y%m%dT%H%M%SZ")"
  random="$(printf '%04x%04x' "$RANDOM" "$RANDOM")"
  file="${dir}/${stamp}-${kind}-${random}.yml"
  if [[ -e "$file" ]]; then
    file="${dir}/${stamp}-${kind}-${random}-${RANDOM}.yml"
  fi
  if ! : >"$file" 2>/dev/null; then
    return 1
  fi
  printf '%s\n' "$file"
}

gc_reports_write_issue() {
  local kind="${1:-generic}"
  local summary="${2:-}"
  local definition="${3:-}"
  local priority="${4:-P2-medium}"
  local file
  file="$(gc_reports_issue_file "$kind")" || return 1
  local summary_escaped
  summary_escaped="$(gc_reports_escape "$summary")"
  local reporter
  reporter="$(gc_reports_current_user)"
  local timestamp
  timestamp="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
  {
    printf 'summary: "%s"\n' "$summary_escaped"
    printf 'priority: %s\n' "$priority"
    printf 'issue_definition: |\n'
    if [[ -n "$definition" ]]; then
      while IFS= read -r line || [[ -n "$line" ]]; do
        printf '  %s\n' "$line"
      done <<<"$definition"
    else
      printf '  (no additional details provided)\n'
    fi
    printf 'metadata:\n'
    printf '  type: %s\n' "$kind"
    printf '  timestamp: "%s"\n' "$timestamp"
    printf '  reporter: "%s"\n' "$(gc_reports_escape "$reporter")"
    printf '  command: "%s"\n' "$(gc_reports_escape "${GC_INVOCATION:-$0}")"
    if [[ -n "${GC_LAST_ERROR_STATUS:-}" ]]; then
      printf '  exit_code: %s\n' "${GC_LAST_ERROR_STATUS}"
    fi
    if [[ -n "${GC_LAST_ERROR_CMD:-}" ]]; then
      printf '  last_command: "%s"\n' "$(gc_reports_escape "${GC_LAST_ERROR_CMD}")"
    fi
    printf '  working_dir: "%s"\n' "$(gc_reports_escape "$PWD")"
    printf '  status: open\n'
    printf '  likes: 0\n'
    printf '  comments: 0\n'
  } >"$file"
  printf '%s\n' "$file"
}

gc_reports_sync_github() {
  local report_file="${1:-}"
  local kind="${2:-generic}"
  local summary="${3:-}"
  local definition="${4:-}"
  local priority="${5:-P2-medium}"
  local cli_version="${VERSION:-}"
  local binary_path="${GC_SELF_PATH:-}"

  if [[ -z "$binary_path" || ! -r "$binary_path" ]]; then
    if [[ -n "${CLI_ROOT:-}" && -r "${CLI_ROOT}/bin/${APP_NAME}" ]]; then
      binary_path="${CLI_ROOT}/bin/${APP_NAME}"
    else
      binary_path="$(command -v "${APP_NAME}" 2>/dev/null || true)"
    fi
  fi

  local repo="${GC_GITHUB_REPO:-}"
  local token="${GC_GITHUB_TOKEN:-}"
  if [[ -z "$repo" || -z "$token" ]]; then
    return 0
  fi

  local helper_path
  helper_path="$(gc_clone_python_tool "reports_sync_github.py" "${PROJECT_ROOT:-$PWD}")" || return 1

  local response
  if ! response="$(
    GC_REPORT_SUMMARY="$summary" \
    GC_REPORT_DEFINITION="$definition" \
    GC_REPORT_PRIORITY="$priority" \
    GC_REPORT_KIND="$kind" \
    GC_REPORT_FILE="$report_file" \
    GC_REPORT_COMMAND="${GC_INVOCATION:-}" \
    GC_REPORT_EXIT="${GC_LAST_ERROR_STATUS:-}" \
    GC_REPORT_LAST_CMD="${GC_LAST_ERROR_CMD:-}" \
    GC_REPORT_WORKDIR="$PWD" \
    GC_REPORT_PROJECT="${PROJECT_ROOT:-$PWD}" \
    GC_REPORTER="${GC_REPORTER:-}" \
    GC_REPORT_VERSION="$cli_version" \
    GC_REPORT_BINARY="${binary_path:-}" \
    python3 "$helper_path" "$repo" "$token"
  )"; then
    warn "Failed to create GitHub issue for $(basename "$report_file")."
    return 0
  fi

  local issue_url=""
  local issue_number=""
  IFS=$'\n' read -r issue_url issue_number <<<"$response"
  if [[ -n "$issue_url" ]]; then
    gc_reports_set_metadata_field "$report_file" issue_url "\"$issue_url\""
    if [[ -n "$issue_number" ]]; then
      gc_reports_set_metadata_field "$report_file" issue_number "$issue_number"
    fi
    info "GitHub issue created -> ${issue_url}"
  fi
}

gc_reports_record_issue() {
  local kind="${1:-generic}"
  local summary="${2:-}"
  local definition="${3:-}"
  local priority="${4:-P2-medium}"
  local report_file
  report_file="$(gc_reports_write_issue "$kind" "$summary" "$definition" "$priority")" || return 1
  gc_reports_sync_github "$report_file" "$kind" "$summary" "$definition" "$priority"
  printf '%s\n' "$report_file"
}

gc_reports_touch_activity() {
  local timestamp="${1:-$(date +%s)}"
  local command="${2:-}"
  local file="${GC_REPORTS_ACTIVITY_FILE:-}"
  [[ -n "$file" ]] || return 0
  if [[ -n "$command" ]]; then
    command="${command//$'\n'/ }"
    printf '%s\t%s\n' "$timestamp" "$command" >"$file" 2>/dev/null || true
  else
    printf '%s\n' "$timestamp" >"$file" 2>/dev/null || true
  fi
}

gc_reports_activity_trap() {
  gc_reports_touch_activity "$(date +%s)" "$1"
  return 0
}

gc_reports_handle_crash() {
  local status="${1:-1}"
  gc_reports_enabled || return 0
  local summary
  printf -v summary "Crash (exit %s) while running '%s'" "$status" "${GC_INVOCATION:-$0}"
  local log_dir
  if ! log_dir="$(gc_logs_dir)"; then
    log_dir="<unknown>"
  fi
  local -a lines
  lines=("The CLI exited unexpectedly with status ${status}.")
  if [[ -n "${GC_LAST_ERROR_CMD:-}" ]]; then
    lines+=("Last command observed before exit: ${GC_LAST_ERROR_CMD}")
  fi
  if [[ -n "${GC_LAST_CRASH_LOG:-}" ]]; then
    lines+=("Crash log stored at: ${GC_LAST_CRASH_LOG}")
  fi
  lines+=("Inspect logs under ${log_dir} for further diagnostics.")
  local definition=""
  if ((${#lines[@]})); then
    printf -v definition '%s\n' "${lines[@]}"
    definition="${definition%$'\n'}"
  fi
  local path
  if path="$(gc_reports_record_issue "crash" "$summary" "$definition" "P0-critical")"; then
    warn "Issue report recorded for crash → ${path}"
  fi
}

gc_reports_handle_idle() {
  local idle_seconds="${1:-0}"
  local heartbeat="${2:-}"
  local last_command="${3:-}"
  gc_reports_enabled || return 0
  local summary
  printf -v summary "Idle/stall detected after %ss while running '%s'" "$idle_seconds" "${GC_INVOCATION:-$0}"
  local log_dir
  if ! log_dir="$(gc_logs_dir)"; then
    log_dir="<unknown>"
  fi
  if [[ -n "$last_command" ]]; then
    last_command="${last_command//$'\n'/ }"
  fi
  local -a lines
  lines=("No CLI activity recorded for ${idle_seconds} seconds.")
  if [[ -n "$heartbeat" ]]; then
    lines+=("Heartbeat file: ${heartbeat}")
  fi
  if [[ -n "$last_command" ]]; then
    lines+=("Last command observed: ${last_command}")
  fi
  lines+=("Inspect processes and logs under ${log_dir} to verify whether the command stalled.")
  local definition=""
  if ((${#lines[@]})); then
    printf -v definition '%s\n' "${lines[@]}"
    definition="${definition%$'\n'}"
  fi
  local path
  if path="$(gc_reports_record_issue "idle" "$summary" "$definition" "P1-high")"; then
    warn "Issue report recorded for idle stall → ${path}"
  fi
}

gc_reports_watchdog_loop() {
  local timeout="${1:-0}"
  local interval="${2:-0}"
  local activity_file="${3:-}"
  local main_pid="${4:-0}"
  local sentinel="${5:-}"

  (( timeout > 0 )) || return 0
  (( interval > 0 )) || interval="$timeout"
  [[ -n "$activity_file" && -n "$main_pid" ]] || return 0

  while kill -0 "$main_pid" 2>/dev/null; do
    sleep "$interval" || break
    [[ -f "$activity_file" ]] || continue
    local raw
    raw="$(cat "$activity_file" 2>/dev/null)" || continue
    local payload="$raw"
    local last="${payload%%$'\t'*}"
    local activity_command=""
    if [[ "$payload" == *$'\t'* ]]; then
      activity_command="${payload#*$'\t'}"
    fi
    [[ "$last" =~ ^[0-9]+$ ]] || continue
    local now
    now="$(date +%s)"
    local delta=$(( now - last ))
    if (( delta >= timeout )); then
      if [[ -n "$sentinel" ]]; then
        printf '%s\n' "$now" >"$sentinel" 2>/dev/null || true
      fi
      gc_reports_handle_idle "$delta" "$activity_file" "$activity_command"
      break
    fi
  done
}

gc_reports_start_watchdog() {
  local timeout="${1:-0}"
  local interval="${2:-0}"
  local activity="${3:-}"
  local main_pid="${4:-0}"
  local sentinel="${5:-}"

  if (( timeout <= 0 )); then
    return 0
  fi
  if (( interval <= 0 || interval > timeout )); then
    interval="$timeout"
  fi
  if [[ -z "$activity" || -z "$main_pid" ]]; then
    return 0
  fi
  if [[ -n "${GC_REPORTS_WATCHDOG_PID:-}" ]] && kill -0 "$GC_REPORTS_WATCHDOG_PID" 2>/dev/null; then
    return 0
  fi

  gc_reports_watchdog_loop "$timeout" "$interval" "$activity" "$main_pid" "$sentinel" &
  GC_REPORTS_WATCHDOG_PID=$!
}

gc_reports_initialize() {
  gc_reports_enabled || return 0
  if (( GC_REPORTS_INITIALIZED )); then
    return 0
  fi
  local log_dir
  if ! log_dir="$(gc_logs_dir)"; then
    warn "Unable to determine log directory for issue reporting"
    return 0
  fi
  GC_REPORTS_STORE_DIR="${log_dir}/issue-reports"
  if ! mkdir -p "$GC_REPORTS_STORE_DIR" 2>/dev/null; then
    warn "Unable to prepare reports directory at ${GC_REPORTS_STORE_DIR}"
    return 0
  fi
  GC_REPORTS_ACTIVITY_FILE="${GC_REPORTS_STORE_DIR}/heartbeat-${GC_MAIN_PID}.txt"
  GC_REPORTS_IDLE_SENTINEL="${GC_REPORTS_STORE_DIR}/idle-${GC_MAIN_PID}.flag"
  if ! : >"$GC_REPORTS_ACTIVITY_FILE" 2>/dev/null; then
    warn "Unable to initialize heartbeat tracking for issue reporting"
    return 0
  fi
  GC_REPORTS_INITIALIZED=1
  gc_reports_touch_activity "$(date +%s)"
  gc_reports_start_watchdog "$GC_REPORTS_IDLE_TIMEOUT" "$GC_REPORTS_CHECK_INTERVAL" "$GC_REPORTS_ACTIVITY_FILE" "$GC_MAIN_PID" "$GC_REPORTS_IDLE_SENTINEL"
  return 0
}

gc_reports_cleanup() {
  if [[ -n "${GC_REPORTS_WATCHDOG_PID:-}" ]]; then
    kill "$GC_REPORTS_WATCHDOG_PID" 2>/dev/null || true
    wait "$GC_REPORTS_WATCHDOG_PID" 2>/dev/null || true
    GC_REPORTS_WATCHDOG_PID=""
  fi
}

gc_reports_dir() {
  if [[ -n "${GC_REPORTS_STORE_DIR:-}" ]]; then
    printf '%s\n' "$GC_REPORTS_STORE_DIR"
    return 0
  fi
  local base
  base="$(gc_logs_dir)" || return 1
  local dir="${base}/issue-reports"
  if ! mkdir -p "$dir" 2>/dev/null; then
    return 1
  fi
  printf '%s\n' "$dir"
}

gc_reports_set_metadata_field() {
  local report_file="${1:?report file required}"
  local key="${2:?metadata key required}"
  local value="${3:-}"
  local helper_path
  helper_path="$(gc_clone_python_tool "reports_set_metadata_field.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$report_file" "$key" "$value"
}

gc_reports_resolve_slug() {
  local slug="${1:-}"
  [[ -n "$slug" ]] || return 1
  local dir
  dir="$(gc_reports_dir)" || return 1
  local candidate
  for ext in yml yaml; do
    candidate="${dir}/${slug}.${ext}"
    if [[ -f "$candidate" ]]; then
      printf '%s\n' "$candidate"
      return 0
    fi
  done
  candidate="${dir}/${slug}"
  if [[ -f "$candidate" ]]; then
    printf '%s\n' "$candidate"
    return 0
  fi
  local -a matches=()
  while IFS= read -r file; do
    local base
    base="$(basename "$file")"
    base="${base%.*}"
    if [[ "$base" == "$slug"* ]]; then
      matches+=("$file")
    fi
  done < <(find "$dir" -maxdepth 1 -type f \( -name '*.yml' -o -name '*.yaml' \) -print 2>/dev/null)
  local count="${#matches[@]}"
  if (( count == 1 )); then
    printf '%s\n' "${matches[0]}"
    return 0
  fi
  if (( count > 1 )); then
    warn "Multiple reports match slug '${slug}'."
    local entry
    for entry in "${matches[@]}"; do
      warn "  $(basename "$entry")"
    done
    return 2
  fi
  return 1
}

gc_reports_run_work() {
  local slug="${1:?slug required}"
  local branch_hint="${2:-}"
  local push_after="${3:-1}"
  local prompt_only="${4:-0}"
  local assignee_override="${5:-}"

  local report_path
  if ! report_path="$(gc_reports_resolve_slug "$slug")"; then
    warn "No issue report found for slug: ${slug}"
    return 1
  fi

  local branch="${branch_hint:-report/${slug}}"
  local push_flag="$push_after"
  if [[ "$push_flag" != "0" ]]; then
    push_flag=1
  fi
  local prompt_only_flag="$prompt_only"
  if [[ "$prompt_only_flag" != "0" ]]; then
    prompt_only_flag=1
  fi

  local report_dir="${GC_DIR}/reports/${slug}"
  mkdir -p "$report_dir"
  local prompt_path="${report_dir}/work.md"
  local project_root="${PROJECT_ROOT:-$PWD}"

  local helper_path
  helper_path="$(gc_clone_python_tool reports_run_work.py "${PROJECT_ROOT:-$PWD}")" || return 1

  if ! python3 "$helper_path" "$report_path" "$prompt_path" "$project_root" "$slug" "$branch" "$push_flag"
  then
    warn "Failed to prepare Codex prompt for report ${slug}"
    return 1
  fi

  info "Prepared Codex prompt → ${prompt_path}"

  local now_utc
  now_utc="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
  local assignee="${assignee_override:-${GC_REPORT_ASSIGNEE:-$(gc_reports_current_user)}}"
  if (( prompt_only_flag )); then
    [[ -n "$assignee" ]] && gc_reports_set_metadata_field "$report_path" assigned "\"$(gc_reports_escape "$assignee")\""
    gc_reports_set_metadata_field "$report_path" branch "\"$(gc_reports_escape "$branch")\""
    gc_reports_set_metadata_field "$report_path" status open
    info "Prompt generated (skipping Codex execution due to --prompt-only)."
    info "Run: ${CODEX_BIN:-codex} exec --model ${CODEX_MODEL} --cd \"${PROJECT_ROOT:-$PWD}\" < ${prompt_path}"
    return 0
  fi

  if [[ -n "$assignee" ]]; then
    gc_reports_set_metadata_field "$report_path" assigned "\"$(gc_reports_escape "$assignee")\""
  fi
  gc_reports_set_metadata_field "$report_path" status in-progress
  gc_reports_set_metadata_field "$report_path" branch "\"$(gc_reports_escape "$branch")\""
  gc_reports_set_metadata_field "$report_path" last_started "\"$now_utc\""

  if codex_call "report-${slug}" --prompt "$prompt_path"; then
    local completed_utc
    completed_utc="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
    gc_reports_set_metadata_field "$report_path" status resolved
    gc_reports_set_metadata_field "$report_path" last_completed "\"$completed_utc\""
    ok "Codex resolved report ${slug}"
    return 0
  else
    local failed_utc
    failed_utc="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
    gc_reports_set_metadata_field "$report_path" status open
    gc_reports_set_metadata_field "$report_path" last_failed "\"$failed_utc\""
    warn "Codex failed to resolve report ${slug}"
    return 1
  fi
}

gc_reports_set_idle_timeout() {
  local value="${1:-}"
  if [[ -z "$value" ]]; then
    die "--reports-idle-timeout requires a value in seconds"
  fi
  if ! [[ "$value" =~ ^[0-9]+$ ]]; then
    die "--reports-idle-timeout expects an integer number of seconds (received: $value)"
  fi
  GC_REPORTS_IDLE_TIMEOUT="$value"
}

gc_reports_extract_global_flags() {
  GC_FILTERED_ARGS=()
  local -a args=("$@")
  local idx=0
  while (( idx < ${#args[@]} )); do
    local arg="${args[idx]}"
    case "$arg" in
      --)
        GC_FILTERED_ARGS+=("${args[@]:idx}")
        return 0
        ;;
      --reports-on)
        GC_REPORTS_ON=1
        ;;
      --reports-off)
        GC_REPORTS_ON=0
        ;;
      --reports-idle-timeout=*)
        gc_reports_set_idle_timeout "${arg#*=}"
        ;;
      --reports-idle-timeout)
        (( idx + 1 < ${#args[@]} )) || die "--reports-idle-timeout requires a value in seconds"
        idx=$((idx + 1))
        gc_reports_set_idle_timeout "${args[idx]}"
        ;;
      *)
        GC_FILTERED_ARGS+=("$arg")
        ;;
    esac
    idx=$((idx + 1))
  done
}

gc_write_crash_log() {
  local status="${1:-1}"
  if (( GC_CRASH_LOGGED )); then
    return
  fi

  local log_dir
  log_dir="$(gc_logs_dir)" || return

  local log_file="${log_dir}/crash.log"
  local timestamp
  timestamp="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
  local invocation="${GC_INVOCATION:-$0}"
  local last_command="${GC_LAST_ERROR_CMD:-}"

  {
    printf 'timestamp=%s\n' "$timestamp"
    printf 'command=%s\n' "$invocation"
    printf 'exit_code=%s\n' "$status"
    if [[ -n "$last_command" ]]; then
      printf 'last_command=%s\n' "$last_command"
    fi
    if [[ -n "${PROJECT_ROOT:-}" ]]; then
      printf 'project_root=%s\n' "$PROJECT_ROOT"
    fi
    printf 'working_dir=%s\n' "$PWD"
    printf -- '---\n'
  } >>"$log_file" 2>/dev/null || return

  GC_LAST_CRASH_LOG="$log_file"
  GC_CRASH_LOGGED=1
}

gc_exit_handler() {
  local status="${1:-0}"
  if (( status != 0 )); then
    if [[ -z "${GC_LAST_ERROR_STATUS:-}" || "${GC_LAST_ERROR_STATUS}" -eq 0 ]]; then
      GC_LAST_ERROR_STATUS="$status"
    fi
    gc_write_crash_log "$status"
    if gc_reports_enabled; then
      gc_reports_handle_crash "$status"
    fi
  fi
  gc_reports_cleanup
}

gc_record_codex_usage() {
  local log_file="${1:-}"
  local task="${2:-}"
  local model="${3:-}"
  local prompt_file="${4:-}"
  local exit_code="${5:-0}"

  GC_LAST_CODEX_PROMPT_TOKENS=0
  GC_LAST_CODEX_COMPLETION_TOKENS=0
  GC_LAST_CODEX_TOTAL_TOKENS=0
  GC_LAST_CODEX_STDOUT_TAIL_HASH="none"
  GC_LAST_CODEX_STDOUT_TAIL_BASE64=""
  GC_LAST_CODEX_TURN_DIFF_HASH=""
  GC_LAST_CODEX_TURN_DIFF_BLOCKS=0

  [[ -n "$log_file" && -f "$log_file" ]] || return 0

  local usage_dir="${LOG_DIR:-${PROJECT_ROOT:-$PWD}/.gpt-creator/logs}"
  mkdir -p "$usage_dir"
  local usage_file="${usage_dir}/codex-usage.ndjson"
  local timestamp
  timestamp="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"

  local py_output=""
  local tmp_output
  tmp_output="$(mktemp "${TMPDIR:-/tmp}/gc-usage-XXXXXX")" || {
    warn "Failed to record Codex usage for task=${task} model=${model}."
    return 1
  }

  local usage_script_py
  usage_script_py="$(mktemp "${TMPDIR:-/tmp}/gc-usage-script-XXXXXX.py")" || {
    warn "Failed to prepare Codex usage recorder."
    rm -f "$tmp_output" || true
    return 1
  }

  local usage_source="${CLI_ROOT}/scripts/record_codex_usage.py"
  if [[ ! -f "$usage_source" ]]; then
    warn "Codex usage recorder missing at ${usage_source}"
    rm -f "$tmp_output" "$usage_script_py" || true
    return 1
  fi

  if ! cp "$usage_source" "$usage_script_py"; then
    warn "Failed to copy Codex usage recorder."
    rm -f "$tmp_output" "$usage_script_py" || true
    return 1
  fi


  if python3 "$usage_script_py" "$log_file" "$usage_file" "$timestamp" "$task" "$model" "$prompt_file" "$exit_code" "${GC_COMMAND_FAILURE_CACHE:-}" "${GC_COMMAND_STREAM_CACHE:-}" "${GC_COMMAND_FILE_CACHE:-}" "${GC_COMMAND_SCAN_CACHE:-}" >"$tmp_output"
  then
    py_output="$(<"$tmp_output")"
    rm -f "$usage_script_py" || true
    rm -f "$tmp_output" || true
    if [[ -n "$py_output" ]]; then
      while IFS= read -r line; do
        [[ -z "$line" ]] && continue
        case "$line" in
          LIMIT_DETECTED$'\t'*)
            GC_CODEX_USAGE_LIMIT_REACHED=1
            GC_CODEX_USAGE_LIMIT_CONFIRMED=0
            GC_CODEX_USAGE_LIMIT_MESSAGE="${line#LIMIT_DETECTED$'\t'}"
            if gc_usage_limit_is_provider_signal "${GC_CODEX_USAGE_LIMIT_MESSAGE}"; then
              GC_CODEX_USAGE_LIMIT_CONFIRMED=1
            fi
            ;;
          LIMIT_DETECTED)
            GC_CODEX_USAGE_LIMIT_REACHED=1
            GC_CODEX_USAGE_LIMIT_CONFIRMED=0
            GC_CODEX_USAGE_LIMIT_MESSAGE=""
            ;;
          USAGE$'\t'*)
            IFS=$'\t' read -r _ prompt_val completion_val total_val <<<"$line"
            if [[ "$prompt_val" =~ ^[0-9]+$ ]]; then
              GC_LAST_CODEX_PROMPT_TOKENS=$((prompt_val))
            else
              GC_LAST_CODEX_PROMPT_TOKENS=0
            fi
            if [[ "$completion_val" =~ ^[0-9]+$ ]]; then
              GC_LAST_CODEX_COMPLETION_TOKENS=$((completion_val))
            else
              GC_LAST_CODEX_COMPLETION_TOKENS=0
            fi
            if [[ "$total_val" =~ ^[0-9]+$ ]]; then
              GC_LAST_CODEX_TOTAL_TOKENS=$((total_val))
            else
              GC_LAST_CODEX_TOTAL_TOKENS=$((GC_LAST_CODEX_PROMPT_TOKENS + GC_LAST_CODEX_COMPLETION_TOKENS))
            fi
            ;;
          CMDFAIL$'\t'*)
            local rest="${line#CMDFAIL$'\t'}"
          local repeat_flag total_fail _ digest encoded_command encoded_summary
          IFS=$'\t' read -r repeat_flag total_fail _ digest encoded_command encoded_summary <<<"$rest"
            local command_text summary_text
            command_text="$(gc_decode_base64 "${encoded_command:-}")"
            summary_text="$(gc_decode_base64 "${encoded_summary:-}")"
            command_text="${command_text//$'\r'/ }"
            command_text="${command_text//$'\n'/ }"
            summary_text="${summary_text//$'\r'/ }"
            summary_text="${summary_text//$'\n'/ }"
            command_text="${command_text#"${command_text%%[![:space:]]*}"}"
            command_text="${command_text%"${command_text##*[![:space:]]}"}"
            summary_text="${summary_text#"${summary_text%%[![:space:]]*}"}"
            summary_text="${summary_text%"${summary_text##*[![:space:]]}"}"
            if [[ ${#summary_text} -gt 160 ]]; then
              summary_text="${summary_text:0:157}..."
            fi
            if [[ "$repeat_flag" == "1" && -n "$digest" ]]; then
              if [[ "$GC_COMMAND_FAILURE_WARN_DIGESTS" != *"|$digest|"* ]]; then
                if [[ -n "$command_text" ]]; then
                  warn "Repeated command failure detected (x${total_fail}): ${command_text}"
                else
                  warn "Repeated command failure detected (x${total_fail})."
                fi
                if [[ -n "$summary_text" ]]; then
                  warn "  Last failure: ${summary_text}"
                fi
                GC_COMMAND_FAILURE_WARN_DIGESTS+="|$digest|"
              fi
            fi
            ;;
          CMDSTREAM$'\t'*)
            local rest="${line#CMDSTREAM$'\t'}"
            local digest repeat_flag total_seen encoded_summary encoded_advice
            IFS=$'\t' read -r digest repeat_flag total_seen encoded_summary encoded_advice <<<"$rest"
            local summary_text advice_text
            summary_text="$(gc_decode_base64 "${encoded_summary:-}")"
            advice_text="$(gc_decode_base64 "${encoded_advice:-}")"
            summary_text="${summary_text//$'\r'/ }"
            summary_text="${summary_text//$'\n'/ }"
            advice_text="${advice_text//$'\r'/ }"
            advice_text="${advice_text//$'\n'/ }"
            summary_text="${summary_text#"${summary_text%%[![:space:]]*}"}"
            summary_text="${summary_text%"${summary_text##*[![:space:]]}"}"
            advice_text="${advice_text#"${advice_text%%[![:space:]]*}"}"
            advice_text="${advice_text%"${advice_text##*[![:space:]]}"}"
            if [[ "$GC_COMMAND_STREAM_WARN_DIGESTS" != *"|$digest|"* ]]; then
              local repeat_note=""
              if [[ "$repeat_flag" == "1" ]]; then
                repeat_note=" (repeat)"
              fi
              warn "Sequential sed/cat streaming detected${repeat_note}: ${summary_text}"
              if [[ -n "$advice_text" ]]; then
                warn "  Recommendation: ${advice_text}"
              fi
              if [[ "$total_seen" =~ ^[0-9]+$ && "$total_seen" -gt 1 ]]; then
                warn "  Observed ${total_seen} times; pivot to targeted searches to cut token usage."
              fi
              GC_COMMAND_STREAM_WARN_DIGESTS+="|$digest|"
            fi
            ;;
          CMDSCAN$'\t'*)
            local rest="${line#CMDSCAN$'\t'}"
            local digest repeat_flag total_seen encoded_command encoded_message
            IFS=$'\t' read -r digest repeat_flag total_seen encoded_command encoded_message <<<"$rest"
            local command_text message_text
            command_text="$(gc_decode_base64 "${encoded_command:-}")"
            message_text="$(gc_decode_base64 "${encoded_message:-}")"
            command_text="${command_text//$'\r'/ }"
            command_text="${command_text//$'\n'/ }"
            message_text="${message_text//$'\r'/ }"
            message_text="${message_text//$'\n'/ }"
            command_text="${command_text#"${command_text%%[![:space:]]*}"}"
            command_text="${command_text%"${command_text##*[![:space:]]}"}"
            message_text="${message_text#"${message_text%%[![:space:]]*}"}"
            message_text="${message_text%"${message_text##*[![:space:]]}"}"
            if [[ "$GC_COMMAND_SCAN_WARN_DIGESTS" != *"|$digest|"* ]]; then
              local repeat_note=""
              if [[ "$repeat_flag" == "1" ]]; then
                repeat_note=" (repeat)"
              fi
              warn "Directory crawl detected${repeat_note}: ${command_text:-<command unavailable>}"
              if [[ -n "$message_text" ]]; then
                warn "  Reason: ${message_text}"
              fi
              if [[ "$total_seen" =~ ^[0-9]+$ && "$total_seen" -gt 1 ]]; then
                warn "  Observed ${total_seen} times; declare new focus before exploring other areas."
              fi
              GC_COMMAND_SCAN_WARN_DIGESTS+="|$digest|"
            fi
            ;;
          CMDGUARD$'\t'*)
            local rest="${line#CMDGUARD$'\t'}"
            local digest repeat_flag issue_count encoded_command encoded_message
            IFS=$'\t' read -r digest repeat_flag issue_count encoded_command encoded_message <<<"$rest"
            local command_text message_text
            command_text="$(gc_decode_base64 "${encoded_command:-}")"
            message_text="$(gc_decode_base64 "${encoded_message:-}")"
            command_text="${command_text//$'\r'/ }"
            command_text="${command_text//$'\n'/ }"
            message_text="${message_text//$'\r'/ }"
            message_text="${message_text//$'\n'/ }"
            command_text="${command_text#"${command_text%%[![:space:]]*}"}"
            command_text="${command_text%"${command_text##*[![:space:]]}"}"
            message_text="${message_text#"${message_text%%[![:space:]]*}"}"
            message_text="${message_text%"${message_text##*[![:space:]]}"}"
            if [[ "$GC_COMMAND_GUARD_WARN_DIGESTS" != *"|$digest|"* ]]; then
              warn "Guardrails blocked expensive command: ${command_text:-pnpm}"
              if [[ -n "$message_text" ]]; then
                warn "  Fix before retry: ${message_text}"
              fi
              if [[ "$issue_count" =~ ^[0-9]+$ && "$issue_count" -gt 1 ]]; then
                warn "  Multiple pre-check issues detected; update plan/focus once resolved."
              fi
              GC_COMMAND_GUARD_WARN_DIGESTS+="|$digest|"
            fi
            ;;
          CMDFILE$'\t'*)
            local rest="${line#CMDFILE$'\t'}"
            local digest repeat_flag total_seen encoded_summary encoded_excerpt
            IFS=$'\t' read -r digest repeat_flag total_seen encoded_summary encoded_excerpt <<<"$rest"
            local summary_text excerpt_text
            summary_text="$(gc_decode_base64 "${encoded_summary:-}")"
            excerpt_text="$(gc_decode_base64 "${encoded_excerpt:-}")"
            summary_text="${summary_text//$'\r'/ }"
            summary_text="${summary_text//$'\n'/ }"
            excerpt_text="${excerpt_text//$'\r'/ }"
            excerpt_text="${excerpt_text//$'\n'/ }"
            summary_text="${summary_text#"${summary_text%%[![:space:]]*}"}"
            summary_text="${summary_text%"${summary_text##*[![:space:]]}"}"
            excerpt_text="${excerpt_text#"${excerpt_text%%[![:space:]]*}"}"
            excerpt_text="${excerpt_text%"${excerpt_text##*[![:space:]]}"}"
            if [[ "$repeat_flag" == "1" ]]; then
              if [[ "$GC_COMMAND_FILE_WARN_DIGESTS" != *"|$digest|"* ]]; then
                warn "Repeated file read detected: ${summary_text}"
                if [[ -n "$excerpt_text" ]]; then
                  local preview="$excerpt_text"
                  if [[ ${#preview} -gt 140 ]]; then
                    preview="${preview:0:137}..."
                  fi
                  warn "  Refer to cached excerpt instead of re-running cat/sed: \"${preview}\""
                else
                  warn "  Refer to the cached excerpt listed in the prompt instead of re-reading the file."
                fi
                if [[ "$total_seen" =~ ^[0-9]+$ && "$total_seen" -gt 1 ]]; then
                  warn "  Observed ${total_seen} reads for this slice; reuse the cached snippet unless the file changed."
                fi
                GC_COMMAND_FILE_WARN_DIGESTS+="|$digest|"
              fi
            else
              if [[ -n "$summary_text" ]]; then
                info "Cached file excerpt saved: ${summary_text}"
              fi
            fi
            ;;
        esac
      done <<<"$py_output"
    fi
    local limit_value="${GC_CODEX_MAX_TOKENS_PER_TASK:-0}"
    if [[ "$limit_value" =~ ^[0-9]+$ ]]; then
      limit_value=$((limit_value))
      if (( limit_value > 0 && GC_LAST_CODEX_TOTAL_TOKENS > limit_value )); then
        # shellcheck disable=SC2034
        GC_CODEX_USAGE_LIMIT_REACHED=1
        # shellcheck disable=SC2034
        GC_CODEX_USAGE_LIMIT_CONFIRMED=1
        if [[ -z "${GC_CODEX_USAGE_LIMIT_MESSAGE:-}" ]]; then
          GC_CODEX_USAGE_LIMIT_MESSAGE="Codex tokens ${GC_LAST_CODEX_TOTAL_TOKENS} exceeded per-task limit ${limit_value}."
        fi
      fi
    fi
    if command -v python3 >/dev/null 2>&1; then
      local tail_info=""
      local tail_helper=""
      if tail_helper="$(gc_clone_python_tool "codex_log_tail.py" "${PROJECT_ROOT:-$PWD}")"; then
        tail_info="$(python3 "$tail_helper" "$log_file")" || tail_info=""
      else
        tail_info=""
      fi
      if [[ -n "$tail_info" ]]; then
        local tail_hash tail_b64 turn_hash turn_blocks
        IFS=$'\n' read -r tail_hash tail_b64 turn_hash turn_blocks <<<"$tail_info"
        # shellcheck disable=SC2034
        [[ -n "$tail_hash" ]] && GC_LAST_CODEX_STDOUT_TAIL_HASH="$tail_hash"
        # shellcheck disable=SC2034
        [[ -n "$tail_b64" ]] && GC_LAST_CODEX_STDOUT_TAIL_BASE64="$tail_b64"
        # shellcheck disable=SC2034
        [[ -n "$turn_hash" ]] && GC_LAST_CODEX_TURN_DIFF_HASH="$turn_hash"
        if [[ "$turn_blocks" =~ ^[0-9]+$ ]]; then
          # shellcheck disable=SC2034
          GC_LAST_CODEX_TURN_DIFF_BLOCKS="$turn_blocks"
        fi
      fi
    fi
    rm -f "$log_file" || true
    return 0
  else
    rm -f "$tmp_output" || true
    warn "Failed to record Codex usage for task=${task} model=${model}."
    return 1
  fi
}

gc_bootstrap_reset_state() {
  local file
  file="$(gc_bootstrap_state_file)"
  rm -f "$file"
}

gc_bootstrap_mark_step() {
  local step="${1:?step required}"
  local status="${2:?status required}"
  local file
  file="$(gc_bootstrap_state_file)"
  mkdir -p "$(dirname "$file")"
  local helper_path
  helper_path="$(gc_clone_python_tool "bootstrap_mark_step.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$file" "$step" "$status" || return 1
}

gc_bootstrap_mark_complete() {
  local file
  file="$(gc_bootstrap_state_file)"
  mkdir -p "$(dirname "$file")"
  local helper_path
  helper_path="$(gc_clone_python_tool "bootstrap_mark_complete.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$file" || return 1
}

gc_bootstrap_step_is_done() {
  local step="${1:?step required}"
  local file
  file="$(gc_bootstrap_state_file)"
  [[ -f "$file" ]] || return 1
  local helper_path
  helper_path="$(gc_clone_python_tool "bootstrap_step_is_done.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$file" "$step"
}

gc_bootstrap_run_step() {
  local step="${1:?step required}"
  shift
  if gc_bootstrap_step_is_done "$step"; then
    info "Step '${step}' already completed; skipping."
    return 0
  fi
  if "$@"; then
    gc_bootstrap_mark_step "$step" "done"
    return 0
  else
    gc_bootstrap_mark_step "$step" "failed"
    return 1
  fi
}

gc_bootstrap_have_rfp() {
  local stage_dir="${STAGING_DIR:-}"
  local input_dir="${INPUT_DIR:-}"
  [[ -n "$stage_dir" && -f "$stage_dir/docs/rfp.md" ]] && return 0
  [[ -n "$stage_dir" && -f "$stage_dir/rfp.md" ]] && return 0
  [[ -n "$input_dir" && -f "$input_dir/rfp.md" ]] && return 0
  return 1
}

gc_auto_project_template() {
  local project_root="${1:?project root required}"
  local templates_root="${2:?templates root required}"
  shift 2
  local -a template_dirs=("$@")
  local count=${#template_dirs[@]}
  (( count )) || return 1
  if (( count == 1 )); then
    printf '%s\n' "${template_dirs[0]}"
    return 0
  fi

  local rfp_path
  rfp_path="$(gc_find_primary_rfp "$project_root")"
  [[ -n "$rfp_path" ]] || return 1

  local helper_path
  helper_path="$(gc_clone_python_tool "auto_project_template.py" "$project_root")" || return 1
  python3 "$helper_path" "$rfp_path" "${template_dirs[@]}"
}

gc_copy_project_template() {
  local template_dir="${1:?template directory required}"
  local project_root="${2:?project root required}"
  local helper_path
  helper_path="$(gc_clone_python_tool "copy_project_template.py" "$project_root")" || return 1
  python3 "$helper_path" "$template_dir" "$project_root"
}

gc_apply_project_template() {
  local project_root="${1:?project root required}"
  local template_request="${2:-auto}"
  local templates_root
  templates_root="$(gc_project_templates_root)"

  mapfile -t available_templates < <(find "$templates_root" -mindepth 1 -maxdepth 1 -type d | sort)
  (( ${#available_templates[@]} )) || {
    info "No project templates available under ${templates_root}; continuing without scaffolding."
    return 0
  }

  local chosen=""
  local request_lower
  request_lower="$(to_lower "$template_request")"
  if [[ "$request_lower" == "skip" ]]; then
    info "Skipping project template scaffolding (per flag)."
    return 0
  fi

  if [[ "$request_lower" != "auto" ]]; then
      for tpl in "${available_templates[@]}"; do
        local tpl_name
        tpl_name="$(basename "$tpl")"
      if [[ "$(to_lower "$tpl_name")" == "$request_lower" ]]; then
        chosen="$tpl"
        template_request="$tpl_name"
        break
      fi
    done
    if [[ -z "$chosen" ]]; then
      warn "Template '${template_request}' not found under ${templates_root}; available: $(printf '%s ' "${available_templates[@]##*/}")"
      return 1
    fi
  else
    chosen="$(gc_auto_project_template "$project_root" "$templates_root" "${available_templates[@]}")"
    if [[ -z "$chosen" ]]; then
      info "No matching project template determined automatically; continuing without scaffolding."
      return 0
    fi
  fi

  local template_name
  template_name="$(basename "$chosen")"
  info "Applying project template → ${template_name}"
  if ! gc_copy_project_template "$chosen" "$project_root"; then
    warn "Failed to copy template '${template_name}'"
    return 1
  fi
}

gc_parse_jira_tasks() {
  local jira_file="${1:?jira markdown path required}"
  local out_json="${2:?output json path required}"
  local helper_path
  helper_path="$(gc_clone_python_tool "parse_jira_tasks.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$jira_file" "$out_json"
}

gc_build_tasks_db() {
  local tasks_json="${1:?tasks json path required}"
  local db_path="${2:?sqlite db path required}"
  local force_flag="${3:-0}"
  local helper_path
  helper_path="$(gc_clone_python_tool "build_tasks_db.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$tasks_json" "$db_path" "$force_flag"
}

gc_build_context_file() {
  local dest_file="${1:?context destination required}"
  local staging_dir="${2:-$GC_STAGING_DIR}"
  local max_lines="${GC_CONTEXT_FILE_LINES:-200}"
  local allow_ui_dump="${GC_CONTEXT_INCLUDE_UI:-0}"
  local doc_snippet_env="${GC_PROMPT_DOC_SNIPPETS:-}"
  local doc_snippet_mode=0
  if [[ -n "$doc_snippet_env" ]]; then
    case "${doc_snippet_env,,}" in
      0|false|no) doc_snippet_mode=0 ;;
      *) doc_snippet_mode=1 ;;
    esac
  fi
  local pointer_digest_helper=""
  local context_doc_snippet_helper=""
  local context_dump_helper=""
  if (( doc_snippet_mode )); then
    pointer_digest_helper="$(gc_clone_python_tool "context_pointer_digest.py" "${PROJECT_ROOT:-$PWD}")" || return 1
    context_doc_snippet_helper="$(gc_clone_python_tool "context_doc_snippet_dump.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  fi
  context_dump_helper="$(gc_clone_python_tool "context_dump_file.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  local -A gc_context_pointer_seen=()
  local -a skip_patterns=()
  if declare -p GC_CONTEXT_SKIP_PATTERNS >/dev/null 2>&1; then
    skip_patterns=("${GC_CONTEXT_SKIP_PATTERNS[@]}")
  fi
  mkdir -p "$(dirname "$dest_file")"
  {
    echo "# Project Context (auto-generated)"
    echo
    shopt -s nullglob
    shopt -s globstar 2>/dev/null || true
    local f
    local -a patterns=(
      "$staging_dir"/pdr.* 
      "$staging_dir"/sds.* 
      "$staging_dir"/openapi.* 
      "$staging_dir"/*.md 
      "$staging_dir"/*.mdx 
      "$staging_dir"/*.adoc 
      "$staging_dir"/*.mmd 
      "$staging_dir"/*.sql 
      "$staging_dir"/*.yml 
      "$staging_dir"/*.yaml
    )
    if [[ "$allow_ui_dump" == "1" ]]; then
      patterns+=("$staging_dir"/*ui*pages*.* "$staging_dir"/*rfp*.*)
    fi
    for f in "${patterns[@]}"; do
      [[ -f "$f" ]] || continue
      local base_name
      base_name="$(basename "$f")"

      local pointer_digest=""
      if (( doc_snippet_mode )); then
        pointer_digest="$(GC_CONTEXT_POINTER_FILE="$f" python3 "$pointer_digest_helper")" || pointer_digest=""
        if [[ -n "$pointer_digest" ]]; then
          local existing=""
          if [[ -n "${gc_context_pointer_seen[$pointer_digest]+_}" ]]; then
            existing="${gc_context_pointer_seen[$pointer_digest]}"
          fi
          if [[ -n "$existing" ]]; then
            echo ""
            echo "----- FILE: ${base_name} -----"
            echo "(duplicate staged doc skipped; same initial content as ${existing})"
            continue
          fi
          gc_context_pointer_seen[$pointer_digest]="$base_name"
        fi
      fi

      local skip_file=0
      if ((${#skip_patterns[@]} > 0)); then
        local pattern
        for pattern in "${skip_patterns[@]}"; do
          [[ -n "$pattern" ]] || continue
          if [[ "$base_name" == "$pattern" || "$f" == "$pattern" ]]; then
            skip_file=1
            break
          fi
          # Treat plain patterns without glob characters as substring matches.
          if [[ "$pattern" != *'*'* && "$pattern" != *'?'* ]]; then
            if [[ "$f" == *"$pattern"* ]]; then
              skip_file=1
              break
            fi
          fi
        done
      fi
      (( skip_file )) && continue

      echo ""
      echo "----- FILE: ${base_name} -----"
      local mime=""
      if command -v file >/dev/null 2>&1; then
        mime="$(file -b --mime-type "$f" 2>/dev/null || true)"
      fi
      if [[ -n "$mime" ]]; then
        case "$mime" in
          text/*|application/json|application/vnd.openxmlformats-officedocument*|application/xml|application/xhtml+xml)
            ;;
          *)
            echo "(skipped non-text file; path: $f)"
            continue
            ;;
        esac
      fi
      if (( doc_snippet_mode )); then
        GC_CONTEXT_POINTER_MODE=1 GC_CONTEXT_POINTER_DIGEST="$pointer_digest" GC_DUMP_FILE="$f" GC_MAX_LINES="$max_lines" python3 "$context_doc_snippet_helper"
      else
        GC_DUMP_FILE="$f" GC_MAX_LINES="$max_lines" python3 "$context_dump_helper"
      fi
    done
    shopt -u globstar 2>/dev/null || true
    shopt -u nullglob || true
  } >"$dest_file"
}

gc_discovery_is_stale() {
  local discovery_file="${1:-}"
  shift || true
  local -a required_keys=("$@")
  if [[ -z "$discovery_file" || ! -f "$discovery_file" || ! -s "$discovery_file" ]]; then
    return 0
  fi
  local helper_path
  helper_path="$(gc_clone_python_tool "discovery_is_stale.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$discovery_file" "${required_keys[@]}"
}

gc_discovery_fill_defaults() {
  local discovery_file="${1:-}"
  shift || true
  local -a required_keys=("$@")
  [[ -n "$discovery_file" && -f "$discovery_file" ]] || return 1
  local helper_path
  helper_path="$(gc_clone_python_tool "discovery_fill_defaults.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$discovery_file" "${required_keys[@]}"
}

gc_refresh_discovery_if_needed() {
  local discovery_file="${STAGING_DIR}/discovery.yaml"
  local -a required=("pdr" "sds" "rfp" "jira" "ui_pages" "openapi")
  if gc_discovery_is_stale "$discovery_file" "${required[@]}"; then
    info "Discovery manifest missing context; running scan + normalize."
    if ! cmd_scan --project "$PROJECT_ROOT"; then
      warn "Automatic scan failed; rerun 'gpt-creator scan' manually."
      return
    fi
    if ! cmd_normalize --project "$PROJECT_ROOT"; then
      warn "Automatic normalize failed; rerun 'gpt-creator normalize' manually."
      return
    fi
    if ! gc_discovery_is_stale "$discovery_file" "${required[@]}"; then
      ok "Discovery manifest refreshed."
    else
      warn "Discovery manifest still incomplete after refresh; check project docs."
      if gc_discovery_fill_defaults "$discovery_file" "${required[@]}"; then
        if ! gc_discovery_is_stale "$discovery_file" "${required[@]}"; then
          ok "Discovery manifest supplemented with placeholder entries."
        fi
      fi
    fi
  fi
}

gc_clear_active_task() {
  unset GC_ACTIVE_TASK_DB
  unset GC_ACTIVE_TASK_SLUG
  unset GC_ACTIVE_TASK_INDEX
  unset GC_ACTIVE_RUN_STAMP
  unset GC_ACTIVE_TASK_NUMBER
  unset GC_ACTIVE_TASK_ID
  unset GC_ACTIVE_TASK_REPORT
  unset GC_ACTIVE_TASK_ARCHIVE
  unset GC_ACTIVE_TASK_PROMPT
  unset GC_ACTIVE_TASK_OUTPUT
}

gc_finalize_active_task() {
  local signal="${1:-INT}"
  local reason="${2:-Interrupted}"
  local report_path="${GC_ACTIVE_TASK_REPORT:-}"
  [[ -n "$report_path" ]] || return 0

  local archive_path="${GC_ACTIVE_TASK_ARCHIVE:-}"
  local prompt_path="${GC_ACTIVE_TASK_PROMPT:-}"
  local output_path="${GC_ACTIVE_TASK_OUTPUT:-}"
  local task_number="${GC_ACTIVE_TASK_NUMBER:-unknown}"
  local task_id="${GC_ACTIVE_TASK_ID:-}"
  local slug="${GC_ACTIVE_TASK_SLUG:-unknown}"
  local tasks_db="${GC_ACTIVE_TASK_DB:-}"
  local task_index="${GC_ACTIVE_TASK_INDEX:-}"
  local run_stamp="${GC_ACTIVE_RUN_STAMP:-manual}"
  local timestamp
  timestamp="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"

  mkdir -p "$(dirname "$report_path")"
  local prompt_entry="$prompt_path"
  local output_entry="$output_path"
  local report_entry_display="$report_path"
  if [[ -n "$PROJECT_ROOT" ]]; then
    local project_prefix="${PROJECT_ROOT%/}/"
    if [[ "$prompt_entry" == "$project_prefix"* ]]; then
      prompt_entry="${prompt_entry#$project_prefix}"
    fi
    if [[ "$output_entry" == "$project_prefix"* ]]; then
      output_entry="${output_entry#$project_prefix}"
    fi
    if [[ "$report_entry_display" == "$project_prefix"* ]]; then
      report_entry_display="${report_entry_display#$project_prefix}"
    fi
  fi

  {
    printf 'task_number: %s\n' "$task_number"
    printf 'task_id: %s\n' "${task_id:-}"
    printf 'task_title: %s\n' "(interrupted)"
    printf 'story_slug: %s\n' "$slug"
    printf 'status: %s\n' "interrupted"
    printf 'timestamp: %s\n' "$timestamp"
    printf 'attempts: %s\n' "0"
    printf 'apply_status: %s\n' "interrupted"
    printf 'changes_applied: %s\n' "false"
    printf 'prompt_path: %s\n' "${prompt_entry:-"(none)"}"
    printf 'output_path: %s\n' "${output_entry:-"(none)"}"
    printf 'notes:\n'
    printf '  - %s by signal %s; rerun work-on-tasks to resume.\n' "$reason" "$signal"
  } >"$report_path"

  if [[ -n "$archive_path" ]]; then
    mkdir -p "$(dirname "$archive_path")"
    cp -f "$report_path" "$archive_path" 2>/dev/null || true
  fi

  if [[ -n "$tasks_db" && -n "$slug" && -n "$task_index" ]]; then
    local report_entry_db="$report_entry_display"
    local prompt_db="$prompt_entry"
    local output_db="$output_entry"
    local notes_payload="Interrupted by signal ${signal}; rerun work-on-tasks to resume."
    gc_record_task_progress "$tasks_db" "$slug" "$task_index" "$run_stamp" "on-hold" "$report_entry_db" "$prompt_db" "$output_db" "0" "0" "0" "interrupted" "false" "$notes_payload" "" "" "" "$timestamp"
    gc_update_task_state "$tasks_db" "$slug" "$task_index" "on-hold" "$run_stamp"
  fi

  gc_clear_active_task
}

gc_interrupt_handler() {
  local signal="${1:-INT}"
  trap - INT TERM TSTP QUIT
  warn "Received ${signal}; finalising active task and exiting."
  gc_finalize_active_task "$signal" "Interrupted"
  local code=128
  case "$signal" in
    INT) code=130 ;;
    TERM) code=143 ;;
    TSTP) code=148 ;;
    QUIT) code=131 ;;
  esac
  exit "$code"
}

gc_build_context_digest() {
  local context_file="${1:?context file required}"
  local digest_file="${2:?digest destination required}"
  local limit_lines="${3:-400}"
  [[ -f "$context_file" ]] || {
    printf '%s\n' "(warn) context digest skipped; missing source ${context_file}" >&2
    return 1
  }
  local helper_path
  helper_path="$(gc_clone_python_tool "build_context_digest.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$context_file" "$digest_file" "$limit_lines"
}

gc_refresh_context_tail() {
  local context_file="${1:?context file required}"
  local tail_file="${2:?tail file required}"
  local mode="${3:-digest}"
  local limit="${4:-0}"

  [[ -n "$tail_file" ]] || {
    printf '%s\n' "$mode"
    return 0
  }

  if (( limit <= 0 )); then
    : >"$tail_file"
    printf '%s\n' "$mode"
    return 0
  fi

  local effective_mode="$mode"
  case "$mode" in
    digest)
      if gc_build_context_digest "$context_file" "$tail_file" "$limit"; then
        printf '%s\n' "$effective_mode"
        return 0
      fi
      warn "Failed to build context digest (limit=${limit}); falling back to raw tail."
      effective_mode="raw"
      ;;
  esac

  case "$effective_mode" in
    raw)
      if ! tail -n "$limit" "$context_file" >"$tail_file" 2>/dev/null; then
        cp "$context_file" "$tail_file"
      fi
      ;;
    *)
      if ! tail -n "$limit" "$context_file" >"$tail_file" 2>/dev/null; then
        cp "$context_file" "$tail_file"
      fi
      ;;
  esac

  printf '%s\n' "$effective_mode"
}

gc_update_work_state() {
  local db_path="${1:?tasks database path required}"
  local story_slug="${2:?story slug required}"
  local status="${3:?status required}"
  local completed="${4:-0}"
  local total="${5:-0}"
  local run_stamp="${6:-manual}"
  local helper_path
  helper_path="$(gc_clone_python_tool "update_work_state.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$db_path" "$story_slug" "$status" "$completed" "$total" "$run_stamp"
}

gc_update_task_state() {
  local db_path="${1:?tasks database path required}"
  local story_slug="${2:?story slug required}"
  local position="${3:?task position required}"
  local status="${4:?status required}"
  local run_stamp="${5:-manual}"
  local helper_path
  helper_path="$(gc_clone_python_tool "update_task_state.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$db_path" "$story_slug" "$position" "$status" "$run_stamp"
}

gc_record_task_progress() {
  local db_path="${1:?tasks database path required}"
  local story_slug="${2:?story slug required}"
  local position="${3:?task position required}"
  local run_stamp="${4:-manual}"
  local status="${5:-}"
  local log_path="${6:-}"
  local prompt_path="${7:-}"
  local output_path="${8:-}"
  local attempts="${9:-0}"
  local tokens_total="${10:-0}"
  local duration_seconds="${11:-0}"
  local apply_status="${12:-}"
  local changes_applied="${13:-0}"
  local notes_text="${14:-}"
  local written_text="${15:-}"
  local patched_text="${16:-}"
  local commands_text="${17:-}"
  local occurred_at="${18:-}"
  local helper_path
  helper_path="$(gc_clone_python_tool "record_task_progress.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" \
    "$db_path" \
    "$story_slug" \
    "$position" \
    "$run_stamp" \
    "$status" \
    "$log_path" \
    "$prompt_path" \
    "$output_path" \
    "$attempts" \
    "$tokens_total" \
    "$duration_seconds" \
    "$apply_status" \
    "$changes_applied" \
    "$notes_text" \
    "$written_text" \
    "$patched_text" \
    "$commands_text" \
    "$occurred_at"
}

gc_update_throughput_metrics() {
  local db_path="${1:?tasks database path required}"
  local action="${2:?throughput action required}"
  local story_slug="${3:-}"
  local position="${4:-}"
  local helper_path
  helper_path="$(gc_clone_python_tool "update_throughput_metrics.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$db_path" "$action" "$story_slug" "$position"
}

gc_rewind_backlog_from_task() {
  local db_path="${1:?tasks database path required}"
  local task_ref_raw="${2:?task reference required}"
  local story_hint_raw="${3:-}"
  local helper_path
  helper_path="$(gc_clone_python_tool "rewind_backlog_from_task.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$db_path" "$task_ref_raw" "$story_hint_raw"
}

gc_align_task_story_slugs() {
  local db_path="${1:?tasks database path required}"
  local helper_path
  helper_path="$(gc_clone_python_tool "align_task_story_slugs.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$db_path"
}

gc_reset_task_progress() {
  local db_path="${1:?tasks database path required}"
  local helper_path
  helper_path="$(gc_clone_python_tool "reset_task_progress.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$db_path"
}

gc_sync_story_totals() {
  local db_path="${1:?tasks database path required}"
  local helper_path
  helper_path="$(gc_clone_python_tool "sync_story_totals.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$db_path"
}

gc_fetch_story_task_counts() {
  local db_path="${1:?tasks database path required}"
  local story_slug="${2:?story slug required}"
  local helper_path
  helper_path="$(gc_clone_python_tool "fetch_story_task_counts.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$db_path" "$story_slug"
}

gc_trim_memory() {
  local phase="${1:-memory-cycle}"
  info "[memory] Reclaiming resources (${phase})"

  local helper_path
  helper_path="$(gc_clone_python_tool "trim_memory.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  if command -v python3 >/dev/null 2>&1; then
    python3 "$helper_path" >/dev/null 2>&1 || true
  fi
}

gc_count_pending_tasks() {
  local db_path="${1:?tasks database path required}"
  local helper_path
  helper_path="$(gc_clone_python_tool "count_pending_tasks.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$db_path"
}

gc_count_unstarted_tasks() {
  local db_path="${1:?tasks database path required}"
  local helper_path
  helper_path="$(gc_clone_python_tool "count_unstarted_tasks.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$db_path"
}

gc_tasks_db_has_rows() {
  local db_path="${1:?tasks database path required}"
  [[ -f "$db_path" ]] || return 1
  local helper_path
  helper_path="$(gc_clone_python_tool "tasks_db_has_rows.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$db_path"
}

gc_has_legacy_tasks_json() {
  local json_dir="${PLAN_DIR}/create-jira-tasks/json"
  [[ -f "${json_dir}/epics.json" ]] || return 1
  [[ -d "${json_dir}/stories" ]] || return 1
  [[ -d "${json_dir}/tasks" ]] || return 1
  return 0
}

gc_rebuild_tasks_db_from_json() {
  local force_flag="${1:-0}"
  local json_dir="${PLAN_DIR}/create-jira-tasks/json"
  local epics_json="${json_dir}/epics.json"
  local stories_dir="${json_dir}/stories"
  local tasks_dir="${json_dir}/tasks"
  local refined_dir="${json_dir}/refined"
  [[ -f "$epics_json" ]] || return 1
  [[ -d "$stories_dir" ]] || return 1
  [[ -d "$tasks_dir" ]] || return 1

  local payload="${json_dir}/tasks_payload.json"
  python3 "${CLI_ROOT}/src/lib/create-jira-tasks/to_payload.py" \
    "$epics_json" "$stories_dir" "$tasks_dir" "$refined_dir" "$payload" || return 1

  local tasks_workspace="${PLAN_DIR}/tasks"
  mkdir -p "$tasks_workspace"
  local db_path="${tasks_workspace}/tasks.db"
  python3 "${CLI_ROOT}/src/lib/create-jira-tasks/to_sqlite.py" \
    "$payload" "$db_path" "$force_flag" || return 1

  return 0
}

# docker compose helper (prefers "docker compose" then docker-compose)
docker_compose() {
  local project_name="${GC_DOCKER_PROJECT_NAME:-${COMPOSE_PROJECT_NAME:-}}"
  if [[ -z "$project_name" ]]; then
    local base
    base="$(basename "${PROJECT_ROOT:-$PWD}")"
    project_name="$(slugify_name "$base")"
  fi
  local compose_verbose_flag=""
  if [[ -n "${GC_DOCKER_VERBOSE:-}" ]]; then
    compose_verbose_flag="--verbose"
  fi
  if command -v docker >/dev/null 2>&1 && docker compose version >/dev/null 2>&1; then
    if [[ -n "$compose_verbose_flag" ]]; then
      COMPOSE_PROJECT_NAME="$project_name" docker compose "$compose_verbose_flag" "$@"
    else
      COMPOSE_PROJECT_NAME="$project_name" docker compose "$@"
    fi
  elif command -v docker-compose >/dev/null 2>&1; then
    if [[ -n "$compose_verbose_flag" ]]; then
      docker-compose -p "$project_name" "$compose_verbose_flag" "$@"
    else
      docker-compose -p "$project_name" "$@"
    fi
  else
    die "Docker CLI not available. Install Docker Desktop or docker-compose before running this command."
  fi
}

gc_compose_port() {
  local compose_file="$1" service="$2" container_port="${3:-}"
  [[ -f "$compose_file" ]] || return 1
  local output=""
  if command -v docker >/dev/null 2>&1 && docker compose version >/dev/null 2>&1; then
    output="$(COMPOSE_PROJECT_NAME="$GC_DOCKER_PROJECT_NAME" docker compose -f "$compose_file" port "$service" "$container_port" 2>/dev/null | head -n1)"
  fi
  if [[ -z "$output" ]] && command -v docker-compose >/dev/null 2>&1; then
    output="$(docker-compose -p "$GC_DOCKER_PROJECT_NAME" -f "$compose_file" port "$service" "$container_port" 2>/dev/null | head -n1)"
  fi
  [[ -n "$output" ]] || return 1
  output="${output##*:}"
  [[ "$output" =~ ^[0-9]+$ ]] || return 1
  printf '%s\n' "$output"
}

gc_container_name() {
  local service="$1"
  printf '%s-%s\n' "${GC_DOCKER_PROJECT_NAME}" "$service"
}

container_exists() {
  local name="$1"
  docker ps -a --format '{{.Names}}' | grep -Fxq "$name"
}

container_state() {
  local name="$1"
  docker inspect -f '{{.State.Status}}' "$name" 2>/dev/null || echo "absent"
}

gc_start_created_containers() {
  local compose_file="$1"
  shift || true
  local -a services=("$@")
  local service container state any_started=0

  for service in "${services[@]}"; do
    container="$(gc_container_name "$service")"
    if ! container_exists "$container"; then
      continue
    fi
    state="$(container_state "$container")"
    if [[ "$state" == "created" ]]; then
      info "Container ${container} stuck in 'created'; attempting manual start"
      if docker start "$container" >/dev/null 2>&1; then
        any_started=1
      else
        warn "Failed to start ${container}; attempting compose start fallback"
        docker_compose -f "$compose_file" start "$service" >/dev/null 2>&1 || true
      fi
    fi
  done

  if (( any_started == 1 )); then
    local wait_seconds=0
    local timeout="${GC_DOCKER_HEALTH_TIMEOUT:-10}"
    local poll_interval="${GC_DOCKER_HEALTH_INTERVAL:-1}"
    (( poll_interval <= 0 )) && poll_interval=1
    while (( wait_seconds < timeout )); do
      local all_ready=1
      for service in "${services[@]}"; do
        container="$(gc_container_name "$service")"
        if ! container_exists "$container"; then
          continue
        fi
        state="$(container_state "$container")"
        case "$state" in
          running|healthy) continue ;;
          exited|dead)
            warn "Container ${container} exited unexpectedly (state=${state}). Check logs."
            all_ready=0
            ;;
          *)
            all_ready=0
            ;;
        esac
      done
      if (( all_ready == 1 )); then
        break
      fi
      sleep "$poll_interval"
      (( wait_seconds += poll_interval )) || true
    done
  fi
}

port_in_use() {
  local port="$1"
  if command -v lsof >/dev/null 2>&1; then
    lsof -nP -iTCP:"$port" -sTCP:LISTEN >/dev/null 2>&1 && return 0
  elif command -v netstat >/dev/null 2>&1; then
    netstat -an 2>/dev/null | grep -E "\.${port} .*LISTEN" >/dev/null && return 0
  fi
  return 1
}

find_free_port() {
  local start="${1:-3306}"
  local port="$start"; local limit=$((start+100))
  while (( port <= limit )); do
    if ! port_in_use "$port" && ! gc_port_is_reserved "$port"; then
      echo "$port"
      return 0
    fi
    ((port++)) || true
  done
  echo "$start"  # fallback
}

gc_pick_port() {
  local label="$1"
  local default="$2"
  shift 2
  local service_key
  service_key="$(slugify_name "$label")"
  [[ -n "$service_key" ]] || service_key="$label"
  local existing_port=""
  existing_port="$(gc_port_for_service "$service_key" 2>/dev/null || true)"
  gc_unreserve_port "$service_key"
  local port=""
  local env_name value
  for env_name in "$@"; do
    value="${!env_name:-}"
    if [[ -n "$value" ]] && [[ "$value" =~ ^[0-9]+$ ]]; then
      port="$value"
      break
    fi
  done
  [[ -n "$port" ]] || port="$default"
  if [[ ! "$port" =~ ^[0-9]+$ ]] || (( port < 1 || port > 65535 )); then
    port="$default"
  fi
  local original="$port"
  local attempts=0
  local limit=200
  while (( attempts < limit )); do
    if (( port < 1 || port > 65535 )); then
      port="$default"
    fi
    if ! port_in_use "$port" && ! gc_port_reserved_by_other "$port" "$service_key"; then
      break
    fi
    ((port++))
    ((attempts++))
  done
  while gc_port_reserved_by_other "$port" "$service_key"; do
    ((port++))
  done
  if (( attempts >= limit )); then
    warn "Unable to find free port for ${label}; using ${port}" >&2
  elif [[ -n "$original" && "$port" != "$original" ]]; then
    info "Port ${original} in use; remapping ${label} to ${port}" >&2
  elif [[ -z "$existing_port" && "$port" != "$default" ]]; then
    info "Port ${default} in use; remapping ${label} to ${port}" >&2
  fi
  gc_reserve_port "$service_key" "$port"
  echo "$port"
}

wait_for_endpoint() {
  local url="$1" label="$2"
  local max_time="${3:-${GC_DOCKER_HEALTH_TIMEOUT:-10}}"
  local delay="${4:-${GC_DOCKER_HEALTH_INTERVAL:-1}}"
  (( delay <= 0 )) && delay=1
  (( max_time <= 0 )) && max_time=1
  local attempts=$(( (max_time + delay - 1) / delay ))
  (( attempts < 1 )) && attempts=1
  local i=1
  while (( i <= attempts )); do
    if curl -fsS --max-time 2 "$url" >/dev/null 2>&1; then
      ok "${label} ready → ${url}"
      return 0
    fi
    sleep "$delay"
    ((i++)) || true
  done
  warn "${label} not ready after ${max_time}s → ${url}"
  return 1
}

gc_sha256_file() {
  local path="$1"
  [[ -f "$path" ]] || return 1
  local helper_path
  helper_path="$(gc_clone_python_tool "sha256_file.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$path"
}

gc_host_prepare_pnpm() {
  if command -v pnpm >/dev/null 2>&1; then
    return 0
  fi
  if command -v corepack >/dev/null 2>&1; then
    corepack enable pnpm >/dev/null 2>&1 || true
    local version="${GC_PNPM_VERSION:-10.17.1}"
    corepack prepare "pnpm@${version}" --activate >/dev/null 2>&1 || \
      corepack use "pnpm@${version}" >/dev/null 2>&1 || true
  fi
  command -v pnpm >/dev/null 2>&1
}

gc_refresh_stack_prepare_node_modules() {
  [[ "${GC_SKIP_HOST_PNPM_INSTALL:-0}" == "1" ]] && return 0
  local root="${PROJECT_ROOT:-$PWD}"
  local lock_file="${root}/pnpm-lock.yaml"
  local has_manifest=0
  if [[ -f "${root}/pnpm-workspace.yaml" || -f "${root}/package.json" ]]; then
    has_manifest=1
  fi
  (( has_manifest )) || return 0

  local modules_dir="${root}/node_modules/.pnpm"
  local stamp_file="${root}/node_modules/.pnpm-lock.hash"
  local need_install=0
  local lock_hash=""

  if [[ -f "$lock_file" ]]; then
    lock_hash="$(gc_sha256_file "$lock_file" 2>/dev/null || true)"
  fi

  if [[ ! -d "$modules_dir" ]]; then
    need_install=1
  else
    if [[ -z "$lock_hash" ]]; then
      need_install=1
    else
      local stamp_hash=""
      [[ -f "$stamp_file" ]] && stamp_hash="$(cat "$stamp_file" 2>/dev/null || true)"
      if [[ "$lock_hash" != "$stamp_hash" ]]; then
        need_install=1
      fi
    fi
  fi

  if (( need_install )); then
    if [[ ! -f "$lock_file" ]]; then
      info "pnpm lockfile missing; generating via install"
    fi
    info "Installing workspace dependencies via pnpm (host)"
    if ! gc_host_prepare_pnpm; then
      warn "pnpm is not available on the host; skipping host install (containers may retry)."
      return 0
    fi
    local install_rc=0
    if (cd "$root" && CI=1 PNPM_IGNORE_NODE_VERSION=1 pnpm install --frozen-lockfile --unsafe-perm --prefer-offline --engine-strict=false --reporter=append-only); then
      install_rc=0
    else
      warn "pnpm install --frozen-lockfile failed; retrying without frozen lockfile"
      if (cd "$root" && CI=1 PNPM_IGNORE_NODE_VERSION=1 pnpm install --unsafe-perm --prefer-offline --engine-strict=false --no-frozen-lockfile --reporter=append-only); then
        install_rc=0
      else
        install_rc=1
      fi
    fi
    if (( install_rc == 0 )); then
      lock_hash="$(gc_sha256_file "$lock_file" 2>/dev/null || true)"
      if [[ -n "$lock_hash" ]]; then
        mkdir -p "${root}/node_modules"
        printf '%s' "$lock_hash" > "$stamp_file"
      else
        rm -f "$stamp_file"
      fi
      ok "Host dependencies installed"
    else
      warn "Host pnpm install failed; containers will attempt dependency installation."
    fi
  fi
}

render_template_file() {
  local src="$1" dest="$2"
  local db_name="${GC_DB_NAME:-${DB_NAME:-app}}"
  local db_user="${GC_DB_USER:-${DB_USER:-app}}"
  local db_pass="${GC_DB_PASSWORD:-${DB_PASSWORD:-app_pass}}"
  local db_host_port="${GC_DB_HOST_PORT:-${DB_HOST_PORT:-3306}}"
  local db_root_pass="${GC_DB_ROOT_PASSWORD:-${DB_ROOT_PASSWORD:-root}}"
  local project_slug="${GC_DOCKER_PROJECT_NAME:-${PROJECT_SLUG:-$(slugify_name "$(basename "${PROJECT_ROOT:-$PWD}")")}}"
  local api_host_port="${GC_API_HOST_PORT:-${API_HOST_PORT:-3000}}"
  local web_host_port="${GC_WEB_HOST_PORT:-${WEB_HOST_PORT:-5173}}"
  local admin_host_port="${GC_ADMIN_HOST_PORT:-${ADMIN_HOST_PORT:-5174}}"
  local proxy_host_port="${GC_PROXY_HOST_PORT:-${PROXY_HOST_PORT:-8080}}"
  local helper_path
  helper_path="$(gc_clone_python_tool "render_template_file.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$src" "$dest" "$db_name" "$db_user" "$db_pass" "$db_host_port" "$db_root_pass" "$project_slug" "$api_host_port" "$web_host_port" "$admin_host_port" "$proxy_host_port"
}

gc_render_sql() {
  local src="$1" dest="$2" database="$3" app_user="$4" app_pass="$5"
  local helper_path
  helper_path="$(gc_clone_python_tool "render_sql.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$src" "$dest" "$database" "$app_user" "$app_pass"
}

gc_temp_file() {
  local dir="$1" prefix="$2" suffix="$3"
  local helper_path
  helper_path="$(gc_clone_python_tool "temp_file.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$dir" "$prefix" "$suffix"
}

gc_execute_sql() {
  local compose_file="$1" sql_file="$2" database="$3"
  local root_user="$4" root_pass="$5" app_user="$6" app_pass="$7" fallback_init="$8" label="$9"
  local import_ok=0
  [[ -n "$label" ]] || label="operation"
  local container_host="127.0.0.1"

  if [[ -f "$compose_file" ]]; then
    if docker_compose -f "$compose_file" ps >/dev/null 2>&1; then
      info "Using docker-compose db service for ${label}"
      docker_compose -f "$compose_file" up -d db >/dev/null 2>&1 || true
      if [[ -n "$root_pass" ]]; then
        if docker_compose -f "$compose_file" exec -T db mysql -h"${container_host}" -u"${root_user}" -p"${root_pass}" "${database}" < "${sql_file}"; then
          import_ok=1
        else
          warn "Root ${label} failed; retrying as ${app_user}"
        fi
      else
        if docker_compose -f "$compose_file" exec -T db mysql -h"${container_host}" -u"${root_user}" "${database}" < "${sql_file}"; then
          import_ok=1
        fi
      fi
      if [[ "$import_ok" -ne 1 ]]; then
        if docker_compose -f "$compose_file" exec -T db mysql -h"${container_host}" -u"${app_user}" ${app_pass:+-p"${app_pass}"} "${database}" < "${sql_file}"; then
          import_ok=1
        fi
      fi
      if [[ "$import_ok" -ne 1 && -n "$fallback_init" && -f "$fallback_init" ]]; then
        local fallback_output fallback_user fallback_pass
        fallback_output="$(python3 - "$fallback_init" <<'PY'
import re, sys
text = open(sys.argv[1]).read()
user = re.search(r"CREATE USER IF NOT EXISTS '([^']+)'", text)
password = re.search(r"IDENTIFIED BY '([^']+)'", text)
if user and password:
    print(user.group(1))
    print(password.group(1))
PY
)"
        if [[ -n "$fallback_output" ]]; then
          IFS=$'\n' read -r fallback_user fallback_pass _ <<<"$fallback_output"
          unset IFS
          if [[ -n "$fallback_user" && -n "$fallback_pass" ]]; then
            if docker_compose -f "$compose_file" exec -T db mysql -h"${container_host}" -u"${fallback_user}" ${fallback_pass:+-p"${fallback_pass}"} "${database}" < "${sql_file}"; then
              import_ok=1
            fi
          fi
        fi
      fi
      if [[ "$import_ok" -eq 1 ]]; then
        return 0
      fi
    fi
  fi

  local host="${DB_HOST:-127.0.0.1}"
  local port="${DB_HOST_PORT:-${GC_DB_HOST_PORT:-${DB_PORT:-3306}}}"
  if ${MYSQL_BIN} -h "$host" -P "$port" -u "$root_user" ${root_pass:+-p"${root_pass}"} "$database" < "$sql_file"; then
    return 0
  fi

  if ${MYSQL_BIN} -h "$host" -P "$port" -u "$app_user" ${app_pass:+-p"${app_pass}"} "$database" < "$sql_file"; then
    return 0
  fi

  return 1
}
gc_refresh_stack_collect_sql() {
  local root="$1"
  local helper_path
  helper_path="$(gc_clone_python_tool "refresh_stack_collect_sql.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$root"
}

gc_refresh_stack_exec_mysql() {
  local container_id="$1" sql_file="$2" user="$3" password="$4" database="$5"
  local port="${6:-3306}"

  [[ -n "$container_id" ]] || return 1
  [[ -f "$sql_file" ]] || return 1
  local -a cmd=(docker exec -i "$container_id" mysql --protocol=TCP -h 127.0.0.1 -P "$port" "-u${user}")
  if [[ -n "$password" ]]; then
    cmd+=("-p${password}")
  fi
  if [[ -n "$database" ]]; then
    cmd+=("$database")
  fi
  if ! "${cmd[@]}" <"$sql_file"; then
    return 1
  fi
  return 0
}

gc_refresh_stack_exec_inline_sql() {
  local container_id="$1" user="$2" password="$3" database="$4"
  local port="${5:-3306}"
  local sql_content
  sql_content="$(cat)"
  local -a cmd=(docker exec -i "$container_id" mysql --protocol=TCP -h 127.0.0.1 -P "$port" "-u${user}")
  if [[ -n "$password" ]]; then
    cmd+=("-p${password}")
  fi
  if [[ -n "$database" ]]; then
    cmd+=("$database")
  fi
  if ! printf "%s" "$sql_content" | "${cmd[@]}"; then
    return 1
  fi
  return 0
}

gc_refresh_stack_inspect_containers() {
  local compose_file="${1:?compose file required}"
  local -a container_ids=()
  mapfile -t container_ids < <(docker_compose -f "$compose_file" ps --all -q 2>/dev/null | awk 'NF')
  if (( ${#container_ids[@]} == 0 )); then
    mapfile -t container_ids < <(docker_compose -f "$compose_file" ps -q 2>/dev/null | awk 'NF')
  fi
  if (( ${#container_ids[@]} == 0 )); then
    printf '%s\n' "No containers found for project ${GC_DOCKER_PROJECT_NAME}."
    return 1
  fi

  local inspect_json=""
  if ! inspect_json="$(docker inspect "${container_ids[@]}" 2>/dev/null)"; then
    printf '%s\n' "Failed to inspect Docker containers for project ${GC_DOCKER_PROJECT_NAME}."
    return 1
  fi

  local helper_path
  helper_path="$(gc_clone_python_tool "refresh_stack_inspect_containers.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  INSPECT_JSON="${inspect_json}" python3 "$helper_path"
}

gc_refresh_stack_wait_for_containers() {
  local compose_file="${1:?compose file required}"
  local timeout="${2:-${GC_DOCKER_HEALTH_TIMEOUT:-10}}"
  local interval="${3:-${GC_DOCKER_HEALTH_INTERVAL:-1}}"
  (( interval <= 0 )) && interval=1
  local elapsed=0
  local output rc

  while (( elapsed <= timeout )); do
    output="$(gc_refresh_stack_inspect_containers "$compose_file")"
    rc=$?
    if (( rc == 0 )); then
      while IFS= read -r line; do
        [[ -z "$line" ]] && continue
        info "$line"
      done <<<"$output"
      return 0
    elif (( rc == 2 )); then
      while IFS= read -r line; do
        [[ -z "$line" ]] && continue
        info "$line"
      done <<<"$output"
      sleep "$interval"
      (( elapsed += interval ))
      continue
    else
      while IFS= read -r line; do
        [[ -z "$line" ]] && continue
        warn "$line"
      done <<<"$output"
      return 1
    fi
  done

  warn "Timed out after ${timeout}s waiting for containers to report healthy state."
  output="$(gc_refresh_stack_inspect_containers "$compose_file")"
  rc=$?
  local log_fn=warn
  if (( rc == 0 )); then
    log_fn=info
  fi
  while IFS= read -r line; do
    [[ -z "$line" ]] && continue
    "$log_fn" "$line"
  done <<<"$output"
  (( rc == 0 )) || return 1
  return 0
}

# ---------- Scan helpers ----------
has_pattern() { LC_ALL=C grep -E -i -m 1 -q -- "$1" "$2" 2>/dev/null; }
classify_file() {
  local path="$1"
  local name="${path##*/}"
  local lower
  lower="$(to_lower "$name")"
  local path_norm
  path_norm="$(to_lower "$path")"
  local ext="${lower##*.}"
  local type="" conf=0

  case "$ext" in
    md)
      if [[ "$lower" == *pdr* ]]; then type="pdr"; conf=0.95
      elif [[ "$lower" == *sds* ]]; then type="sds"; conf=0.92
      elif [[ "$lower" == *rfp* ]]; then type="rfp"; conf=0.9
      elif [[ "$lower" == *jira* ]]; then type="jira"; conf=0.88
      elif [[ "$lower" == *ui*pages* || "$lower" == *website*ui*pages* ]]; then type="ui_pages"; conf=0.85
      elif has_pattern '\bJIRA\b|Issue Key' "$path"; then type="jira"; conf=0.6
      fi
      ;;
    yml|yaml|json)
      if has_pattern '^[[:space:]]*openapi[[:space:]]*:[[:space:]]*3' "$path" || has_pattern '"openapi"[[:space:]]*:' "$path" || has_pattern '"swagger"[[:space:]]*:' "$path"; then
        type="openapi"; conf=0.94
      fi
      ;;
    sql)
      type="sql"; conf=0.65
      if has_pattern 'CREATE[[:space:]]+TABLE' "$path"; then conf=0.8; fi
      ;;
    mmd)
      type="mermaid"; conf=0.7
      ;;
    html)
      local is_html=0
      if [[ "$path_norm" == *"page_samples"* || "$path_norm" == *"page-samples"* ]]; then
        is_html=1
      elif echo "$lower" | grep -Eq '(abo|auth|prg|evt|ctn)[0-9]+\.html'; then
        is_html=1
      fi
      if [[ $is_html -eq 1 ]]; then
        type="page_sample_html"; conf=0.7
      fi
      ;;
    css)
      local is_css=0
      if [[ "$path_norm" == *"page_samples"* || "$path_norm" == *"samples"* ]]; then
        is_css=1
      elif [[ "$lower" == *style.css ]]; then
        is_css=1
      fi
      if [[ $is_css -eq 1 ]]; then
        type="page_sample_css"; conf=0.6
      fi
      ;;
  esac

  if [[ -n "$type" ]]; then
    printf '%s|%.2f\n' "$type" "$conf"
  fi
}

cmd_scan() {
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"

  local runtime_dir="$GC_DIR"
  local manifest_dir="${runtime_dir}/manifests"
  mkdir -p "$manifest_dir" "${PLAN_DIR}/tasks"
  local scan_stamp
  scan_stamp="$(date +%Y%m%d-%H%M%S)"
  local manifest="${manifest_dir}/discovery_${scan_stamp}.tsv"
  local manifest_tmp="${manifest}.tmp"
  local python_bin="${PYTHON_BIN:-python3}"
  local python_available=0
  if command -v "$python_bin" >/dev/null 2>&1; then
    python_available=1
  fi

  local -a scan_dirs=()
  if [[ -n "${GC_SCAN_ROOTS:-}" ]]; then
    local IFS=',:'
    read -ra scan_tokens <<<"${GC_SCAN_ROOTS}"
    for token in "${scan_tokens[@]}"; do
      token="${token//[[:space:]]/}"
      [[ -z "$token" ]] && continue
      if [[ -d "$PROJECT_ROOT/$token" ]]; then
        scan_dirs+=("$PROJECT_ROOT/$token")
      elif [[ -d "$token" ]]; then
        scan_dirs+=("$(abs_path "$token")")
      fi
    done
  fi
  if [[ ${#scan_dirs[@]} -eq 0 ]]; then
    local -a defaults=(apps docs db src packages qa tests ops)
    for candidate in "${defaults[@]}"; do
      if [[ -d "$PROJECT_ROOT/$candidate" ]]; then
        scan_dirs+=("$PROJECT_ROOT/$candidate")
      fi
    done
    if [[ ${#scan_dirs[@]} -eq 0 ]]; then
      scan_dirs=("$PROJECT_ROOT")
    fi
  fi

  local -a prune_dirs=(
    ".git"
    "node_modules"
    ".pnpm-store"
    "dist"
    "build"
    ".venv"
    ".gpt-creator"
    "tmp"
    "Library"
    "ansible"
    "docker.bak"
    "vendor"
    ".cache"
  )
  if [[ -n "${GC_SCAN_PRUNE_DIRS:-}" ]]; then
    local IFS=',:'
    read -ra prune_tokens <<<"${GC_SCAN_PRUNE_DIRS}"
    for token in "${prune_tokens[@]}"; do
      token="${token//[[:space:]]/}"
      [[ -n "$token" ]] && prune_dirs+=("$token")
    done
  fi

  info "Scanning project artifacts under: ${scan_dirs[*]}"
  printf "type\tconfidence\tpath\n" > "$manifest_tmp"

  local -a find_prune_expr=()
  if [[ ${#prune_dirs[@]} -gt 0 ]]; then
    find_prune_expr+=( "(" )
    for dir in "${prune_dirs[@]}"; do
      find_prune_expr+=( -name "$dir" -o )
    done
    unset 'find_prune_expr[${#find_prune_expr[@]}-1]'
    find_prune_expr+=( ")" -prune -o )
  fi

  local -a find_args=("${scan_dirs[@]}")
  find_args+=( "${find_prune_expr[@]}" -type f -print0 )

  while IFS= read -r -d '' f; do
    local hit
    hit="$(classify_file "$f")" || true
    if [[ -n "$hit" ]]; then
      local type conf
      IFS='|' read -r type conf <<<"$hit"
      printf "%s\t%.2f\t%s\n" "$type" "$conf" "$f" >> "$manifest_tmp"
    fi
  done < <(find "${find_args[@]}")

  mv "$manifest_tmp" "$manifest"
  if ! cp -f "$manifest" "${runtime_dir}/scan.tsv"; then
    warn "Unable to persist discovery manifest copy at ${runtime_dir}/scan.tsv."
  fi
  info "Discovery TSV → ${manifest}"

  local scan_json="${STAGING_DIR}/scan.json"
  if (( python_available )); then
    "$python_bin" - <<'PY' "$manifest" "$PROJECT_ROOT" "$scan_json"
import csv, json, sys, time, pathlib
manifest, root, out = sys.argv[1:4]
rows = []
with open(manifest, newline='') as fh:
    reader = csv.DictReader(fh, delimiter='\t')
    for row in reader:
        if not row['type']:
            continue
        rows.append({
            "type": row['type'],
            "confidence": float(row['confidence'] or 0),
            "path": str(pathlib.Path(row['path']).resolve())
        })
scan = {
    "project_root": str(pathlib.Path(root).resolve()),
    "generated_at": time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
    "artifacts": rows
}
pathlib.Path(out).parent.mkdir(parents=True, exist_ok=True)
with open(out, 'w') as fh:
    json.dump(scan, fh, indent=2)
print(out)
PY
  else
    warn "Skipping scan.json export; ${python_bin} not available."
  fi

  if (( python_available )); then
    local doc_registry_tool="${CLI_ROOT}/src/lib/doc_registry.py"
    if [[ -f "$doc_registry_tool" ]]; then
      if "$python_bin" "$doc_registry_tool" sync-scan \
        --project-root "$PROJECT_ROOT" \
        --runtime-dir "$runtime_dir" \
        --scan-tsv "$manifest"; then
        info "Documentation registry synced."
      else
        warn "Documentation registry sync failed; inspect ${runtime_dir}/logs."
      fi
    else
      warn "Skipping documentation registry sync; tool not found at ${doc_registry_tool}."
    fi

    local doc_catalog_tool="${CLI_ROOT}/src/lib/doc_catalog.py"
    local doc_catalog_json="${STAGING_DIR}/doc-catalog.json"
    local doc_catalog_library="${STAGING_DIR}/doc-library.md"
    local doc_catalog_index="${STAGING_DIR}/doc-index.md"
    if [[ -f "$doc_catalog_tool" ]]; then
      if "$python_bin" "$doc_catalog_tool" \
        --project-root "$PROJECT_ROOT" \
        --staging-dir "$STAGING_DIR" \
        --out-json "$doc_catalog_json" \
        --out-library "$doc_catalog_library" \
        --out-index "$doc_catalog_index"; then
        info "Documentation catalog rebuilt."
      else
        warn "Documentation catalog build failed."
      fi
    else
      warn "Skipping documentation catalog build; tool not found at ${doc_catalog_tool}."
    fi

    local doc_pipeline_tool="${CLI_ROOT}/src/lib/doc_pipeline.py"
    if [[ -f "$doc_pipeline_tool" ]]; then
      if "$python_bin" "$doc_pipeline_tool" \
        --project-root "$PROJECT_ROOT" \
        --runtime-dir "$runtime_dir"; then
        info "Documentation summaries refreshed."
      else
        warn "Documentation summaries refresh failed."
      fi
    else
      warn "Skipping documentation summaries refresh; tool not found at ${doc_pipeline_tool}."
    fi

    local doc_indexer_pkg_root="${CLI_ROOT}/src"
    local doc_indexer_ready=0
    if PYTHONPATH="${doc_indexer_pkg_root}${PYTHONPATH:+:$PYTHONPATH}" \
      "$python_bin" - <<'PY' "$doc_indexer_pkg_root"; then
import importlib
import sys
root = sys.argv[1]
sys.path.insert(0, root)
try:
    module = importlib.import_module("lib.doc_indexer")
    getattr(module, "DocIndexer")
except Exception:
    raise SystemExit(1)
else:
    raise SystemExit(0)
PY
      doc_indexer_ready=1
    fi
    if (( doc_indexer_ready )); then
      if PYTHONPATH="${doc_indexer_pkg_root}${PYTHONPATH:+:$PYTHONPATH}" \
        "$python_bin" -m lib.doc_indexer --runtime-dir "$runtime_dir"; then
        info "Documentation indexes rebuilt."
      else
        warn "Documentation indexing failed."
      fi
    else
      info "Skipping documentation indexing; doc_indexer module not importable."
    fi
  else
    warn "Skipping documentation registry and catalog refresh; ${python_bin} not available."
  fi

  if [[ -f "$scan_json" ]]; then
    ok "Scan manifest → ${scan_json}"
  else
    warn "Scan manifest export missing (${scan_json}); rerun scan after installing ${python_bin}."
  fi
}

cmd_normalize() {
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"

  local scan_json="${STAGING_DIR}/scan.json"
  if [[ ! -f "$scan_json" ]]; then
    warn "No scan.json found, running scan first."
    cmd_scan --project "$PROJECT_ROOT"
  fi

  scan_json="${STAGING_DIR}/scan.json"
  python3 - <<'PY' "$scan_json" "$INPUT_DIR" "$PLAN_DIR"
import json, sys, shutil, pathlib, time
scan_path, input_dir, plan_dir = sys.argv[1:4]
input_dir = pathlib.Path(input_dir)
plan_dir = pathlib.Path(plan_dir)
input_dir.mkdir(parents=True, exist_ok=True)
plan_dir.mkdir(parents=True, exist_ok=True)

data = json.load(open(scan_path))
project_root = pathlib.Path(data.get('project_root', '.')).resolve()
artifacts = data.get('artifacts', [])

unique_types = {"pdr", "sds", "rfp", "jira", "ui_pages", "openapi"}
unique = {}
for entry in artifacts:
    t = entry.get('type')
    if t in unique_types:
        if t not in unique or entry['confidence'] > unique[t]['confidence']:
            unique[t] = entry

multi_map = {
    "sql": pathlib.Path('sql'),
    "mermaid": pathlib.Path('mermaid'),
    "page_sample_html": pathlib.Path('page_samples'),
    "page_sample_css": pathlib.Path('page_samples')
}
collected = {key: [] for key in multi_map}
for entry in artifacts:
    t = entry.get('type')
    if t in multi_map:
        collected[t].append(entry)

provenance = []

def copy_file(src_path, rel_dest, entry):
    src = pathlib.Path(src_path)
    dest = input_dir / rel_dest
    dest.parent.mkdir(parents=True, exist_ok=True)
    shutil.copy2(src, dest)
    provenance.append({
        "type": entry.get('type'),
        "source": str(src),
        "destination": str(dest.relative_to(input_dir)),
        "confidence": entry.get('confidence', 0)
    })

name_map = {
    "pdr": pathlib.Path('pdr.md'),
    "sds": pathlib.Path('sds.md'),
    "rfp": pathlib.Path('rfp.md'),
    "jira": pathlib.Path('jira.md'),
    "ui_pages": pathlib.Path('ui-pages.md')
}

for t, entry in unique.items():
    src = entry['path']
    if t == 'openapi':
        suffix = pathlib.Path(src).suffix.lower()
        if suffix in {'.yaml', '.yml'}:
            rel = pathlib.Path('openapi.yaml')
        elif suffix == '.json':
            rel = pathlib.Path('openapi.json')
        else:
            rel = pathlib.Path('openapi.src')
    else:
        rel = name_map[t]
    copy_file(src, rel, entry)

for t, entries in collected.items():
    dest_root = multi_map[t]
    for entry in entries:
        src = pathlib.Path(entry['path'])
        try:
            rel = src.resolve().relative_to(project_root)
        except ValueError:
            rel = pathlib.Path(src.name)
        rel = dest_root / rel
        copy_file(src, rel, entry)

# discovery.yaml (summary)
import io
from textwrap import indent
summary = io.StringIO()
summary.write('generated_at: %s\n' % time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()))
summary.write('project_root: %s\n' % project_root)
summary.write('artifacts:\n')
for entry in artifacts:
    summary.write('  - type: %s\n' % entry.get('type'))
    summary.write('    confidence: %.2f\n' % entry.get('confidence', 0))
    summary.write('    path: %s\n' % entry.get('path'))
(input_dir / '..' / 'discovery.yaml').resolve().write_text(summary.getvalue())

focus_targets = []
core_types = {"pdr", "sds", "rfp", "jira", "openapi", "ui_pages", "sql"}
seen_focus = set()
for entry in provenance:
    dest = entry.get('destination')
    if not dest:
        continue
    t = entry.get('type')
    dest_rel = str(pathlib.Path(dest).as_posix())
    if t in core_types and dest_rel not in seen_focus:
        focus_targets.append(dest_rel)
        seen_focus.add(dest_rel)
if not focus_targets:
    for entry in provenance:
        dest = entry.get('destination')
        if dest:
            dest_rel = str(pathlib.Path(dest).as_posix())
            if dest_rel not in seen_focus:
                focus_targets.append(dest_rel)
                seen_focus.add(dest_rel)

prov = {
    'generated_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
    'entries': provenance,
    'focus': focus_targets,
}
(plan_dir / 'provenance.json').write_text(json.dumps(prov, indent=2))
PY

  ok "Normalized inputs → ${INPUT_DIR}"
}

cmd_plan() {
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"

  local openapi=""
  for cand in "$INPUT_DIR/openapi.yaml" "$INPUT_DIR/openapi.yml" "$INPUT_DIR/openapi.json" "$INPUT_DIR/openapi.src"; do
    [[ -f "$cand" ]] && { openapi="$cand"; break; }
  done
  local sql_dir="$INPUT_DIR/sql"

  python3 - <<'PY' "$openapi" "$sql_dir" "$PLAN_DIR"
import json, os, re, sys, time, pathlib
from collections import OrderedDict
openapi_path, sql_dir, plan_dir = sys.argv[1:4]
plan_dir = pathlib.Path(plan_dir)
plan_dir.mkdir(parents=True, exist_ok=True)

routes = []
schemas = []
openapi_loaded = False
if openapi_path:
    try:
        text = pathlib.Path(openapi_path).read_text()
        if openapi_path.endswith('.json'):
            data = json.loads(text)
            openapi_loaded = True
        else:
            try:
                import yaml  # type: ignore
                data = yaml.safe_load(text)  # type: ignore
                openapi_loaded = True
            except Exception:
                data = None
        if openapi_loaded and isinstance(data, dict):
            for path, methods in (data.get('paths') or {}).items():
                if isinstance(methods, dict):
                    for method, body in methods.items():
                        if not isinstance(body, dict):
                            continue
                        routes.append({
                            'method': method.upper(),
                            'path': path,
                            'summary': body.get('summary') or ''
                        })
            schemas = list((data.get('components') or {}).get('schemas') or {})
        else:
            raise ValueError('fallback parser')
    except Exception:
        routes = []
        schemas = []
        text = pathlib.Path(openapi_path).read_text() if openapi_path else ''
        current_path = None
        for line in text.splitlines():
            if re.match(r'^\s*/[^\s]+:\s*$', line):
                current_path = line.strip().rstrip(':')
                continue
            if current_path:
                m = re.match(r'^\s{2,}(get|post|put|patch|delete|options|head):\s*$', line, re.I)
                if m:
                    routes.append({'method': m.group(1).upper(), 'path': current_path, 'summary': ''})
                    continue
                if re.match(r'^\S', line):
                    current_path = None

        in_components = False
        in_schemas = False
        for line in text.splitlines():
            stripped = line.strip()
            if not stripped:
                continue
            if re.match(r'^components:\s*$', stripped):
                in_components = True
                in_schemas = False
                continue
            if in_components and re.match(r'^schemas:\s*$', stripped):
                in_schemas = True
                continue
            indent = len(line) - len(line.lstrip(' '))
            if in_schemas:
                if indent <= 2 and not stripped.startswith('#') and not stripped.startswith('schemas:'):
                    in_schemas = False
                    continue
                if indent == 4 and re.match(r'^[A-Za-z0-9_.-]+:\s*$', stripped):
                    name = stripped.split(':', 1)[0]
                    schemas.append(name)

sql_tables = []
sql_dir_path = pathlib.Path(sql_dir)
if sql_dir and sql_dir_path.is_dir():
    for sql_file in sql_dir_path.rglob('*.sql'):
        try:
            text = sql_file.read_text()
        except Exception:
            continue
        for m in re.finditer(r'CREATE\s+TABLE\s+`?([A-Za-z0-9_]+)`?', text, flags=re.IGNORECASE):
            sql_tables.append(m.group(1))

schema_set = {s.lower() for s in schemas}
table_set = {t.lower() for t in sql_tables}
only_in_openapi = sorted(schema_set - table_set)
only_in_sql = sorted(table_set - schema_set)

routes_path = plan_dir / 'routes.md'
entities_path = plan_dir / 'entities.md'
tasks_path = plan_dir / 'tasks.json'
plan_todo = plan_dir / 'PLAN_TODO.md'

def write_routes():
    lines = ['# Routes', '']
    if routes:
        for item in sorted(routes, key=lambda r: (r['path'], r['method'])):
            summary = f" — {item['summary']}" if item.get('summary') else ''
            lines.append(f"- `{item['method']} {item['path']}`{summary}")
    else:
        lines.append('No routes detected — ensure openapi.yaml is present in staging/inputs.')
    routes_path.write_text('\n'.join(lines) + '\n')


def write_entities():
    lines = ['# Entities', '']
    lines.append('## OpenAPI Schemas')
    if schemas:
        for name in sorted(schemas):
            lines.append(f'- {name}')
    else:
        lines.append('- (none found)')
    lines.append('')
    lines.append('## SQL Tables')
    if sql_tables:
        for name in sorted(sql_tables):
            lines.append(f'- {name}')
    else:
        lines.append('- (none found)')
    lines.append('')
    lines.append('## Detected deltas')
    if only_in_openapi:
        lines.append('- Only in OpenAPI: ' + ', '.join(only_in_openapi))
    if only_in_sql:
        lines.append('- Only in SQL: ' + ', '.join(only_in_sql))
    if not only_in_openapi and not only_in_sql:
        lines.append('- None (schemas and tables aligned on name)')
    entities_path.write_text('\n'.join(lines) + '\n')


def write_tasks():
    tasks = []
    if only_in_openapi:
        tasks.append({
            'id': 'align-openapi-sql',
            'title': 'Align OpenAPI schemas with SQL tables',
            'details': f"Create tables or update schemas for: {', '.join(only_in_openapi)}"
        })
    if only_in_sql:
        tasks.append({
            'id': 'document-sql-gap',
            'title': 'Document SQL tables missing from OpenAPI',
            'details': f"Expose or document SQL tables not covered by API: {', '.join(only_in_sql)}"
        })
    tasks_path.write_text(json.dumps({'generated_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()), 'tasks': tasks}, indent=2))


def write_plan_todo():
    lines = [
        '# Build Plan',
        '',
        '- Validate discovery outputs under `staging/inputs`.',
        '- Review `routes.md` & `entities.md` for coverage and deltas.',
        '- Implement generation steps for API, DB, Web, Admin, Docker.',
        '- Run `gpt-creator generate all --project <path>` if not already executed.',
        '- Bring the stack up with `gpt-creator run up` and smoke test.',
        '- Execute `gpt-creator verify all` to satisfy acceptance & NFR gates.',
        '- Iterate on Jira tasks using `gpt-creator iterate` until checks pass.'
    ]
    plan_todo.write_text('\n'.join(lines) + '\n')

write_routes()
write_entities()
write_tasks()
write_plan_todo()
PY

  ok "Plan artifacts created under ${PLAN_DIR}"
}

copy_template_tree() {
  local src="$1" dest="$2"
  [[ -d "$src" ]] || die "Template directory not found: $src"
  find "$src" -type d ! -name '.DS_Store' | while IFS= read -r dir; do
    local rel="${dir#$src}"
    mkdir -p "$dest/$rel"
  done
  find "$src" -type f | while IFS= read -r file; do
    local base
    base="$(basename "$file")"
    [[ "$base" == '.DS_Store' ]] && continue
    local rel="${file#$src/}"
    local target="$dest/$rel"
    if [[ "$target" == *.tmpl ]]; then
      target="${target%.tmpl}"
      mkdir -p "$(dirname "$target")"
      render_template_file "$file" "$target"
    else
      mkdir -p "$(dirname "$target")"
      cp "$file" "$target"
    fi
  done
}

cmd_generate() {
  local facet="${1:-}"; shift || true
  [[ -n "$facet" ]] || die "generate requires a facet: api|web|admin|db|docker|all"
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"
  local templates="$CLI_ROOT/templates"

  case "$facet" in
    api)
      local out="$PROJECT_ROOT/apps/api"
      mkdir -p "$out"
      copy_template_tree "$templates/api/nestjs" "$out"
      ok "API scaffolded → ${out}"
      ;;
    web)
      local out="$PROJECT_ROOT/apps/web"
      mkdir -p "$out"
      copy_template_tree "$templates/web/vue3" "$out"
      ok "Web scaffolded → ${out}"
      ;;
    admin)
      local out="$PROJECT_ROOT/apps/admin"
      mkdir -p "$out"
      copy_template_tree "$templates/admin/vue3" "$out"
      ok "Admin scaffolded → ${out}"
      ;;
    db)
      local out="$PROJECT_ROOT/db"
      mkdir -p "$out"
      copy_template_tree "$templates/db/mysql" "$out"
      ok "DB artifacts scaffolded → ${out}"
      ;;
    docker)
      local out="$PROJECT_ROOT/docker"
      mkdir -p "$out"
      local preferred="${GC_DB_HOST_PORT:-${DB_HOST_PORT:-${MYSQL_HOST_PORT:-3306}}}"
      gc_unreserve_port db
      if port_in_use "$preferred"; then
        local next; next="$(find_free_port "$preferred")"
        if [[ "$next" != "$preferred" ]]; then
          info "Port $preferred in use; remapping MySQL to $next"
          preferred="$next"
        fi
      fi
      GC_DB_HOST_PORT="$preferred"
      DB_HOST_PORT="$GC_DB_HOST_PORT"
      MYSQL_HOST_PORT="$GC_DB_HOST_PORT"
      gc_reserve_port db "$GC_DB_HOST_PORT"
      gc_set_env_var DB_HOST_PORT "$GC_DB_HOST_PORT"
      gc_set_env_var MYSQL_HOST_PORT "$GC_DB_HOST_PORT"
      gc_set_env_var GC_DB_HOST_PORT "$GC_DB_HOST_PORT"
      local api_host_port
      api_host_port="$(gc_pick_port "API" 3000 GC_API_HOST_PORT API_HOST_PORT)"
      GC_API_HOST_PORT="$api_host_port"
      API_HOST_PORT="$GC_API_HOST_PORT"
      gc_set_env_var API_HOST_PORT "$API_HOST_PORT"
      gc_set_env_var GC_API_HOST_PORT "$GC_API_HOST_PORT"
      gc_reserve_port api "$GC_API_HOST_PORT"
      local web_host_port
      web_host_port="$(gc_pick_port "Web" 5173 GC_WEB_HOST_PORT WEB_HOST_PORT)"
      GC_WEB_HOST_PORT="$web_host_port"
      WEB_HOST_PORT="$GC_WEB_HOST_PORT"
      gc_set_env_var WEB_HOST_PORT "$WEB_HOST_PORT"
      gc_set_env_var GC_WEB_HOST_PORT "$GC_WEB_HOST_PORT"
      gc_reserve_port web "$GC_WEB_HOST_PORT"
      local admin_host_port
      admin_host_port="$(gc_pick_port "Admin" 5174 GC_ADMIN_HOST_PORT ADMIN_HOST_PORT)"
      GC_ADMIN_HOST_PORT="$admin_host_port"
      ADMIN_HOST_PORT="$GC_ADMIN_HOST_PORT"
      gc_set_env_var ADMIN_HOST_PORT "$ADMIN_HOST_PORT"
      gc_set_env_var GC_ADMIN_HOST_PORT "$GC_ADMIN_HOST_PORT"
      gc_reserve_port admin "$GC_ADMIN_HOST_PORT"
      local proxy_host_port
      proxy_host_port="$(gc_pick_port "Proxy" 8080 GC_PROXY_HOST_PORT PROXY_HOST_PORT)"
      GC_PROXY_HOST_PORT="$proxy_host_port"
      PROXY_HOST_PORT="$GC_PROXY_HOST_PORT"
      gc_set_env_var PROXY_HOST_PORT "$PROXY_HOST_PORT"
      gc_set_env_var GC_PROXY_HOST_PORT "$GC_PROXY_HOST_PORT"
      gc_reserve_port proxy "$GC_PROXY_HOST_PORT"
      local local_url="mysql://${GC_DB_USER}:${GC_DB_PASSWORD}@127.0.0.1:${GC_DB_HOST_PORT}/${GC_DB_NAME}"
      gc_set_env_var DATABASE_URL "$local_url"
      local api_base_url="http://localhost:${GC_API_HOST_PORT}/api/v1"
      gc_set_env_var GC_API_BASE_URL "$api_base_url"
      gc_set_env_var VITE_API_BASE "$api_base_url"
      local api_health_url="${api_base_url%/}/health"
      gc_set_env_var GC_API_HEALTH_URL "$api_health_url"
      local proxy_base="http://localhost:${GC_PROXY_HOST_PORT}"
      gc_set_env_var GC_WEB_URL "${proxy_base}/"
      gc_set_env_var GC_ADMIN_URL "${proxy_base}/admin/"
      gc_load_env
      copy_template_tree "$templates/docker" "$out"
      if [[ -f "$out/pnpm-entry.sh" ]]; then
        chmod +x "$out/pnpm-entry.sh" || true
      fi
      ok "Docker assets scaffolded → ${out}"
      ;;
    all)
      for f in api db web admin docker; do
        cmd_generate "$f" --project "$PROJECT_ROOT"
      done
      return 0
      ;;
    *) die "Unknown facet: ${facet}";;
  esac
}

cmd_db() {
  local action="${1:-}"; shift || true
  [[ -n "$action" ]] || die "db requires: provision|import|seed"
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"
  local compose_file="$PROJECT_ROOT/docker/docker-compose.yml"

  case "$action" in
    provision)
      [[ -f "$compose_file" ]] || die "Compose file not found at ${compose_file}; run 'gpt-creator generate docker'."
      info "Starting database service via docker compose"
      docker_compose -f "$compose_file" up -d db
      ok "MySQL container provisioned"
      ;;
    import)
      local sql_file
      sql_file="$(find "$INPUT_DIR/sql" -maxdepth 2 -type f -name '*.sql' | head -n1 || true)"
      [[ -n "$sql_file" ]] || die "No staged SQL found under ${INPUT_DIR}/sql"
      info "Importing SQL from ${sql_file}"
      local database="${DB_NAME:-$GC_DB_NAME}"
      local root_user="${DB_ROOT_USER:-root}"
      local root_pass="${DB_ROOT_PASSWORD:-${GC_DB_ROOT_PASSWORD:-}}"
      local app_user="${DB_USER:-$GC_DB_USER}"
      local app_pass="${DB_PASSWORD:-$GC_DB_PASSWORD}"
      local cleanup_files=()
      trap 'for f in "${cleanup_files[@]}"; do [[ -n "$f" && -f "$f" ]] && rm -f "$f"; done; trap - RETURN' RETURN
      local rendered_sql
      rendered_sql="$(gc_temp_file "$STAGING_DIR" "import-" ".sql")"
      cleanup_files+=("$rendered_sql")
      gc_render_sql "$sql_file" "$rendered_sql" "$database" "$app_user" "$app_pass"
      local init_sql="${INPUT_DIR}/sql/db/init.sql"
      if gc_execute_sql "$compose_file" "$rendered_sql" "$database" "$root_user" "$root_pass" "$app_user" "$app_pass" "$init_sql" "import"; then
        ok "Database import finished"
      else
        die "Database import failed"
      fi
      ;;
    seed)
      local seed_file="${PROJECT_ROOT}/db/seed.sql"
      [[ -f "$seed_file" ]] || die "Seed file not found: ${seed_file}"
      info "Seeding database from ${seed_file}"
      local database="${DB_NAME:-$GC_DB_NAME}"
      local root_user="${DB_ROOT_USER:-root}"
      local root_pass="${DB_ROOT_PASSWORD:-${GC_DB_ROOT_PASSWORD:-}}"
      local app_user="${DB_USER:-$GC_DB_USER}"
      local app_pass="${DB_PASSWORD:-$GC_DB_PASSWORD}"
      local cleanup_files=()
      trap 'for f in "${cleanup_files[@]}"; do [[ -n "$f" && -f "$f" ]] && rm -f "$f"; done; trap - RETURN' RETURN
      local rendered_seed
      rendered_seed="$(gc_temp_file "$STAGING_DIR" "seed-" ".sql")"
      cleanup_files+=("$rendered_seed")
      gc_render_sql "$seed_file" "$rendered_seed" "$database" "$app_user" "$app_pass"
      local fallback_init="${PROJECT_ROOT}/db/init.sql"
      if [[ ! -f "$fallback_init" ]]; then
        fallback_init="${INPUT_DIR}/sql/db/init.sql"
      fi
      if gc_execute_sql "$compose_file" "$rendered_seed" "$database" "$root_user" "$root_pass" "$app_user" "$app_pass" "$fallback_init" "seed"; then
        ok "Database seed applied"
      else
        die "Database seed failed"
      fi
      ;;
    *) die "Unknown db action: ${action}";;
  esac
}

cmd_run() {
  local action="${1:-}"; shift || true
  [[ -n "$action" ]] || die "run requires: up|down|logs|open"
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"
  local compose_file="$PROJECT_ROOT/docker/docker-compose.yml"

  case "$action" in
    up)
      [[ -f "$compose_file" ]] || die "Compose file not found at ${compose_file}; generate docker assets first."
      gc_refresh_stack_prepare_node_modules
      docker_compose -f "$compose_file" up -d
      ok "Stack is starting (check docker compose ps)"
      local api_base="${GC_API_BASE_URL:-http://localhost:3000/api/v1}"
      local web_url="${GC_WEB_URL:-http://localhost:8080/}"
      local admin_url="${GC_ADMIN_URL:-http://localhost:8080/admin/}"
      local health_timeout="${GC_DOCKER_HEALTH_TIMEOUT:-10}"
      local health_interval="${GC_DOCKER_HEALTH_INTERVAL:-1}"
      wait_for_endpoint "${api_base%/}/health" "API /health" "$health_timeout" "$health_interval" || true
      local web_ping="${web_url%/}/__vite_ping"
      if ! wait_for_endpoint "$web_ping" "Web (vite ping)" "$health_timeout" "$health_interval"; then
        wait_for_endpoint "${web_url%/}/" "Web" "$health_timeout" "$health_interval" || true
      fi
      local admin_ping="${admin_url%/}/__vite_ping"
      if ! wait_for_endpoint "$admin_ping" "Admin (vite ping)" "$health_timeout" "$health_interval"; then
        wait_for_endpoint "${admin_url%/}/" "Admin" "$health_timeout" "$health_interval" || true
      fi
      ;;
    down)
      [[ -f "$compose_file" ]] || die "Compose file not found at ${compose_file}"
      docker_compose -f "$compose_file" down
      ok "Stack shut down"
      ;;
    logs)
      [[ -f "$compose_file" ]] || die "Compose file not found at ${compose_file}"
      docker_compose -f "$compose_file" logs -f
      ;;
    open)
      if command -v open >/dev/null 2>&1; then
        open "http://localhost:8080" || open "http://localhost:5173" || true
      else
        ${EDITOR_CMD} "$PROJECT_ROOT" || true
      fi
      ;;
    *) die "Unknown run action: ${action}";;
  esac
}

cmd_refresh_stack() {
  local root="" compose_override="" sql_override="" seed_override=""
  local skip_import=0 skip_seed=0
  local only_services="" skip_services=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --compose) compose_override="$(abs_path "$2")"; shift 2;;
      --sql) sql_override="$(abs_path "$2")"; shift 2;;
      --seed) seed_override="$(abs_path "$2")"; shift 2;;
      --no-import) skip_import=1; shift;;
      --no-seed) skip_seed=1; shift;;
      --only-services) only_services="${2:-}"; shift 2;;
      --skip-services) skip_services="${2:-}"; shift 2;;
      -h|--help)
        cat <<'EOHELP'
Usage: gpt-creator refresh-stack [options]

Tear down, rebuild, and prime the local Docker stack (schema + seeds).

Options:
  --project PATH   Project root (defaults to current directory)
  --compose FILE   Override docker-compose file
  --sql FILE       Explicit SQL dump to import (auto-discovered if omitted)
  --seed FILE      Seed SQL file to apply after import
  --only-services LIST  Comma/space separated subset of services to start (e.g. "web,api")
  --skip-services LIST  Comma/space separated services to skip when starting (e.g. "db,admin")
  --no-import      Skip schema import step
  --no-seed        Skip seeding step
  -h, --help       Show this help
EOHELP
        return 0
        ;;
      *) break;;
    esac
  done

  ensure_ctx "$root"

  local -a refresh_sql_init_files=() refresh_sql_schema_files=() refresh_sql_seed_files=() refresh_sql_all_files=()
  local refresh_sql_default_db_name="" refresh_sql_default_db_user="" refresh_sql_default_db_password="" refresh_sql_default_user_host=""
  eval "$(gc_refresh_stack_collect_sql "$PROJECT_ROOT")"

  if [[ -n "$sql_override" ]]; then
    refresh_sql_schema_files=("$sql_override")
  fi
  if [[ -n "$seed_override" ]]; then
    refresh_sql_seed_files=("$seed_override")
  fi

  local env_updated=0
  if [[ -n "$refresh_sql_default_db_name" && "$refresh_sql_default_db_name" != "$GC_DB_NAME" ]]; then
    gc_set_env_var DB_NAME "$refresh_sql_default_db_name"
    gc_set_env_var GC_DB_NAME "$refresh_sql_default_db_name"
    env_updated=1
  fi
  if [[ -n "$refresh_sql_default_db_user" && "$refresh_sql_default_db_user" != "$GC_DB_USER" ]]; then
    gc_set_env_var DB_USER "$refresh_sql_default_db_user"
    gc_set_env_var GC_DB_USER "$refresh_sql_default_db_user"
    env_updated=1
  fi
  if [[ -n "$refresh_sql_default_db_password" && "$refresh_sql_default_db_password" != "$GC_DB_PASSWORD" ]]; then
    gc_set_env_var DB_PASSWORD "$refresh_sql_default_db_password"
    gc_set_env_var GC_DB_PASSWORD "$refresh_sql_default_db_password"
    env_updated=1
  fi
  if (( env_updated )); then
    gc_load_env
    local host_port="${GC_DB_HOST_PORT:-${DB_HOST_PORT:-3306}}"
    local database_url="mysql://${GC_DB_USER}:${GC_DB_PASSWORD}@127.0.0.1:${host_port}/${GC_DB_NAME}"
    gc_set_env_var DATABASE_URL "$database_url"
  fi

  info "Using database '${GC_DB_NAME}' with user '${GC_DB_USER}'"

  local compose_file="$compose_override"
  if [[ -n "$compose_file" ]]; then
    compose_file="$(abs_path "$compose_file")"
  else
    info "Rendering docker assets from templates"
    if ! cmd_generate docker --project "$PROJECT_ROOT"; then
      die "Failed to generate docker assets"
    fi
    if [[ -f "${PROJECT_ROOT}/docker/compose.yaml" ]]; then
      compose_file="${PROJECT_ROOT}/docker/compose.yaml"
    elif [[ -f "${PROJECT_ROOT}/docker/docker-compose.yml" ]]; then
      compose_file="${PROJECT_ROOT}/docker/docker-compose.yml"
    elif [[ -f "${PROJECT_ROOT}/docker-compose.yml" ]]; then
      compose_file="${PROJECT_ROOT}/docker-compose.yml"
    else
      die "Compose file not found after generation. Expected docker/compose.yaml or docker-compose.yml"
    fi
  fi

  info "Refreshing Docker stack for ${GC_DOCKER_PROJECT_NAME}"

  info "Stopping existing containers (removing volumes)"
  docker_compose -f "$compose_file" down -v --remove-orphans || true

  local slug="$GC_DOCKER_PROJECT_NAME"
  local -a stale_containers=(
    "${slug}-db"
    "${slug}-api"
    "${slug}-web"
    "${slug}-admin"
    "${slug}-proxy"
    "${slug}_db"
    "${slug}_api"
    "${slug}_web"
    "${slug}_admin"
    "${slug}_proxy"
  )
  local container
  for container in "${stale_containers[@]}"; do
    if docker ps -a --format '{{.Names}}' | grep -Fxq "$container"; then
      info "Removing leftover container ${container}"
      docker rm -f "$container" >/dev/null 2>&1 || true
    fi
  done

  if (( ${#refresh_sql_all_files[@]} > 0 )); then
    info "Discovered SQL assets:"
    local listed
    for listed in "${refresh_sql_all_files[@]}"; do
      if [[ "$listed" == "$PROJECT_ROOT/"* ]]; then
        info "  - ${listed#$PROJECT_ROOT/}"
      else
        info "  - ${listed}"
      fi
    done
  else
    info "No SQL assets discovered automatically."
  fi

  local -a all_services=(db api web admin proxy)
  local -a services_to_start=()
  if [[ -n "$only_services" ]]; then
    local normalized_only="${only_services//,/ }"
    read -r -a services_to_start <<< "$normalized_only"
  else
    services_to_start=("${all_services[@]}")
  fi
  if [[ -n "$skip_services" ]]; then
    local -a skip_list=()
    local normalized_skip="${skip_services//,/ }"
    read -r -a skip_list <<< "$normalized_skip"
    if (( ${#skip_list[@]} > 0 )); then
      local -a filtered=()
      local svc skip_flag skip_item
      for svc in "${services_to_start[@]}"; do
        skip_flag=0
        for skip_item in "${skip_list[@]}"; do
          [[ -z "$skip_item" ]] && continue
          if [[ "$svc" == "$skip_item" ]]; then
            skip_flag=1
            break
          fi
        done
        if (( skip_flag == 0 )); then
          filtered+=("$svc")
        fi
      done
      services_to_start=("${filtered[@]}")
    fi
  fi
  if (( ${#services_to_start[@]} > 0 )); then
    # Deduplicate and drop empties
    local -a deduped=()
    local svc seen_services=""
    for svc in "${services_to_start[@]}"; do
      [[ -z "$svc" ]] && continue
      case " $seen_services " in
        *" $svc "*) continue ;;
      esac
      deduped+=("$svc")
      seen_services+=" $svc"
    done
    services_to_start=("${deduped[@]}")
  fi

  if (( ${#services_to_start[@]} == 0 )); then
    warn "No services selected to start; skipping docker compose up."
  else
    info "Building and starting containers (${services_to_start[*]})"
    GC_DOCKER_VERBOSE="${GC_DOCKER_VERBOSE:-1}"
    gc_refresh_stack_prepare_node_modules
    docker_compose -f "$compose_file" up -d --build "${services_to_start[@]}"
    gc_start_created_containers "$compose_file" "${services_to_start[@]}"
  fi

  local db_requested=0
  local svc
  for svc in "${services_to_start[@]}"; do
    if [[ "$svc" == "db" ]]; then
      db_requested=1
      break
    fi
  done

  local db_container=""
  if (( db_requested )); then
    db_container="$(docker_compose -f "$compose_file" ps -q db || true)"
    if [[ -n "$db_container" ]]; then
      info "Waiting for MySQL to be ready…"
      local waited=0
      local mysql_timeout="${GC_DOCKER_HEALTH_TIMEOUT:-10}"
      local sleep_interval="${GC_DOCKER_HEALTH_INTERVAL:-1}"
      (( sleep_interval <= 0 )) && sleep_interval=1
      while (( waited < mysql_timeout )); do
        if docker exec -i "$db_container" sh -lc 'mysqladmin ping -h 127.0.0.1 --silent' >/dev/null 2>&1; then
          info "MySQL is ready."
          break
        fi
        sleep "$sleep_interval"
        ((waited += sleep_interval)) || true
      done
      if (( waited >= mysql_timeout )); then
        warn "MySQL readiness timeout after ${mysql_timeout}s (continuing)."
      fi
    else
      warn "Database container did not start; SQL import will be skipped."
    fi
  else
    info "Database service excluded from start; skipping readiness wait."
  fi

  docker_compose -f "$compose_file" ps

  local db_port="3306"
  local root_user="${DB_ROOT_USER:-root}"
  local root_pass="${DB_ROOT_PASSWORD:-${GC_DB_ROOT_PASSWORD:-}}"
  local app_user="${DB_USER:-$GC_DB_USER}"
  local app_pass="${DB_PASSWORD:-$GC_DB_PASSWORD}"
  local db_name="${DB_NAME:-$GC_DB_NAME}"
  local app_host="${refresh_sql_default_user_host:-%}"

  local import_rc=0 seed_rc=0
  local schema_attempted=0 seed_attempted=0

  if [[ -z "$db_container" ]]; then
    if (( skip_import == 0 )); then
      import_rc=1
    fi
    if (( skip_seed == 0 )); then
      seed_rc=1
    fi
  else
    if (( skip_import == 0 || skip_seed == 0 )); then
      local ensure_sql
      ensure_sql="$(python3 - <<'PY' "$db_name" "$app_user" "$app_pass" "$app_host"
import sys

db, user, password, host = sys.argv[1:5]
if not host:
    host = '%'
if not db:
    db = 'app'
if not user:
    user = 'app'

def quote_identifier(name: str) -> str:
    return '`' + name.replace('`', '``') + '`'

def quote_string(value: str) -> str:
    return "'" + value.replace("'", "''") + "'"

statements = [
    f"CREATE DATABASE IF NOT EXISTS {quote_identifier(db)} CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;",
    f"CREATE USER IF NOT EXISTS {quote_string(user)}@{quote_string(host)} IDENTIFIED BY {quote_string(password)};",
    f"GRANT ALL PRIVILEGES ON {quote_identifier(db)}.* TO {quote_string(user)}@{quote_string(host)};",
    "FLUSH PRIVILEGES;",
]
print("\n".join(statements))
PY
)"
      if ! gc_refresh_stack_exec_inline_sql "$db_container" "$root_user" "$root_pass" "" "$db_port" <<<"$ensure_sql"; then
        warn "Failed to ensure database or user; continuing with imports."
      else
        info "Ensured database ${db_name} and user ${app_user}"
      fi
    fi

    if (( skip_import == 0 )) && (( ${#refresh_sql_init_files[@]} + ${#refresh_sql_schema_files[@]} == 0 )); then
      info "No schema SQL files found; skipping import."
      skip_import=1
    fi
    if (( skip_seed == 0 )) && (( ${#refresh_sql_seed_files[@]} == 0 )); then
      info "No seed SQL files found; skipping seeding."
      skip_seed=1
    fi

    if (( skip_import == 0 )); then
      local file display
      for file in "${refresh_sql_init_files[@]}"; do
        [[ -f "$file" ]] || { warn "Init SQL not found: $file"; import_rc=1; continue; }
        display="$file"
        [[ "$display" == "$PROJECT_ROOT/"* ]] && display="${display#$PROJECT_ROOT/}"
        info "Applying init SQL: ${display}"
        ((schema_attempted++))
        if ! gc_refresh_stack_exec_mysql "$db_container" "$file" "$root_user" "$root_pass" "" "$db_port"; then
          warn "Init SQL failed as ${root_user}; retrying as ${app_user}"
          if ! gc_refresh_stack_exec_mysql "$db_container" "$file" "$app_user" "$app_pass" "" "$db_port"; then
            warn "Init SQL failed: ${display}"
            import_rc=1
            continue
          fi
        fi
      done

      for file in "${refresh_sql_schema_files[@]}"; do
        [[ -f "$file" ]] || { warn "Schema SQL not found: $file"; import_rc=1; continue; }
        display="$file"
        [[ "$display" == "$PROJECT_ROOT/"* ]] && display="${display#$PROJECT_ROOT/}"
        info "Importing schema SQL: ${display}"
        ((schema_attempted++))
        if ! gc_refresh_stack_exec_mysql "$db_container" "$file" "$root_user" "$root_pass" "$db_name" "$db_port"; then
          warn "Schema import failed as ${root_user}; retrying as ${app_user}"
          if ! gc_refresh_stack_exec_mysql "$db_container" "$file" "$app_user" "$app_pass" "$db_name" "$db_port"; then
            warn "Schema SQL failed: ${display}"
            import_rc=1
            continue
          fi
        fi
      done
    else
      info "Skipping schema import (--no-import)"
    fi

    if (( skip_seed == 0 )); then
      local seed_file seed_display
      for seed_file in "${refresh_sql_seed_files[@]}"; do
        [[ -f "$seed_file" ]] || { warn "Seed SQL not found: $seed_file"; seed_rc=1; continue; }
        seed_display="$seed_file"
        [[ "$seed_display" == "$PROJECT_ROOT/"* ]] && seed_display="${seed_display#$PROJECT_ROOT/}"
        info "Applying seed SQL: ${seed_display}"
        ((seed_attempted++))
        if ! gc_refresh_stack_exec_mysql "$db_container" "$seed_file" "$root_user" "$root_pass" "$db_name" "$db_port"; then
          warn "Seed import failed as ${root_user}; retrying as ${app_user}"
          if ! gc_refresh_stack_exec_mysql "$db_container" "$seed_file" "$app_user" "$app_pass" "$db_name" "$db_port"; then
            warn "Seed SQL failed: ${seed_display}"
            seed_rc=1
            continue
          fi
        fi
      done
    else
      info "Skipping seeding (--no-seed)"
    fi
  fi

  info "Verifying Docker service health"
  local stack_health_rc=0
  if gc_refresh_stack_wait_for_containers "$compose_file" "${GC_DOCKER_HEALTH_TIMEOUT:-10}" "${GC_DOCKER_HEALTH_INTERVAL:-1}"; then
    ok "Docker services healthy"
  else
    stack_health_rc=1
    warn "Docker services reported issues; inspect compose logs for details."
  fi

  local status=0
  if (( import_rc != 0 )); then
    status=1
  elif (( skip_import == 0 && schema_attempted > 0 )); then
    ok "Database schema imported"
  fi
  if (( seed_rc != 0 )); then
    status=1
  elif (( skip_seed == 0 && seed_attempted > 0 )); then
    ok "Database seeds applied"
  fi
  if (( stack_health_rc != 0 )); then
    status=1
  fi

  if (( status == 0 )); then
    ok "Stack refreshed successfully"
  else
    warn "Stack refresh completed with issues; inspect logs above."
  fi
  return $status
}


cmd_verify() {
  local kind="${1:-all}"; shift || true
  local root=""
  local api_base="${GC_API_BASE_URL:-http://localhost:3000/api/v1}"
  local api_health="${GC_API_HEALTH_URL:-}"
  local web_url="${GC_WEB_URL:-http://localhost:8080/}"
  local admin_url="${GC_ADMIN_URL:-http://localhost:8080/admin/}"
  local api_base_override=0 api_health_override=0 web_url_override=0 admin_url_override=0

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --api-url) api_base="$2"; api_base_override=1; shift 2;;
      --api-health) api_health="$2"; api_health_override=1; shift 2;;
      --web-url) web_url="$2"; web_url_override=1; shift 2;;
      --admin-url) admin_url="$2"; admin_url_override=1; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"

  kind="$(printf '%s' "$kind" | tr '[:upper:]' '[:lower:]')"

  local compose_file="${PROJECT_ROOT}/docker/docker-compose.yml"
  local ports_updated=0
  if [[ -f "$compose_file" ]]; then
    local detected
    if detected="$(gc_compose_port "$compose_file" api 3000)"; then
      if [[ -n "$detected" && "$detected" != "$GC_API_HOST_PORT" ]]; then
        GC_API_HOST_PORT="$detected"; API_HOST_PORT="$detected"; ports_updated=1
      fi
    fi
    if detected="$(gc_compose_port "$compose_file" web 5173)"; then
      if [[ -n "$detected" && "$detected" != "$GC_WEB_HOST_PORT" ]]; then
        GC_WEB_HOST_PORT="$detected"; WEB_HOST_PORT="$detected"; ports_updated=1
      fi
    fi
    if detected="$(gc_compose_port "$compose_file" admin 5173)"; then
      if [[ -n "$detected" && "$detected" != "$GC_ADMIN_HOST_PORT" ]]; then
        GC_ADMIN_HOST_PORT="$detected"; ADMIN_HOST_PORT="$detected"; ports_updated=1
      fi
    fi
    if detected="$(gc_compose_port "$compose_file" proxy 80)"; then
      if [[ -n "$detected" && "$detected" != "$GC_PROXY_HOST_PORT" ]]; then
        GC_PROXY_HOST_PORT="$detected"; PROXY_HOST_PORT="$detected"; ports_updated=1
      fi
    fi
  fi
  (( ports_updated )) && gc_env_sync_ports

  if (( api_base_override == 0 )); then
    api_base="${GC_API_BASE_URL:-$api_base}"
  fi
  if (( web_url_override == 0 )); then
    web_url="${GC_WEB_URL:-$web_url}"
  fi
  if (( admin_url_override == 0 )); then
    admin_url="${GC_ADMIN_URL:-$admin_url}"
  fi
  if (( api_health_override == 0 )); then
    api_health="${GC_API_HEALTH_URL:-$api_health}"
  fi

  local trimmed_base="${api_base%/}"
  api_health="${api_health:-${trimmed_base}/health}"

  local verify_root="$CLI_ROOT/verify"
  [[ -d "$verify_root" ]] || die "verify scripts directory missing at ${verify_root}"

  case "$kind" in
    program_filters) kind="program-filters" ;;
    program-filters|acceptance|openapi|a11y|lighthouse|consent|telemetry|nfr|all) ;;
    *) die "Unknown verify target: ${kind}";;
  esac

  local -a check_names
  case "$kind" in
    acceptance) check_names=(acceptance) ;;
    openapi|a11y|lighthouse|consent|program-filters|telemetry)
      check_names=("$kind")
      ;;
    nfr)
      check_names=(openapi a11y lighthouse consent program-filters telemetry)
      ;;
    all)
      check_names=(acceptance openapi a11y lighthouse consent program-filters telemetry)
      ;;
  esac

  local summary_dir="${PROJECT_ROOT}/.gpt-creator/staging/verify"
  local logs_dir="${summary_dir}/logs"
  mkdir -p "$summary_dir" "$logs_dir"

  local summary_path="${summary_dir}/summary.json"
  local python_available=0
  local python_bin=""
  if command -v python3 >/dev/null 2>&1; then
    python_available=1
    python_bin="$(command -v python3)"
  fi
  local check_order="acceptance,openapi,lighthouse,a11y,consent,program-filters,telemetry"

  local pass=0 fail=0 skip=0

  update_verify_summary() {
    [[ "$python_available" -eq 1 ]] || return 0
    local name="$1"
    local status="$2"
    local label="$3"
    local message="$4"
    local log_path="$5"
    local report_path="$6"
    local score="$7"
    local duration="$8"
    local run_kind="$9"
    local timestamp="${10}"
    local event=""
    event="$(
      CHECK_NAME="$name" \
      CHECK_STATUS="$status" \
      CHECK_LABEL="$label" \
      CHECK_MESSAGE="$message" \
      CHECK_LOG="$log_path" \
      CHECK_REPORT="$report_path" \
      CHECK_SCORE="$score" \
      CHECK_DURATION="$duration" \
      CHECK_RUN_KIND="$run_kind" \
      CHECK_TIMESTAMP="$timestamp" \
      CHECK_ORDER="$check_order" \
      PROJECT_ROOT="$PROJECT_ROOT" \
      "$python_bin" - "$summary_path" <<'PY'
import json, os, sys, datetime
path = sys.argv[1]
root = os.environ.get("PROJECT_ROOT", "")
name = os.environ.get("CHECK_NAME", "")
if not name:
    sys.exit(0)
label = os.environ.get("CHECK_LABEL", name.title())
status = os.environ.get("CHECK_STATUS", "unknown")
message = os.environ.get("CHECK_MESSAGE", "")
log_path = os.environ.get("CHECK_LOG", "")
report_path = os.environ.get("CHECK_REPORT", "")
score_raw = os.environ.get("CHECK_SCORE", "")
duration_raw = os.environ.get("CHECK_DURATION", "")
run_kind = os.environ.get("CHECK_RUN_KIND", "")
timestamp = os.environ.get("CHECK_TIMESTAMP", datetime.datetime.utcnow().replace(microsecond=0).isoformat() + "Z")
order_raw = os.environ.get("CHECK_ORDER", "")

def relify(path_value):
    if not path_value:
        return ""
    if not os.path.isabs(path_value) and root:
        abs_path = os.path.normpath(os.path.join(root, path_value))
    else:
        abs_path = os.path.normpath(path_value)
    if root:
        try:
            rel = os.path.relpath(abs_path, root)
        except Exception:
            rel = abs_path
    else:
        rel = abs_path
    return rel.replace(os.sep, "/")

score = None
if score_raw:
    try:
        score = float(score_raw)
    except Exception:
        score = None

duration = None
if duration_raw:
    try:
        duration = float(duration_raw)
    except Exception:
        duration = None

try:
    with open(path, "r", encoding="utf-8") as fh:
        data = json.load(fh)
except Exception:
    data = {}

checks = data.get("checks")
if not isinstance(checks, dict):
    checks = {}

entry = checks.get(name, {})
entry["name"] = name
entry["label"] = label
entry["status"] = status
if message:
    entry["message"] = message
elif "message" in entry:
    del entry["message"]

log_rel = relify(log_path)
if log_rel:
    entry["log"] = log_rel
elif "log" in entry:
    del entry["log"]

report_rel = relify(report_path)
if report_rel:
    entry["report"] = report_rel
elif "report" in entry:
    del entry["report"]

if score is not None:
    entry["score"] = score
elif "score" in entry:
    del entry["score"]

if duration is not None:
    entry["duration_seconds"] = duration
elif "duration_seconds" in entry:
    del entry["duration_seconds"]

entry["updated"] = timestamp
if run_kind:
    entry["run_kind"] = run_kind

checks[name] = entry
data["checks"] = checks
data["last_updated"] = timestamp
if run_kind:
    data["last_run_kind"] = run_kind

stats = {"passed": 0, "failed": 0, "skipped": 0, "total": 0}
for chk in checks.values():
    status_value = str(chk.get("status", "")).lower()
    if status_value in ("pass", "passed", "ok", "success"):
        stats["passed"] += 1
    elif status_value in ("skip", "skipped"):
        stats["skipped"] += 1
    else:
        stats["failed"] += 1
    stats["total"] += 1
data["stats"] = stats

order = [part.strip() for part in order_raw.split(",") if part.strip()]
if order:
    merged = []
    seen = set()
    for item in order:
        if item not in seen:
            merged.append(item); seen.add(item)
    for item in checks.keys():
        if item not in seen:
            merged.append(item); seen.add(item)
    data["order"] = merged
else:
    existing = data.get("order")
    merged = []
    seen = set()
    if isinstance(existing, list):
        for item in existing:
            if isinstance(item, str) and item not in seen:
                merged.append(item); seen.add(item)
    for item in checks.keys():
        if item not in seen:
            merged.append(item); seen.add(item)
    data["order"] = merged

with open(path, "w", encoding="utf-8") as fh:
    json.dump(data, fh, indent=2)

event = {
    "name": name,
    "label": label,
    "status": status,
    "message": message,
    "log": entry.get("log", ""),
    "report": entry.get("report", ""),
    "score": entry.get("score"),
    "updated": timestamp,
    "run_kind": run_kind,
    "stats": stats,
    "duration_seconds": entry.get("duration_seconds"),
}
print(json.dumps(event))
PY
    )" || event=""
    if [[ -n "$event" ]]; then
      printf '::verify::%s\n' "$event"
    fi
  }

  run_check() {
    local name="$1"; shift
    local label="$1"; shift
    local -a cmd=("$@")
    local timestamp
    timestamp="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
    local stamp
    stamp="$(date -u +"%Y%m%d-%H%M%S")"
    local log_file="${logs_dir}/${stamp}-${name}.log"
    SECONDS=0
    set +e
    "${cmd[@]}" 2>&1 | tee "$log_file"
    local exit_status=${PIPESTATUS[0]}
    set -e
    local duration="$SECONDS"
    local status message
    case "$exit_status" in
      0)
        status="pass"
        message="${label} checks passed."
        ((pass++))
        ;;
      3)
        status="skip"
        message="${label} check skipped (missing dependency)."
        ((skip++))
        warn "${label} check skipped (missing dependency)"
        ;;
      *)
        status="fail"
        message="${label} check failed (exit ${exit_status})."
        ((fail++))
        warn "${label} check failed (exit ${exit_status})"
        ;;
    esac
    cp -f "$log_file" "${logs_dir}/${name}-latest.log" 2>/dev/null || true
    local log_rel="${log_file#$PROJECT_ROOT/}"
    log_rel="${log_rel#./}"
    update_verify_summary "$name" "$status" "$label" "$message" "$log_rel" "" "" "$duration" "$kind" "$timestamp"
    return 0
  }

  find_openapi_candidate() {
    local spec=""
    for cand in "$INPUT_DIR/openapi.yaml" "$INPUT_DIR/openapi.yml" "$INPUT_DIR/openapi.json"; do
      if [[ -f "$cand" ]]; then
        spec="$cand"
        break
      fi
    done
    printf '%s' "$spec"
  }

  for name in "${check_names[@]}"; do
    case "$name" in
      acceptance)
        run_check "acceptance" "Acceptance" \
          env PROJECT_ROOT="$PROJECT_ROOT" GC_COMPOSE_FILE="$compose_file" \
          bash "$verify_root/acceptance.sh" "${api_base}" "${web_url}" "${admin_url}" "${api_health}"
        ;;
      openapi)
        run_check "openapi" "OpenAPI" \
          bash "$verify_root/check-openapi.sh" "$(find_openapi_candidate)"
        ;;
      a11y)
        run_check "a11y" "Accessibility" \
          bash "$verify_root/check-a11y.sh" "${web_url}" "${admin_url}"
        ;;
      lighthouse)
        run_check "lighthouse" "Lighthouse" \
          bash "$verify_root/check-lighthouse.sh" "${web_url}" "${admin_url}"
        ;;
      consent)
        run_check "consent" "Consent" \
          bash "$verify_root/check-consent.sh" "${web_url}"
        ;;
      program-filters)
        run_check "program-filters" "Program Filters" \
          bash "$verify_root/check-program-filters.sh" "${api_base}"
        ;;
      telemetry)
        run_check "telemetry" "Telemetry" \
          bash "$verify_root/check-telemetry.sh"
        ;;
    esac
  done

  if (( fail > 0 )); then
    die "Verify failed — pass=${pass} fail=${fail} skip=${skip}"
  fi
  ok "Verify complete — pass=${pass} skip=${skip}"
}

gc_exec_with_timeout() {
  local timeout="${1:-0}"
  local stdin_file="${2:-}"
  local log_file="${3:-}"
  shift 3 || true
  local -a cmd=("$@")

  if (( ${#cmd[@]} == 0 )); then
    return 1
  fi

  python3 - "$timeout" "$stdin_file" "$log_file" "${cmd[@]}" <<'PY'
import hashlib
import os
import re
import select
import signal
import subprocess
import sys
import time
from collections import deque
from pathlib import Path

def to_int(value: str) -> int:
    try:
        return max(0, int(value))
    except (TypeError, ValueError):
        return 0

timeout = to_int(sys.argv[1] if len(sys.argv) > 1 else "0")
stdin_path = sys.argv[2] if len(sys.argv) > 2 else ""
log_path = sys.argv[3] if len(sys.argv) > 3 else ""
cmd = sys.argv[4:]
max_duration = to_int(os.environ.get("GC_CODEX_EXEC_MAX_DURATION", "0"))
idle_ping_interval = to_int(os.environ.get("GC_CODEX_IDLE_PING_INTERVAL", "180"))
idle_ping_max = to_int(os.environ.get("GC_CODEX_IDLE_PING_MAX", "3"))
idle_ping_count = 0
next_idle_ping = idle_ping_interval if idle_ping_interval > 0 else 0

def describe_command(args):
    if not args:
        return "command"
    first = os.path.basename(args[0])
    tail = []
    for token in args[1:3]:
        if token.strip():
            tail.append(token)
    if tail:
        first = f"{first} {' '.join(tail)}"
    if len(args) > 3:
        first += " …"
    return first

cmd_label = describe_command(cmd)

if not cmd:
    sys.exit(1)

stdin = None
if stdin_path:
    try:
        stdin = open(stdin_path, "rb")
    except FileNotFoundError:
        print(f"{stdin_path}: No such file or directory", file=sys.stderr, flush=True)
        sys.exit(1)

log = None
if log_path:
    log_dir = os.path.dirname(log_path)
    if log_dir:
        os.makedirs(log_dir, exist_ok=True)
    log = open(log_path, "w", encoding="utf-8", buffering=1)

diff_cache = {}
diff_dir = None
diff_counter = 0
capturing_diff = False
diff_header = ""
diff_lines = []
diff_repeat_limit = to_int(os.environ.get("GC_CODEX_DIFF_REPEAT_LIMIT", "6"))
diff_repeat_counts = {}
diff_recent = deque(maxlen=max(5, diff_repeat_limit if diff_repeat_limit > 0 else 5))
last_diff_digest = None
last_diff_streak = 0
abort_now = False
abort_reason = ""
abort_repeat_count = 0
turn_limit = to_int(os.environ.get("GC_CODEX_MAX_TURNS", "0"))
turn_count = 0

def normalize_diff_for_digest(diff_text: str) -> str:
    lines = []
    for raw_line in diff_text.splitlines():
        if raw_line.startswith("index "):
            continue
        line = re.sub(r"\b[0-9a-f]{7,40}\b", "<sha>", raw_line)
        lines.append(line)
    return "\n".join(lines)

def ensure_diff_dir():
    global diff_dir
    if diff_dir is None and log_path:
        base = Path(log_path)
        diff_dir_path = base.parent / f"{base.name}.diffs"
        diff_dir_path.mkdir(parents=True, exist_ok=True)
        diff_dir = diff_dir_path
    return diff_dir

def emit(text: str) -> None:
    if not text:
        return
    try:
        sys.stdout.write(text)
        sys.stdout.flush()
    except OSError:
        pass
    if log:
        try:
            log.write(text)
            log.flush()
        except OSError:
            pass

def is_diff_line(line: str) -> bool:
    if line == "":
        return True
    prefixes = (
        "diff --git ",
        "index ",
        "@@",
        "--- ",
        "+++ ",
        "+",
        "-",
        " ",
        "Binary files ",
        "No newline at end of file",
        "rename ",
        "similarity index",
        "dissimilarity index",
        "copy from",
        "copy to",
        "new file mode",
        "deleted file mode",
        "old mode",
        "new mode",
    )
    return any(line.startswith(prefix) for prefix in prefixes)

def flush_diff():
    global capturing_diff, diff_header, diff_lines, diff_counter
    global abort_now, abort_reason, abort_repeat_count, timed_out, timeout_type
    global diff_recent, last_diff_digest, last_diff_streak
    if not diff_header:
        capturing_diff = False
        diff_lines = []
        return
    diff_text = "".join(diff_lines)
    normalized_diff = normalize_diff_for_digest(diff_text)
    digest_source = normalized_diff if normalized_diff.strip() else diff_text
    digest = hashlib.sha256(digest_source.encode("utf-8")).hexdigest()[:12]
    count = diff_repeat_counts.get(digest, 0) + 1
    diff_repeat_counts[digest] = count
    if last_diff_digest == digest:
        last_diff_streak = last_diff_streak + 1
    else:
        last_diff_digest = digest
        last_diff_streak = 1
    diff_recent.append(digest)
    recent_occurrences = diff_recent.count(digest)
    oscillates = False
    if len(diff_recent) >= 3:
        a, b, c = diff_recent[-3], diff_recent[-2], diff_recent[-1]
        if a == c and a != b:
            oscillates = True
    diff_dir_path = ensure_diff_dir()
    base_dir = diff_dir_path.parent if diff_dir_path is not None else os.getcwd()
    base_dir_str = str(base_dir)
    if digest in diff_cache:
        stored_path = diff_cache[digest]
        if stored_path:
            rel = os.path.relpath(str(stored_path), base_dir_str)
            emit(f"{diff_header.strip()} (repeat #{count}) see {rel}\n")
        else:
            emit(f"{diff_header.strip()} (repeat #{count} diff cached earlier)\n")
    else:
        if diff_dir_path is not None:
            diff_counter += 1
            stored_path = diff_dir_path / f"turn-{diff_counter:03d}-{digest}.patch"
            stored_path.write_text(diff_text, encoding="utf-8")
            diff_cache[digest] = stored_path
            rel = os.path.relpath(str(stored_path), base_dir_str)
            emit(f"{diff_header.strip()} stored patch {rel}\n")
        else:
            diff_cache[digest] = None
            emit(f"{diff_header.strip()} (diff of {len(diff_text)} bytes captured)\n")
    if diff_repeat_limit and diff_repeat_limit > 0 and not abort_now:
        triggered = False
        if last_diff_streak >= diff_repeat_limit:
            abort_reason = f"turn diff repeated {last_diff_streak} times consecutively"
            abort_repeat_count = last_diff_streak
            timeout_type = "diff-repeat"
            triggered = True
        elif len(diff_recent) >= diff_repeat_limit and recent_occurrences >= diff_repeat_limit:
            abort_reason = f"turn diff repeated {recent_occurrences} times within last {len(diff_recent)} turns"
            abort_repeat_count = recent_occurrences
            timeout_type = "diff-repeat"
            triggered = True
        elif oscillates:
            abort_reason = "turn diff oscillation detected (A→B→A pattern)"
            timeout_type = "diff-oscillation"
            triggered = True
        if triggered:
            abort_now = True
            timed_out = True
    capturing_diff = False
    diff_header = ""
    diff_lines.clear()

buffer = ""

def process_line(line: str):
    global capturing_diff, diff_header, diff_lines
    global abort_now, turn_count, abort_reason, timed_out, timeout_type
    stripped = line.rstrip("\n")
    lowered = stripped.lower()
    if turn_limit:
        turn_pos = lowered.find("turn ")
        if turn_pos != -1:
            prev_char = lowered[turn_pos - 1] if turn_pos > 0 else ""
            if turn_pos == 0 or prev_char in {"", " ", "\t", "]"}:
                turn_count += 1
                if turn_count >= turn_limit and not abort_now:
                    abort_now = True
                    timeout_type = "turn-limit"
                    abort_reason = f"turn limit {turn_limit} reached"
                    timed_out = True
                    return
    if capturing_diff:
        if is_diff_line(stripped):
            diff_lines.append(line)
            return
        flush_diff()
        # fall through to treat current line normally
        if abort_now:
            return
    if stripped.lower().startswith("turn diff"):
        capturing_diff = True
        diff_header = stripped
        diff_lines = []
        return
    emit(line)

def process_text(text: str):
    global buffer
    global abort_now
    buffer += text
    while True:
        if "\n" in buffer:
            line, buffer = buffer.split("\n", 1)
            process_line(line + "\n")
            if abort_now:
                break
        else:
            break

preexec = os.setsid if hasattr(os, "setsid") else None

try:
    proc = subprocess.Popen(
        cmd,
        stdin=stdin,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        bufsize=0,
        preexec_fn=preexec,
    )
except FileNotFoundError:
    emit(f"{cmd[0]} not found\n")
    if log:
        log.close()
    sys.exit(127)

start = time.monotonic()
last_activity = start
timed_out = False
timeout_type = None

if not proc.stdout:
    if stdin:
        stdin.close()
    if log:
        log.close()
    sys.exit(1)

fd = proc.stdout.fileno()

interrupted = False

try:
    while True:
        now = time.monotonic()
        elapsed = now - start
        if max_duration and elapsed >= max_duration:
            timed_out = True
            timeout_type = "hard"
            break
        if timeout:
            idle_for = now - last_activity
            if (
                idle_ping_interval
                and idle_ping_max
                and next_idle_ping
                and idle_ping_count < idle_ping_max
                and idle_for >= next_idle_ping
                and not abort_now
            ):
                idle_ping_count += 1
                elapsed_sec = int(elapsed)
                idle_sec = int(idle_for)
                emit(
                    f"[gc] Waiting on {cmd_label}: no output for {idle_sec}s (elapsed {elapsed_sec}s)\n"
                )
                last_activity = time.monotonic()
                idle_for = 0.0
                if idle_ping_count < idle_ping_max:
                    next_idle_ping = idle_ping_interval
                else:
                    next_idle_ping = 0
            if idle_for >= timeout:
                timed_out = True
                timeout_type = "idle"
                break
            remaining = timeout - idle_for
            wait_time = remaining if remaining < 0.2 else 0.2
        else:
            wait_time = 0.2
        ready, _, _ = select.select([fd], [], [], wait_time)
        if ready:
            chunk = os.read(fd, 8192)
            if chunk:
                text = chunk.decode("utf-8", errors="replace")
                process_text(text)
                last_activity = time.monotonic()
                if idle_ping_interval:
                    idle_ping_count = 0
                    next_idle_ping = idle_ping_interval if idle_ping_interval > 0 else 0
                if abort_now:
                    break
            else:
                break
        else:
            if proc.poll() is not None:
                break
        if abort_now:
            break
    if not timed_out and not abort_now:
        while True:
            chunk = os.read(fd, 8192)
            if not chunk:
                break
            text = chunk.decode("utf-8", errors="replace")
            process_text(text)
            if idle_ping_interval:
                idle_ping_count = 0
                next_idle_ping = idle_ping_interval if idle_ping_interval > 0 else 0
            if abort_now:
                break
except KeyboardInterrupt:
    interrupted = True
    abort_now = True
    try:
        if preexec:
            os.killpg(proc.pid, signal.SIGINT)
        else:
            proc.send_signal(signal.SIGINT)
    except ProcessLookupError:
        pass
    try:
        proc.wait(5)
    except subprocess.TimeoutExpired:
        try:
            if preexec:
                os.killpg(proc.pid, signal.SIGTERM)
            else:
                proc.terminate()
        except ProcessLookupError:
            pass
finally:
    if stdin:
        stdin.close()

proc.stdout.close()

if buffer:
    process_line(buffer)
    buffer = ""

flush_diff()
if abort_now and not timed_out:
    timed_out = True

if timed_out:
    try:
        if preexec:
            os.killpg(proc.pid, signal.SIGTERM)
        else:
            proc.terminate()
    except ProcessLookupError:
        pass
    try:
        proc.wait(5)
    except subprocess.TimeoutExpired:
        try:
            if preexec:
                os.killpg(proc.pid, signal.SIGKILL)
            else:
                proc.kill()
        except ProcessLookupError:
            pass
        proc.wait()
    if timeout_type == "hard":
        emit(f"\n[gc] Command exceeded max runtime of {max_duration} seconds\n")
    elif timeout_type == "diff-repeat":
        limit_msg = diff_repeat_limit if diff_repeat_limit else abort_repeat_count
        emit(f"\n[gc] Command emitted the same diff {abort_repeat_count or limit_msg} times; aborting to prevent an infinite loop\n")
    elif timeout_type == "diff-oscillation":
        emit("\n[gc] Command diff output oscillated between patterns (A→B→A); aborting to prevent an infinite loop\n")
    elif timeout_type == "turn-limit":
        emit(f"\n[gc] Command exceeded the maximum of {turn_limit} Codex turns; aborting to prevent an infinite loop\n")
    else:
        emit(f"\n[gc] Command produced no output for {timeout} seconds\n")
    if log:
        log.flush()
        log.close()
    if timeout_type in {"diff-repeat", "diff-oscillation"}:
        sys.exit(125)
    elif timeout_type == "turn-limit":
        sys.exit(126)
    else:
        sys.exit(124)
elif interrupted:
    if log:
        log.flush()
        log.close()
    emit("\n[gc] Command interrupted by user (KeyboardInterrupt)\n")
    sys.exit(130)

exit_code = proc.poll()
if exit_code is None:
    exit_code = proc.wait()

if log:
    log.flush()
    log.close()

sys.exit(exit_code)
PY
  return "$?"
}

codex_call() {
  local task="${1:?task}"; shift || true
  local prompt_dir="${GC_DIR}/prompts"
  mkdir -p "$prompt_dir"
  GC_CODEX_CALL_TOKEN_ACCUM=0

  local prompt_file=""
  local output_file=""

  if [[ $# -gt 0 && -f "$1" ]]; then
    prompt_file="$1"
    shift || true
  fi

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --prompt) prompt_file="$2"; shift 2;;
      --output) output_file="$2"; shift 2;;
      *) break;;
    esac
  done

  if [[ -z "$prompt_file" ]]; then
    prompt_file="${prompt_dir}/${task}.md"
    if [[ ! -f "$prompt_file" ]]; then
      cat >"$prompt_file" <<'PROMPT'
# Instruction
You are Codex (gpt-5-codex) assisting the gpt-creator pipeline. Apply requested changes deterministically.
PROMPT
    fi
  fi

  if [[ -n "$prompt_file" && -f "$prompt_file" ]]; then
    gc_trim_prompt_file "$prompt_file"
  fi

  if [[ "${GC_CODEX_USAGE_LIMIT_REACHED:-0}" == "1" && "${GC_CODEX_USAGE_LIMIT_CONFIRMED:-0}" == "1" ]]; then
    warn "Codex usage limit previously reached; skipping ${task}."
    return 95
  fi

  if command -v "$CODEX_BIN" >/dev/null 2>&1; then
    info "Codex ${task} → model=${CODEX_MODEL}"
    local fallback_model="${CODEX_FALLBACK_MODEL:-gpt-5-codex}"
    # shellcheck disable=SC2034  # local function assigned below for lexical scoping
    local run_codex_model
    run_codex_model() {
      local model="$1"
      shift || true
      local args=(exec --model "$model")
      if [[ -n "${CODEX_PROFILE:-}" ]]; then
        args+=(--profile "$CODEX_PROFILE")
      fi
      if [[ -n "${PROJECT_ROOT:-}" ]]; then
        args+=(--cd "$PROJECT_ROOT")
      fi
      if [[ -n "${CODEX_REASONING_EFFORT:-}" ]]; then
        args+=(-c "model_reasoning_effort=\"${CODEX_REASONING_EFFORT}\"")
      fi
      args+=(--full-auto --sandbox workspace-write --skip-git-repo-check)
      if [[ -n "$output_file" ]]; then
        mkdir -p "$(dirname "$output_file")"
        args+=(--output-last-message "$output_file")
      fi
      local usage_dir="${LOG_DIR:-${PROJECT_ROOT:-$PWD}/.gpt-creator/logs}"
      mkdir -p "$usage_dir"
      local task_slug
      task_slug="$(printf '%s' "$task" | tr '[:upper:]' '[:lower:]')"
      task_slug="$(printf '%s' "$task_slug" | tr -c 'a-z0-9' '_')"
      [[ -n "$task_slug" ]] || task_slug="codex"
      local model_slug
      model_slug="$(printf '%s' "$model" | tr '[:upper:]' '[:lower:]')"
      model_slug="$(printf '%s' "$model_slug" | tr -c 'a-z0-9' '_')"
      [[ -n "$model_slug" ]] || model_slug="model"
      local codex_log=""
      if ! codex_log="$(mktemp "${usage_dir}/codex-${task_slug}-${model_slug}.XXXXXX.log" 2>/dev/null)"; then
        codex_log="$(mktemp 2>/dev/null)" || codex_log=""
      fi
      local exec_timeout=0
      if [[ "${GC_CODEX_EXEC_TIMEOUT:-}" =~ ^[0-9]+$ ]]; then
        exec_timeout=$((GC_CODEX_EXEC_TIMEOUT))
      fi
      if [[ -n "$codex_log" ]]; then
        local cmd_status=0
        if gc_exec_with_timeout "$exec_timeout" "$prompt_file" "$codex_log" "$CODEX_BIN" "${args[@]}"; then
          cmd_status=0
        else
          cmd_status=$?
        fi
        gc_record_codex_usage "$codex_log" "$task" "$model" "$prompt_file" "$cmd_status"
        local attempt_tokens="${GC_LAST_CODEX_TOTAL_TOKENS:-0}"
        if [[ "$attempt_tokens" =~ ^[0-9]+$ ]]; then
          GC_CODEX_CALL_TOKEN_ACCUM=$((GC_CODEX_CALL_TOKEN_ACCUM + attempt_tokens))
        fi
        if [[ "${GC_CODEX_USAGE_LIMIT_REACHED:-0}" == "1" ]]; then
          return 95
        fi
        return "$cmd_status"
      else
        local cmd_status=0
        if gc_exec_with_timeout "$exec_timeout" "$prompt_file" "" "$CODEX_BIN" "${args[@]}"; then
          cmd_status=0
        else
          cmd_status=$?
        fi
        if [[ "${GC_CODEX_USAGE_LIMIT_REACHED:-0}" == "1" ]]; then
          return 95
        fi
        return "$cmd_status"
      fi
    }
    if run_codex_model "$CODEX_MODEL"; then
      return 0
    fi
    local primary_status="$?"
    if [[ "${GC_CODEX_USAGE_LIMIT_REACHED:-0}" == "1" ]]; then
      return "$primary_status"
    fi
    if [[ "$CODEX_MODEL" != "$fallback_model" ]]; then
      warn "Codex model ${CODEX_MODEL} failed; retrying with ${fallback_model}."
      if run_codex_model "$fallback_model"; then
        return 0
      fi
      primary_status="$?"
      if [[ "${GC_CODEX_USAGE_LIMIT_REACHED:-0}" == "1" ]]; then
        return "$primary_status"
      fi
      warn "Codex invocation returned non-zero."
      return "$primary_status"
    fi
    warn "Codex invocation returned non-zero."
    return "$primary_status"
  else
    warn "Codex binary (${CODEX_BIN}) not found — skipping ${task}."
  fi
}

ensure_go_runtime() {
  local min_major=1
  local min_minor=21
  local target_version="1.22.4"
  local requested_bin="${GO_BIN:-}"
  local resolved_bin=""

  if [[ -n "$requested_bin" ]]; then
    if command -v "$requested_bin" >/dev/null 2>&1; then
      resolved_bin="$(command -v "$requested_bin")"
    else
      warn "GO_BIN is set to '${requested_bin}' but that binary was not found on PATH."
    fi
  fi

  if [[ -z "$resolved_bin" ]] && command -v go >/dev/null 2>&1; then
    resolved_bin="$(command -v go)"
  fi

  if [[ -n "$resolved_bin" ]]; then
    local raw_version
    raw_version="$("$resolved_bin" version 2>/dev/null | awk '{print $3}')"
    local version="${raw_version#go}"
    version="${version%%[^0-9.]*}"
    local IFS=.
    read -r version_major version_minor _ <<<"${version}"
    version_major="${version_major:-0}"
    version_minor="${version_minor:-0}"
    if (( 10#$version_major > min_major )) || { (( 10#$version_major == min_major )) && (( 10#$version_minor >= min_minor )); }; then
      GC_GO_BIN="$resolved_bin"
      export GO_BIN="$GC_GO_BIN"
      return
    fi
    warn "Found Go ${version:-unknown} at ${resolved_bin}, but need ≥ ${min_major}.${min_minor}."
  fi

  local os_name arch_name go_os go_arch
  os_name="$(uname -s)"
  arch_name="$(uname -m)"
  case "$os_name" in
    Linux) go_os="linux" ;;
    Darwin) go_os="darwin" ;;
    *)
      die "Unsupported OS (${os_name}) for automatic Go installation. Install Go ${min_major}.${min_minor}+ and set GO_BIN."
      ;;
  esac

  case "$arch_name" in
    x86_64|amd64) go_arch="amd64" ;;
    arm64|aarch64) go_arch="arm64" ;;
    *)
      die "Unsupported architecture (${arch_name}) for automatic Go installation. Install Go ${min_major}.${min_minor}+ and set GO_BIN."
      ;;
  esac

  local base_dir
  if [[ -n "${PLAN_DIR:-}" ]]; then
    base_dir="$PLAN_DIR"
  else
    base_dir="${XDG_CACHE_HOME:-$HOME/.cache}/gpt-creator"
    mkdir -p "$base_dir"
  fi
  local runtime_root="${base_dir}/.runtime/go"
  local archive_basename="go${target_version}.${go_os}-${go_arch}"
  local target_dir="${runtime_root}/${archive_basename}"
  local go_root="${target_dir}/go"
  local go_binary="${go_root}/bin/go"

  mkdir -p "$runtime_root"

  if [[ ! -x "$go_binary" ]]; then
    info "Installing Go ${target_version} (${go_os}-${go_arch})"
    local url="https://go.dev/dl/${archive_basename}.tar.gz"
    local tmp_archive
    tmp_archive="$(mktemp "${runtime_root}/go-${target_version}.XXXXXX")"
    if ! curl -fsSL "$url" -o "$tmp_archive"; then
      rm -f "$tmp_archive"
      die "Failed to download Go toolchain from ${url}"
    fi
    rm -rf "$target_dir"
    mkdir -p "$target_dir"
    if ! tar -xzf "$tmp_archive" -C "$target_dir"; then
      rm -f "$tmp_archive"
      die "Failed to extract Go toolchain archive (${url})"
    fi
    rm -f "$tmp_archive"
  fi

  if [[ ! -x "$go_binary" ]]; then
    die "Go toolchain installation incomplete; expected executable at ${go_binary}"
  fi

  local gopath="${target_dir}/gopath"
  local cache_dir="${target_dir}/cache"
  mkdir -p "${gopath}/bin" "${gopath}/pkg/mod" "${cache_dir}"
  export PATH="${go_root}/bin:${PATH}"
  export GOROOT="${go_root}"
  export GOPATH="${gopath}"
  export GOMODCACHE="${gopath}/pkg/mod"
  export GOCACHE="${cache_dir}"
  GC_GO_BIN="$go_binary"
  export GO_BIN="$GC_GO_BIN"
  ok "Go runtime pinned to ${target_version}"
}

ensure_node_runtime() {
  local root_dir="${1:-$PROJECT_ROOT}"
  local target_version="20.10.0"
  local runtime_root="${PLAN_DIR}/.runtime"

  # If current node already satisfies v20.x with sufficient minor, skip.
  if command -v node >/dev/null 2>&1; then
    local current
    current="$(node -v 2>/dev/null | sed 's/^v//')"
    if [[ "$current" == 20.* ]]; then
      local minor="${current#20.}"
      minor="${minor%%.*}"
      local patch="${current#20.${minor}.}"
      [[ -z "$patch" ]] && patch=0
      if (( 10#${minor:-0} > 10 )) || { (( 10#${minor:-0} == 10 )) && (( 10#${patch:-0} >= 0 )); }; then
        return
      fi
    fi
  fi

  mkdir -p "$runtime_root"

  local os_name
  os_name="$(uname -s)"
  local arch_name
  arch_name="$(uname -m)"
  local os_tag="" arch_tag="" ext="tar.xz"

  case "$os_name" in
    Darwin)
      os_tag="darwin"
      ext="tar.gz"
      ;;
    Linux)
      os_tag="linux"
      ;;
    *)
      warn "Unsupported OS (${os_name}) for automatic Node runtime; continuing with system Node."
      return
      ;;
  esac

  case "$arch_name" in
    x86_64|amd64)
      arch_tag="x64"
      ;;
    arm64|aarch64)
      arch_tag="arm64"
      ;;
    *)
      warn "Unsupported CPU architecture (${arch_name}) for automatic Node runtime; continuing with system Node."
      return
      ;;
  esac

  local archive_name="node-v${target_version}-${os_tag}-${arch_tag}"
  local target_dir="${runtime_root}/${archive_name}"

  if [[ ! -d "$target_dir" ]]; then
    local url="https://nodejs.org/dist/v${target_version}/${archive_name}.${ext}"
    info "Downloading Node.js ${target_version} (${os_tag}-${arch_tag})"
    local tmp_archive
    tmp_archive="$(mktemp "${runtime_root}/node-${target_version}.XXXXXX")"
    if ! curl -fsSL "$url" -o "$tmp_archive"; then
      warn "Failed to download Node.js runtime from ${url}; continuing with system Node."
      rm -f "$tmp_archive"
      return
    fi
    mkdir -p "$runtime_root"
    if [[ "$ext" == "tar.gz" ]]; then
      tar -xzf "$tmp_archive" -C "$runtime_root"
    else
      tar -xJf "$tmp_archive" -C "$runtime_root"
    fi
    rm -f "$tmp_archive"
  fi

  if [[ ! -d "$target_dir" ]]; then
    warn "Node runtime directory ${target_dir} missing after download; continuing with system Node."
    return
  fi

  export PATH="${target_dir}/bin:${PATH}"
  export NODE_HOME="$target_dir"
  export npm_config_cache="${runtime_root}/npm-cache"
  export PNPM_HOME="${runtime_root}/pnpm-home"
  mkdir -p "$npm_config_cache" "$PNPM_HOME"
  export PATH="${PNPM_HOME}:${PATH}"
  export GC_NODE_RUNTIME="${target_dir}"
  ok "Node.js runtime pinned to ${target_version}"
}

ensure_node_dependencies() {
  local root_dir="${1:-$PROJECT_ROOT}"
  local sentinel="${PLAN_DIR}/.deps-installed"

  ensure_node_runtime "$root_dir"

  if [[ -f "$sentinel" ]]; then
    return
  fi

  local bootstrap_script="${PROJECT_ROOT}/scripts/bootstrap_dependencies.sh"
  if [[ ! -x "$bootstrap_script" ]]; then
    warn "Dependency bootstrap script missing at ${bootstrap_script}; skipping automatic install."
    return
  fi

  if "$bootstrap_script" "$root_dir"; then
    touch "$sentinel"
    return
  fi

  die "Dependency bootstrap failed; see the bootstrap log for details."
}

gc_apply_codex_changes() {
  local output_file="${1:?output file required}"
  local project_root="${2:?project root required}"
  local helper_path
  helper_path="$(gc_clone_python_tool "focus_text.py" "$project_root")" || return 1
  python3 "$helper_path" "$output_file" "$project_root"
}

cmd_create_jira_tasks() {
  local args=()
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project)
        args+=(--project "$(abs_path "$2")")
        shift 2
        ;;
      --model)
        args+=(--model "$2")
        shift 2
        ;;
      --force|--dry-run)
        args+=("$1")
        shift
        ;;
      -h|--help)
        # Let the dedicated CLI script handle help output.
        args+=("$1")
        shift
        ;;
      *)
        args+=("$1")
        shift
        ;;
    esac
  done

  local shell_bin="${BASH:-bash}"
  "$shell_bin" "$CLI_ROOT/src/cli/create-jira-tasks.sh" "${args[@]}"
}

cmd_create_tasks() {
  local root="" jira="" force=0
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --jira) jira="$(abs_path "$2")"; shift 2;;
      --force) force=1; shift;;
      -h|--help)
        cat <<'EOUSAGE'
Usage: gpt-creator create-tasks [--project PATH] [--jira FILE] [--force]

Convert Jira markdown tasks into a project-scoped SQLite database stored under .gpt-creator/staging/plan/tasks.

Options:
  --project PATH  Project root (defaults to current directory)
  --jira FILE     Jira markdown source (defaults to staging/inputs/jira.md)
  --force         Rebuild the tasks database without restoring prior progress metadata
EOUSAGE
        return 0
        ;;
      *) break;;
    esac
  done

  ensure_ctx "$root"
  [[ -n "$jira" ]] || jira="${INPUT_DIR}/jira.md"
  [[ -f "$jira" ]] || die "Jira tasks file not found: ${jira}"

  local tasks_dir="${PLAN_DIR}/tasks"
  local parsed_local="${tasks_dir}/parsed.local.json"
  local tasks_db="${tasks_dir}/tasks.db"
  mkdir -p "$tasks_dir"

  info "Parsing Jira backlog → ${parsed_local}"
  gc_parse_jira_tasks "$jira" "$parsed_local"

  info "Building tasks database → ${tasks_db}"
  local db_stats
  if ! db_stats="$(gc_build_tasks_db "$parsed_local" "$tasks_db" "$force")"; then
    die "Failed to build Jira tasks SQLite database"
  fi

  local story_count=0 task_count=0 restored_stories=0 restored_tasks=0
  while IFS= read -r line; do
    case "$line" in
      STORIES\ *) story_count="${line#STORIES }" ;;
      TASKS\ *) task_count="${line#TASKS }" ;;
      RESTORED_STORIES\ *) restored_stories="${line#RESTORED_STORIES }" ;;
      RESTORED_TASKS\ *) restored_tasks="${line#RESTORED_TASKS }" ;;
    esac
  done <<<"$db_stats"

  info "Stories: ${story_count:-0} (restored: ${restored_stories:-0})"
  info "Tasks: ${task_count:-0} (restored statuses: ${restored_tasks:-0})"
  ok "Tasks database updated → ${tasks_db}"

  local legacy_manifest="${tasks_dir}/manifest.json"
  local legacy_stories="${tasks_dir}/stories"
  if [[ -f "$legacy_manifest" || -d "$legacy_stories" ]]; then
    warn "Legacy task manifest/JSON artifacts detected under ${tasks_dir}; they are no longer used now that tasks live in SQLite. Remove them when convenient to avoid confusion."
  fi
}

cmd_backlog() {
  local root="" type_arg="" item_children="" show_progress=0 task_details=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project|--root)
        root="$(abs_path "$2")"
        shift 2
        ;;
      --type)
        type_arg="$2"
        shift 2
        ;;
      --item-children)
        item_children="$2"
        shift 2
        ;;
      --progress)
        show_progress=1
        shift
        ;;
      --task-details)
        task_details="$2"
        shift 2
        ;;
      -h|--help)
        cat <<'EOUSAGE'
Usage: gpt-creator backlog [--project PATH|--root PATH]
                           [--type epics|stories]
                           [--item-children ID]
                           [--progress]
                           [--task-details ID]

Inspect the backlog stored in .gpt-creator/staging/plan/tasks/tasks.db without an interactive prompt.

  --type epics|stories     List backlog summaries (defaults to 'epics' when no other flag is provided).
  --item-children ID       Show the direct children of the epic/story identified by ID (slug, key, or ID).
  --progress               Print an overall progress bar summarising task completion.
  --task-details ID        Show a detailed view for the matching task ID (case-insensitive).

Pass any combination of the flags above; each requested view is printed sequentially.
EOUSAGE
        return 0
        ;;
      *)
        die "Unknown argument for backlog: $1"
        ;;
    esac
  done

  if [[ -z "$type_arg" && -z "$item_children" && "$show_progress" -eq 0 && -z "$task_details" ]]; then
    type_arg="epics"
  fi

  ensure_ctx "$root"
  local tasks_db="${PLAN_DIR}/tasks/tasks.db"
  if [[ ! -f "$tasks_db" ]]; then
    die "Tasks database not found at ${tasks_db}. Run 'gpt-creator create-tasks' first."
  fi

  local backlog_helper
  backlog_helper="$(gc_clone_python_tool "fetch_stories.py" "${PROJECT_ROOT:-$PWD}")" || die "Failed to prepare backlog helper script"
  python3 "$backlog_helper" "$tasks_db" "${type_arg:-}" "${item_children:-}" "$show_progress" "${task_details:-}"
}

cmd_migrate_tasks_json() {
  local root="" force=0
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --force) force=1; shift;;
      -h|--help)
        cat <<'EOUSAGE'
Usage: gpt-creator migrate-tasks [--project PATH] [--force]

Populate the tasks SQLite database from the JSON outputs produced by
`gpt-creator create-jira-tasks` (under plan/create-jira-tasks/json).

Options:
  --project PATH  Project root (defaults to current directory)
  --force         Rebuild the database without restoring prior task status metadata
EOUSAGE
        return 0
        ;;
      *) break;;
    esac
  done

  ensure_ctx "$root"

  local pipeline_dir="${PLAN_DIR}/create-jira-tasks"
  local json_dir="${pipeline_dir}/json"
  local epics_json="${json_dir}/epics.json"
  local stories_dir="${json_dir}/stories"
  local tasks_dir="${json_dir}/tasks"

  [[ -f "$epics_json" ]] || die "Epics JSON not found: ${epics_json}"
  [[ -d "$stories_dir" ]] || die "Stories JSON directory not found: ${stories_dir}"
  [[ -d "$tasks_dir" ]] || die "Tasks JSON directory not found: ${tasks_dir}"

  local payload="${json_dir}/tasks_payload.json"
  local tasks_workspace="${PLAN_DIR}/tasks"
  mkdir -p "$tasks_workspace"
  local db_path="${tasks_workspace}/tasks.db"

  info "Building tasks payload from JSON → ${payload}"
  info "Updating tasks database → ${db_path}"

  if ! gc_rebuild_tasks_db_from_json "$force"; then
    die "Failed to rebuild tasks database from JSON payload"
  fi

  ok "Tasks database updated → ${db_path}"
}

cmd_refine_tasks() {
  local root="" story_filter="" model_override="" dry_run=0 force=0
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --story) story_filter="$2"; shift 2;;
      --model) model_override="$2"; shift 2;;
      --dry-run) dry_run=1; shift;;
      --force) force=1; shift;;
      -h|--help)
        cat <<'EOUSAGE'
Usage: gpt-creator refine-tasks [--project PATH] [--story SLUG] [--model NAME] [--dry-run] [--force]

Refine tasks stored in the SQLite backlog using Codex, updating each task
record and the refined JSON artifacts as soon as a task is enriched.

Options:
  --project PATH  Project root (defaults to current directory)
  --story SLUG    Limit refinement to a single story slug (optional)
  --model NAME    Override Codex model (defaults to CODEX_MODEL/GC defaults)
  --dry-run       Build prompts without invoking Codex
  --force         Reset refinement progress and reprocess every task
EOUSAGE
        return 0
        ;;
      *) break;;
    esac
  done

  ensure_ctx "$root"

  local tasks_db="${PLAN_DIR}/tasks/tasks.db"
  [[ -f "$tasks_db" ]] || die "Tasks database not found: ${tasks_db}"

  local pipeline_dir="${PLAN_DIR}/create-jira-tasks"
  local json_tasks_dir="${pipeline_dir}/json/tasks"
  [[ -d "$json_tasks_dir" ]] || die "Tasks JSON directory not found: ${json_tasks_dir}"

  local have_refined
  have_refined=$(python3 - <<'PY' "$tasks_db"
import sqlite3, sys

path = sys.argv[1]
conn = sqlite3.connect(path)
cur = conn.cursor()
cur.execute("PRAGMA table_info(tasks)")
cols = {row[1] for row in cur.fetchall()}
added = False
if "refined" not in cols:
    cur.execute("ALTER TABLE tasks ADD COLUMN refined INTEGER DEFAULT 0")
    added = True
if "refined_at" not in cols:
    cur.execute("ALTER TABLE tasks ADD COLUMN refined_at TEXT")
    added = True
if added:
    conn.commit()
cur.execute("PRAGMA table_info(tasks)")
cols = {row[1] for row in cur.fetchall()}
conn.close()
print("1" if "refined" in cols else "0")
PY
  )

  local summary
  summary=$(python3 - <<'PY' "$tasks_db" "$story_filter"
import sqlite3, sys

db_path = sys.argv[1]
filters_raw = sys.argv[2] if len(sys.argv) > 2 else ''
filters = {value.strip().lower() for value in filters_raw.split(',') if value.strip()}

conn = sqlite3.connect(db_path)
cur = conn.cursor()

cur.execute("PRAGMA table_info(tasks)")
cols = {row[1] for row in cur.fetchall()}
have_refined = 'refined' in cols

story_map = {
    (row[0] or '').strip(): (row[1] or '').strip().lower()
    for row in cur.execute("SELECT story_slug, story_id FROM stories")
}

def story_in_scope(slug: str) -> bool:
    lower = slug.lower()
    if not filters:
        return True
    if lower in filters:
        return True
    story_id_lower = story_map.get(slug, '')
    if story_id_lower in filters:
        return True
    return False

if have_refined:
    rows = cur.execute("SELECT story_slug, COALESCE(refined, 0) FROM tasks").fetchall()
else:
    rows = [(slug, 0) for (slug,) in cur.execute("SELECT story_slug FROM tasks").fetchall()]
conn.close()

total_tasks = 0
refined_tasks = 0
stories_total = set()
stories_pending = set()

for slug, refined in rows:
    slug = (slug or '').strip()
    if not slug or not story_in_scope(slug):
        continue
    stories_total.add(slug)
    total_tasks += 1
    try:
        refined_value = int(refined)
    except Exception:
        refined_value = 0
    if refined_value:
        refined_tasks += 1
    else:
        stories_pending.add(slug)

pending_tasks = total_tasks - refined_tasks
print(total_tasks, refined_tasks, pending_tasks, len(stories_total), len(stories_pending))
PY
  ) || die "Failed to summarise tasks backlog"

  local total_tasks refined_tasks pending_tasks total_stories pending_stories
  read -r total_tasks refined_tasks pending_tasks total_stories pending_stories <<<"$summary"

  if (( force )); then
    python3 - <<'PY' "$tasks_db"
import sqlite3, sys
conn = sqlite3.connect(sys.argv[1])
cur = conn.cursor()
cur.execute("UPDATE tasks SET refined = 0, refined_at = NULL")
conn.commit()
conn.close()
PY
    refined_tasks=0
    pending_tasks=$total_tasks
  fi

  info "Backlog summary → tasks: total=${total_tasks}, refined=${refined_tasks}, pending=${pending_tasks}; stories: total=${total_stories}, pending=${pending_stories}${story_filter:+ (filter='${story_filter}')}."

  local codex_cmd="${CODEX_BIN:-${CODEX_CMD:-codex}}"
  if (( dry_run == 0 )) && ! command -v "$codex_cmd" >/dev/null 2>&1; then
    warn "Codex CLI '$codex_cmd' not found; switching to --dry-run."
    dry_run=1
  fi

  local model_name="${model_override:-${CODEX_MODEL:-$GC_DEFAULT_MODEL}}"

  # shellcheck source=src/lib/create-jira-tasks/pipeline.sh
  source "${CLI_ROOT}/src/lib/create-jira-tasks/pipeline.sh"

  local force_flag=0
  local skip_refine=0
  local dry_flag="$dry_run"
  cjt::init "$PROJECT_ROOT" "$model_name" "$force_flag" "$skip_refine" "$dry_flag"
  CJT_DOC_FILES=()
  cjt::build_context_files

  CJT_SYNC_DB=1
  CJT_TASKS_DB_PATH="$tasks_db"
  CJT_IGNORE_REFINE_STATE=1
  CJT_REFINE_FORCE=$force
  CJT_HAVE_REFINED_COLUMN="$have_refined"
  CJT_REFINE_TOTAL_TASKS="$total_tasks"
  CJT_REFINE_REFINED_TASKS="$refined_tasks"
  CJT_REFINE_PENDING_TASKS="$pending_tasks"
  CJT_REFINE_TOTAL_STORIES="$total_stories"
  CJT_REFINE_PENDING_STORIES="$pending_stories"
  if [[ -n "$story_filter" ]]; then
    CJT_ONLY_STORY_SLUG="$story_filter"
  fi

  cjt::refine_tasks
  ok "Task refinement complete"
}

gc_write_task_prompt() {
  local db_path="${1:?tasks db path required}"
  local story_slug="${2:?story slug required}"
  local task_index="${3:?task index required}"
  local prompt_path="${4:?prompt path required}"
  local context_tail="${5:-}"
  local model_name="${6:-$CODEX_MODEL}"
  local project_root="${7:-$PROJECT_ROOT}"
  local staging_dir="${8:-$GC_STAGING_DIR}"
  local prompt_helper
  prompt_helper="$(gc_clone_python_tool "document_index.py" "$project_root")" || return 1
  python3 "$prompt_helper" "$db_path" "$story_slug" "$task_index" "$prompt_path" "$context_tail" "$model_name" "$project_root" "$staging_dir"
}

# Return success (0) when note language implies manual follow-up is required.
gc_note_requires_followup() {
  local note_text="${1:-}"
  local note_lower="${note_text,,}"

  [[ -n "$note_lower" ]] || return 1

  if [[ "$note_lower" == *"parse-error"* ]]; then
    return 0
  fi

  if [[ "$note_lower" == *"no manual"* || "$note_lower" == *"no manual steps"* || "$note_lower" == *"no manual verification"* || "$note_lower" == *"no manual review"* || "$note_lower" == *"no review needed"* || "$note_lower" == *"no review required"* || "$note_lower" == *"no action required"* || "$note_lower" == *"no follow-up"* || "$note_lower" == *"no follow up"* ]]; then
    return 1
  fi

  if [[ "$note_lower" == *"optional"* || "$note_lower" == *"recommended"* || "$note_lower" == *"informational"* || "$note_lower" == *"for reference"* ]]; then
    if [[ "$note_lower" != *"requires"* && "$note_lower" != *"required"* && "$note_lower" != *"must"* && "$note_lower" != *"need"* && "$note_lower" != *"needs"* && "$note_lower" != *"needed"* && "$note_lower" != *"blocked"* && "$note_lower" != *"blocker"* && "$note_lower" != *"pending"* ]]; then
      return 1
    fi
  fi

  if [[ "$note_lower" == *"error"* || "$note_lower" == *"failure"* || "$note_lower" == *"failed"* ]]; then
    if [[ "$note_lower" == *"no error"* || "$note_lower" == *"no errors"* || "$note_lower" == *"without error"* || "$note_lower" == *"error free"* || "$note_lower" == *"error-free"* || "$note_lower" == *"errors resolved"* || "$note_lower" == *"errors addressed"* || "$note_lower" == *"failure resolved"* || "$note_lower" == *"failure addressed"* ]]; then
      :
    else
      return 0
    fi
  fi

  local contains_manual=0
  if [[ "$note_lower" == *"manual"* || "$note_lower" == *"manually"* ]]; then
    contains_manual=1
  fi

  if (( contains_manual )); then
    if [[ "$note_lower" == *"manual steps optional"* || "$note_lower" == *"manual testing optional"* || "$note_lower" == *"manual qa optional"* || "$note_lower" == *"manual testing recommended"* || "$note_lower" == *"manual qa recommended"* || "$note_lower" == *"manual review optional"* || "$note_lower" == *"manual verification optional"* ]]; then
      contains_manual=0
    fi
  fi

  if (( contains_manual )); then
    local -a manual_triggers=(
      "requires"
      "required"
      "require"
      "must"
      "need"
      "needs"
      "needed"
      "manual follow-up"
      "manual follow up"
      "manual followup"
      "manual verification"
      "manual review"
      "manual steps"
      "manual patch"
      "manual fix"
      "manual merge"
      "manual deploy"
      "manual migration"
      "manual intervention"
      "manual action"
      "apply manually"
      "manually apply"
      "manually patch"
      "manually merge"
      "manually verify"
      "could not"
      "can't"
      "cannot"
      "unable"
      "failed"
      "failure"
      "todo"
      "tbd"
      "pending"
      "block"
      "blocked"
      "follow-up"
      "follow up"
      "followup"
    )
    for trigger in "${manual_triggers[@]}"; do
      if [[ "$note_lower" == *"$trigger"* ]]; then
        return 0
      fi
    done
  fi

  if [[ "$note_lower" == *"review"* ]]; then
    if [[ "$note_lower" == *"no review"* || "$note_lower" == *"reviewed"* ]]; then
      return 1
    fi
    local -a review_triggers=(
      "needs review"
      "need review"
      "required review"
      "requires review"
      "review required"
      "pending review"
      "awaiting review"
      "please review"
      "for review"
      "manual review"
      "review manually"
      "review and apply"
      "review this change"
    )
    for trigger in "${review_triggers[@]}"; do
      if [[ "$note_lower" == *"$trigger"* ]]; then
        return 0
      fi
    done
  fi

  return 1
}

cmd_work_on_tasks() {
  local root="" resume=1 story_filter="" start_task_ref="" no_verify=0 keep_artifacts=0 memory_cycle=0 force_reset=0
  local batch_size=0 sleep_between=0 context_lines=400 context_file_lines=200 prompt_compact=1 sample_lines=0 doc_snippets=1
  local -a context_skip_patterns=()
  local token_limit="${GC_CODEX_MAX_TOKENS_PER_TASK:-0}"
  local token_limit_override=0
  local idle_timeout="${GC_WORK_ON_TASKS_IDLE_TIMEOUT:-0}"
  local throughput_checkpoint_interval=300
  local throughput_next_checkpoint=0
  local diff_repeat_limit_override=""

  local codex_timeout_default=600
  local codex_timeout_value="${GC_CODEX_EXEC_TIMEOUT:-}"
  if [[ -z "$codex_timeout_value" ]]; then
    GC_CODEX_EXEC_TIMEOUT="$codex_timeout_default"
  elif [[ "$codex_timeout_value" =~ ^[0-9]+$ ]]; then
    if (( codex_timeout_value <= 0 )); then
      if [[ -n "${GC_CODEX_EXEC_TIMEOUT_INITIAL:-}" ]]; then
        warn "GC_CODEX_EXEC_TIMEOUT (idle timeout) must be greater than zero; defaulting to ${codex_timeout_default}s."
      fi
      GC_CODEX_EXEC_TIMEOUT="$codex_timeout_default"
    fi
  else
    warn "GC_CODEX_EXEC_TIMEOUT ('${codex_timeout_value}') is not numeric; defaulting idle timeout to ${codex_timeout_default}s."
    GC_CODEX_EXEC_TIMEOUT="$codex_timeout_default"
  fi
  local codex_timeout_seconds="${GC_CODEX_EXEC_TIMEOUT}"

  local loop_guard_triggered=0
  local loop_guard_exit_code="${GC_LOOP_GUARD_EXIT_CODE:-72}"
  if ! [[ "$loop_guard_exit_code" =~ ^[0-9]+$ ]]; then
    loop_guard_exit_code=72
  fi

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --story|--from-story) story_filter="$2"; shift 2;;
      --from-task|--fresh-from|--task)
        start_task_ref="${2:-}"
        [[ -n "$start_task_ref" ]] || die "--from-task requires a task id or story:position reference"
        shift 2
        ;;
      --fresh) resume=0; shift;;
      --force)
        resume=0
        force_reset=1
        shift
        ;;
      --no-verify) no_verify=1; shift;;
      --keep-artifacts) keep_artifacts=1; shift;;
      --memory-cycle) memory_cycle=1; shift;;
      --batch-size) batch_size="${2:-0}"; shift 2;;
      --sleep-between) sleep_between="${2:-0}"; shift 2;;
      --context-lines)
        context_lines="${2:-}"
        shift 2
        ;;
      --context-none)
        context_lines=0
        shift
        ;;
      --context-file-lines)
        context_file_lines="${2:-}"
        shift 2
        ;;
      --context-skip)
        context_skip_patterns+=("$2")
        shift 2
        ;;
      --prompt-compact)
        prompt_compact=1
        shift
        ;;
      --prompt-expanded)
        prompt_compact=0
        shift
        ;;
      --context-doc-snippets|--doc-snippets)
        doc_snippets=1
        shift
        ;;
      --no-context-doc-snippets|--no-doc-snippets)
        doc_snippets=0
        shift
        ;;
      --sample-lines)
        sample_lines="${2:-}"
        shift 2
        ;;
      --token-limit)
        token_limit="${2:-}"
        token_limit_override=1
        shift 2
        ;;
      --diff-repeat-limit)
        diff_repeat_limit_override="${2:-}"
        [[ -n "$diff_repeat_limit_override" ]] || die "--diff-repeat-limit requires a value"
        [[ "$diff_repeat_limit_override" =~ ^[0-9]+$ ]] || die "Invalid --diff-repeat-limit value: ${diff_repeat_limit_override}"
        export GC_CODEX_DIFF_REPEAT_LIMIT="$diff_repeat_limit_override"
        shift 2
        ;;
      --idle-timeout)
        idle_timeout="${2:-}"
        shift 2
        ;;
      --)
        shift
        break
        ;;
      -h|--help)
        cat <<'EOHELP'
Usage: gpt-creator work-on-tasks [options]

Execute tasks from the project SQLite backlog using Codex, with resumable progress.

Options:
  --project PATH       Project root (defaults to current directory)
  --story ID|SLUG      Start from the matching story id or slug (inclusive)
  --from-task REF      Resume from task REF (task id or story-slug:position) and continue forward
  --fresh              Ignore saved progress and start from the first story
  --no-verify          Skip running gpt-creator verify after tasks complete
  --keep-artifacts     Retain Codex prompt/output files for each task (default cleans up)
  --memory-cycle       Process one task at a time, prune caches, and auto-resume to limit memory usage
  --batch-size NUM     Process at most NUM tasks this run, then pause (default: unlimited)
  --sleep-between SEC  Sleep SEC seconds between tasks (default: 0)
  --context-lines NUM  Include the last NUM lines of shared context in each prompt (default: 400)
  --context-none       Skip attaching shared context to task prompts
  --context-file-lines NUM
                        Limit each shared-context file to NUM lines before summarising (default: 200)
  --context-skip GLOB  Ignore matching files when building shared context (repeatable)
  --prompt-compact     Use the compact instruction/schema block (default setting)
  --prompt-expanded    Restore the legacy verbose instruction/schema block
  --context-doc-snippets
                        (default) Pull scoped excerpts for referenced docs/endpoints when available
  --no-context-doc-snippets
                        Disable doc-snippet mode and include staged docs verbatim
  --sample-lines NUM   Include at most NUM chunks of minified sample payloads (default: 0; increase to view raw content)
  --token-limit NUM    Stop after a task exceeds NUM Codex tokens (default: 0 → unlimited)
  --diff-repeat-limit NUM
                        Override Codex diff repeat guard threshold (default: GC_CODEX_DIFF_REPEAT_LIMIT or 6)
  --idle-timeout SEC   Abort the run if no forward progress occurs for SEC seconds (default: 0 → disabled)
EOHELP
        return 0
        ;;
      *)
        die "Unknown work-on-tasks option: ${1}"
        ;;
    esac
  done

  if [[ $# -gt 0 ]]; then
    die "Unexpected argument for work-on-tasks: ${1}"
  fi

  [[ "$batch_size" =~ ^[0-9]+$ ]] || die "Invalid --batch-size value: ${batch_size}"
  [[ "$sleep_between" =~ ^[0-9]+$ ]] || die "Invalid --sleep-between value: ${sleep_between}"
  [[ "$context_lines" =~ ^[0-9]+$ ]] || die "Invalid --context-lines value: ${context_lines}"
  [[ "$context_file_lines" =~ ^[0-9]+$ ]] || die "Invalid --context-file-lines value: ${context_file_lines}"
  [[ "$sample_lines" =~ ^-?[0-9]+$ ]] || die "Invalid --sample-lines value: ${sample_lines}"
  if [[ -z "$idle_timeout" ]]; then
    idle_timeout=0
  elif [[ "$idle_timeout" =~ ^[0-9]+$ ]]; then
    :
  else
    die "Invalid --idle-timeout value: ${idle_timeout}"
  fi
  if [[ -z "$token_limit" ]]; then
    token_limit=0
  elif [[ "$token_limit" =~ ^[0-9]+$ ]]; then
    :
  else
    if (( token_limit_override )); then
      die "Invalid --token-limit value: ${token_limit}"
    else
      warn "GC_CODEX_MAX_TOKENS_PER_TASK ('${token_limit}') is not numeric; disabling per-task token guard."
      token_limit=0
    fi
  fi
  batch_size=$((batch_size))
  sleep_between=$((sleep_between))
  context_lines=$((context_lines))
  context_file_lines=$((context_file_lines))
  sample_lines=$((sample_lines))
  token_limit=$((token_limit))
  idle_timeout=$((idle_timeout))
  (( sample_lines >= 0 )) || die "--sample-lines must be zero or positive (got ${sample_lines})"

  local token_budget_raw="${GC_TOKENS_PER_TASK_BUDGET:-${GC_PER_TASK_TOKEN_BUDGET:-}}"
  local token_budget=0
  if [[ -n "${token_budget_raw:-}" ]]; then
    token_budget="$(gc_parse_int "$token_budget_raw" "$token_limit")"
  else
    if (( token_limit > 0 )); then
      token_budget="$token_limit"
    else
      token_budget=100000
    fi
  fi
  token_budget=$((token_budget))
  if (( token_budget < 0 )); then
    token_budget=0
  fi
  token_limit="$token_budget"

  local token_soft_ratio_raw="${GC_TOKENS_SOFT_RATIO:-0.9}"
  local token_soft_threshold=0
  if (( token_limit > 0 )); then
    token_soft_threshold=$(python3 - "$token_limit" "$token_soft_ratio_raw" <<'PY'
import sys
budget = int(sys.argv[1])
try:
    ratio = float(sys.argv[2])
except Exception:
    ratio = 0.9
if ratio <= 0:
    print(0)
else:
    if ratio > 1:
        ratio = 1.0
    print(int(budget * ratio))
PY
    ) || token_soft_threshold=0
  fi

  local token_min_headroom
  token_min_headroom="$(gc_parse_int "${GC_TOKENS_MIN_HEADROOM:-3000}" 3000)"
  local token_next_estimate
  token_next_estimate="$(gc_parse_int "${GC_TOKENS_NEXT_STEP_ESTIMATE:-5000}" 5000)"

  export GC_CODEX_MAX_TOKENS_PER_TASK="$token_limit"
  if (( token_limit > 0 )); then
    info "Per-task Codex token guard active → ${token_limit} tokens."
  fi
  if (( idle_timeout > 0 )); then
    info "Idle watchdog active → ${idle_timeout}s without progress."
  fi
  if [[ -n "$diff_repeat_limit_override" ]]; then
    info "Codex diff repeat guard limit → ${diff_repeat_limit_override} diff(s)."
  fi

  local diff_guard_stall_limit="${GC_DIFF_GUARD_STALL_LIMIT:-1}"
  local diff_guard_token_threshold="${GC_DIFF_GUARD_TOKEN_THRESHOLD:-12}"
  local diff_guard_stdout_slice="${GC_DIFF_GUARD_STDOUT_SLICE:-2048}"
  local diff_guard_file_cooldown="${GC_DIFF_GUARD_FILE_COOLDOWN:-2}"
  local diff_guard_file_min_bytes="${GC_DIFF_GUARD_FILE_MIN_BYTES:-120}"
  local diff_guard_turn_repeat_limit="${GC_DIFF_GUARD_TURN_REPEAT_LIMIT:-3}"
  local diff_guard_history_limit="${GC_DIFF_GUARD_HISTORY_LIMIT:-200}"
  if ! [[ "$diff_guard_stall_limit" =~ ^-?[0-9]+$ ]]; then
    diff_guard_stall_limit=1
  fi
  if ! [[ "$diff_guard_token_threshold" =~ ^[0-9]+$ ]]; then
    diff_guard_token_threshold=12
  fi
  if ! [[ "$diff_guard_stdout_slice" =~ ^[0-9]+$ ]]; then
    diff_guard_stdout_slice=2048
  fi
  if ! [[ "$diff_guard_file_cooldown" =~ ^[0-9]+$ ]]; then
    diff_guard_file_cooldown=2
  fi
  if ! [[ "$diff_guard_file_min_bytes" =~ ^[0-9]+$ ]]; then
    diff_guard_file_min_bytes=120
  fi
  if ! [[ "$diff_guard_turn_repeat_limit" =~ ^[0-9]+$ ]]; then
    diff_guard_turn_repeat_limit=3
  fi
  if ! [[ "$diff_guard_history_limit" =~ ^[0-9]+$ ]]; then
    diff_guard_history_limit=200
  fi
  export GC_DIFF_GUARD_STDOUT_SLICE="$diff_guard_stdout_slice"
  local diff_guard_history=""
  local diff_guard_global_attempt=0

  if [[ -n "$start_task_ref" && $resume -eq 0 ]]; then
    info "--fresh ignored when --from-task is provided; resuming from the specified task instead."
    resume=1
  fi

  if (( memory_cycle )); then
    if (( batch_size == 0 || batch_size > 1 )); then
      info "Memory-cycle enabled; forcing --batch-size 1 for iterative runs."
    fi
    batch_size=1
  fi

  ensure_ctx "$root"
  local tasks_dir="${PLAN_DIR}/tasks"
  local tasks_db="${tasks_dir}/tasks.db"
  mkdir -p "$tasks_dir"
  export GC_DOCUMENTATION_DB_PATH="$tasks_db"
  if ! gc_require_documentation_catalog "$PROJECT_ROOT"; then
    local catalog_root="${PROJECT_ROOT:-$PWD}"
    die "Failed to prepare documentation catalog. Run 'gpt-creator scan --project \"${catalog_root}\"' and retry."
  fi
  gc_bootstrap_docs_registry "$tasks_db"

  if ! gc_tasks_db_has_rows "$tasks_db"; then
    die "Task database missing or empty. Run 'gpt-creator create-tasks' (or create-jira-tasks + migrate-tasks) before work-on-tasks."
  fi

  local now_ts
  now_ts="$(date +%s)"
  throughput_next_checkpoint=$((now_ts + throughput_checkpoint_interval))

  local throughput_msg=""
  if throughput_msg="$(gc_update_throughput_metrics "$tasks_db" "flush")"; then
    if [[ -n "$throughput_msg" ]]; then
      info "$throughput_msg"
    fi
  else
    warn "Failed to prime throughput metrics (flush)."
  fi
  if ! gc_update_throughput_metrics "$tasks_db" "init" >/dev/null; then
    warn "Failed to start throughput metrics window."
  fi

  gc_align_task_story_slugs "$tasks_db"
  gc_sync_story_totals "$tasks_db"

  if (( force_reset )); then
    info "Resetting backlog progress to pending (--force)."
    gc_reset_task_progress "$tasks_db"
    gc_sync_story_totals "$tasks_db"
  fi

  local start_task_story_slug="" start_task_story_title="" start_task_position="" start_task_id="" start_task_title=""
  if [[ -n "$start_task_ref" ]]; then
    info "Rewinding backlog starting from task reference '${start_task_ref}'."
    local rewind_info=""
    local original_story_filter="$story_filter"
    if ! rewind_info="$(gc_rewind_backlog_from_task "$tasks_db" "$start_task_ref" "$original_story_filter")"; then
      die "Unable to rewind backlog from task reference '${start_task_ref}'."
    fi
    IFS=$'\t' read -r start_task_story_slug start_task_story_title start_task_position start_task_id start_task_title <<<"$rewind_info"
    if [[ -z "$start_task_story_slug" || -z "$start_task_position" ]]; then
      die "Invalid response while rewinding backlog; aborting."
    fi
    if [[ -n "$original_story_filter" && "${original_story_filter,,}" != "${start_task_story_slug,,}" ]]; then
      info "Normalizing story filter '${original_story_filter}' to story slug '${start_task_story_slug}'."
    fi
    story_filter="$start_task_story_slug"
    gc_sync_story_totals "$tasks_db"
    local story_display="$start_task_story_slug"
    if [[ -n "$start_task_story_title" && "${start_task_story_title,,}" != "${start_task_story_slug,,}" ]]; then
      story_display+=" — ${start_task_story_title}"
    fi
    info "Starting from task ${start_task_position} (${start_task_id:-no-id}) in story ${story_display}."
    if [[ -n "$start_task_title" ]]; then
      info "  ${start_task_title}"
    fi
  fi

  ensure_node_dependencies "$PROJECT_ROOT"
  gc_refresh_discovery_if_needed
  gc_clear_active_task

  if (( prompt_compact )); then
    export GC_PROMPT_COMPACT=1
  else
    unset GC_PROMPT_COMPACT
  fi
  if (( doc_snippets )); then
    export GC_PROMPT_DOC_SNIPPETS=1
  else
    unset GC_PROMPT_DOC_SNIPPETS
  fi

  local state_dir="${PLAN_DIR}/work"
  local runs_dir="${state_dir}/runs"
  local work_logs_root="${LOG_DIR}/work-on-tasks"
  mkdir -p "$runs_dir" "$work_logs_root"

  GC_CONTEXT_FILE_LINES="$context_file_lines"
  if ((${#context_skip_patterns[@]} > 0)); then
    GC_CONTEXT_SKIP_PATTERNS=("${context_skip_patterns[@]}")
  else
    unset GC_CONTEXT_SKIP_PATTERNS
  fi

  export GC_PROMPT_SAMPLE_LINES="$sample_lines"

  local run_stamp
  run_stamp="$(date +%Y%m%d_%H%M%S)"
  local run_dir="${runs_dir}/${run_stamp}"
  local run_log_dir="${work_logs_root}/${run_stamp}"
  mkdir -p "$run_dir" "$run_log_dir"
  local doc_catalog_path="${state_dir}/doc-catalog.json"
  if [[ ! -f "$doc_catalog_path" ]]; then
    mkdir -p "$(dirname "$doc_catalog_path")"
    printf '{"version":1,"documents":{}}\n' >"$doc_catalog_path"
  fi
  export GC_DOC_CATALOG_PATH="$doc_catalog_path"
  local last_progress_ts
  local idle_timeout_triggered=0
  local ctx_file="${run_dir}/context.md"
  gc_build_context_file "$ctx_file" "$STAGING_DIR"
  local context_tail=""
  local context_tail_mode="none"
  export GC_CONTEXT_TAIL_LIMIT="$context_lines"
  if (( context_lines > 0 )); then
    context_tail_mode="digest"
    context_tail="${run_dir}/context_digest.md"
    if ! gc_build_context_digest "$ctx_file" "$context_tail" "$context_lines"; then
      warn "Failed to build context digest; falling back to raw tail."
      context_tail_mode="raw"
      context_tail="${run_dir}/context_tail.md"
      if ! tail -n "$context_lines" "$ctx_file" >"$context_tail" 2>/dev/null; then
        cp "$ctx_file" "$context_tail"
      fi
    fi
  fi
  GC_CONTEXT_TAIL_MODE="$context_tail_mode"
  export GC_CONTEXT_TAIL_MODE

  local context_lines_current="$context_lines"
  local context_file_lines_current="$context_file_lines"
  local context_lines_min=0
  local context_file_lines_min=0
  local context_lines_min_default="${GC_CONTEXT_MIN_LINES:-80}"
  local context_file_lines_min_default="${GC_CONTEXT_MIN_FILE_LINES:-60}"
  if ! [[ "$context_lines_min_default" =~ ^[0-9]+$ ]]; then
    context_lines_min_default=80
  fi
  if ! [[ "$context_file_lines_min_default" =~ ^[0-9]+$ ]]; then
    context_file_lines_min_default=60
  fi
  if (( context_lines_current > 0 )); then
    context_lines_min="$context_lines_min_default"
    if (( context_lines_current < context_lines_min )); then
      context_lines_min="$context_lines_current"
    fi
  fi
  if (( context_file_lines_current > 0 )); then
    context_file_lines_min="$context_file_lines_min_default"
    if (( context_file_lines_current < context_file_lines_min )); then
      context_file_lines_min="$context_file_lines_current"
    fi
  fi
  local context_shrink_iterations=0
  local context_last_shrink_tokens=0
  local context_auto_shrink_threshold="${GC_CONTEXT_AUTO_SHRINK_THRESHOLD:-60000}"
  if ! [[ "$context_auto_shrink_threshold" =~ ^[0-9]+$ ]]; then
    context_auto_shrink_threshold=60000
  fi
  if (( token_limit > 0 )); then
    local derived_threshold=$(( (token_limit * 85) / 100 ))
    if (( derived_threshold <= 0 )); then
      derived_threshold="$token_limit"
    fi
    if (( derived_threshold >= token_limit )); then
      local delta=$(( token_limit / 10 ))
      (( delta < 1 )) && delta=1
      derived_threshold=$(( token_limit - delta ))
    fi
    if (( derived_threshold <= 0 )); then
      derived_threshold="$token_limit"
    fi
    context_auto_shrink_threshold="$derived_threshold"
  fi

  info "Work run directory → ${run_dir}"

  local resume_flag=1
  [[ $resume -eq 1 ]] || resume_flag=0

  local work_failed=0
  local any_changes=0
  local manual_followups=0
  local usage_limit_triggered=0
  local batch_limit_reached=0

  gc_touch_progress() {
    last_progress_ts="$(date +%s)"
  }

  gc_check_idle_timeout() {
    if (( idle_timeout > 0 )); then
      local now_ts
      now_ts="$(date +%s)"
      if (( now_ts - last_progress_ts >= idle_timeout )); then
        if (( idle_timeout_triggered == 0 )); then
          idle_timeout_triggered=1
          manual_followups=1
          work_failed=1
          warn "Idle timeout reached (${idle_timeout}s without progress); halting run."
        fi
        return 1
      fi
    fi
    return 0
  }

  gc_auto_commit_task() {
    local commit_message="${1:-}"
    shift || true
    local -a raw_paths=("$@")
    GC_LAST_AUTO_COMMIT_HASH=""
    GC_LAST_AUTO_COMMIT_STATUS="skipped"

    if [[ -z "${PROJECT_ROOT:-}" ]]; then
      info "    Auto-commit skipped: project root unavailable."
      return 0
    fi
    if ! command -v git >/dev/null 2>&1; then
      info "    Auto-commit skipped: git command not found."
      return 0
    fi
    if ! git -C "$PROJECT_ROOT" rev-parse --is-inside-work-tree >/dev/null 2>&1; then
      info "    Auto-commit skipped: not a git repository."
      return 0
    fi

    local -a stage_paths=()
    local -A seen_stage_paths=()
    local raw path
    for raw in "${raw_paths[@]}"; do
      path="$raw"
      path="${path%% (*}"
      while [[ "$path" == " "* ]]; do
        path="${path# }"
      done
      while [[ "$path" == *" " ]]; do
        path="${path% }"
      done
      if [[ "$path" == "$PROJECT_ROOT/"* ]]; then
        path="${path#$PROJECT_ROOT/}"
      fi
      [[ -n "$path" ]] || continue
      if [[ "$path" == *.rej ]]; then
        continue
      fi
      if [[ "$path" == tmp/* ]]; then
        continue
      fi
      case "$path" in
        .gpt-creator/staging/plan/work/runs/*|.gpt-creator/work/*|.gpt-creator/logs/*|.gpt-creator/cache/*)
          continue
          ;;
      esac
      if [[ -z "${seen_stage_paths[$path]:-}" ]]; then
        stage_paths+=("$path")
        seen_stage_paths[$path]=1
      fi
    done

    if (( ${#stage_paths[@]} == 0 )); then
      info "    Auto-commit skipped: no paths to stage."
      GC_LAST_AUTO_COMMIT_STATUS="clean"
      return 0
    fi

    local sanitized_message="$commit_message"
    if command -v python3 >/dev/null 2>&1; then
      sanitized_message="$(python3 - "$commit_message" <<'PY'
import sys
msg = sys.argv[1]
msg = ' '.join(msg.replace('\r', ' ').replace('\n', ' ').split())
print((msg[:69] + '...') if len(msg) > 72 else msg)
PY
      )" || sanitized_message="$commit_message"
    else
      sanitized_message="${sanitized_message//$'\r'/ }"
      sanitized_message="${sanitized_message//$'\n'/ }"
    fi

    local add_output
    if ! add_output="$(git -C "$PROJECT_ROOT" add -- "${stage_paths[@]}" 2>&1)"; then
      warn "    Auto-commit staging failed: ${add_output}"
      GC_LAST_AUTO_COMMIT_STATUS="failed"
      return 0
    fi

    if git -C "$PROJECT_ROOT" diff --cached --quiet --exit-code; then
      info "    Auto-commit skipped: staged state clean."
      GC_LAST_AUTO_COMMIT_STATUS="clean"
      return 0
    fi

    local commit_output
    if ! commit_output="$(git -C "$PROJECT_ROOT" commit -m "$sanitized_message" 2>&1)"; then
      warn "    Auto-commit failed: ${commit_output}"
      GC_LAST_AUTO_COMMIT_STATUS="failed"
      return 0
    fi

    local commit_hash
    commit_hash="$(git -C "$PROJECT_ROOT" rev-parse HEAD 2>/dev/null)" || commit_hash=""
    GC_LAST_AUTO_COMMIT_HASH="$commit_hash"
    GC_LAST_AUTO_COMMIT_STATUS="committed"
    if [[ -n "$commit_hash" ]]; then
      info "    Auto-commit ${commit_hash:0:7}: ${sanitized_message}"
    else
      info "    Auto-commit created: ${sanitized_message}"
    fi
    return 0
  }

  gc_diff_fingerprint() {
    if [[ -z "${PROJECT_ROOT:-}" ]]; then
      printf 'no-project-%s\n' "$RANDOM$RANDOM$$"
      return 0
    fi
    if ! command -v git >/dev/null 2>&1; then
      printf 'nogit-%s\n' "$RANDOM$RANDOM$$"
      return 0
    fi
    local diff_payload
    if ! diff_payload="$( (git -C "$PROJECT_ROOT" diff --shortstat --no-ext-diff || true; printf '\n'; git -C "$PROJECT_ROOT" diff --numstat --no-ext-diff || true; printf '\n'; git -C "$PROJECT_ROOT" diff --name-only --no-ext-diff || true) 2>/dev/null )"; then
      printf 'diff-error-%s\n' "$RANDOM$RANDOM$$"
      return 0
    fi
    if command -v python3 >/dev/null 2>&1; then
      python3 - "$diff_payload" <<'PY'
import hashlib, sys
data = sys.argv[1].encode('utf-8', 'replace')
print(hashlib.sha1(data).hexdigest())
PY
    elif command -v shasum >/dev/null 2>&1; then
      printf '%s' "$diff_payload" | shasum -a 1 2>/dev/null | awk '{print $1}'
    elif command -v sha1sum >/dev/null 2>&1; then
      printf '%s' "$diff_payload" | sha1sum 2>/dev/null | awk '{print $1}'
    else
      printf 'hash-%s\n' "$(printf '%s' "$diff_payload" | wc -c | awk '{print $1}')"
    fi
  }

  gc_create_empty_apply_checkpoint() {
    local story_slug="$1"
    local task_id="$2"
    local root="${PROJECT_ROOT:-$PWD}"
    [[ -n "$story_slug" ]] || story_slug="story"
    [[ -n "$task_id" ]] || task_id="task"
    local checkpoint_dir="${root}/docs/qa/evidence/${story_slug}/${task_id}"
    local checkpoint_file="${checkpoint_dir}/checkpoint.md"
    mkdir -p "$checkpoint_dir" || return 1
    if [[ ! -f "$checkpoint_file" ]]; then
      cat >"$checkpoint_file" <<EOF
# Checkpoint — ${story_slug}/${task_id}

Generated automatically because the agent response contained no actionable `changes`.

- Timestamp: $(date -u +%FT%TZ)
- Runner: gc-empty-apply-fallback

## What was attempted
- Review the raw agent output under .gpt-creator/staging/plan/work/runs for detailed context.
EOF
    fi
    git -C "$root" add "$checkpoint_file" >/dev/null 2>&1 || true
    python3 - <<'PY' "$checkpoint_file" "$root"
import sys
from pathlib import Path
file_path = Path(sys.argv[1])
root = Path(sys.argv[2])
try:
    rel = file_path.relative_to(root)
except ValueError:
    rel = file_path
print(rel)
PY
  }

  gc_touch_progress

  local processed_total=0
  local processed_any_total=0
  local remaining_tasks=0
  local memory_cycle_single=0

  while :; do
    if gc_check_idle_timeout; then :; else break; fi
    local iteration_processed_any=0
    local iteration_processed=0
    local continue_current_run=0
    local pending_tasks=0

    local effective_batch_size="$batch_size"
    if (( memory_cycle )); then
      if (( memory_cycle_single )); then
        effective_batch_size=1
      else
        effective_batch_size="$batch_size"
      fi
    fi

    while IFS=$'\t' read -r sequence slug story_id story_title epic_id epic_title total_tasks next_task completed status; do
      if [[ -z "${sequence}${slug}${story_id}${story_title}${epic_id}${epic_title}" ]]; then
        continue
      fi

      if ! gc_check_idle_timeout; then
        break
      fi

      iteration_processed_any=1

      if (( throughput_checkpoint_interval > 0 )); then
        now_ts="$(date +%s)"
        if (( throughput_next_checkpoint == 0 || now_ts >= throughput_next_checkpoint )); then
          local throughput_checkpoint_msg=""
          if throughput_checkpoint_msg="$(gc_update_throughput_metrics "$tasks_db" "checkpoint")"; then
            if [[ -n "$throughput_checkpoint_msg" ]]; then
              info "$throughput_checkpoint_msg"
            fi
          else
            warn "Failed to checkpoint throughput metrics."
          fi
          throughput_next_checkpoint=$((now_ts + throughput_checkpoint_interval))
        fi
      fi

    : "${total_tasks:=0}"; : "${next_task:=0}"
    local total_tasks_int=0
    if [[ "$total_tasks" =~ ^[0-9]+$ ]]; then
      total_tasks_int=$((total_tasks))
    fi
    local next_task_int=0
    if [[ "$next_task" =~ ^[0-9]+$ ]]; then
      next_task_int=$((next_task))
    fi

    printf -v story_prefix "%03d" "${sequence:-0}"
    [[ -n "$slug" ]] || slug="story-${story_prefix}"
    local story_run_dir="${run_dir}/story_${story_prefix}_${slug}"
    local story_log_dir="${run_log_dir}/story_${story_prefix}_${slug}"
    mkdir -p "${story_run_dir}/prompts" "${story_run_dir}/out" "${story_run_dir}/reports" "$story_log_dir"
    local report_dir="${story_run_dir}/reports"

    info "Story ${story_prefix} (${story_id:-$slug}) — ${story_title:-Unnamed}"

    if (( total_tasks_int == 0 )); then
      local stats=""
      if stats="$(gc_fetch_story_task_counts "$tasks_db" "$slug" 2>/dev/null)"; then
        local actual_total=0 actual_completed=0
        IFS=$'\t' read -r actual_total actual_completed <<<"$stats"
        if [[ "$actual_total" =~ ^[0-9]+$ ]] && (( actual_total > 0 )); then
          info "  Found ${actual_total} task(s) in backlog despite zero metadata; synchronising."
          total_tasks_int=$actual_total
          completed="${actual_completed}"
          if [[ "$completed" =~ ^[0-9]+$ ]]; then
            (( completed > actual_total )) && completed="$actual_total"
            if (( next_task_int < completed )); then
              next_task_int=$completed
            fi
          fi
          gc_update_work_state "$tasks_db" "$slug" "pending" "$completed" "$total_tasks_int" "$run_stamp"
        fi
      fi
    fi

    if (( total_tasks_int == 0 )); then
      info "  No tasks for this story; marking complete."
      gc_update_work_state "$tasks_db" "$slug" "complete" 0 0 "$run_stamp"
      if (( keep_artifacts == 0 )); then
        rmdir "${story_run_dir}/prompts" 2>/dev/null || true
        rmdir "${story_run_dir}/out" 2>/dev/null || true
      fi
      continue
    fi

    gc_update_work_state "$tasks_db" "$slug" "in-progress" "$next_task_int" "$total_tasks_int" "$run_stamp"

    local task_index
    local story_failed=0
    for (( task_index = next_task_int; task_index < total_tasks_int; task_index++ )); do
      gc_clear_active_task
      if ! gc_check_idle_timeout; then
        break
      fi
      if (( throughput_checkpoint_interval > 0 )); then
        now_ts="$(date +%s)"
        if (( throughput_next_checkpoint == 0 || now_ts >= throughput_next_checkpoint )); then
          local throughput_checkpoint_msg=""
          if throughput_checkpoint_msg="$(gc_update_throughput_metrics "$tasks_db" "checkpoint")"; then
            if [[ -n "$throughput_checkpoint_msg" ]]; then
              info "$throughput_checkpoint_msg"
            fi
          else
            warn "Failed to checkpoint throughput metrics."
          fi
          throughput_next_checkpoint=$((now_ts + throughput_checkpoint_interval))
        fi
      fi
      if (( effective_batch_size > 0 && iteration_processed >= effective_batch_size )); then
        batch_limit_reached=1
        break
      fi
      local task_number
      printf -v task_number "%03d" $((task_index + 1))
      local prompt_path="${story_run_dir}/prompts/task_${task_number}.prompt.md"
      local output_path="${story_run_dir}/out/task_${task_number}.out.md"

      local prompt_meta
      if ! prompt_meta="$(gc_write_task_prompt "$tasks_db" "$slug" "$task_index" "$prompt_path" "$context_tail" "$CODEX_MODEL" "$PROJECT_ROOT" "$STAGING_DIR")"; then
        warn "  Failed to build prompt for task index ${task_index}"
        work_failed=1
        break
      fi

      local task_id="" task_title="" task_story_points=""
      IFS=$'\t' read -r task_id task_title task_story_points <<<"$prompt_meta"
      local banner_task_id="${task_id:-no-id}"
      local task_start_epoch
      task_start_epoch="$(date +%s)"
      local task_tokens_total=0
      local task_duration_seconds=0
      local task_duration_display=""
      local task_tokens_display=""
      local task_story_points_display="—"
      if [[ -n "$task_story_points" ]]; then
        task_story_points_display="$task_story_points"
      fi

      printf '\n'
      gc_render_task_banner "START OF A NEW TASK" "START TASK ID" "$banner_task_id"
      info "  → Working on task ${task_number} (${banner_task_id})"
      info "    ${task_title:-(untitled)}"

      local call_name="story-${slug}-task-${task_number}"
      local codex_ok=0
      local attempt=0
      local max_attempts=2
      gc_touch_progress
      local prompt_augmented=0 prompt_change_reminder=0
      local keep_output=$keep_artifacts
      local break_after_update=0
      local task_result_status="in-progress"
      local task_needs_review=0
      local -a task_notes=()
      local -a task_written_paths=()
      local -a task_patched_paths=()
      local -a task_commands=()
      local task_changes_applied=0
      local apply_status="pending"
      local task_report_path="${report_dir}/task_${task_number}.log"
      local task_log_archive_path="${story_log_dir}/task_${task_number}.log"
      local task_change_sizes=""
      local diff_last_transition=""
      local diff_prev_after_sig=""
      local diff_prev_prev_after_sig=""
      local diff_stall_count=0
      local turn_diff_prev_hash=""
      local turn_diff_repeat_count=0
      local diff_guard_enabled=1
      if (( diff_guard_stall_limit <= 0 && diff_guard_file_cooldown <= 0 && diff_guard_turn_repeat_limit <= 0 )); then
        diff_guard_enabled=0
      fi
      local token_soft_warned=0
      local token_lean_attempted=0
      local empty_checkpoint_created=0
      gc_update_task_state "$tasks_db" "$slug" "$task_index" "in-progress" "$run_stamp"
      export GC_ACTIVE_TASK_DB="$tasks_db"
      export GC_ACTIVE_TASK_SLUG="$slug"
      export GC_ACTIVE_TASK_INDEX="$task_index"
      export GC_ACTIVE_RUN_STAMP="$run_stamp"
      export GC_ACTIVE_TASK_NUMBER="$task_number"
      export GC_ACTIVE_TASK_ID="$task_id"
      export GC_ACTIVE_TASK_REPORT="$task_report_path"
      export GC_ACTIVE_TASK_ARCHIVE="$task_log_archive_path"
      export GC_ACTIVE_TASK_PROMPT="$prompt_path"
      export GC_ACTIVE_TASK_OUTPUT="$output_path"

      while (( attempt < max_attempts )); do
        (( ++attempt ))
        (( ++diff_guard_global_attempt ))
        local diff_guard_current_step="$diff_guard_global_attempt"
        gc_touch_progress
        if (( token_budget > 0 )); then
          local token_budget_remaining=$(( token_budget - task_tokens_total ))
          if (( token_soft_threshold > 0 )) && (( token_soft_warned == 0 )) && (( task_tokens_total >= token_soft_threshold )); then
            local token_usage_pct=0
            if (( token_budget > 0 )); then
              token_usage_pct=$(( (task_tokens_total * 100) / token_budget ))
            fi
            warn "  Token usage ${task_tokens_total}/${token_budget} (~${token_usage_pct}%%); nearing per-task budget."
            token_soft_warned=1
          fi
          local token_required=$token_min_headroom
          if (( token_next_estimate > token_required )); then
            token_required=$token_next_estimate
          fi
          if (( token_budget_remaining <= token_required )); then
            if (( token_lean_attempted == 0 )); then
              warn "  Token budget nearly exhausted; pruning prompt for lean retry."
              gc_trim_prompt_file_lean "$prompt_path"
              token_lean_attempted=1
              task_notes+=("Lean retry initiated after trimming prompt context for token budget.")
              (( attempt-- ))
              continue
            fi
            warn "  Token budget hit; stopping this task early → Status: BLOCKED_BUDGET."
            task_result_status="blocked-budget"
            task_notes+=("Token budget exhausted after lean retry; trim scope or increase CODEX_MAX_PROMPT_TOKENS.")
            codex_ok=1
            break_after_update=1
            break
          fi
        fi
        local diff_before=""
        task_change_sizes=""
        if (( diff_guard_enabled )); then
          diff_before="$(gc_diff_fingerprint)"
        fi
        if codex_call "$call_name" --prompt "$prompt_path" --output "$output_path"; then
          local attempt_tokens="${GC_CODEX_CALL_TOKEN_ACCUM:-${GC_LAST_CODEX_TOTAL_TOKENS:-0}}"
          if [[ "$attempt_tokens" =~ ^[0-9]+$ ]]; then
            task_tokens_total=$((task_tokens_total + attempt_tokens))
          fi
          if [[ ! -s "$output_path" ]]; then
            local checkpoint_story="${story_id:-$slug}"
            local checkpoint_task="${task_id:-$banner_task_id}"
            local checkpoint_rel=""
            if (( empty_checkpoint_created == 0 )) && checkpoint_rel="$(gc_create_empty_apply_checkpoint "$checkpoint_story" "$checkpoint_task")"; then
              empty_checkpoint_created=1
              info "    Created checkpoint evidence ${checkpoint_rel} because Codex returned no output."
              local checkpoint_abs="${PROJECT_ROOT:-$PWD}/${checkpoint_rel}"
              local checkpoint_size
              checkpoint_size="$(python3 - <<'PY' "$checkpoint_abs"
import sys
from pathlib import Path
path = Path(sys.argv[1])
print(path.stat().st_size if path.exists() else 0)
PY
)"
              task_change_sizes+="${checkpoint_rel}"$'\t'"${checkpoint_size}"$'\n'
              task_written_paths+=("$checkpoint_rel")
              task_notes+=("Auto-generated checkpoint evidence ${checkpoint_rel} because the Codex reply was empty.")
              task_changes_applied=1
              any_changes=1
              apply_status="ok"
              task_needs_review=0
              manual_followups=0
              keep_output=0
              task_result_status="complete"
              codex_ok=1
              break
            fi
            warn "  Codex produced no output for ${call_name}; manual review required."
            task_needs_review=1
            manual_followups=1
            keep_output=1
            task_result_status="on-hold"
            task_notes+=("Codex produced no output; manual review required.")
            codex_ok=1
            break
          fi
          local apply_output
          if ! apply_output="$(gc_apply_codex_changes "$output_path" "$PROJECT_ROOT")"; then
            warn "  Failed to apply changes for ${call_name}; manual review required (see ${output_path})."
            task_needs_review=1
            manual_followups=1
            keep_output=1
            task_result_status="on-hold"
            task_notes+=("Codex changes could not be applied automatically; review ${output_path}.")
            codex_ok=1
            break
          fi
          if [[ "$apply_output" == "no-output" || "$apply_output" == "empty-output" ]]; then
            warn "  Codex produced no actionable JSON for ${call_name}; skipping task."
            task_needs_review=0
            manual_followups=0
            keep_output=0
            task_result_status="skipped-no-changes"
            task_notes+=("No actionable changes; auto-skip.")
            codex_ok=1
            break
          fi
          apply_status="ok"
          while IFS= read -r change_line; do
            case "$change_line" in
              STATUS\ *)
                apply_status="${change_line#STATUS }"
                ;;
              APPLIED)
                info "    Changes applied."
                ;;
              WRITE\ *)
                local written_path="${change_line#WRITE }"
                info "    Wrote ${written_path}"
                any_changes=1
                task_changes_applied=1
                task_written_paths+=("$written_path")
                ;;
              PATCH\ *)
                local patched_path="${change_line#PATCH }"
                info "    Patched ${patched_path}"
                any_changes=1
                task_changes_applied=1
                task_patched_paths+=("$patched_path")
                ;;
              SIZE\ *)
                local size_entry="${change_line#SIZE }"
                local size_path="${size_entry%%$'\t'*}"
                local size_bytes="${size_entry#*$'\t'}"
                if [[ -z "$size_path" || "$size_path" == "$size_bytes" ]]; then
                  size_bytes="${size_entry##* }"
                  size_path="${size_entry%% *}"
                fi
                task_change_sizes+="${size_path}"$'\t'"${size_bytes}"$'\n'
                ;;
              NOOP\ *)
                local noop_path="${change_line#NOOP }"
                warn "    No-op: ${noop_path}"
                task_notes+=("Codex proposed change already applied: ${noop_path}")
                ;;
              CMD\ *)
                local suggested_cmd="${change_line#CMD }"
                info "    Suggested command: ${suggested_cmd}"
                task_commands+=("$suggested_cmd")
                ;;
              NOTE\ *)
                local note_text="${change_line#NOTE }"
                warn "    Note: ${note_text}"
                task_notes+=("$note_text")
                if gc_note_requires_followup "$note_text"; then
                  task_needs_review=1
                fi
                ;;
            esac
          done <<<"$apply_output"

          if (( task_changes_applied == 0 )); then
            if (( empty_checkpoint_created == 0 )); then
              local checkpoint_story="${story_id:-$slug}"
              local checkpoint_task="${task_id:-$banner_task_id}"
              local checkpoint_rel=""
              if checkpoint_rel="$(gc_create_empty_apply_checkpoint "$checkpoint_story" "$checkpoint_task")"; then
                empty_checkpoint_created=1
                info "    Created checkpoint evidence ${checkpoint_rel} to avoid empty apply."
                local checkpoint_abs="${PROJECT_ROOT:-$PWD}/${checkpoint_rel}"
                local checkpoint_size
                checkpoint_size="$(python3 - <<'PY' "$checkpoint_abs"
import sys
from pathlib import Path
path = Path(sys.argv[1])
print(path.stat().st_size if path.exists() else 0)
PY
)"
                task_change_sizes+="${checkpoint_rel}"$'\t'"${checkpoint_size}"$'\n'
                task_written_paths+=("$checkpoint_rel")
                task_notes+=("Auto-generated checkpoint evidence ${checkpoint_rel} because the agent returned no actionable changes.")
                task_changes_applied=1
                any_changes=1
                apply_status="ok"
              fi
            fi
            if (( task_changes_applied == 0 )); then
              if (( attempt < max_attempts )); then
                warn "  Codex response contained no file changes; retrying (attempt $((attempt + 1)) of ${max_attempts})."
                if (( prompt_change_reminder == 0 )); then
                  cat >>"$prompt_path" <<'REM'

## Reminder
- Provide valid unified diffs or file writes in the `changes` array for every edit; planning alone is insufficient.
- Only conclude once the required modifications are present in the `changes` payload.
REM
                  prompt_change_reminder=1
                fi
                task_changes_applied=0
                apply_status="pending"
                task_commands=()
                task_written_paths=()
                task_patched_paths=()
                continue
              fi
              warn "  Codex produced no actionable changes; skipping task."
              task_needs_review=0
              manual_followups=0
              keep_output=0
              task_result_status="skipped-no-changes"
              apply_status="no-changes"
              task_notes+=("No actionable changes; auto-skip.")
              codex_ok=1
              break_after_update=1
              break
            fi
          fi

          if [[ "$apply_status" == "parse-error" ]]; then
            if (( attempt < max_attempts )); then
              warn "  Codex returned invalid JSON; retrying (attempt $((attempt + 1)) of ${max_attempts})."
              if (( prompt_augmented == 0 )); then
                cat >>"$prompt_path" <<'REM'

## Reminder
- Output a single JSON object exactly as described above; do not include any explanatory text outside the JSON.
- If no changes are required, return the JSON with an empty `changes` array and clear notes explaining why.
- Diff entries must remain valid unified diffs.
REM
                prompt_augmented=1
              fi
              continue
            else
              warn "  Codex output was invalid JSON after retry; manual review required (see ${output_path})."
              task_needs_review=1
              manual_followups=1
              keep_output=1
              task_result_status="on-hold"
              task_notes+=("Codex output remained invalid JSON after retries; inspect ${output_path}.")
              codex_ok=1
              break_after_update=1
            fi
          fi

          if (( task_changes_applied > 0 )); then
            gc_note_mutation
          fi

          local stdout_hash="${GC_LAST_CODEX_STDOUT_TAIL_HASH:-none}"
          local stdout_base64="${GC_LAST_CODEX_STDOUT_TAIL_BASE64:-}"
          local turn_hash="${GC_LAST_CODEX_TURN_DIFF_HASH:-}"
          local turn_blocks="${GC_LAST_CODEX_TURN_DIFF_BLOCKS:-0}"
          local diff_after=""
          if (( diff_guard_enabled )); then
            diff_after="$(gc_diff_fingerprint)"
          fi
          local after_signature="${diff_after:-}:${stdout_hash}"
          local attempt_tokens_value=0
          if [[ "$attempt_tokens" =~ ^[0-9]+$ ]]; then
            attempt_tokens_value=$((attempt_tokens))
          fi
          local progress_sig="${diff_before:-}:${diff_after:-}:${stdout_hash}:${attempt_tokens_value}"
          local guard_reason=""
          if (( diff_guard_enabled )); then
            if (( attempt_tokens_value <= diff_guard_token_threshold )) && [[ "$progress_sig" == "$diff_last_transition" ]]; then
              diff_stall_count=$((diff_stall_count + 1))
            else
              diff_stall_count=0
            fi
            if (( diff_guard_stall_limit > 0 )) && (( diff_stall_count >= diff_guard_stall_limit )); then
              guard_reason="no-progress"
            fi
          fi
          if [[ -z "$guard_reason" && -n "$diff_prev_prev_after_sig" && "$after_signature" == "$diff_prev_prev_after_sig" && "$diff_prev_after_sig" != "$after_signature" ]]; then
            guard_reason="oscillation"
          fi
          if (( diff_guard_enabled )) && [[ -z "$guard_reason" ]]; then
            if [[ -n "$turn_hash" && "$turn_blocks" -gt 0 ]]; then
              if [[ "$turn_hash" == "$turn_diff_prev_hash" ]]; then
                turn_diff_repeat_count=$((turn_diff_repeat_count + 1))
              else
                turn_diff_repeat_count=1
              fi
            else
              turn_diff_repeat_count=0
            fi
            turn_diff_prev_hash="$turn_hash"
            if [[ -n "$turn_hash" ]] && (( diff_guard_turn_repeat_limit > 0 )) && (( turn_diff_repeat_count >= diff_guard_turn_repeat_limit )); then
              guard_reason="turn-diff"
            fi
          fi
          if [[ -z "$guard_reason" ]] && (( diff_guard_file_cooldown > 0 )) && [[ -n "$task_change_sizes" ]]; then
            while IFS=$'	' read -r base_path size_bytes; do
              [[ -z "$base_path" ]] && continue
              if ! [[ "$size_bytes" =~ ^[0-9]+$ ]]; then
                continue
              fi
              local change_size=$((size_bytes))
              if (( change_size >= diff_guard_file_min_bytes )); then
                continue
              fi
              local last_step=""
              while IFS=$'	' read -r hist_path hist_step _hist_bytes; do
                [[ -z "$hist_path" ]] && continue
                if [[ "$hist_path" == "$base_path" ]]; then
                  last_step="$hist_step"
                  break
                fi
              done <<<"$diff_guard_history"
              if [[ -n "$last_step" ]]; then
                local step_delta=$((diff_guard_current_step - last_step))
                if (( step_delta <= diff_guard_file_cooldown )); then
                  guard_reason="file-cooldown:${base_path}"
                  break
                fi
              fi
            done <<<"$task_change_sizes"
          fi
          if (( diff_guard_enabled )); then
            diff_prev_prev_after_sig="$diff_prev_after_sig"
            diff_prev_after_sig="$after_signature"
            diff_last_transition="$progress_sig"
          fi
          if [[ -n "$task_change_sizes" ]]; then
            local new_history=""
            while IFS=$'\t' read -r size_path size_bytes; do
              [[ -z "$size_path" ]] && continue
              new_history+="${size_path}"$'\t'"${diff_guard_current_step}"$'\t'"${size_bytes}"$'\n'
            done <<<"$task_change_sizes"
            if [[ -n "$new_history" ]]; then
              diff_guard_history="${new_history}${diff_guard_history}"
              diff_guard_history="$(printf '%s' "$diff_guard_history" | head -n "$diff_guard_history_limit")"
            fi
          fi
          if [[ -n "$guard_reason" ]]; then
            warn "    Loop guard (${guard_reason}) detected; halting task."
            task_needs_review=1
            manual_followups=1
            keep_output=1
            task_result_status="on-hold"
            apply_status="loop-guard"
            codex_ok=1
            break_after_update=1
            story_failed=1
            loop_guard_triggered=1
            work_failed=1
            local guard_log="${story_run_dir}/loop_guard_${task_number}_${attempt}.log"
            {
              printf 'LOOP_GUARD_TRIPPED\n'
              printf 'reason: %s\n' "$guard_reason"
              printf 'timestamp: %s\n' "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
              printf 'attempt_tokens: %s\n' "$attempt_tokens_value"
              printf 'diff_before: %s\n' "${diff_before:-}"
              printf 'diff_after: %s\n' "${diff_after:-}"
              printf 'stdout_hash: %s\n' "$stdout_hash"
              printf 'turn_hash: %s\n' "$turn_hash"
              printf 'turn_blocks: %s\n' "$turn_blocks"
              printf '\n-- git diff --stat --\n'
              git -C "$PROJECT_ROOT" diff --stat || true
              printf '\n-- git diff --numstat --\n'
              git -C "$PROJECT_ROOT" diff --numstat || true
              printf '\n-- stdout tail (latest %s bytes) --\n' "$diff_guard_stdout_slice"
              if [[ -n "$stdout_base64" ]]; then
                if command -v python3 >/dev/null 2>&1; then
                  python3 - <<'PY' "$stdout_base64"
import base64, sys, textwrap
data = base64.b64decode(sys.argv[1])
print(data.decode('utf-8', errors='replace'))
PY
                else
                  printf '(python3 unavailable to decode base64 tail)\n'
                fi
              else
                printf '(unavailable)\n'
              fi
            } >"$guard_log" 2>&1 || true
            local guard_rel="$guard_log"
            if [[ -n "$PROJECT_ROOT" ]]; then
              guard_rel="${guard_rel#$PROJECT_ROOT/}"
            fi
            task_notes+=("Loop guard tripped (${guard_reason}); see ${guard_rel}.")
            task_notes+=("LOOP_GUARD_TRIPPED exit=${loop_guard_exit_code}")
            break
          fi

          if (( keep_output == 0 )); then
            rm -f "$prompt_path" "$output_path"
          fi
          codex_ok=1
        else
          local codex_status=$?
          local attempt_tokens="${GC_CODEX_CALL_TOKEN_ACCUM:-${GC_LAST_CODEX_TOTAL_TOKENS:-0}}"
          if [[ "$attempt_tokens" =~ ^[0-9]+$ ]]; then
            task_tokens_total=$((task_tokens_total + attempt_tokens))
          fi
          if [[ "${GC_CODEX_USAGE_LIMIT_REACHED:-0}" == "1" ]]; then
            local limit_message="${GC_CODEX_USAGE_LIMIT_MESSAGE:-}"
            local tokens_this_task="$task_tokens_total"
            if ! [[ "$tokens_this_task" =~ ^[0-9]+$ ]]; then
              tokens_this_task=0
            fi
            local usage_limit_confirmed="${GC_CODEX_USAGE_LIMIT_CONFIRMED:-0}"

            if (( tokens_this_task == 0 )); then
              warn "  Quota suspicion but tokens=0 for ${call_name} — continuing (soft gate)."
              if [[ -n "$limit_message" ]]; then
                warn "    $limit_message"
              fi
              GC_CODEX_USAGE_LIMIT_REACHED=0
              GC_CODEX_USAGE_LIMIT_MESSAGE=""
              GC_CODEX_USAGE_LIMIT_CONFIRMED=0
              continue
            fi

            if (( usage_limit_confirmed == 0 )); then
              warn "  Codex quota warning for ${call_name} not confirmed by provider — continuing."
              if [[ -n "$limit_message" ]]; then
                warn "    $limit_message"
              fi
              GC_CODEX_USAGE_LIMIT_REACHED=0
              GC_CODEX_USAGE_LIMIT_MESSAGE=""
              GC_CODEX_USAGE_LIMIT_CONFIRMED=0
              continue
            fi

            local wait_seconds
            wait_seconds="$(python3 - "${limit_message}" <<'PY'
import re
import sys

def parse_wait(msg: str) -> int:
    msg = (msg or "").lower()
    total = 0
    for value, unit in re.findall(r'(\d+)\s*(hour|hours|hr|hrs|h|minute|minutes|min|mins|m|second|seconds|sec|secs|s)', msg):
        count = int(value)
        if unit.startswith(('hour', 'hr', 'h')):
            total += count * 3600
        elif unit.startswith(('minute', 'min', 'm')):
            total += count * 60
        elif unit.startswith(('second', 'sec', 's')):
            total += count
    return total

msg = sys.argv[1] if len(sys.argv) > 1 else ""
wait = parse_wait(msg)
if wait <= 0:
    wait = 3600
print(wait)
PY
)"
            if ! [[ "$wait_seconds" =~ ^[0-9]+$ ]]; then
              wait_seconds=3600
            fi
            local wait_minutes=$((wait_seconds / 60))
            if (( wait_minutes > 0 )); then
              warn "  Codex usage limit confirmed; pausing for approximately ${wait_minutes} minute(s) before retry."
            else
              warn "  Codex usage limit confirmed; pausing for ${wait_seconds} second(s) before retry."
            fi
            if [[ -n "$limit_message" ]]; then
              warn "    $limit_message"
            fi
            sleep "$wait_seconds" || true
            GC_CODEX_USAGE_LIMIT_REACHED=0
            GC_CODEX_USAGE_LIMIT_MESSAGE=""
            GC_CODEX_USAGE_LIMIT_CONFIRMED=0
            (( attempt-- ))
            continue
          elif (( codex_status == 124 )); then
            warn "  Codex was idle for ${codex_timeout_seconds}s during ${call_name}."
            if (( attempt < max_attempts )); then
              info "    Retrying task ${task_number} after timeout (attempt $((attempt + 1)) of ${max_attempts})."
              continue
            fi
            task_result_status="on-hold"
            task_needs_review=1
            manual_followups=1
            keep_output=1
            task_notes+=("Codex produced no output for ${codex_timeout_seconds}s; rerun work-on-tasks to continue from this task.")
            codex_ok=1
            break_after_update=1
            work_failed=1
            break
          elif (( codex_status == 125 )); then
            warn "  Codex loop guard detected repeated diffs/oscillation; halting task."
            task_result_status="on-hold"
            task_needs_review=1
            manual_followups=1
            keep_output=1
            task_notes+=("Codex loop guard triggered (repeated diffs); inspect Codex log for details.")
            apply_status="loop-guard"
            codex_ok=1
            break_after_update=1
            story_failed=1
            loop_guard_triggered=1
            work_failed=1
            break
          elif (( codex_status == 126 )); then
            warn "  Codex turn limit reached for ${call_name}; stopping task."
            task_result_status="on-hold"
            task_needs_review=1
            manual_followups=1
            keep_output=1
            task_notes+=("Codex turn limit reached; adjust focus or retry after inspecting Codex log.")
            apply_status="loop-guard"
            codex_ok=1
            break_after_update=1
            story_failed=1
            loop_guard_triggered=1
            work_failed=1
            break
          fi
          warn "  Codex execution failed for ${call_name}; progress saved."
          task_result_status="blocked"
          task_needs_review=1
          manual_followups=1
          keep_output=1
          task_notes+=("Codex execution failed; no automated changes were applied.")
          codex_ok=1
          break
        fi
        break
      done

      if (( task_needs_review )); then
        manual_followups=1
        if [[ "$task_result_status" != "blocked" ]]; then
          task_result_status="on-hold"
        fi
      fi

      if (( codex_ok == 0 )); then
        task_result_status="blocked"
        task_notes+=("Codex execution did not complete; no changes were applied.")
      fi

      if (( codex_ok )) && (( context_lines_current > context_lines_min || context_file_lines_current > context_file_lines_min )); then
        local last_tokens="${GC_LAST_CODEX_TOTAL_TOKENS:-0}"
        local shrink_threshold="$context_auto_shrink_threshold"
        local trigger_shrink=0
        if [[ "$last_tokens" =~ ^[0-9]+$ ]] && (( last_tokens > shrink_threshold )); then
          trigger_shrink=1
        elif (( token_limit > 0 )) && [[ "$last_tokens" =~ ^[0-9]+$ ]] && (( last_tokens >= token_limit )); then
          trigger_shrink=1
        fi
        if (( trigger_shrink )); then
          if (( context_shrink_iterations > 0 )) && (( last_tokens <= context_last_shrink_tokens )); then
            trigger_shrink=0
          fi
        fi
        if (( trigger_shrink )); then
          local new_context_lines="$context_lines_current"
          local new_context_file_lines="$context_file_lines_current"
          local reduced=0
          if (( context_lines_current > context_lines_min )); then
            local decrement_lines=$(( context_lines_current / 3 ))
            (( decrement_lines < 40 )) && decrement_lines=40
            new_context_lines=$(( context_lines_current - decrement_lines ))
            if (( new_context_lines < context_lines_min )); then
              new_context_lines="$context_lines_min"
            fi
            if (( new_context_lines < context_lines_current )); then
              reduced=1
            fi
          fi
          if (( context_file_lines_current > context_file_lines_min )); then
            local decrement_files=$(( context_file_lines_current / 2 ))
            (( decrement_files < 20 )) && decrement_files=20
            new_context_file_lines=$(( context_file_lines_current - decrement_files ))
            if (( new_context_file_lines < context_file_lines_min )); then
              new_context_file_lines="$context_file_lines_min"
            fi
            if (( new_context_file_lines < context_file_lines_current )); then
              reduced=1
            fi
          fi
          if (( reduced )); then
            info "    Token usage ${last_tokens} exceeded budget (~${shrink_threshold}); pruning shared context to ${new_context_lines} lines (per-file ${new_context_file_lines})."
            context_lines_current="$new_context_lines"
            context_file_lines_current="$new_context_file_lines"
            context_last_shrink_tokens="$last_tokens"
            context_shrink_iterations=$((context_shrink_iterations + 1))
            GC_CONTEXT_FILE_LINES="$context_file_lines_current"
            export GC_CONTEXT_FILE_LINES
            GC_CONTEXT_TAIL_LIMIT="$context_lines_current"
            export GC_CONTEXT_TAIL_LIMIT
            if ! gc_build_context_file "$ctx_file" "$STAGING_DIR"; then
              warn "Failed to rebuild shared context after pruning."
            else
              if [[ -n "$context_tail" && -f "$ctx_file" ]]; then
                local new_mode
                new_mode="$(gc_refresh_context_tail "$ctx_file" "$context_tail" "$context_tail_mode" "$context_lines_current")"
                if [[ "$new_mode" == "raw" && "$context_tail_mode" != "raw" ]]; then
                  context_tail="${run_dir}/context_tail.md"
                  new_mode="$(gc_refresh_context_tail "$ctx_file" "$context_tail" "raw" "$context_lines_current")"
                fi
                context_tail_mode="$new_mode"
                GC_CONTEXT_TAIL_MODE="$context_tail_mode"
                export GC_CONTEXT_TAIL_MODE
              fi
            fi
          else
            context_last_shrink_tokens="$last_tokens"
          fi
        fi
      fi

      if [[ "$task_result_status" == "in-progress" ]]; then
        task_result_status="complete"
      fi

      local task_end_epoch
      task_end_epoch="$(date +%s)"
      task_duration_seconds=$((task_end_epoch - task_start_epoch))
      if (( task_duration_seconds < 0 )); then
        task_duration_seconds=0
      fi
      task_duration_display="$(gc_format_duration_compact "$task_duration_seconds")"
      task_tokens_display="$(gc_format_tokens_compact "$task_tokens_total")"

      local story_status_hint="in-progress"
      case "$task_result_status" in
        blocked|blocked-budget) story_status_hint="blocked" ;;
        on-hold) story_status_hint="on-hold" ;;
      esac

      local completed_hint="$task_index"
      if [[ "$task_result_status" == "complete" ]]; then
        completed_hint=$((task_index + 1))
      fi

      gc_update_task_state "$tasks_db" "$slug" "$task_index" "$task_result_status" "$run_stamp"
      gc_update_work_state "$tasks_db" "$slug" "$story_status_hint" "$completed_hint" "$total_tasks_int" "$run_stamp"

      if [[ "$task_result_status" == "complete" ]]; then
        local throughput_task_msg=""
        if throughput_task_msg="$(gc_update_throughput_metrics "$tasks_db" "task-complete" "$slug" "$task_index")"; then
          if [[ -n "$throughput_task_msg" ]]; then
            info "  ${throughput_task_msg}"
            now_ts="$(date +%s)"
            throughput_next_checkpoint=$((now_ts + throughput_checkpoint_interval))
          fi
        else
          warn "  Failed to record throughput metrics for task ${task_number}."
        fi
      fi

      if [[ "$task_result_status" == "complete" && $task_changes_applied -gt 0 ]]; then
        local -a commit_targets=("${task_written_paths[@]}")
        commit_targets+=("${task_patched_paths[@]}")
        local apply_status_label="${apply_status:-unknown}"
        local commit_label="${banner_task_id}: ${task_title:-Task ${task_number}}"
        local commit_message="chore(tasks): ${banner_task_id} snapshot (apply_status: ${apply_status_label})"
        gc_auto_commit_task "$commit_message" "${commit_targets[@]}"
        case "${GC_LAST_AUTO_COMMIT_STATUS:-skipped}" in
          committed)
            local note_message="${commit_label//$'\n'/ }"
            note_message="${note_message//$'\r'/ }"
            if [[ -n "${GC_LAST_AUTO_COMMIT_HASH:-}" ]]; then
              task_notes+=("Auto-commit ${GC_LAST_AUTO_COMMIT_HASH:0:7} — ${note_message} (${apply_status_label})")
            else
              task_notes+=("Auto-commit recorded — ${note_message} (${apply_status_label})")
            fi
            ;;
          failed)
            task_notes+=("Auto-commit failed; inspect git status and commit manually.")
            manual_followups=1
            ;;
        esac
      fi

      if [[ "$task_result_status" == "complete" ]]; then
        local finalize_script="${PROJECT_ROOT}/scripts/auto_finalize_task.sh"
        if [[ -x "$finalize_script" ]]; then
          if ! bash "$finalize_script"; then
            warn "  Auto finalize failed; inspect git status."
          fi
        fi
      fi

      local timestamp_utc
      timestamp_utc="$(date -u +%Y-%m-%dT%H:%M:%SZ)"

      local prompt_entry="$prompt_path"
      local output_entry="$output_path"
      local report_entry_path="$task_log_archive_path"
      local report_entry_display="$report_entry_path"
      local report_entry_db=""
      local project_prefix="${PROJECT_ROOT}/"
      if [[ -n "$PROJECT_ROOT" ]]; then
        if [[ "$prompt_entry" == "$project_prefix"* ]]; then
          prompt_entry="${prompt_entry#$project_prefix}"
        fi
        if [[ "$output_entry" == "$project_prefix"* ]]; then
          output_entry="${output_entry#$project_prefix}"
        fi
        if [[ "$report_entry_display" == "$project_prefix"* ]]; then
          report_entry_display="${report_entry_display#$project_prefix}"
        fi
      fi
      report_entry_db="$report_entry_display"
      if [[ ! -f "$prompt_path" ]]; then
        if (( keep_artifacts == 0 )); then
          prompt_entry="(discarded)"
        else
          prompt_entry="(missing)"
        fi
      fi
      if [[ ! -f "$output_path" ]]; then
        if (( keep_output == 0 )); then
          output_entry="(discarded)"
        else
          output_entry="(missing)"
        fi
      fi
      if [[ ! -f "$task_log_archive_path" ]]; then
        report_entry_db=""
        report_entry_display="(missing)"
      fi

      local changes_flag="false"
      if (( task_changes_applied > 0 )); then
        changes_flag="true"
      fi

      local notes_payload=""
      if ((${#task_notes[@]} > 0)); then
        notes_payload="$(printf '%s\n' "${task_notes[@]}")"
      fi
      local written_payload=""
      if ((${#task_written_paths[@]} > 0)); then
        written_payload="$(printf '%s\n' "${task_written_paths[@]}")"
      fi
      local patched_payload=""
      if ((${#task_patched_paths[@]} > 0)); then
        patched_payload="$(printf '%s\n' "${task_patched_paths[@]}")"
      fi
      local commands_payload=""
      if ((${#task_commands[@]} > 0)); then
        commands_payload="$(printf '%s\n' "${task_commands[@]}")"
      fi

      gc_record_task_progress "$tasks_db" "$slug" "$task_index" "$run_stamp" "$task_result_status" "$report_entry_db" "$prompt_entry" "$output_entry" "$attempt" "$task_tokens_total" "$task_duration_seconds" "$apply_status" "$changes_flag" "$notes_payload" "$written_payload" "$patched_payload" "$commands_payload" "$timestamp_utc"

      {
        printf 'task_number: %s\n' "$task_number"
        printf 'task_id: %s\n' "${task_id:-}"
        printf 'task_title: %s\n' "${task_title//$'\n'/ }"
        printf 'story_slug: %s\n' "$slug"
        printf 'status: %s\n' "$task_result_status"
        printf 'timestamp: %s\n' "$timestamp_utc"
        printf 'attempts: %s\n' "$attempt"
        printf 'apply_status: %s\n' "$apply_status"
        printf 'changes_applied: %s\n' "$changes_flag"
        printf 'prompt_path: %s\n' "$prompt_entry"
        printf 'output_path: %s\n' "$output_entry"
        if ((${#task_written_paths[@]} > 0)); then
          printf 'written:\n'
          for path in "${task_written_paths[@]}"; do
            printf '  - %s\n' "$path"
          done
        fi
        if ((${#task_patched_paths[@]} > 0)); then
          printf 'patched:\n'
          for path in "${task_patched_paths[@]}"; do
            printf '  - %s\n' "$path"
          done
        fi
        if ((${#task_commands[@]} > 0)); then
          printf 'commands:\n'
          for cmd in "${task_commands[@]}"; do
            printf '  - %s\n' "$cmd"
          done
        fi
        printf 'notes:\n'
        if ((${#task_notes[@]} > 0)); then
          for note in "${task_notes[@]}"; do
            printf '  - %s\n' "${note//$'\n'/ }"
          done
        else
          printf '  - (none)\n'
        fi
      } >"$task_report_path"
      if ! cp -f "$task_report_path" "$task_log_archive_path"; then
        warn "  Failed to archive task log to ${task_log_archive_path}."
      fi
      gc_clear_active_task

      case "$task_result_status" in
        complete)
          info "  ✓ Task ${task_number} (${task_id:-no-id}) completed with status: ${task_result_status}"
          ;;
        on-hold)
          warn "  Task ${task_number} (${task_id:-no-id}) marked ${task_result_status}; review ${report_entry_display}."
          ;;
        blocked|blocked-budget)
          warn "  Task ${task_number} (${task_id:-no-id}) blocked; see ${report_entry_display}."
          ;;
        *)
          info "  Task ${task_number} (${task_id:-no-id}) finished with status: ${task_result_status}"
          ;;
      esac

      local task_status_display="${task_result_status:-unknown}"
      if [[ "$task_status_display" == "complete" ]]; then
        task_status_display="COMPLETED"
      else
        task_status_display="${task_status_display//-/ }"
        task_status_display="${task_status_display//_/ }"
        task_status_display="$(printf '%s' "$task_status_display" | tr '[:lower:]' '[:upper:]')"
      fi

      printf '\n'
      gc_render_task_banner --header-bottom "END OF THE TASK WORK" \
        "END TASK ID" "$banner_task_id" \
        "REPORT:" \
        "TOKENS USED: ${task_tokens_display}" \
        "STORY POINTS: ${task_story_points_display}" \
        "TIME SPENT: ${task_duration_display}" \
        "STATUS: ${task_status_display}"

      (( ++processed_total ))
      (( ++iteration_processed ))

      if [[ "$task_result_status" == "blocked" || "$task_result_status" == "blocked-budget" ]]; then
        story_failed=1
        break
      fi

      if (( break_after_update )); then
        continue
      fi

      if (( sleep_between > 0 )); then
        sleep "$sleep_between"
      fi

    done

    if (( batch_limit_reached )); then
      break
    fi

    if (( story_failed )); then
      warn "Stopping at story ${slug} due to previous error."
      break
    fi

    if (( idle_timeout_triggered )); then
      break
    fi

    gc_update_work_state "$tasks_db" "$slug" "complete" "$total_tasks_int" "$total_tasks_int" "$run_stamp"
    gc_touch_progress
    if (( keep_artifacts == 0 )); then
      rmdir "${story_run_dir}/prompts" 2>/dev/null || true
      rmdir "${story_run_dir}/out" 2>/dev/null || true
    fi
  done < <(
    python3 - "$tasks_db" "${story_filter}" "$resume_flag" <<'PY'
import sqlite3
import sys
import re

DB_PATH = sys.argv[1]
story_filter = (sys.argv[2] or '').strip().lower()
resume_flag = sys.argv[3] == "1"

conn = sqlite3.connect(DB_PATH)
conn.row_factory = sqlite3.Row
cur = conn.cursor()

stories = cur.execute('SELECT story_slug, story_id, story_title, epic_key, epic_title, sequence, status FROM stories ORDER BY sequence ASC, story_slug ASC').fetchall()

def norm(value):
    return (value or "").strip().lower()

def normalize(value):
    return (value or "").strip()

def slug_norm(value):
    value = norm(value)
    if not value:
        return ""
    return re.sub(r'[^a-z0-9]+', '-', value).strip('-')

start_allowed = not story_filter

for story in stories:
    slug = normalize(story["story_slug"])
    sequence = story["sequence"] or 0
    story_id = normalize(story["story_id"])
    epic_key = normalize(story["epic_key"])
    epic_title = normalize(story["epic_title"])
    story_title = normalize(story["story_title"])

    story_title_clean = story_title.replace('\t', ' ').replace('\n', ' ')
    epic_title_clean = epic_title.replace('\t', ' ').replace('\n', ' ')
    epic_key_clean = epic_key.replace('\t', ' ').replace('\n', ' ')
    story_id_clean = story_id.replace('\t', ' ').replace('\n', ' ')

    if story_filter and not start_allowed:
        keys = {norm(story_id), norm(slug), norm(epic_key), norm(str(sequence))}
        if story_filter in keys:
            start_allowed = True
        else:
            continue

    task_rows = []
    slug_lower = norm(slug)
    if slug_lower:
        task_rows = cur.execute(
            'SELECT position, status FROM tasks WHERE LOWER(COALESCE(story_slug, "")) = ? ORDER BY position ASC',
            (slug_lower,),
        ).fetchall()

    if not task_rows and story_id:
        story_id_lower = norm(story_id)
        if story_id_lower:
            rows = cur.execute(
                'SELECT id, position, status, story_slug FROM tasks WHERE LOWER(COALESCE(story_id, "")) = ? ORDER BY position ASC',
                (story_id_lower,),
            ).fetchall()
            if rows:
                task_rows = [(row["position"], row["status"]) for row in rows]
                if slug_lower:
                    cur.execute(
                        'UPDATE tasks SET story_slug = ? WHERE LOWER(COALESCE(story_id, "")) = ?',
                        (slug, story_id_lower),
                    )
                    conn.commit()

    if not task_rows and slug_lower:
        slug_key = slug_norm(slug)
        if slug_key:
            rows = cur.execute(
                'SELECT position, status FROM tasks WHERE LOWER(COALESCE(story_slug, "")) = ? ORDER BY position ASC',
                (slug_key,),
            ).fetchall()
            task_rows = rows

    total = len(task_rows)
    completed = 0
    next_index = 0
    for row in task_rows:
        status = (row[1] or "").strip().lower()
        if status == "complete":
            completed += 1
            continue
        next_index = row[0] or 0
        break
    else:
        next_index = total

    current_status = (story["status"] or "").strip()

    if resume_flag and not story_filter and current_status.lower() == "complete":
        continue

    if resume_flag:
        if total == 0:
            next_index = 0
        elif completed >= total:
            if story_filter:
                next_index = total
            else:
                continue
    else:
        next_index = 0 if total > 0 else 0

    print("	".join([
        str(sequence),
        slug,
        story_id_clean,
        story_title_clean,
        epic_key_clean,
        epic_title_clean,
        str(total),
        str(next_index),
        str(completed),
        current_status,
    ]))

conn.close()
PY
  )

    if (( idle_timeout_triggered )); then
      break
    fi

    if (( iteration_processed_any )); then
      processed_any_total=1
    else
      if (( processed_any_total == 0 )); then
        info "No stories to process (already complete)."
      fi
      break
    fi

    if (( batch_limit_reached )); then
      remaining_tasks="$(gc_count_pending_tasks "$tasks_db" || echo 0)"
      [[ "$remaining_tasks" =~ ^[0-9]+$ ]] || remaining_tasks=0
      break
    fi

    if (( memory_cycle )); then
      pending_tasks="$(gc_count_pending_tasks "$tasks_db" || echo 0)"
      [[ "$pending_tasks" =~ ^[0-9]+$ ]] || pending_tasks=0
      remaining_tasks="$pending_tasks"
      if (( work_failed == 0 )); then
        if (( iteration_processed > 0 )) && (( pending_tasks > 0 )); then
          gc_trim_memory "memory-cycle"
          info "Memory-cycle paused after ${iteration_processed} task(s); ${pending_tasks} pending."
          memory_cycle_single=1
          continue_current_run=1
        elif (( pending_tasks == 0 )); then
          gc_trim_memory "memory-cycle-final"
        else
          gc_trim_memory "memory-cycle"
        fi
      else
        gc_trim_memory "memory-cycle-error"
      fi
    fi

    if (( continue_current_run == 0 )); then
      remaining_tasks="$(gc_count_pending_tasks "$tasks_db" || echo 0)"
      [[ "$remaining_tasks" =~ ^[0-9]+$ ]] || remaining_tasks=0
      local unstarted_tasks
      unstarted_tasks="$(gc_count_unstarted_tasks "$tasks_db" || echo 0)"
      [[ "$unstarted_tasks" =~ ^[0-9]+$ ]] || unstarted_tasks=0

      if (( manual_followups > 0 )); then
        if (( unstarted_tasks > 0 )); then
          info "Manual follow-ups recorded; ${unstarted_tasks} task(s) still pending — continuing backlog."
          continue_current_run=1
        elif (( remaining_tasks > 0 )); then
          warn "Manual follow-ups detected; backlog paused with ${remaining_tasks} review task(s)."
        fi
      elif (( work_failed == 0 && manual_followups == 0 && memory_cycle == 0 && batch_limit_reached == 0 && effective_batch_size == 0 && iteration_processed > 0 && remaining_tasks > 0 )); then
        if [[ -n "$story_filter" ]]; then
          info "Remaining tasks detected beyond filtered story; rerun with a broader filter to continue."
        else
          info "${remaining_tasks} task(s) remain; continuing work-on-tasks automatically."
          continue_current_run=1
        fi
      fi
    fi

    if (( continue_current_run )); then
      continue
    fi

    break
  done

  if (( idle_timeout_triggered )); then
    warn "work-on-tasks halted by idle timeout after ${idle_timeout}s without progress."
  fi

  if (( processed_any_total == 0 )); then
    gc_clear_active_task
    return 0
  fi

  gc_clear_active_task

  throughput_msg=""
  if throughput_msg="$(gc_update_throughput_metrics "$tasks_db" "flush")"; then
    if [[ -n "$throughput_msg" ]]; then
      info "$throughput_msg"
    fi
  else
    warn "Failed to finalise throughput metrics."
  fi

  if [[ $work_failed -eq 0 && $batch_limit_reached -eq 0 && $no_verify -eq 0 && $remaining_tasks -eq 0 ]]; then
    if (( any_changes == 0 )); then
      info "No repository changes detected; skipping verify."
    else
      info "Re-running verify after work run"
      if ! cmd_verify all --project "$PROJECT_ROOT"; then
        warn "Verify command reported failures."
        work_failed=1
      fi
    fi
  fi

  if (( batch_limit_reached )); then
    info "Batch size limit hit after ${processed_total} task(s); rerun to continue from the next pending task."
  fi

  if (( usage_limit_triggered )); then
    warn "Codex usage limit confirmed by provider; halt further work until additional quota is available."
  fi

  if (( loop_guard_triggered )); then
    warn "LOOP_GUARD_TRIPPED: work-on-tasks halted to avoid infinite loop."
    return "$loop_guard_exit_code"
  fi

  if [[ $work_failed -eq 0 ]]; then
    if (( batch_limit_reached )); then
      ok "work-on-tasks paused → ${run_dir}"
    else
      ok "work-on-tasks complete → ${run_dir}"
    fi
    if (( manual_followups )); then
      warn "Manual review needed for some tasks — see notes above and preserved output artifacts."
    fi
  else
    warn "work-on-tasks completed with issues — inspect ${run_dir}"
    return 1
  fi
}


cmd_iterate() {
  warn "'iterate' is deprecated; prefer 'gpt-creator create-tasks' followed by 'gpt-creator work-on-tasks'."

  local root="" jira="" reverify=1
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --jira) jira="$(abs_path "$2")"; shift 2;;
      --no-verify) reverify=0; shift;;
      *) break;;
    esac
  done
  ensure_ctx "$root"
  [[ -n "$jira" ]] || jira="${INPUT_DIR}/jira.md"
  [[ -f "$jira" ]] || die "Jira tasks file not found: ${jira}"

  local tasks_json_local="${PLAN_DIR}/jira-tasks.local.json"
  gc_parse_jira_tasks "$jira" "$tasks_json_local"
  ok "Parsed Jira tasks → ${tasks_json_local}"
  local tasks_json="${tasks_json_local}"

  local codex_parse_prompt="${PLAN_DIR}/iterate-codex-parse.md"
  local codex_raw_json="${PLAN_DIR}/jira-tasks.codex.raw.txt"
  local codex_json="${PLAN_DIR}/jira-tasks.codex.json"
  {
    cat >"$codex_parse_prompt" <<'PROMPT'
# Instruction
You are a structured-data assistant. Convert the following Jira backlog markdown into strict JSON.

## Requirements
- Output **only** valid JSON (no prose, no code fences).
- Structure: { "tasks": [ { "epic_id": str, "epic_title": str, "story_id": str, "story_title": str, "id": str, "title": str, "assignees": [str], "tags": [str], "estimate": str, "description": str, "acceptance_criteria": [str], "dependencies": [str] } ] }.
- Each task begins with a bold identifier such as **T18.5.2**; treat every such block as a separate task and capture its parent story/epic when present.
- Preserve bullet details verbatim inside the description and acceptance criteria lists. Do not repeat metadata (assignee/tags/estimate) inside the description.
- Use empty strings/arrays when information is missing.
- Do not include explanatory text.

## Jira Markdown
PROMPT
    cat "$jira" >>"$codex_parse_prompt"
    cat >>"$codex_parse_prompt" <<'PROMPT'
## End Markdown
PROMPT
  }

  if codex_call "iterate-parse" --prompt "$codex_parse_prompt" --output "$codex_raw_json"; then
    if python3 - <<'PY' "$codex_raw_json" "$codex_json"
import json, pathlib, re, sys
raw_path, out_path = sys.argv[1:3]
text = pathlib.Path(raw_path).read_text().strip()
if not text:
    raise SystemExit(1)
if text.startswith('```'):
    text = re.sub(r'^```[a-zA-Z0-9_-]*\s*', '', text)
    text = re.sub(r'```\s*$', '', text)
data = json.loads(text)
if isinstance(data, list):
    data = {'tasks': data}
elif 'tasks' not in data:
    data = {'tasks': [data]}
pathlib.Path(out_path).write_text(json.dumps(data, indent=2) + '\n')
PY
      then
      ok "Codex parsed Jira tasks → ${codex_json}"
      tasks_json="$codex_json"
    else
      warn "Codex JSON output invalid; falling back to local parser results."
    fi
  else
    warn "Codex parsing step failed; using local parser output."
  fi

  local iterate_dir="${PLAN_DIR}/iterate"
  mkdir -p "$iterate_dir"
  local order_file="${iterate_dir}/tasks-order.txt"

  python3 - <<'PY' "$tasks_json" "$iterate_dir" "$PROJECT_ROOT"
import json, pathlib, sys
source, out_dir, project_root = sys.argv[1:4]
tasks = json.load(open(source)).get('tasks', [])
out = pathlib.Path(out_dir)
out.mkdir(parents=True, exist_ok=True)
index_path = out / 'tasks-order.txt'
with index_path.open('w') as idx:
    for i, task in enumerate(tasks, 1):
        title = (task.get('title') or '').strip() or f'Task {i}'
        task_id = (task.get('id') or '').strip()
        description = (task.get('description') or '').strip() or '(No additional details provided.)'
        estimate = (task.get('estimate') or '').strip()
        tags = ', '.join(task.get('tags') or [])
        assignees = ', '.join(task.get('assignees') or [])
        story_bits = [part for part in [(task.get('story_id') or '').strip(), (task.get('story_title') or '').strip()] if part]
        prompt_path = out / f'task-{i:02d}.md'
        idx.write(str(prompt_path) + '\n')
        lines = []
        if task_id and not title.startswith(task_id):
            lines.append(f"# Task {i}: {task_id} — {title}")
        else:
            lines.append(f"# Task {i}: {title}")
        lines.append('')
        lines.append('## Context')
        lines.append(f'- Working directory: {project_root}')
        if task_id:
            lines.append(f'- Task ID: {task_id}')
        if story_bits:
            lines.append(f"- Story: {' — '.join(story_bits)}")
        if assignees:
            lines.append(f'- Assignees: {assignees}')
        if estimate:
            lines.append(f'- Estimate: {estimate}')
        if tags:
            lines.append(f'- Tags: {tags}')
        lines.append('')
        lines.append('## Description')
        lines.append(description or '(No additional details provided.)')
        lines.append('')
        if task.get('acceptance_criteria'):
            lines.append('## Acceptance Criteria')
            for ac in task['acceptance_criteria']:
                lines.append(f'- {ac}')
            lines.append('')
        if task.get('dependencies'):
            lines.append('## Dependencies')
            for dep in task['dependencies']:
                lines.append(f'- {dep}')
            lines.append('')
        lines.append('')
        lines.append('## Instructions')
        lines.append('- Outline your plan before modifying files.')
        lines.append('- Implement the task in the repository; commits are not required.')
        lines.append('- Show relevant diffs (git snippets) and command results.')
        lines.append('- Verify acceptance criteria for this task.')
        lines.append('- If blocked, explain why and propose next steps.')
        lines.append('')
        lines.append('## Output Format')
        lines.append('- Begin with a heading `Task {i}`.')
        lines.append('- Summarise changes, tests, and outstanding follow-ups.')
        prompt_path.write_text('\n'.join(lines) + '\n')
PY

  if [[ -s "$order_file" ]]; then
    while IFS= read -r prompt_path; do
      [[ -z "$prompt_path" ]] && continue
      local base_name
      base_name="$(basename "$prompt_path" .md)"
      local output_path
      output_path="${prompt_path%.md}.output.md"
      info "Running Codex for ${base_name}"
      codex_call "$base_name" --prompt "$prompt_path" --output "$output_path" || warn "Codex task ${base_name} returned non-zero"
    done < "$order_file"
  else
    warn "No Jira tasks to process after parsing."
  fi

  local summary_prompt="${iterate_dir}/summary.md"
  local summary_output="${iterate_dir}/summary.output.md"
  python3 - <<'PY' "$tasks_json" "$order_file" "$summary_prompt"
import json, pathlib, sys
tasks = json.load(open(sys.argv[1])).get('tasks', [])
order_file = pathlib.Path(sys.argv[2])
prompt_path = pathlib.Path(sys.argv[3])
lines = ['# Summary Request', '', 'Summarise the completed Jira work and list follow-up actions.']
lines.append('')
lines.append('## Task Reports')
if order_file.exists():
    for i, prompt in enumerate(order_file.read_text().splitlines(), 1):
        if not prompt:
            continue
        title = tasks[i-1].get('title') if i-1 < len(tasks) else f'Task {i}'
        out_path = pathlib.Path(prompt).with_suffix('.output.md')
        lines.append(f'- Task {i}: {title}')
        if out_path.exists():
            content = out_path.read_text().strip()
            if content:
                snippet = content[:2000]
                lines.append('  ```')
                lines.append(snippet)
                lines.append('  ```')
        else:
            lines.append('  (No output captured)')
else:
    lines.append('- No outputs available.')
lines.append('')
lines.append('## Output Requirements')
lines.append('- Provide an overall summary of work completed.')
lines.append('- List follow-up items or blockers.')
lines.append('- Use markdown headings and bullet lists.')
prompt_path.write_text('\n'.join(lines) + '\n')
PY

  codex_call "iterate-summary" --prompt "$summary_prompt" --output "$summary_output" || warn "Codex summary step returned non-zero"

  if [[ "$reverify" -eq 1 ]]; then
    info "Re-running verify after iteration"
    cmd_verify all --project "$PROJECT_ROOT"
  fi
}

cmd_estimate() {
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project)
        root="$(abs_path "$2")"
        shift 2
        ;;
      -h|--help)
        cat <<'USAGE'
Usage: gpt-creator estimate [--project PATH]

Estimate how long it will take to finish the remaining tasks based on story points (default 15 SP/hour; refined by work-on-tasks throughput).
USAGE
        return 0
        ;;
      *)
        die "Unknown estimate option: $1"
        ;;
    esac
  done

  ensure_ctx "$root"
  local tasks_db="${PLAN_DIR}/tasks/tasks.db"
  if [[ ! -f "$tasks_db" ]]; then
    die "Tasks database not found at ${tasks_db}. Run 'gpt-creator create-tasks' first."
  fi

  local helper_path
  helper_path="$(gc_clone_python_tool "estimate_remaining_work.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$tasks_db"
}


cmd_tokens() {
  local root="" details=0 json_output=0
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project)
        root="$(abs_path "$2")"
        shift 2
        ;;
      --details)
        details=1
        shift
        ;;
      --json)
        json_output=1
        shift
        ;;
      -h|--help)
        cat <<'USAGE'
Usage: gpt-creator tokens [--project PATH] [--details] [--json]

Print aggregated Codex token usage captured under .gpt-creator/logs/codex-usage.ndjson.
USAGE
        return 0
        ;;
      *)
        die "Unknown tokens option: $1"
        ;;
    esac
  done

  local project_root=""
  if [[ -n "$root" ]]; then
    project_root="$root"
  elif [[ -n "${PROJECT_ROOT:-}" ]]; then
    project_root="$PROJECT_ROOT"
  else
    project_root="$PWD"
  fi

  local usage_file="${project_root}/.gpt-creator/logs/codex-usage.ndjson"
  if [[ ! -f "$usage_file" ]]; then
    warn "No Codex usage data found at ${usage_file}. Run a codex-enabled command first."
    return 1
  fi
  local helper_path
  helper_path="$(gc_clone_python_tool "tokens_report.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$usage_file" "$details" "$json_output"
}


cmd_reports() {
  local root=""
  local mode="list"
  local slug=""
  local open_editor=0
  local work_branch=""
  local push_after=1
  local prompt_only=0
  local reporter_filter=""
  local close_invalid=0
  local close_comment="Authenticity failed (automated by gpt-creator reports audit)."
  local close_comment_set=0
  local include_closed=0
  local audit_limit=""
  local audit_limit_set=0
  local digests_path=""
  local digests_path_set=0
  local invalid_label=""
  local invalid_label_set=0
  local allow_pairs=()

  while [[ $# -gt 0 ]]; do
    case "$1" in
      list)
        mode="list"
        shift
        ;;
      backlog)
        mode="backlog"
        shift
        ;;
      auto)
        mode="auto"
        shift
        ;;
      audit)
        mode="audit"
        shift
        ;;
      work)
        mode="work"
        shift
        if [[ $# -eq 0 ]]; then
          die "reports work requires a slug identifier"
        fi
        slug="$1"
        shift
        ;;
      show)
        mode="show"
        shift
        if [[ $# -eq 0 ]]; then
          die "reports show requires a slug identifier"
        fi
        slug="$1"
        shift
        ;;
      --project)
        root="$(abs_path "$2")"
        shift 2
        ;;
      --open)
        open_editor=1
        shift
        ;;
      --branch)
        work_branch="$2"
        shift 2
        ;;
      --no-push)
        push_after=0
        shift
        ;;
      --push)
        push_after=1
        shift
        ;;
      --prompt-only)
        prompt_only=1
        shift
        ;;
      --reporter)
        reporter_filter="$2"
        shift 2
        ;;
      --close-invalid)
        close_invalid=1
        shift
        ;;
      --no-close-invalid)
        close_invalid=0
        shift
        ;;
      --comment)
        close_comment="${2:?--comment requires text}"
        close_comment_set=1
        shift 2
        ;;
      --include-closed)
        include_closed=1
        shift
        ;;
      --limit)
        audit_limit="${2:?--limit requires a positive integer}"
        audit_limit_set=1
        shift 2
        ;;
      --digests)
        digests_path="${2:?--digests requires a file path}"
        digests_path_set=1
        shift 2
        ;;
      --allow)
        allow_pairs+=("${2:?--allow requires VERSION=SHA256}")
        shift 2
        ;;
      --label-invalid)
        invalid_label="${2:?--label-invalid requires a value}"
        invalid_label_set=1
        shift 2
        ;;
      --no-label-invalid)
        invalid_label=""
        invalid_label_set=1
        shift
        ;;
      -h|--help)
        cat <<'USAGE'
Usage:
  gpt-creator reports [--project PATH]
  gpt-creator reports list [--project PATH]
  gpt-creator reports backlog [--project PATH]
  gpt-creator reports auto [--project PATH] [--reporter NAME] [--no-push] [--prompt-only]
  gpt-creator reports work [--project PATH] [--branch NAME] [--no-push] [--prompt-only] <slug>
  gpt-creator reports [--project PATH] [--open] <slug>
  gpt-creator reports show [--project PATH] [--open] <slug>
  gpt-creator reports audit [--close-invalid] [--include-closed] [--limit N]
                               [--digests FILE] [--allow VERSION=SHA256]
                               [--label-invalid NAME|--no-label-invalid]
                               [--comment TEXT]

List captured crash/stall reports, show the open backlog, automatically resolve matching issues, assign Codex to resolve a single report, display a specific entry, or audit GitHub auto-reports for authenticity.
USAGE
        return 0
        ;;
      *)
        if [[ -z "$slug" && "$mode" == "list" ]]; then
          mode="show"
          slug="$1"
          shift
        else
          die "Unknown reports argument: $1"
        fi
        ;;
    esac
  done

  if [[ "$mode" == "show" && -z "$slug" ]]; then
    die "reports show requires a slug identifier"
  fi

  if [[ "$mode" == "work" && -z "$slug" ]]; then
    die "reports work requires a slug identifier"
  fi

  if [[ "$mode" == "work" && "$open_editor" -ne 0 ]]; then
    die "--open cannot be combined with reports work"
  fi

  if [[ "$mode" == "auto" && "$open_editor" -ne 0 ]]; then
    die "--open cannot be combined with reports auto"
  fi

  if [[ "$mode" == "auto" && -n "$work_branch" ]]; then
    die "--branch is not supported for reports auto"
  fi

  if [[ "$mode" != "work" && "$mode" != "auto" ]]; then
    if [[ -n "$work_branch" || "$prompt_only" -ne 0 || "$push_after" -ne 1 ]]; then
      die "reports options --branch/--no-push/--prompt-only are only valid with 'work' or 'auto'"
    fi
  fi

  if [[ "$mode" != "auto" && -n "$reporter_filter" ]]; then
    die "--reporter is only supported with reports auto"
  fi

  if [[ -n "$audit_limit" ]]; then
    if [[ ! "$audit_limit" =~ ^[0-9]+$ ]]; then
      die "--limit expects a non-negative integer"
    fi
  fi

  if [[ "$mode" != "audit" ]]; then
    if (( close_invalid != 0 || include_closed != 0 || close_comment_set != 0 || audit_limit_set != 0 || digests_path_set != 0 || invalid_label_set != 0 || ${#allow_pairs[@]} != 0 )); then
      die "reports options --close-invalid/--include-closed/--limit/--digests/--allow/--label-invalid/--comment are only valid with 'audit'"
    fi
  fi

  local pair_entry
  for pair_entry in "${allow_pairs[@]}"; do
    if [[ ! "$pair_entry" =~ ^[^=]+=[0-9a-fA-F]{64}$ ]]; then
      die "--allow expects VERSION=SHA256 (got '${pair_entry}')"
    fi
  done

  if [[ "$mode" != "audit" ]]; then
    if [[ -n "$root" ]]; then
      ensure_ctx "$root"
    else
      ensure_ctx "${PROJECT_ROOT:-$PWD}"
    fi
  elif [[ -n "$root" ]]; then
    ensure_ctx "$root"
  fi

  local reports_dir=""
  if [[ "$mode" != "audit" ]]; then
    reports_dir="$(gc_reports_dir)" || die "Unable to access issue report directory"
  fi

  case "$mode" in
    audit)
      local repo="${GC_GITHUB_REPO:-}"
      local token="${GC_GITHUB_TOKEN:-}"
      if [[ -z "$repo" || -z "$token" ]]; then
        die "reports audit requires GC_GITHUB_REPO and GC_GITHUB_TOKEN to be set"
      fi
      local state="open"
      if (( include_closed )); then
        state="all"
      fi
      if [[ -z "$digests_path" ]]; then
        if [[ -n "${GC_REPORT_DIGESTS_PATH:-}" ]]; then
          digests_path="${GC_REPORT_DIGESTS_PATH}"
        elif [[ -n "${CLI_ROOT:-}" && -f "${CLI_ROOT}/config/release-digests.json" ]]; then
          digests_path="${CLI_ROOT}/config/release-digests.json"
        fi
      fi
      local allow_join=""
      if ((${#allow_pairs[@]} > 0)); then
        allow_join="$(printf '%s\n' "${allow_pairs[@]}")"
      fi
      python3 - <<'PY' "$repo" "$token" "$state" "$close_invalid" "$close_comment" "$audit_limit" "$digests_path" "$invalid_label" "$allow_join"
import json
import os
import sys
import urllib.error
import urllib.parse
import urllib.request
import hashlib
from textwrap import indent

repo, token, state, close_flag, close_comment, limit_value, digests_path, invalid_label, allow_payload = sys.argv[1:10]
close_invalid = close_flag == "1"
limit = None
if limit_value:
    try:
        parsed_limit = int(limit_value)
        if parsed_limit > 0:
            limit = parsed_limit
    except ValueError:
        limit = None

session_headers = {
    "Authorization": f"Bearer {token}",
    "Accept": "application/vnd.github+json",
    "Content-Type": "application/json",
    "X-GitHub-Api-Version": "2022-11-28",
    "User-Agent": "gpt-creator-reports-audit",
}

def github_request(url, method="GET", payload=None):
    data = None
    if payload is not None:
        data = json.dumps(payload).encode("utf-8")
    request = urllib.request.Request(url, data=data, headers=session_headers, method=method)
    with urllib.request.urlopen(request) as resp:
        if resp.status == 204:
            return None
        body = resp.read().decode("utf-8")
        if not body:
            return None
        return json.loads(body)

def load_allowlist(path, inline_pairs):
    mapping = {}
    inline_pairs = [line.strip() for line in inline_pairs.splitlines() if line.strip()]
    for entry in inline_pairs:
        if "=" not in entry:
            continue
        version, digest = entry.split("=", 1)
        version = version.strip()
        digest = digest.strip().lower()
        if not version or len(digest) != 64:
            continue
        mapping.setdefault(version, set()).add(digest)
    if not path:
        return mapping
    try:
        with open(path, "r", encoding="utf-8") as fh:
            data = json.load(fh)
    except FileNotFoundError:
        return mapping
    except Exception as exc:
        print(f"Failed to read digest allowlist '{path}': {exc}", file=sys.stderr)
        return mapping
    if isinstance(data, dict):
        iterable = data.items()
    elif isinstance(data, list):
        iterable = []
        for item in data:
            if isinstance(item, dict):
                version = str(item.get("version") or "").strip()
                if not version:
                    continue
                sha_values = []
                sha_field = item.get("sha256")
                if isinstance(sha_field, str):
                    sha_values = [sha_field]
                elif isinstance(sha_field, list):
                    sha_values = [val for val in sha_field if isinstance(val, str)]
                elif isinstance(sha_field, dict):
                    sha_values = []
                    for val in sha_field.values():
                        if isinstance(val, str):
                            sha_values.append(val)
                        elif isinstance(val, list):
                            sha_values.extend(x for x in val if isinstance(x, str))
                for digest in sha_values:
                    mapping.setdefault(version, set()).add(digest.lower())
            elif isinstance(item, str):
                if "=" in item:
                    version, digest = item.split("=", 1)
                    mapping.setdefault(version.strip(), set()).add(digest.strip().lower())
        return mapping
    else:
        return mapping
    for version, values in iterable:
        if isinstance(values, str):
            mapping.setdefault(str(version), set()).add(values.lower())
        elif isinstance(values, list):
            mapping.setdefault(str(version), set()).update(val.lower() for val in values if isinstance(val, str))
        elif isinstance(values, dict):
            for val in values.values():
                if isinstance(val, str):
                    mapping.setdefault(str(version), set()).add(val.lower())
                elif isinstance(val, list):
                    mapping.setdefault(str(version), set()).update(v.lower() for v in val if isinstance(v, str))
    return mapping

digest_allowlist = load_allowlist(digests_path, allow_payload or "")

def fetch_issues():
    collected = []
    page = 1
    per_page = 100
    while True:
        url = f"https://api.github.com/repos/{repo}/issues?state={urllib.parse.quote(state)}&labels=auto-report&per_page={per_page}&page={page}"
        try:
            items = github_request(url)
        except urllib.error.HTTPError as err:
            text = err.read().decode("utf-8", "ignore")
            print(f"GitHub API error while fetching issues: {err.code} {text}", file=sys.stderr)
            return collected
        except Exception as exc:
            print(f"Failed to fetch GitHub issues: {exc}", file=sys.stderr)
            return collected
        if not items:
            break
        for issue in items:
            if "pull_request" in issue:
                continue
            collected.append(issue)
            if limit is not None and len(collected) >= limit:
                return collected
        if len(items) < per_page:
            break
        page += 1
    return collected

def parse_issue(issue):
    body = issue.get("body") or ""
    metadata = {}
    for line in body.splitlines():
        stripped = line.strip()
        if stripped.startswith("- **") and "**:" in stripped:
            head, value = stripped.split("**:", 1)
            label = head.replace("- **", "").strip()
            metadata[label] = value.strip()
    watermark_token = ""
    start = body.find("<!--")
    while start != -1:
        end = body.find("-->", start + 4)
        if end == -1:
            break
        comment = body[start + 4:end].strip()
        if comment.lower().startswith("gpt-creator:"):
            watermark_token = comment.split(":", 1)[1].strip()
            break
        start = body.find("<!--", end + 3)
    version = metadata.get("CLI Version", "").strip()
    binary_hash = metadata.get("CLI Binary SHA256", "").strip().lower()
    signature = metadata.get("CLI Signature", "").strip().lower()
    expected_signature = ""
    signature_valid = False
    watermark_valid = False
    reasons = []
    if version or binary_hash:
        expected_signature = hashlib.sha256(f"{version}:{binary_hash}".encode("utf-8")).hexdigest()
    if not binary_hash or len(binary_hash) != 64:
        reasons.append("missing CLI binary hash")
    if not version:
        reasons.append("missing CLI version")
    if signature and expected_signature:
        if signature == expected_signature:
            signature_valid = True
        else:
            reasons.append("signature mismatch")
    else:
        reasons.append("missing CLI signature")
    if watermark_token:
        watermark_expected = f"{version or 'unknown'}:{expected_signature}" if expected_signature else ""
        if watermark_expected and watermark_token.lower() == watermark_expected.lower():
            watermark_valid = True
        elif version and signature_valid and watermark_token.lower() == f"{version.lower()}:unsigned":
            watermark_valid = False
            reasons.append("watermark unsigned but signature present")
        else:
            reasons.append("watermark mismatch")
    else:
        reasons.append("missing watermark")
    digest_allowed = True
    if digest_allowlist:
        allowed_hashes = digest_allowlist.get(version, set())
        if allowed_hashes:
            if binary_hash not in allowed_hashes:
                digest_allowed = False
                reasons.append("binary hash not in allowlist")
        else:
            digest_allowed = False
            reasons.append("version not present in allowlist")
    valid = signature_valid and watermark_valid and digest_allowed
    return {
        "number": issue.get("number"),
        "title": issue.get("title") or "",
        "url": issue.get("html_url") or "",
        "state": issue.get("state"),
        "version": version,
        "binary_hash": binary_hash,
        "signature": signature,
        "expected_signature": expected_signature,
        "signature_valid": signature_valid,
        "watermark": watermark_token,
        "watermark_valid": watermark_valid,
        "digest_allowed": digest_allowed,
        "valid": valid,
        "reasons": reasons,
        "metadata": metadata,
    }

def ensure_label(issue_number):
    if not invalid_label:
        return
    try:
        issue = github_request(f"https://api.github.com/repos/{repo}/issues/{issue_number}")
    except Exception:
        return
    if not issue:
        return
    labels = issue.get("labels") or []
    label_names = {lbl["name"] if isinstance(lbl, dict) else str(lbl) for lbl in labels}
    if invalid_label in label_names:
        return
    label_names.add(invalid_label)
    payload = {"labels": sorted(label_names)}
    try:
        github_request(f"https://api.github.com/repos/{repo}/issues/{issue_number}", method="PATCH", payload=payload)
    except Exception:
        pass

def close_issue(item):
    number = item["number"]
    if not close_invalid:
        return
    if str(item.get("state")) == "closed":
        return
    try:
        github_request(
            f"https://api.github.com/repos/{repo}/issues/{number}/comments",
            method="POST",
            payload={"body": close_comment},
        )
    except Exception as exc:
        print(f"Failed to comment on issue #{number}: {exc}", file=sys.stderr)
    try:
        github_request(
            f"https://api.github.com/repos/{repo}/issues/{number}",
            method="PATCH",
            payload={"state": "closed"},
        )
    except Exception as exc:
        print(f"Failed to close issue #{number}: {exc}", file=sys.stderr)
    else:
        ensure_label(number)
        print(f"Closed issue #{number} ({item['title']}) as authenticity failed.")

issues = fetch_issues()
if not issues:
    print("No GitHub issues labelled 'auto-report' found.")
    sys.exit(0)

parsed = [parse_issue(issue) for issue in issues]
valid_count = sum(1 for item in parsed if item["valid"])
invalid_items = [item for item in parsed if not item["valid"]]

print(f"Audited {len(parsed)} auto-report issue(s) [{state}].")
print(f" - Valid:   {valid_count}")
print(f" - Invalid: {len(invalid_items)}")
if digest_allowlist:
    print(f" - Allowlist source: {digests_path or 'inline overrides only'}")

def summarize(item):
    header = f"#{item['number']} [{item['state']}] {item['title']}"
    verdict = "VALID" if item["valid"] else "INVALID"
    print(f"\n{header}\nResult: {verdict}")
    print(f"URL: {item['url']}")
    print(f"Version: {item['version'] or '(missing)'}")
    print(f"Binary SHA256: {item['binary_hash'] or '(missing)'}")
    print(f"Signature: {item['signature'] or '(missing)'}")
    print(f"Watermark: {item['watermark'] or '(missing)'}")
    print(f"Signature OK: {'yes' if item['signature_valid'] else 'no'}")
    print(f"Watermark OK: {'yes' if item['watermark_valid'] else 'no'}")
    if digest_allowlist:
        print(f"Allowlist OK: {'yes' if item['digest_allowed'] else 'no'}")
    if item["reasons"]:
        detail = "\n".join(f"- {reason}" for reason in item["reasons"])
        print("Notes:\n" + indent(detail, "  "))

for item in parsed:
    summarize(item)
    if not item["valid"]:
        close_issue(item)

PY
      return
      ;;
    list|backlog)
      if [[ -z "$(find "$reports_dir" -maxdepth 1 -type f \( -name '*.yml' -o -name '*.yaml' \) -print -quit 2>/dev/null)" ]]; then
        info "No issue reports recorded yet."
        return 0
      fi
      python3 - <<'PY' "$reports_dir" "$mode"
import datetime
import os
import sys

dir_path = sys.argv[1]
mode = sys.argv[2]
entries = []

for name in os.listdir(dir_path):
    lower = name.lower()
    if not (lower.endswith(".yml") or lower.endswith(".yaml")):
        continue
    path = os.path.join(dir_path, name)
    if not os.path.isfile(path):
        continue
    try:
        stat = os.stat(path)
    except OSError:
        continue
    slug = os.path.splitext(name)[0]
    summary = ""
    priority = ""
    timestamp = ""
    issue_type = ""
    status = ""
    likes = 0
    comments = 0
    reporter = ""
    metadata = False
    try:
        with open(path, "r", encoding="utf-8", errors="replace") as fh:
            for line in fh:
                stripped = line.strip()
                if stripped.startswith("summary:") and not summary:
                    value = stripped.split(":", 1)[1].strip()
                    if value.startswith('"') and value.endswith('"') and len(value) >= 2:
                        value = value[1:-1]
                    summary = value
                elif stripped.startswith("priority:") and not priority:
                    priority = stripped.split(":", 1)[1].strip()
                elif stripped.startswith("type:") and not issue_type:
                    issue_type = stripped.split(":", 1)[1].strip()
                elif stripped.startswith("timestamp:") and not timestamp:
                    timestamp = stripped.split(":", 1)[1].strip().strip('"')
                if stripped == "metadata:":
                    metadata = True
                    continue
                if metadata:
                    if not line.startswith("  "):
                        metadata = False
                    else:
                        meta = line.strip()
                        if meta.startswith("timestamp:") and not timestamp:
                            timestamp = meta.split(":", 1)[1].strip().strip('"')
                        elif meta.startswith("status:") and not status:
                            status = meta.split(":", 1)[1].strip()
                        elif meta.startswith("likes:") and likes == 0:
                            try:
                                likes = int(meta.split(":", 1)[1].strip())
                            except ValueError:
                                likes = 0
                        elif meta.startswith("comments:") and comments == 0:
                            try:
                                comments = int(meta.split(":", 1)[1].strip())
                            except ValueError:
                                comments = 0
                        elif meta.startswith("reporter:") and not reporter:
                            reporter = meta.split(":", 1)[1].strip().strip('"')
    except OSError:
        continue
    popularity = likes + comments
    entries.append((stat.st_mtime, slug, summary, priority, timestamp, issue_type, status, popularity, likes, comments, reporter))

entries.sort(key=lambda item: item[0], reverse=True)

printed = False
for mtime, slug, summary, priority, timestamp, issue_type, status, popularity, likes, comments, reporter in entries:
    if mode == "backlog" and status and status.lower() not in ("open", "new", "todo"):
        continue
    summary = summary or "(no summary)"
    priority = priority or "unknown"
    issue_type = issue_type or "unknown"
    status_display = status or "open"
    display_time = timestamp or datetime.datetime.utcfromtimestamp(mtime).strftime("%Y-%m-%dT%H:%M:%SZ")
    print(f"[{slug}] {display_time} {priority} ({issue_type}) status={status_display} pop={popularity} (likes={likes}, comments={comments})")
    if reporter:
        print(f"  reporter={reporter}")
    print(f"  {summary}")
    print()
    printed = True

if mode == "backlog" and not printed:
    print("No open reports found.")
PY
      ;;
    auto)
      local reporter="${reporter_filter:-$(gc_reports_current_user)}"
      info "Auto-processing reports for reporter: ${reporter}"
      local auto_slugs=()
      while IFS= read -r slug_entry; do
        [[ -n "$slug_entry" ]] && auto_slugs+=("$slug_entry")
      done < <(python3 - <<'PY' "$reports_dir" "$reporter"
import os
import sys

dir_path = sys.argv[1]
reporter_filter = sys.argv[2].strip()
entries = []

for name in os.listdir(dir_path):
    lower = name.lower()
    if not (lower.endswith('.yml') or lower.endswith('.yaml')):
        continue
    path = os.path.join(dir_path, name)
    if not os.path.isfile(path):
        continue
    try:
        stat = os.stat(path)
    except OSError:
        continue
    slug = os.path.splitext(name)[0]
    metadata = {}
    metadata_active = False
    try:
        with open(path, 'r', encoding='utf-8', errors='replace') as fh:
            for line in fh:
                stripped = line.strip()
                if stripped == 'metadata:':
                    metadata_active = True
                    continue
                if metadata_active:
                    if line.startswith('  '):
                        if ':' in stripped:
                            k, v = stripped.split(':', 1)
                            metadata[k.strip()] = v.strip().strip('"')
                    else:
                        metadata_active = False
    except OSError:
        continue
    reporter = metadata.get('reporter', '')
    status = metadata.get('status', 'open').lower()
    if reporter_filter and reporter.lower() != reporter_filter.lower():
        continue
    if status not in ('open', 'new', 'todo'):
        continue
    entries.append((stat.st_mtime, slug))

entries.sort(key=lambda item: item[0], reverse=True)

for _, slug in entries:
    print(slug)
PY
)
      if ((${#auto_slugs[@]} == 0)); then
        info "No matching reports for reporter '${reporter}'."
        return 0
      fi
      local slug_entry
      for slug_entry in "${auto_slugs[@]}"; do
        info "Resolving report ${slug_entry}"
        if ! gc_reports_run_work "$slug_entry" "" "$push_after" "$prompt_only" "$reporter"; then
          die "Codex failed while processing report ${slug_entry}"
        fi
      done
      ;;
    show)
      local report_path=""
      if ! report_path="$(gc_reports_resolve_slug "$slug")"; then
        die "No issue report found for slug: ${slug}"
      fi
      info "Report file: ${report_path}"
      printf '\n'
      cat "$report_path"
      printf '\n'
      if (( open_editor )); then
        local editor_cmd="${EDITOR_CMD:-${EDITOR:-vi}}"
        if [[ -z "$editor_cmd" ]]; then
          warn "EDITOR_CMD not set; skipping --open"
        else
          info "Opening report in ${editor_cmd}"
          if ! bash -lc "${editor_cmd} \"${report_path}\""; then
            warn "Failed to launch editor command: ${editor_cmd}"
          fi
        fi
      fi
      ;;
    work)
      if ! gc_reports_run_work "$slug" "$work_branch" "$push_after" "$prompt_only" ""; then
        die "Codex failed to resolve report ${slug}"
      fi
      ;;
  esac
}

cmd_create_project() {
  local template_request="auto"
  local path=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --template)
        template_request="${2:?--template requires a value (template name or 'auto')}"
        shift 2
        ;;
      --skip-template)
        template_request="skip"
        shift
        ;;
      -h|--help)
        cat <<'USAGE'
Usage: gpt-creator create-project [--template NAME|auto|skip] [--skip-template] <path>

Create a new project root, optionally scaffold it from a project template, then
run the full build pipeline (scan → normalize → plan → generate → db → run → verify).
USAGE
        return 0
        ;;
      *)
        if [[ -z "$path" ]]; then
          path="$1"
        else
          die "Unexpected argument: $1"
        fi
        shift
        ;;
    esac
  done

  [[ -n "$path" ]] || die "create-project requires a path"

  local project_root
  project_root="$(abs_path "$path")"
  mkdir -p "$project_root"

  if ! gc_apply_project_template "$project_root" "$template_request"; then
    warn "Project template application reported issues; continuing with base scaffolding."
  fi

  ensure_ctx "$project_root"
  info "Project root: ${PROJECT_ROOT}"

  cmd_scan --project "$PROJECT_ROOT"
  cmd_normalize --project "$PROJECT_ROOT"
  cmd_plan --project "$PROJECT_ROOT"
  cmd_generate all --project "$PROJECT_ROOT"
  cmd_db provision --project "$PROJECT_ROOT" || warn "Database provision step reported an error"
  cmd_run up --project "$PROJECT_ROOT" || warn "Stack start reported an error"
  cmd_verify acceptance --project "$PROJECT_ROOT" || warn "Acceptance checks failing — review stack health."
  ok "Project bootstrap complete"
}

cmd_bootstrap() {
  local template_request="auto"
  local path=""
  local fresh=0
  local rfp_path=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --template)
        template_request="${2:?--template requires a value (template name or 'auto')}"
        shift 2
        ;;
      --skip-template)
        template_request="skip"
        shift
        ;;
      --rfp)
        rfp_path="${2:?--rfp requires a file path}"
        shift 2
        ;;
      --fresh)
        fresh=1
        shift
        ;;
      -h|--help)
        cat <<'USAGE'
Usage: gpt-creator bootstrap [--template NAME|auto|skip] [--skip-template] [--rfp FILE] [--fresh] <path>

Generate a full project from an RFP in one step by running:
  • create-pdr → create-sds → create-db-dump → create-jira-tasks → scan/normalize/plan/generate/db/run/verify
Templates from project_templates/ are applied first (auto-selected by default).
USAGE
        return 0
        ;;
      *)
        if [[ -z "$path" ]]; then
          path="$1"
        else
          die "Unexpected argument: $1"
        fi
        shift
        ;;
    esac
  done

  [[ -n "$path" ]] || die "bootstrap requires a path"

  if [[ -n "$rfp_path" ]]; then
    [[ -f "$rfp_path" ]] || die "RFP file not found: ${rfp_path}"
  fi

  local project_root
  project_root="$(abs_path "$path")"
  mkdir -p "$project_root"

  ensure_ctx "$project_root"
  info "Project root: ${PROJECT_ROOT}"

  if (( fresh )); then
    gc_bootstrap_reset_state
  fi

  mkdir -p "$(gc_bootstrap_state_dir)"

  if gc_bootstrap_step_is_done template; then
    info "Step 'template' already completed; skipping."
  else
    if gc_apply_project_template "$PROJECT_ROOT" "$template_request"; then
      gc_bootstrap_mark_step template "done"
    else
      gc_bootstrap_mark_step template "failed"
      die "Project template application failed"
    fi
  fi

  if [[ -n "$rfp_path" ]]; then
    local staged_rfp="${INPUT_DIR}/rfp.md"
    local staged_docs_rfp="${STAGING_DIR}/docs/rfp.md"
    if gc_bootstrap_step_is_done stage-rfp; then
      if [[ ! -f "$staged_rfp" || ! -f "$staged_docs_rfp" ]]; then
        info "Restaging RFP (previous artifacts missing)."
        gc_bootstrap_mark_step stage-rfp "reset"
      fi
    fi
    if ! gc_bootstrap_step_is_done stage-rfp; then
      mkdir -p "${INPUT_DIR}"
      cp "$rfp_path" "$staged_rfp"
      mkdir -p "${STAGING_DIR}/docs"
      cp "$rfp_path" "${STAGING_DIR}/docs/rfp.md"
      gc_bootstrap_mark_step stage-rfp "done"
      info "Staged RFP → ${staged_rfp}"
    else
      info "Step 'stage-rfp' already completed; skipping."
    fi
  fi

  info "[1/10] Scanning documentation"
  if ! gc_bootstrap_run_step scan cmd_scan --project "$PROJECT_ROOT"; then
    die "Bootstrap halted during scan"
  fi

  info "[2/10] Normalizing documentation"
  if ! gc_bootstrap_run_step normalize cmd_normalize --project "$PROJECT_ROOT"; then
    die "Bootstrap halted during normalize"
  fi

  info "[3/10] Generating Product Requirements Document"
  if gc_bootstrap_step_is_done create-pdr; then
    info "Step 'create-pdr' already completed; skipping."
  else
    if ! gc_bootstrap_have_rfp; then
      warn "No RFP found in staging; skipping create-pdr. Provide --rfp or add .gpt-creator/staging/docs/rfp.md to enable this step."
      gc_bootstrap_mark_step create-pdr "done"
    elif bash "$CLI_ROOT/src/cli/create-pdr.sh" --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step create-pdr "done"
    else
      gc_bootstrap_mark_step create-pdr "failed"
      die "create-pdr failed"
    fi
  fi

  info "[4/10] Generating System Design Specification"
  if gc_bootstrap_step_is_done create-sds; then
    info "Step 'create-sds' already completed; skipping."
  else
    if bash "$CLI_ROOT/src/cli/create-sds.sh" --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step create-sds "done"
    else
      gc_bootstrap_mark_step create-sds "failed"
      die "create-sds failed"
    fi
  fi

  info "[5/11] Generating database schema & seed dumps"
  if gc_bootstrap_step_is_done create-db-dump; then
    info "Step 'create-db-dump' already completed; skipping."
  else
    if bash "$CLI_ROOT/src/cli/create-db-dump.sh" --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step create-db-dump "done"
    else
      gc_bootstrap_mark_step create-db-dump "failed"
      die "create-db-dump failed"
    fi
  fi

  info "[6/11] Mining Jira tasks"
  if ! gc_bootstrap_step_is_done create-jira-tasks; then
    if cmd_create_jira_tasks --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step create-jira-tasks "done"
    else
      gc_bootstrap_mark_step create-jira-tasks "failed"
      die "create-jira-tasks failed"
    fi
  else
    info "Step 'create-jira-tasks' already completed; skipping."
  fi

  info "[7/11] Planning build"
  if ! gc_bootstrap_step_is_done plan; then
    if cmd_plan --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step plan "done"
    else
      gc_bootstrap_mark_step plan "failed"
      die "plan step failed"
    fi
  else
    info "Step 'plan' already completed; skipping."
  fi

  if ! gc_bootstrap_step_is_done generate; then
    info "[8/11] Generating stack code"
    if cmd_generate all --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step generate "done"
    else
      gc_bootstrap_mark_step generate "failed"
      die "generate step failed"
    fi
  else
    info "Step 'generate' already completed; skipping."
  fi

  info "[9/11] Provisioning infrastructure"
  if ! gc_bootstrap_step_is_done db-provision; then
    if cmd_db provision --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step db-provision "done"
    else
      gc_bootstrap_mark_step db-provision "failed"
      die "Database provision failed"
    fi
  else
    info "Step 'db-provision' already completed; skipping."
  fi

  info "[10/11] Starting stack"
  if ! gc_bootstrap_step_is_done run-up; then
    if cmd_run up --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step run-up "done"
    else
      gc_bootstrap_mark_step run-up "failed"
      die "Stack start failed"
    fi
  else
    info "Step 'run-up' already completed; skipping."
  fi

  if ! gc_bootstrap_step_is_done verify; then
    if cmd_verify acceptance --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step verify "done"
    else
      gc_bootstrap_mark_step verify "failed"
      die "Acceptance verification failed"
    fi
  else
    info "Step 'verify' already completed; skipping."
  fi

  info "[11/11] Verifying acceptance"
  gc_bootstrap_mark_complete
  ok "Bootstrap complete — code, docs, and backlog generated"
}

cmd_update() {
  local force=0
  local repo_url="${GC_UPDATE_REPO_URL:-https://github.com/bekirdag/gpt-creator.git}"
  local prefix="/usr/local"
  local tmpdir=""

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --force)
        force=1
        shift
        ;;
      -h|--help)
        cat <<EOF
Usage: ${APP_NAME} update [--force]

Fetches the latest gpt-creator sources and reinstalls the CLI.
EOF
        return 0
        ;;
      *)
        die "Unknown argument for update: $1"
        ;;
    esac
  done

  if ! command -v git >/dev/null 2>&1; then
    die "'git' is required for ${APP_NAME} update"
  fi

  tmpdir="$(mktemp -d "${TMPDIR:-/tmp}/gpt-creator-update.XXXXXX")" || die "Unable to create temporary directory"

  info "Cloning repository: $repo_url"
  if ! git clone "$repo_url" "$tmpdir"; then
    rm -rf "$tmpdir"
    die "Failed to clone repository from ${repo_url}"
  fi

  info "Fetching latest changes"
  if ! git -C "$tmpdir" pull --ff-only; then
    rm -rf "$tmpdir"
    die "Failed to update repository in $tmpdir"
  fi

  local install_script="$tmpdir/scripts/install.sh"
  if [[ ! -x "$install_script" ]]; then
    rm -rf "$tmpdir"
    die "Installer not found at $install_script"
  fi

  local -a install_cmd=("$install_script" --prefix "$prefix")
  if (( force )); then
    install_cmd+=("--force")
  fi

  info "Running installer: ${install_cmd[*]}"
  if ! "${install_cmd[@]}"; then
    rm -rf "$tmpdir"
    die "Install script failed"
  fi

  rm -rf "$tmpdir"
  ok "gpt-creator updated successfully"
}

cmd_keys() {
  local action="${1:-list}"
  case "$action" in
    list|status)
      shift || true
      if (($#)); then
        die "keys ${action} does not take additional arguments"
      fi
      gc_api_keys_list
      ;;
    set)
      shift || true
      local target="${1:-}"
      [[ -n "$target" ]] || die "keys set requires a service name or environment variable"
      gc_api_keys_set "$target"
      ;;
    help|-h|--help)
      cat <<EOF
Usage: ${APP_NAME} keys [list|status]
       ${APP_NAME} keys set <service>
       ${APP_NAME} keys <service>

Lists required third-party API keys and stores credentials under your user config directory.
EOF
      ;;
    *)
      if (($# > 1)); then
        die "Unknown keys action: $*"
      fi
      gc_api_keys_set "$action"
      ;;
  esac
}

cmd_tui() {
  ensure_go_runtime
  local go_bin="${GC_GO_BIN:-${GO_BIN:-go}}"
  if ! command -v "$go_bin" >/dev/null 2>&1; then
    die "Go 1.21+ is required to run the gpt-creator TUI. Automatic setup failed; install Go manually and set GO_BIN."
  fi
  local tui_dir="${CLI_ROOT}/tui"
  if [[ ! -d "$tui_dir" ]]; then
    die "TUI sources not found at ${tui_dir}"
  fi
  local skip_tidy="${GC_SKIP_TUI_TIDY:-}"
  info "Launching gpt-creator TUI (preview)"
  (
    cd "$tui_dir"
    local go_dir_readonly=0
    local readonly_path=""
    if [[ ! -w go.mod ]]; then
      go_dir_readonly=1
      readonly_path="${tui_dir}/go.mod"
    elif [[ -e go.sum && ! -w go.sum ]]; then
      go_dir_readonly=1
      readonly_path="${tui_dir}/go.sum"
    fi
    if [[ -z "$skip_tidy" ]]; then
      if (( go_dir_readonly )); then
        warn "Skipping 'go mod tidy' because ${readonly_path} is not writable"
      else
        info "Ensuring TUI Go modules are tidy"
        if ! "$go_bin" mod tidy >/dev/null 2>&1; then
          warn "'go mod tidy' reported issues; retrying with output"
          if ! "$go_bin" mod tidy; then
            die "Failed to tidy Go modules required for the TUI"
          fi
        fi
      fi
    fi
    if (( go_dir_readonly )) && [[ "${GOFLAGS:-}" != *"-mod="* ]]; then
      export GOFLAGS="${GOFLAGS:+${GOFLAGS} }-mod=readonly"
    fi
    "$go_bin" run . "$@"
  )
}

usage() {
cat <<EOF
${APP_NAME} v${VERSION}

Usage:
  ${APP_NAME} [--reports-on|--reports-off] [--reports-idle-timeout SECONDS] <command> [args]
  ${APP_NAME} create-project [--template NAME|auto|skip] [--skip-template] <path>
  ${APP_NAME} bootstrap [--template NAME|auto|skip] [--skip-template] [--rfp FILE] [--fresh] <path>
  ${APP_NAME} scan [--project <path>]
  ${APP_NAME} normalize [--project <path>]
  ${APP_NAME} plan [--project <path>]
  ${APP_NAME} generate <api|web|admin|db|docker|all> [--project <path>]
  ${APP_NAME} db <provision|import|seed> [--project <path>]
  ${APP_NAME} run <up|down|logs|open> [--project <path>]
  ${APP_NAME} refresh-stack [options]
  ${APP_NAME} verify <acceptance|nfr|all> [--project <path>] [--api-url API_BASE] [--api-health URL] [--web-url URL] [--admin-url URL]
  ${APP_NAME} create-sds [--project <path>] [--model NAME] [--dry-run] [--force]
  ${APP_NAME} create-pdr [--project <path>] [--model NAME] [--dry-run] [--force]
  ${APP_NAME} create-jira-tasks [--project <path>] [--model NAME] [--force] [--dry-run]
  ${APP_NAME} create-db-dump [--project <path>] [--model NAME] [--dry-run] [--force]
  ${APP_NAME} migrate-tasks [--project <path>] [--force]
  ${APP_NAME} refine-tasks [--project <path>] [--story SLUG] [--model NAME] [--dry-run]
  ${APP_NAME} create-tasks [--project <path>] [--jira <file>] [--force]
  ${APP_NAME} backlog [--project <path>] [--type epics|stories] [--item-children ID] [--progress] [--task-details ID]
  ${APP_NAME} estimate [--project <path>]
  ${APP_NAME} tokens [--project <path>] [--details] [--json]
  ${APP_NAME} reports [--project <path>] [list|backlog|show|work] [--branch NAME] [--no-push] [--prompt-only] [--open] [slug]
  ${APP_NAME} show-file [--range A:B|--head N|--tail N|--diff|--refresh] <path>
  ${APP_NAME} task-convert [options] (deprecated; runs create-tasks)
  ${APP_NAME} sweep-artifacts [--project <path>] [path...]
  ${APP_NAME} tidy-progress [--project <path>] [path...] (deprecated alias of sweep-artifacts)
  ${APP_NAME} work-on-tasks [--project <path>] [--story ID|SLUG] [--fresh] [--no-verify] [--keep-artifacts]
  ${APP_NAME} iterate [--project <path>] [--jira <file>] [--no-verify] (deprecated)
  ${APP_NAME} keys [list|set <service>]
  ${APP_NAME} tui
  ${APP_NAME} update [--force]
  ${APP_NAME} version
  ${APP_NAME} help

Global flags:
  --reports-on               Enable automatic crash/stall reports for the current invocation
  --reports-off              Disable automatic crash/stall reports (overrides GC_REPORTS_ON=1)
  --reports-idle-timeout <s> Override idle detection threshold in seconds (default: ${GC_REPORTS_IDLE_TIMEOUT})

Environment overrides:
  CODEX_BIN, CODEX_MODEL, DOCKER_BIN, MYSQL_BIN, EDITOR_CMD, GC_API_HEALTH_URL, GC_WEB_URL, GC_ADMIN_URL
EOF
}

main() {
  local cmd="${1:-help}"; shift || true
  local shell_bin="${BASH:-bash}"
  case "$cmd" in
    help|-h|--help) usage ;;
    version|-v|--version) echo "${APP_NAME} ${VERSION}" ;;
    create-project) cmd_create_project "$@" ;;
    bootstrap)      cmd_bootstrap "$@" ;;
    scan)           cmd_scan "$@" ;;
    normalize)      cmd_normalize "$@" ;;
    plan)           cmd_plan "$@" ;;
    generate)       cmd_generate "$@" ;;
    db)             cmd_db "$@" ;;
    run)            cmd_run "$@" ;;
    refresh-stack)  cmd_refresh_stack "$@" ;;
    verify)         cmd_verify "$@" ;;
    migrate-tasks)  cmd_migrate_tasks_json "$@" ;;
    refine-tasks)   cmd_refine_tasks "$@" ;;
    create-sds)     "$shell_bin" "$CLI_ROOT/src/cli/create-sds.sh" "$@" ;;
    create-pdr)     "$shell_bin" "$CLI_ROOT/src/cli/create-pdr.sh" "$@" ;;
    create-jira-tasks) "$CLI_ROOT/src/cli/create-jira-tasks.sh" "$@" ;;
    create-db-dump) "$shell_bin" "$CLI_ROOT/src/cli/create-db-dump.sh" "$@" ;;
    create-tasks)   cmd_create_tasks "$@" ;;
    backlog)        cmd_backlog "$@" ;;
    estimate)       cmd_estimate "$@" ;;
    tokens)         cmd_tokens "$@" ;;
    reports)        cmd_reports "$@" ;;
    task-convert)   cmd_task_convert "$@" ;;
    show-file)      cmd_show_file "$@" ;;
    sweep-artifacts)  cmd_sweep_artifacts "$@" ;;
    tidy-progress)  cmd_tidy_progress "$@" ;;
    work-on-tasks)  cmd_work_on_tasks "$@" ;;
    iterate)        cmd_iterate "$@" ;;
    keys)           cmd_keys "$@" ;;
    tui)            cmd_tui "$@" ;;
    update)         cmd_update "$@" ;;
    *) die "Unknown command: ${cmd}. See '${APP_NAME} help'" ;;
  esac
}

gc_load_api_keys

GC_ORIGINAL_ARGS=("$@")
gc_reports_extract_global_flags "$@"
set -- "${GC_FILTERED_ARGS[@]}"

GC_INVOCATION="$0"
if ((${#GC_ORIGINAL_ARGS[@]})); then
  GC_INVOCATION+=" $(printf '%q ' "${GC_ORIGINAL_ARGS[@]}")"
  GC_INVOCATION="${GC_INVOCATION% }"
fi

if gc_reports_enabled; then
  trap 'gc_reports_activity_trap "$BASH_COMMAND"' DEBUG
fi

trap 'gc_interrupt_handler INT' INT
trap 'gc_interrupt_handler TERM' TERM
trap 'gc_interrupt_handler TSTP' TSTP
trap 'gc_interrupt_handler QUIT' QUIT

trap 'gc_capture_error_context $? "$BASH_COMMAND"' ERR
trap 'gc_exit_handler $?' EXIT

main "$@"
