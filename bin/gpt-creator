#!/usr/bin/env bash
# gpt-creator — scaffolding & orchestration CLI
# Aligns with Product Definition & Requirements (PDR v0.2)
# Usage: gpt-creator <command> [args]

set -Eeuo pipefail

resolve_cli_root() {
  local source="${BASH_SOURCE[0]}"
  while [[ -L "$source" ]]; do
    local dir
    dir="$(cd "$(dirname "$source")" && pwd)"
    source="$(readlink "$source")"
    [[ "$source" != /* ]] && source="$dir/$source"
  done
  cd "$(dirname "$source")/.." && pwd
}

CLI_ROOT="$(resolve_cli_root)"
unset -f resolve_cli_root

gc_env_file() { echo "${PROJECT_ROOT:-$PWD}/.env"; }

gc_random_string() {
  python3 - <<'PY'
import secrets, string
alphabet = string.ascii_letters + string.digits
print(''.join(secrets.choice(alphabet) for _ in range(32)))
PY
}

gc_set_env_var() {
  local key="$1" value="$2"
  local env_file="$(gc_env_file)"
  python3 - <<'PY' "$env_file" "$key" "$value"
import pathlib, sys
path = pathlib.Path(sys.argv[1])
key = sys.argv[2]
value = sys.argv[3]
if path.exists():
    lines = path.read_text().splitlines()
else:
    lines = []
for idx, line in enumerate(lines):
    if line.startswith(f"{key}="):
        lines[idx] = f"{key}={value}"
        break
else:
    lines.append(f"{key}={value}")
path.write_text('\n'.join(lines) + '\n')
PY
}

gc_load_env() {
  local env_file="$(gc_env_file)"
  if [[ -f "$env_file" ]]; then
    set -a
    # shellcheck disable=SC1090
    source "$env_file"
    set +a
  fi
  GC_DB_NAME="${GC_DB_NAME:-${DB_NAME:-app}}"
  GC_DB_USER="${GC_DB_USER:-${DB_USER:-app}}"
  GC_DB_PASSWORD="${GC_DB_PASSWORD:-${DB_PASSWORD:-app_pass}}"
  GC_DB_ROOT_PASSWORD="${GC_DB_ROOT_PASSWORD:-${DB_ROOT_PASSWORD:-root}}"
  GC_DB_HOST_PORT="${GC_DB_HOST_PORT:-${DB_HOST_PORT:-${DB_PORT:-3306}}}"
  DB_NAME="${DB_NAME:-$GC_DB_NAME}"
  DB_USER="${DB_USER:-$GC_DB_USER}"
  DB_PASSWORD="${DB_PASSWORD:-$GC_DB_PASSWORD}"
  DB_ROOT_PASSWORD="${DB_ROOT_PASSWORD:-$GC_DB_ROOT_PASSWORD}"
  DB_HOST_PORT="${DB_HOST_PORT:-$GC_DB_HOST_PORT}"
}

gc_create_env_if_missing() {
  local env_file="$(gc_env_file)"
  if [[ -f "$env_file" ]]; then
    return
  fi
  local slug
  slug="$(basename "${PROJECT_ROOT:-$PWD}")"
  slug=$(printf '%s' "$slug" | tr -c '[:alnum:]' '_')
  slug=$(printf '%s' "$slug" | tr '[:upper:]' '[:lower:]')
  slug=$(printf '%.12s' "$slug")
  [[ -n "$slug" ]] || slug="app"
  local db_name="${slug}_db"
  local db_user="gc_${slug}_user"
  local db_password="$(gc_random_string)"
  local db_root_password="$(gc_random_string)"
  cat > "$env_file" <<EOF
# gpt-creator environment
DB_NAME=${db_name}
DB_USER=${db_user}
DB_PASSWORD=${db_password}
DB_ROOT_USER=root
DB_ROOT_PASSWORD=${db_root_password}
DB_HOST=127.0.0.1
DB_PORT=3306
DB_HOST_PORT=3306
DATABASE_URL=mysql://${db_user}:${db_password}@127.0.0.1:3306/${db_name}
VITE_API_BASE=http://localhost:3000/api/v1
EOF
  chmod 600 "$env_file" || true
}

VERSION="0.2.0"
APP_NAME="gpt-creator"

# Defaults (override via env)
CODEX_BIN="${CODEX_BIN:-codex}"
CODEX_MODEL="${CODEX_MODEL:-gpt-5-codex}"
CODEX_FALLBACK_MODEL="${CODEX_FALLBACK_MODEL:-gpt-5-codex}"
CODEX_REASONING_EFFORT="${CODEX_REASONING_EFFORT:-high}"
EDITOR_CMD="${EDITOR_CMD:-code}"
DOCKER_BIN="${DOCKER_BIN:-docker}"
MYSQL_BIN="${MYSQL_BIN:-mysql}"

# Colors (TTY-only)
if [[ -t 1 ]]; then
  c_reset=$'\033[0m'; c_dim=$'\033[2m'; c_bold=$'\033[1m'
  c_red=$'\033[31m'; c_yellow=$'\033[33m'; c_cyan=$'\033[36m'; c_green=$'\033[32m'
else
  c_reset=; c_dim=; c_bold=; c_red=; c_yellow=; c_cyan=; c_green=
fi

ts() { date +"%Y-%m-%dT%H:%M:%S"; }
die() { echo "${c_red}✖${c_reset} $*" >&2; exit 1; }
info(){ echo "${c_cyan}➜${c_reset} $*"; }
ok()  { echo "${c_green}✔${c_reset} $*"; }
warn(){ echo "${c_yellow}!${c_reset} $*"; }

abs_path() {
  python3 - "$1" <<'PY' 2>/dev/null || perl -MCwd=abs_path -e 'print abs_path(shift)."\n"' "$1" || echo "$1"
import os,sys; print(os.path.abspath(sys.argv[1]))
PY
}

to_lower() {
  printf '%s' "$1" | tr '[:upper:]' '[:lower:]'
}

# Context directories inside project
ensure_ctx() {
  local root="${1:-}"
  if [[ -z "${root}" ]]; then root="${PROJECT_ROOT:-$PWD}"; fi
  PROJECT_ROOT="$(abs_path "$root")"
  GC_DIR="${PROJECT_ROOT}/.gpt-creator"
  STAGING_DIR="${GC_DIR}/staging"
  INPUT_DIR="${STAGING_DIR}/inputs"
  PLAN_DIR="${STAGING_DIR}/plan"
  LOG_DIR="${GC_DIR}/logs"
  ART_DIR="${GC_DIR}/artifacts"
  mkdir -p "$GC_DIR" "$STAGING_DIR" "$INPUT_DIR" "$PLAN_DIR" "$LOG_DIR" "$ART_DIR"
  gc_create_env_if_missing
  gc_load_env
}

gc_parse_jira_tasks() {
  local jira_file="${1:?jira markdown path required}"
  local out_json="${2:?output json path required}"
  python3 - <<'PY' "$jira_file" "$out_json"
import json
import re
import sys
import time
from pathlib import Path

jira_path, out_path = sys.argv[1:3]
lines = Path(jira_path).read_text(encoding='utf-8').splitlines()

epic_id = ""
epic_title = ""
story_id = ""
story_title = ""
tasks = []
current = None
section = None

dashes = r'[\-\u2012\u2013\u2014\u2015]'


def normalise_list(value: str):
    cleaned = value.replace('+', ',').replace('/', ',').replace('&', ',')
    parts = [part.strip() for part in re.split(r',|;|\\band\\b', cleaned, flags=re.I) if part.strip()]
    seen = set()
    ordered = []
    for item in parts:
        key = item.lower()
        if key not in seen:
            seen.add(key)
            ordered.append(item)
    return ordered


def flush_current():
    global current, tasks, section
    if current is None:
        return
    description_lines = [line.rstrip() for line in current['description_lines'] if line.strip()]
    description = "\n".join(description_lines).strip()
    current['description'] = description
    del current['description_lines']
    tasks.append(current)
    current = None
    section = None


for raw in lines:
    stripped = raw.strip()
    if not stripped:
        if current is not None and section == 'description':
            current['description_lines'].append('')
        continue

    if stripped.lower() in {'**epic**', '**story**', '### **story**'}:
        continue

    epic_heading = re.match(r'^##\s+Epic\s+([A-Za-z0-9_.-]+)\s+' + dashes + r'\s+(.*)$', stripped)
    epic_bold = re.match(r'^\*\*([Ee][A-Za-z0-9_.:-]+)\s*' + dashes + r'\s*(.+?)\*\*$', stripped)
    if epic_heading or epic_bold:
        flush_current()
        if epic_heading:
            epic_id, epic_title = epic_heading.group(1).strip(), epic_heading.group(2).strip()
        else:
            epic_id, epic_title = epic_bold.group(1).strip(), epic_bold.group(2).strip()
        story_id = ""
        story_title = ""
        continue

    story_heading = re.match(r'^###\s+Story\s+([A-Za-z0-9_.-]+)\s+' + dashes + r'\s+(.*)$', stripped)
    story_bold = re.match(r'^\*\*([Ss][A-Za-z0-9_.:-]+)\s*' + dashes + r'\s*(.+?)\*\*$', stripped)
    if story_heading or story_bold:
        flush_current()
        if story_heading:
            story_id, story_title = story_heading.group(1).strip(), story_heading.group(2).strip()
        else:
            story_id, story_title = story_bold.group(1).strip(), story_bold.group(2).strip()
        continue

    task_match = re.match(r'^\*\*([Tt][A-Za-z0-9_.:-]+)\s*' + dashes + r'\s*(.+?)\*\*$', stripped)
    if task_match:
        flush_current()
        task_id, task_title = task_match.groups()
        current = {
            'epic_id': epic_id,
            'epic_title': epic_title,
            'story_id': story_id,
            'story_title': story_title,
            'id': task_id.strip(),
            'title': task_title.strip(),
            'assignees': [],
            'tags': [],
            'estimate': '',
            'description_lines': [],
            'acceptance_criteria': [],
            'dependencies': [],
        }
        section = None
        continue

    if current is None:
        continue

    if '**Description:**' in stripped:
        section = 'description'
        after = stripped.split('**Description:**', 1)[1].strip()
        if after:
            current['description_lines'].append(after)
        continue

    if '**Acceptance Criteria:**' in stripped:
        section = 'ac'
        after = stripped.split('**Acceptance Criteria:**', 1)[1].strip()
        if after:
            current['acceptance_criteria'].append(after)
        continue

    if '**Dependencies:**' in stripped:
        section = 'dependencies'
        after = stripped.split('**Dependencies:**', 1)[1].strip()
        if after:
            current['dependencies'] = normalise_list(after)
        continue

    if section == 'ac':
        if stripped.startswith('*') or stripped.startswith('-'):
            current['acceptance_criteria'].append(stripped.lstrip('*- ').rstrip())
            continue
        else:
            section = None

    if section == 'dependencies':
        if stripped.startswith('*') or stripped.startswith('-'):
            current['dependencies'].extend(normalise_list(stripped.lstrip('*- ')))
            continue
        else:
            section = None

    segments = [seg.strip() for seg in re.split(r'[·•]', stripped) if seg.strip()]
    meta_consumed = False
    for seg in segments:
        plain = seg.replace('**', '')
        lower = plain.lower()
        if lower.startswith('assignee:'):
            value = plain.split(':', 1)[1].strip()
            if value:
                current['assignees'] = normalise_list(value)
                meta_consumed = True
        elif lower.startswith('tags:'):
            value = plain.split(':', 1)[1].strip()
            if value:
                current['tags'] = normalise_list(value)
                meta_consumed = True
        elif lower.startswith('estimate:'):
            value = plain.split(':', 1)[1].strip()
            if value and not current['estimate']:
                current['estimate'] = value
                meta_consumed = True

    if section == 'description' or (not meta_consumed and section is None):
        current['description_lines'].append(raw.rstrip())

flush_current()

payload = {
    'generated_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
    'tasks': tasks
}
Path(out_path).write_text(json.dumps(payload, indent=2) + '\n', encoding='utf-8')
PY
}

gc_build_context_file() {
  local dest_file="${1:?context destination required}"
  local staging_dir="${2:-$GC_STAGING_DIR}"
  mkdir -p "$(dirname "$dest_file")"
  {
    echo "# Project Context (auto-generated)"
    echo
    shopt -s nullglob globstar
    local f
    for f in \
      "$staging_dir"/pdr.* \
      "$staging_dir"/sds.* \
      "$staging_dir"/openapi.* \
      "$staging_dir"/*.sql \
      "$staging_dir"/*.mmd \
      "$staging_dir"/*ui*pages*.* \
      "$staging_dir"/*rfp*.* \
      "$staging_dir"/*style*.* \
      "$staging_dir"/*css*; do
      [[ -f "$f" ]] || continue
      echo ""
      echo "----- FILE: $(basename "$f") -----"
      if command -v file >/dev/null 2>&1 && file -b --mime-type "$f" | grep -q 'text'; then
        sed -e 's/\t/  /g' "$f" | sed -e $'s/\r$//'
      else
        echo "(binary or non-text file; path: $f)"
      fi
    done
    shopt -u nullglob globstar
  } >"$dest_file"
}

gc_update_work_state() {
  local state_file="${1:?state file required}"
  local story_slug="${2:?story slug required}"
  local status="${3:?status required}"
  local completed="${4:-0}"
  local total="${5:-0}"
  local run_stamp="${6:-}"
  python3 - <<'PY' "$state_file" "$story_slug" "$status" "$completed" "$total" "$run_stamp"
import json
import sys
import time
from pathlib import Path

state_path = Path(sys.argv[1])
slug = sys.argv[2]
status = sys.argv[3]
completed = int(sys.argv[4] or 0)
total = int(sys.argv[5] or 0)
run_stamp = sys.argv[6] or 'manual'

if state_path.exists():
    try:
        state = json.loads(state_path.read_text())
    except Exception:
        state = {}
else:
    state = {}

stories = state.setdefault('stories', {})
entry = stories.setdefault(slug, {})
entry.update({
    'status': status,
    'completed_tasks': completed,
    'total_tasks': total,
    'run': run_stamp,
    'updated_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())
})

state['last_run'] = run_stamp
state['last_updated'] = entry['updated_at']

state_path.parent.mkdir(parents=True, exist_ok=True)
state_path.write_text(json.dumps(state, indent=2) + '\n')
PY
}

# docker compose helper (prefers "docker compose" then docker-compose)
docker_compose() {
  if command -v docker >/dev/null 2>&1 && docker compose version >/dev/null 2>&1; then
    docker compose "$@"
  elif command -v docker-compose >/dev/null 2>&1; then
    docker-compose "$@"
  else
    die "docker compose is not available. Install Docker Desktop or docker-compose."
  fi
}

port_in_use() {
  local port="$1"
  if command -v lsof >/dev/null 2>&1; then
    lsof -nP -iTCP:"$port" -sTCP:LISTEN >/dev/null 2>&1 && return 0
  elif command -v netstat >/dev/null 2>&1; then
    netstat -an 2>/dev/null | grep -E "\.${port} .*LISTEN" >/dev/null && return 0
  fi
  return 1
}

find_free_port() {
  local start="${1:-3306}"
  local port="$start"; local limit=$((start+100))
  while (( port <= limit )); do
    if ! port_in_use "$port"; then
      echo "$port"
      return 0
    fi
    ((port++)) || true
  done
  echo "$start"  # fallback
}

wait_for_endpoint() {
  local url="$1" label="$2"
  local attempts="${3:-30}" delay="${4:-2}"
  local i=1
  while (( i <= attempts )); do
    if curl -fsS --max-time 2 "$url" >/dev/null 2>&1; then
      ok "${label} ready → ${url}"
      return 0
    fi
    sleep "$delay"
    ((i++)) || true
  done
  warn "${label} not ready after $((attempts * delay))s → ${url}"
  return 1
}

render_template_file() {
  local src="$1" dest="$2"
  local db_name="${GC_DB_NAME:-${DB_NAME:-app}}"
  local db_user="${GC_DB_USER:-${DB_USER:-app}}"
  local db_pass="${GC_DB_PASSWORD:-${DB_PASSWORD:-app_pass}}"
  local db_host_port="${GC_DB_HOST_PORT:-${DB_HOST_PORT:-3306}}"
  local db_root_pass="${GC_DB_ROOT_PASSWORD:-${DB_ROOT_PASSWORD:-root}}"
  python3 - <<'PY' "$src" "$dest" "$db_name" "$db_user" "$db_pass" "$db_host_port" "$db_root_pass"
import pathlib, sys
args = sys.argv[1:]
src, dest, db_name, db_user, db_pass, db_host_port = args[:6]
db_root_pass = args[6] if len(args) > 6 else ''
text = pathlib.Path(src).read_text()
text = text.replace('{{DB_NAME}}', db_name)
text = text.replace('{{DB_USER}}', db_user)
text = text.replace('{{DB_PASSWORD}}', db_pass)
text = text.replace('{{DB_HOST_PORT}}', db_host_port)
text = text.replace('{{DB_ROOT_PASSWORD}}', db_root_pass)
pathlib.Path(dest).write_text(text)
PY
}

gc_render_sql() {
  local src="$1" dest="$2" database="$3" app_user="$4" app_pass="$5"
  python3 - <<'PY' "$src" "$dest" "$database" "$app_user" "$app_pass"
import pathlib, re, sys
src, dest, db_name, app_user, app_pass = sys.argv[1:6]
text = pathlib.Path(src).read_text()

text = text.replace('{{DB_NAME}}', db_name)
text = text.replace('{{DB_USER}}', app_user)
text = text.replace('{{DB_PASSWORD}}', app_pass)

def rewrite_add_column_if_not_exists(sql):
    alter_pattern = re.compile(r'ALTER\s+TABLE\s+(`?)([A-Za-z_][A-Za-z0-9_]*)\1\s+(.*?);', re.IGNORECASE | re.DOTALL)
    parts = []
    last_idx = 0

    def find_clause_end(body, start_idx):
        depth = 0
        i = start_idx
        while i < len(body):
            ch = body[i]
            if ch == '(':
                depth += 1
            elif ch == ')':
                if depth > 0:
                    depth -= 1
            elif ch == ',' and depth == 0:
                return i
            elif ch == ';' and depth == 0:
                return i
            i += 1
        return len(body)

    def split_clauses(body):
        clauses = []
        current = []
        depth = 0
        for ch in body:
            if ch == '(':
                depth += 1
            elif ch == ')':
                if depth > 0:
                    depth -= 1
            if ch == ',' and depth == 0:
                clauses.append(''.join(current))
                current = []
                continue
            current.append(ch)
        tail = ''.join(current)
        if tail.strip():
            clauses.append(tail)
        return clauses

    add_col_pattern = re.compile(r'ADD\s+COLUMN\s+IF\s+NOT\s+EXISTS\s+(`?)([A-Za-z_][A-Za-z0-9_]*)\1\s+', re.IGNORECASE)
    add_key_pattern = re.compile(r'ADD\s+(UNIQUE\s+)?KEY\s+(`?)([A-Za-z_][A-Za-z0-9_]*)\2', re.IGNORECASE)
    add_constraint_pattern = re.compile(r'ADD\s+CONSTRAINT\s+(`?)([A-Za-z_][A-Za-z0-9_]*)\1\s+FOREIGN\s+KEY', re.IGNORECASE)

    for match in alter_pattern.finditer(sql):
        table_name = match.group(2)
        body = match.group(3)
        clauses = split_clauses(body)

        column_additions = []
        index_additions = []
        constraint_additions = []
        leftover_clauses = []

        for clause in clauses:
            clause_stripped = clause.strip()
            if not clause_stripped:
                continue
            col_match = add_col_pattern.match(clause_stripped)
            if col_match:
                definition = clause_stripped[col_match.end():].strip().rstrip(',')
                col_name = col_match.group(2)
                quote = col_match.group(1) or ''
                column_additions.append((col_name, quote, definition))
                continue
            key_match = add_key_pattern.match(clause_stripped)
            if key_match:
                index_name = key_match.group(3)
                index_additions.append((clause_stripped, index_name))
                continue
            constraint_match = add_constraint_pattern.match(clause_stripped)
            if constraint_match:
                constraint_name = constraint_match.group(2)
                constraint_additions.append((clause_stripped, constraint_name))
                continue
            leftover_clauses.append(clause.rstrip())

        if not (column_additions or index_additions or constraint_additions):
            continue

        parts.append(sql[last_idx:match.start()])

        dynamic_sql = []
        for col_name, quote, definition in column_additions:
            column_token = f"{quote}{col_name}{quote}"
            ddl = f"ALTER TABLE `{table_name}` ADD COLUMN {column_token} {definition}".strip()
            ddl_escaped = ddl.replace("'", "''")
            dynamic_sql.append(
                "SET @ddl := (\n"
                "  SELECT IF(\n"
                "    EXISTS(SELECT 1 FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = DATABASE() AND TABLE_NAME = '"
                + table_name + "' AND COLUMN_NAME = '" + col_name + "'),\n"
                "    'DO 0',\n"
                "    '" + ddl_escaped + "'\n"
                "  )\n"
                ");\nPREPARE stmt FROM @ddl;\nEXECUTE stmt;\nDEALLOCATE PREPARE stmt;\n"
            )

        for clause_text, index_name in index_additions:
            ddl = f"ALTER TABLE `{table_name}` {clause_text}".strip()
            ddl_escaped = ddl.replace("'", "''")
            dynamic_sql.append(
                "SET @ddl := (\n"
                "  SELECT IF(\n"
                "    EXISTS(SELECT 1 FROM INFORMATION_SCHEMA.STATISTICS WHERE TABLE_SCHEMA = DATABASE() AND TABLE_NAME = '"
                + table_name + "' AND INDEX_NAME = '" + index_name + "'),\n"
                "    'DO 0',\n"
                "    '" + ddl_escaped + "'\n"
                "  )\n"
                ");\nPREPARE stmt FROM @ddl;\nEXECUTE stmt;\nDEALLOCATE PREPARE stmt;\n"
            )

        for clause_text, constraint_name in constraint_additions:
            ddl = f"ALTER TABLE `{table_name}` {clause_text}".strip()
            ddl_escaped = ddl.replace("'", "''")
            dynamic_sql.append(
                "SET @ddl := (\n"
                "  SELECT IF(\n"
                "    EXISTS(SELECT 1 FROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS WHERE TABLE_SCHEMA = DATABASE() AND TABLE_NAME = '"
                + table_name + "' AND CONSTRAINT_NAME = '" + constraint_name + "'),\n"
                "    'DO 0',\n"
                "    '" + ddl_escaped + "'\n"
                "  )\n"
                ");\nPREPARE stmt FROM @ddl;\nEXECUTE stmt;\nDEALLOCATE PREPARE stmt;\n"
            )

        if leftover_clauses:
            remaining_body = ',\n'.join(leftover_clauses)
            dynamic_sql.append(f"ALTER TABLE `{table_name}`\n{remaining_body}\n;")

        parts.append('\n'.join(dynamic_sql))
        last_idx = match.end()

    parts.append(sql[last_idx:])
    return ''.join(parts)

text = rewrite_add_column_if_not_exists(text)

old_slug_update = """UPDATE instructors
SET slug = LOWER(
  REPLACE(
    REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(
      TRIM(COALESCE(NULLIF(display_name,''), CONCAT(TRIM(first_name),' ',TRIM(last_name)))),
      'ç','c'),'ğ','g'),'ı','i'),'ö','o'),'ş','s'),'ü','u'
    )
  , ' ', '-')
)
WHERE slug IS NULL;"""

new_slug_update = """UPDATE instructors
SET slug = LOWER(
  REPLACE(
    REPLACE(
      REPLACE(
        REPLACE(
          REPLACE(
            REPLACE(
              REPLACE(
                TRIM(COALESCE(NULLIF(display_name,''), CONCAT(TRIM(first_name),' ',TRIM(last_name)))),
                'ç','c'),
              'ğ','g'),
            'ı','i'),
          'ö','o'),
        'ş','s'),
      'ü','u'),
    ' ', '-')
)
WHERE slug IS NULL;"""

text = text.replace(old_slug_update, new_slug_update)

old_unique_block = """ALTER TABLE instructors
  MODIFY slug VARCHAR(120) NOT NULL,
  ADD UNIQUE KEY uq_instructors_slug (slug);"""

new_unique_block = """ALTER TABLE instructors
  MODIFY slug VARCHAR(120) NOT NULL;

SET @ddl := (
  SELECT IF(
    EXISTS(SELECT 1 FROM INFORMATION_SCHEMA.STATISTICS WHERE TABLE_SCHEMA = DATABASE() AND TABLE_NAME = 'instructors' AND INDEX_NAME = 'uq_instructors_slug'),
    'DO 0',
    'ALTER TABLE `instructors` ADD UNIQUE KEY `uq_instructors_slug` (`slug`)' 
  )
);
PREPARE stmt FROM @ddl;
EXECUTE stmt;
DEALLOCATE PREPARE stmt;"""

text = text.replace(old_unique_block, new_unique_block)

text = re.sub(r'^\s*USE\s+`?[^`]+`?;', lambda _m: f"USE `{db_name}`;", text, flags=re.IGNORECASE | re.MULTILINE)
text = re.sub(r'CREATE\s+DATABASE\s+IF\s+NOT\s+EXISTS\s+`[^`]+`', lambda _m: f"CREATE DATABASE IF NOT EXISTS `{db_name}`", text, flags=re.IGNORECASE)
text = re.sub(r'ON\s+`[^`]+`\.\*\s+TO', lambda _m: f"ON `{db_name}`.* TO", text, flags=re.IGNORECASE)
text = re.sub(r"(CREATE\s+USER[^']*')([^']+)(')", lambda m: f"{m.group(1)}{app_user}{m.group(3)}", text, flags=re.IGNORECASE)
text = re.sub(r"(IDENTIFIED\s+BY\s+')([^']+)(')", lambda m: f"{m.group(1)}{app_pass}{m.group(3)}", text, flags=re.IGNORECASE)
text = re.sub(r"(TO\s+')([^']+)('@)", lambda m: f"{m.group(1)}{app_user}{m.group(3)}", text, flags=re.IGNORECASE)

def wrap_add_column(match):
    prefix, name = match.group(1), match.group(2)
    if name.startswith('`') and name.endswith('`'):
        return match.group(0)
    return f"{prefix}`{name}`"

text = re.sub(r"(?i)(ADD\s+COLUMN\s+)([A-Za-z_][A-Za-z0-9_]*)", wrap_add_column, text)

for ident in ("row_number", "field_name", "error_code"):
    pattern = re.compile(rf"(?<![`'])\b{re.escape(ident)}\b(?![`'])", re.IGNORECASE)
    text = pattern.sub(lambda m: f"`{m.group(0)}`", text)

def drop_check_constraint(src_text, name):
    pattern = re.compile(rf"CONSTRAINT\s+{re.escape(name)}\s+CHECK\s*\(", re.IGNORECASE)
    while True:
        match = pattern.search(src_text)
        if not match:
            return src_text
        start = match.start()
        comma_idx = src_text.rfind(',', 0, start)
        if comma_idx == -1:
            comma_idx = start
        depth = 1
        i = match.end()
        while i < len(src_text):
            ch = src_text[i]
            if ch == '(':
                depth += 1
            elif ch == ')':
                depth -= 1
                if depth == 0:
                    i += 1
                    break
            i += 1
        else:
            return src_text
        src_text = src_text[:comma_idx] + src_text[i:]

for constraint in ("ck_seo_target", "ck_legal_rev_publish"):
    text = drop_check_constraint(text, constraint)

pathlib.Path(dest).write_text(text)
PY
}

gc_temp_file() {
  local dir="$1" prefix="$2" suffix="$3"
  python3 - <<'PY' "$dir" "$prefix" "$suffix"
import os, sys, tempfile, pathlib
dir_path, prefix, suffix = sys.argv[1:4]
path = pathlib.Path(dir_path)
path.mkdir(parents=True, exist_ok=True)
fd, temp_path = tempfile.mkstemp(prefix=prefix, suffix=suffix, dir=str(path))
os.close(fd)
print(temp_path)
PY
}

gc_execute_sql() {
  local compose_file="$1" sql_file="$2" database="$3"
  local root_user="$4" root_pass="$5" app_user="$6" app_pass="$7" fallback_init="$8" label="$9"
  local import_ok=0
  [[ -n "$label" ]] || label="operation"
  local container_host="127.0.0.1"

  if [[ -f "$compose_file" ]]; then
    if docker_compose -f "$compose_file" ps >/dev/null 2>&1; then
      info "Using docker-compose db service for ${label}"
      docker_compose -f "$compose_file" up -d db >/dev/null 2>&1 || true
      if [[ -n "$root_pass" ]]; then
        if docker_compose -f "$compose_file" exec -T db mysql -h"${container_host}" -u"${root_user}" -p"${root_pass}" "${database}" < "${sql_file}"; then
          import_ok=1
        else
          warn "Root ${label} failed; retrying as ${app_user}"
        fi
      else
        if docker_compose -f "$compose_file" exec -T db mysql -h"${container_host}" -u"${root_user}" "${database}" < "${sql_file}"; then
          import_ok=1
        fi
      fi
      if [[ "$import_ok" -ne 1 ]]; then
        if docker_compose -f "$compose_file" exec -T db mysql -h"${container_host}" -u"${app_user}" ${app_pass:+-p"${app_pass}"} "${database}" < "${sql_file}"; then
          import_ok=1
        fi
      fi
      if [[ "$import_ok" -ne 1 && -n "$fallback_init" && -f "$fallback_init" ]]; then
        local fallback_output fallback_user fallback_pass
        fallback_output="$(python3 - "$fallback_init" <<'PY'
import re, sys
text = open(sys.argv[1]).read()
user = re.search(r"CREATE USER IF NOT EXISTS '([^']+)'", text)
password = re.search(r"IDENTIFIED BY '([^']+)'", text)
if user and password:
    print(user.group(1))
    print(password.group(1))
PY
)"
        if [[ -n "$fallback_output" ]]; then
          IFS=$'\n' read -r fallback_user fallback_pass _ <<<"$fallback_output"
          unset IFS
          if [[ -n "$fallback_user" && -n "$fallback_pass" ]]; then
            if docker_compose -f "$compose_file" exec -T db mysql -h"${container_host}" -u"${fallback_user}" ${fallback_pass:+-p"${fallback_pass}"} "${database}" < "${sql_file}"; then
              import_ok=1
            fi
          fi
        fi
      fi
      if [[ "$import_ok" -eq 1 ]]; then
        return 0
      fi
    fi
  fi

  local host="${DB_HOST:-127.0.0.1}"
  local port="${DB_HOST_PORT:-${GC_DB_HOST_PORT:-${DB_PORT:-3306}}}"
  if ${MYSQL_BIN} -h "$host" -P "$port" -u "$root_user" ${root_pass:+-p"${root_pass}"} "$database" < "$sql_file"; then
    return 0
  fi

  if ${MYSQL_BIN} -h "$host" -P "$port" -u "$app_user" ${app_pass:+-p"${app_pass}"} "$database" < "$sql_file"; then
    return 0
  fi

  return 1
}

# ---------- Scan helpers ----------
has_pattern() { LC_ALL=C grep -E -i -m 1 -q -- "$1" "$2" 2>/dev/null; }
classify_file() {
  local path="$1"
  local name="${path##*/}"
  local lower="$(to_lower "$name")"
  local path_norm="$(to_lower "$path")"
  local ext="${lower##*.}"
  local type="" conf=0

  case "$ext" in
    md)
      if [[ "$lower" == *pdr* ]]; then type="pdr"; conf=0.95
      elif [[ "$lower" == *sds* ]]; then type="sds"; conf=0.92
      elif [[ "$lower" == *rfp* ]]; then type="rfp"; conf=0.9
      elif [[ "$lower" == *jira* ]]; then type="jira"; conf=0.88
      elif [[ "$lower" == *ui*pages* || "$lower" == *website*ui*pages* ]]; then type="ui_pages"; conf=0.85
      elif has_pattern '\bJIRA\b|Issue Key' "$path"; then type="jira"; conf=0.6
      fi
      ;;
    yml|yaml|json)
      if has_pattern '^[[:space:]]*openapi[[:space:]]*:[[:space:]]*3' "$path" || has_pattern '"openapi"[[:space:]]*:' "$path" || has_pattern '"swagger"[[:space:]]*:' "$path"; then
        type="openapi"; conf=0.94
      fi
      ;;
    sql)
      type="sql"; conf=0.65
      if has_pattern 'CREATE[[:space:]]+TABLE' "$path"; then conf=0.8; fi
      ;;
    mmd)
      type="mermaid"; conf=0.7
      ;;
    html)
      local is_html=0
      if [[ "$path_norm" == *"page_samples"* || "$path_norm" == *"page-samples"* ]]; then
        is_html=1
      elif echo "$lower" | grep -Eq '(abo|auth|prg|evt|ctn)[0-9]+\.html'; then
        is_html=1
      fi
      if [[ $is_html -eq 1 ]]; then
        type="page_sample_html"; conf=0.7
      fi
      ;;
    css)
      local is_css=0
      if [[ "$path_norm" == *"page_samples"* || "$path_norm" == *"samples"* ]]; then
        is_css=1
      elif [[ "$lower" == *style.css ]]; then
        is_css=1
      fi
      if [[ $is_css -eq 1 ]]; then
        type="page_sample_css"; conf=0.6
      fi
      ;;
  esac

  if [[ -n "$type" ]]; then
    printf '%s|%.2f\n' "$type" "$conf"
  fi
}

cmd_scan() {
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"

  info "Scanning ${PROJECT_ROOT} for project artifacts…"
  local manifest="${GC_DIR}/scan.tsv"
  local tmp="${manifest}.tmp"
  printf "type\tconfidence\tpath\n" > "$tmp"

  while IFS= read -r -d '' f; do
    local hit
    hit="$(classify_file "$f")" || true
    if [[ -n "$hit" ]]; then
      local type conf
      IFS='|' read -r type conf <<<"$hit"
      printf "%s\t%.2f\t%s\n" "$type" "$conf" "$f" >> "$tmp"
    fi
  done < <(find "$PROJECT_ROOT" \
      \( -name '.git' -o -name 'node_modules' -o -name 'dist' -o -name 'build' -o -name '.venv' -o -name '.gpt-creator' \) -prune -o -type f -print0)

  mv "$tmp" "$manifest"

  local scan_json="${STAGING_DIR}/scan.json"
  python3 - <<'PY' "$manifest" "$PROJECT_ROOT" "$scan_json"
import csv, json, sys, time, pathlib
manifest, root, out = sys.argv[1:4]
rows = []
with open(manifest, newline='') as fh:
    reader = csv.DictReader(fh, delimiter='\t')
    for row in reader:
        if not row['type']:
            continue
        rows.append({
            "type": row['type'],
            "confidence": float(row['confidence'] or 0),
            "path": str(pathlib.Path(row['path']).resolve())
        })
scan = {
    "project_root": str(pathlib.Path(root).resolve()),
    "generated_at": time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
    "artifacts": rows
}
pathlib.Path(out).parent.mkdir(parents=True, exist_ok=True)
with open(out, 'w') as fh:
    json.dump(scan, fh, indent=2)
print(out)
PY
  ok "Scan manifest → ${scan_json}"
}

cmd_normalize() {
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"

  local scan_json="${STAGING_DIR}/scan.json"
  if [[ ! -f "$scan_json" ]]; then
    warn "No scan.json found, running scan first."
    cmd_scan --project "$PROJECT_ROOT"
  fi

  scan_json="${STAGING_DIR}/scan.json"
  python3 - <<'PY' "$scan_json" "$INPUT_DIR" "$PLAN_DIR"
import json, sys, shutil, pathlib, time
scan_path, input_dir, plan_dir = sys.argv[1:4]
input_dir = pathlib.Path(input_dir)
plan_dir = pathlib.Path(plan_dir)
input_dir.mkdir(parents=True, exist_ok=True)
plan_dir.mkdir(parents=True, exist_ok=True)

data = json.load(open(scan_path))
project_root = pathlib.Path(data.get('project_root', '.')).resolve()
artifacts = data.get('artifacts', [])

unique_types = {"pdr", "sds", "rfp", "jira", "ui_pages", "openapi"}
unique = {}
for entry in artifacts:
    t = entry.get('type')
    if t in unique_types:
        if t not in unique or entry['confidence'] > unique[t]['confidence']:
            unique[t] = entry

multi_map = {
    "sql": pathlib.Path('sql'),
    "mermaid": pathlib.Path('mermaid'),
    "page_sample_html": pathlib.Path('page_samples'),
    "page_sample_css": pathlib.Path('page_samples')
}
collected = {key: [] for key in multi_map}
for entry in artifacts:
    t = entry.get('type')
    if t in multi_map:
        collected[t].append(entry)

provenance = []

def copy_file(src_path, rel_dest, entry):
    src = pathlib.Path(src_path)
    dest = input_dir / rel_dest
    dest.parent.mkdir(parents=True, exist_ok=True)
    shutil.copy2(src, dest)
    provenance.append({
        "type": entry.get('type'),
        "source": str(src),
        "destination": str(dest.relative_to(input_dir)),
        "confidence": entry.get('confidence', 0)
    })

name_map = {
    "pdr": pathlib.Path('pdr.md'),
    "sds": pathlib.Path('sds.md'),
    "rfp": pathlib.Path('rfp.md'),
    "jira": pathlib.Path('jira.md'),
    "ui_pages": pathlib.Path('ui-pages.md')
}

for t, entry in unique.items():
    src = entry['path']
    if t == 'openapi':
        suffix = pathlib.Path(src).suffix.lower()
        if suffix in {'.yaml', '.yml'}:
            rel = pathlib.Path('openapi.yaml')
        elif suffix == '.json':
            rel = pathlib.Path('openapi.json')
        else:
            rel = pathlib.Path('openapi.src')
    else:
        rel = name_map[t]
    copy_file(src, rel, entry)

for t, entries in collected.items():
    dest_root = multi_map[t]
    for entry in entries:
        src = pathlib.Path(entry['path'])
        try:
            rel = src.resolve().relative_to(project_root)
        except ValueError:
            rel = pathlib.Path(src.name)
        rel = dest_root / rel
        copy_file(src, rel, entry)

# discovery.yaml (summary)
import io
from textwrap import indent
summary = io.StringIO()
summary.write('generated_at: %s\n' % time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()))
summary.write('project_root: %s\n' % project_root)
summary.write('artifacts:\n')
for entry in artifacts:
    summary.write('  - type: %s\n' % entry.get('type'))
    summary.write('    confidence: %.2f\n' % entry.get('confidence', 0))
    summary.write('    path: %s\n' % entry.get('path'))
(input_dir / '..' / 'discovery.yaml').resolve().write_text(summary.getvalue())

prov = {
    'generated_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
    'entries': provenance
}
(plan_dir / 'provenance.json').write_text(json.dumps(prov, indent=2))
PY

  ok "Normalized inputs → ${INPUT_DIR}"
}

cmd_plan() {
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"

  local openapi=""
  for cand in "$INPUT_DIR/openapi.yaml" "$INPUT_DIR/openapi.yml" "$INPUT_DIR/openapi.json" "$INPUT_DIR/openapi.src"; do
    [[ -f "$cand" ]] && { openapi="$cand"; break; }
  done
  local sql_dir="$INPUT_DIR/sql"

  python3 - <<'PY' "$openapi" "$sql_dir" "$PLAN_DIR"
import json, os, re, sys, time, pathlib
from collections import OrderedDict
openapi_path, sql_dir, plan_dir = sys.argv[1:4]
plan_dir = pathlib.Path(plan_dir)
plan_dir.mkdir(parents=True, exist_ok=True)

routes = []
schemas = []
openapi_loaded = False
if openapi_path:
    try:
        text = pathlib.Path(openapi_path).read_text()
        if openapi_path.endswith('.json'):
            data = json.loads(text)
            openapi_loaded = True
        else:
            try:
                import yaml  # type: ignore
                data = yaml.safe_load(text)  # type: ignore
                openapi_loaded = True
            except Exception:
                data = None
        if openapi_loaded and isinstance(data, dict):
            for path, methods in (data.get('paths') or {}).items():
                if isinstance(methods, dict):
                    for method, body in methods.items():
                        if not isinstance(body, dict):
                            continue
                        routes.append({
                            'method': method.upper(),
                            'path': path,
                            'summary': body.get('summary') or ''
                        })
            schemas = list((data.get('components') or {}).get('schemas') or {})
        else:
            raise ValueError('fallback parser')
    except Exception:
        routes = []
        schemas = []
        text = pathlib.Path(openapi_path).read_text() if openapi_path else ''
        current_path = None
        for line in text.splitlines():
            if re.match(r'^\s*/[^\s]+:\s*$', line):
                current_path = line.strip().rstrip(':')
                continue
            if current_path:
                m = re.match(r'^\s{2,}(get|post|put|patch|delete|options|head):\s*$', line, re.I)
                if m:
                    routes.append({'method': m.group(1).upper(), 'path': current_path, 'summary': ''})
                    continue
                if re.match(r'^\S', line):
                    current_path = None

        in_components = False
        in_schemas = False
        for line in text.splitlines():
            stripped = line.strip()
            if not stripped:
                continue
            if re.match(r'^components:\s*$', stripped):
                in_components = True
                in_schemas = False
                continue
            if in_components and re.match(r'^schemas:\s*$', stripped):
                in_schemas = True
                continue
            indent = len(line) - len(line.lstrip(' '))
            if in_schemas:
                if indent <= 2 and not stripped.startswith('#') and not stripped.startswith('schemas:'):
                    in_schemas = False
                    continue
                if indent == 4 and re.match(r'^[A-Za-z0-9_.-]+:\s*$', stripped):
                    name = stripped.split(':', 1)[0]
                    schemas.append(name)

sql_tables = []
sql_dir_path = pathlib.Path(sql_dir)
if sql_dir and sql_dir_path.is_dir():
    for sql_file in sql_dir_path.rglob('*.sql'):
        try:
            text = sql_file.read_text()
        except Exception:
            continue
        for m in re.finditer(r'CREATE\s+TABLE\s+`?([A-Za-z0-9_]+)`?', text, flags=re.IGNORECASE):
            sql_tables.append(m.group(1))

schema_set = {s.lower() for s in schemas}
table_set = {t.lower() for t in sql_tables}
only_in_openapi = sorted(schema_set - table_set)
only_in_sql = sorted(table_set - schema_set)

routes_path = plan_dir / 'routes.md'
entities_path = plan_dir / 'entities.md'
tasks_path = plan_dir / 'tasks.json'
plan_todo = plan_dir / 'PLAN_TODO.md'

def write_routes():
    lines = ['# Routes', '']
    if routes:
        for item in sorted(routes, key=lambda r: (r['path'], r['method'])):
            summary = f" — {item['summary']}" if item.get('summary') else ''
            lines.append(f"- `{item['method']} {item['path']}`{summary}")
    else:
        lines.append('No routes detected — ensure openapi.yaml is present in staging/inputs.')
    routes_path.write_text('\n'.join(lines) + '\n')


def write_entities():
    lines = ['# Entities', '']
    lines.append('## OpenAPI Schemas')
    if schemas:
        for name in sorted(schemas):
            lines.append(f'- {name}')
    else:
        lines.append('- (none found)')
    lines.append('')
    lines.append('## SQL Tables')
    if sql_tables:
        for name in sorted(sql_tables):
            lines.append(f'- {name}')
    else:
        lines.append('- (none found)')
    lines.append('')
    lines.append('## Detected deltas')
    if only_in_openapi:
        lines.append('- Only in OpenAPI: ' + ', '.join(only_in_openapi))
    if only_in_sql:
        lines.append('- Only in SQL: ' + ', '.join(only_in_sql))
    if not only_in_openapi and not only_in_sql:
        lines.append('- None (schemas and tables aligned on name)')
    entities_path.write_text('\n'.join(lines) + '\n')


def write_tasks():
    tasks = []
    if only_in_openapi:
        tasks.append({
            'id': 'align-openapi-sql',
            'title': 'Align OpenAPI schemas with SQL tables',
            'details': f"Create tables or update schemas for: {', '.join(only_in_openapi)}"
        })
    if only_in_sql:
        tasks.append({
            'id': 'document-sql-gap',
            'title': 'Document SQL tables missing from OpenAPI',
            'details': f"Expose or document SQL tables not covered by API: {', '.join(only_in_sql)}"
        })
    tasks_path.write_text(json.dumps({'generated_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()), 'tasks': tasks}, indent=2))


def write_plan_todo():
    lines = [
        '# Build Plan',
        '',
        '- Validate discovery outputs under `staging/inputs`.',
        '- Review `routes.md` & `entities.md` for coverage and deltas.',
        '- Implement generation steps for API, DB, Web, Admin, Docker.',
        '- Run `gpt-creator generate all --project <path>` if not already executed.',
        '- Bring the stack up with `gpt-creator run up` and smoke test.',
        '- Execute `gpt-creator verify all` to satisfy acceptance & NFR gates.',
        '- Iterate on Jira tasks using `gpt-creator iterate` until checks pass.'
    ]
    plan_todo.write_text('\n'.join(lines) + '\n')

write_routes()
write_entities()
write_tasks()
write_plan_todo()
PY

  ok "Plan artifacts created under ${PLAN_DIR}"
}

copy_template_tree() {
  local src="$1" dest="$2"
  [[ -d "$src" ]] || die "Template directory not found: $src"
  find "$src" -type d ! -name '.DS_Store' | while IFS= read -r dir; do
    local rel="${dir#$src}"
    mkdir -p "$dest/$rel"
  done
  find "$src" -type f | while IFS= read -r file; do
    local base="$(basename "$file")"
    [[ "$base" == '.DS_Store' ]] && continue
    local rel="${file#$src/}"
    local target="$dest/$rel"
    if [[ "$target" == *.tmpl ]]; then
      target="${target%.tmpl}"
      mkdir -p "$(dirname "$target")"
      render_template_file "$file" "$target"
    else
      mkdir -p "$(dirname "$target")"
      cp "$file" "$target"
    fi
  done
}

cmd_generate() {
  local facet="${1:-}"; shift || true
  [[ -n "$facet" ]] || die "generate requires a facet: api|web|admin|db|docker|all"
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"
  local templates="$CLI_ROOT/templates"

  case "$facet" in
    api)
      local out="$PROJECT_ROOT/apps/api"
      mkdir -p "$out"
      copy_template_tree "$templates/api/nestjs" "$out"
      ok "API scaffolded → ${out}"
      ;;
    web)
      local out="$PROJECT_ROOT/apps/web"
      mkdir -p "$out"
      copy_template_tree "$templates/web/vue3" "$out"
      ok "Web scaffolded → ${out}"
      ;;
    admin)
      local out="$PROJECT_ROOT/apps/admin"
      mkdir -p "$out"
      copy_template_tree "$templates/admin/vue3" "$out"
      ok "Admin scaffolded → ${out}"
      ;;
    db)
      local out="$PROJECT_ROOT/db"
      mkdir -p "$out"
      copy_template_tree "$templates/db/mysql" "$out"
      ok "DB artifacts scaffolded → ${out}"
      ;;
    docker)
      local out="$PROJECT_ROOT/docker"
      mkdir -p "$out"
      local preferred="${GC_DB_HOST_PORT:-${DB_HOST_PORT:-${MYSQL_HOST_PORT:-3306}}}"
      if port_in_use "$preferred"; then
        local next; next="$(find_free_port "$preferred")"
        if [[ "$next" != "$preferred" ]]; then
          info "Port $preferred in use; remapping MySQL to $next"
          preferred="$next"
        fi
      fi
      GC_DB_HOST_PORT="$preferred"
      DB_HOST_PORT="$GC_DB_HOST_PORT"
      MYSQL_HOST_PORT="$GC_DB_HOST_PORT"
      gc_set_env_var DB_HOST_PORT "$GC_DB_HOST_PORT"
      gc_set_env_var MYSQL_HOST_PORT "$GC_DB_HOST_PORT"
      gc_set_env_var GC_DB_HOST_PORT "$GC_DB_HOST_PORT"
      local local_url="mysql://${GC_DB_USER}:${GC_DB_PASSWORD}@127.0.0.1:${GC_DB_HOST_PORT}/${GC_DB_NAME}"
      gc_set_env_var DATABASE_URL "$local_url"
      gc_load_env
      copy_template_tree "$templates/docker" "$out"
      ok "Docker assets scaffolded → ${out}"
      ;;
    all)
      for f in api db web admin docker; do
        cmd_generate "$f" --project "$PROJECT_ROOT"
      done
      return 0
      ;;
    *) die "Unknown facet: ${facet}";;
  esac
}

cmd_db() {
  local action="${1:-}"; shift || true
  [[ -n "$action" ]] || die "db requires: provision|import|seed"
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"
  local compose_file="$PROJECT_ROOT/docker/docker-compose.yml"

  case "$action" in
    provision)
      [[ -f "$compose_file" ]] || die "Compose file not found at ${compose_file}; run 'gpt-creator generate docker'."
      info "Starting database service via docker compose"
      docker_compose -f "$compose_file" up -d db
      ok "MySQL container provisioned"
      ;;
    import)
      local sql_file
      sql_file="$(find "$INPUT_DIR/sql" -maxdepth 2 -type f -name '*.sql' | head -n1 || true)"
      [[ -n "$sql_file" ]] || die "No staged SQL found under ${INPUT_DIR}/sql"
      info "Importing SQL from ${sql_file}"
      local database="${DB_NAME:-$GC_DB_NAME}"
      local root_user="${DB_ROOT_USER:-root}"
      local root_pass="${DB_ROOT_PASSWORD:-${GC_DB_ROOT_PASSWORD:-}}"
      local app_user="${DB_USER:-$GC_DB_USER}"
      local app_pass="${DB_PASSWORD:-$GC_DB_PASSWORD}"
      local cleanup_files=()
      trap 'for f in "${cleanup_files[@]}"; do [[ -n "$f" && -f "$f" ]] && rm -f "$f"; done; trap - RETURN' RETURN
      local rendered_sql
      rendered_sql="$(gc_temp_file "$STAGING_DIR" "import-" ".sql")"
      cleanup_files+=("$rendered_sql")
      gc_render_sql "$sql_file" "$rendered_sql" "$database" "$app_user" "$app_pass"
      local init_sql="${INPUT_DIR}/sql/db/init.sql"
      if gc_execute_sql "$compose_file" "$rendered_sql" "$database" "$root_user" "$root_pass" "$app_user" "$app_pass" "$init_sql" "import"; then
        ok "Database import finished"
      else
        die "Database import failed"
      fi
      ;;
    seed)
      local seed_file="${PROJECT_ROOT}/db/seed.sql"
      [[ -f "$seed_file" ]] || die "Seed file not found: ${seed_file}"
      info "Seeding database from ${seed_file}"
      local database="${DB_NAME:-$GC_DB_NAME}"
      local root_user="${DB_ROOT_USER:-root}"
      local root_pass="${DB_ROOT_PASSWORD:-${GC_DB_ROOT_PASSWORD:-}}"
      local app_user="${DB_USER:-$GC_DB_USER}"
      local app_pass="${DB_PASSWORD:-$GC_DB_PASSWORD}"
      local cleanup_files=()
      trap 'for f in "${cleanup_files[@]}"; do [[ -n "$f" && -f "$f" ]] && rm -f "$f"; done; trap - RETURN' RETURN
      local rendered_seed
      rendered_seed="$(gc_temp_file "$STAGING_DIR" "seed-" ".sql")"
      cleanup_files+=("$rendered_seed")
      gc_render_sql "$seed_file" "$rendered_seed" "$database" "$app_user" "$app_pass"
      local fallback_init="${PROJECT_ROOT}/db/init.sql"
      if [[ ! -f "$fallback_init" ]]; then
        fallback_init="${INPUT_DIR}/sql/db/init.sql"
      fi
      if gc_execute_sql "$compose_file" "$rendered_seed" "$database" "$root_user" "$root_pass" "$app_user" "$app_pass" "$fallback_init" "seed"; then
        ok "Database seed applied"
      else
        die "Database seed failed"
      fi
      ;;
    *) die "Unknown db action: ${action}";;
  esac
}

cmd_run() {
  local action="${1:-}"; shift || true
  [[ -n "$action" ]] || die "run requires: up|down|logs|open"
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"
  local compose_file="$PROJECT_ROOT/docker/docker-compose.yml"

  case "$action" in
    up)
      [[ -f "$compose_file" ]] || die "Compose file not found at ${compose_file}; generate docker assets first."
      docker_compose -f "$compose_file" up -d
      ok "Stack is starting (check docker compose ps)"
      local api_base="${GC_API_BASE_URL:-http://localhost:3000/api/v1}"
      local web_url="${GC_WEB_URL:-http://localhost:8080/}"
      local admin_url="${GC_ADMIN_URL:-http://localhost:8080/admin/}"
      wait_for_endpoint "${api_base%/}/health" "API /health" 40 3 || true
      local web_ping="${web_url%/}/__vite_ping"
      if ! wait_for_endpoint "$web_ping" "Web (vite ping)" 40 3; then
        wait_for_endpoint "${web_url%/}/" "Web" 20 3 || true
      fi
      local admin_ping="${admin_url%/}/__vite_ping"
      if ! wait_for_endpoint "$admin_ping" "Admin (vite ping)" 40 3; then
        wait_for_endpoint "${admin_url%/}/" "Admin" 20 3 || true
      fi
      ;;
    down)
      [[ -f "$compose_file" ]] || die "Compose file not found at ${compose_file}"
      docker_compose -f "$compose_file" down
      ok "Stack shut down"
      ;;
    logs)
      [[ -f "$compose_file" ]] || die "Compose file not found at ${compose_file}"
      docker_compose -f "$compose_file" logs -f
      ;;
    open)
      if command -v open >/dev/null 2>&1; then
        open "http://localhost:8080" || open "http://localhost:5173" || true
      else
        ${EDITOR_CMD} "$PROJECT_ROOT" || true
      fi
      ;;
    *) die "Unknown run action: ${action}";;
  esac
}

cmd_verify() {
  local kind="${1:-all}"; shift || true
  local root=""
  local api_base="${GC_API_BASE_URL:-http://localhost:3000/api/v1}"
  local api_health="${GC_API_HEALTH_URL:-}"
  local web_url="${GC_WEB_URL:-http://localhost:8080/}"
  local admin_url="${GC_ADMIN_URL:-http://localhost:8080/admin/}"

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --api-url) api_base="$2"; shift 2;;
      --api-health) api_health="$2"; shift 2;;
      --web-url) web_url="$2"; shift 2;;
      --admin-url) admin_url="$2"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"

  if [[ -z "$api_health" ]]; then
    local trimmed_base="${api_base%/}"
    api_health="${trimmed_base}/health"
  fi

  local verify_root="$CLI_ROOT/verify"
  [[ -d "$verify_root" ]] || die "verify scripts directory missing at ${verify_root}"

  local -a check_names
  case "$kind" in
    acceptance) check_names=(acceptance) ;;
    nfr) check_names=(openapi a11y lighthouse consent program_filters) ;;
    all) check_names=(acceptance openapi a11y lighthouse consent program_filters) ;;
    *) die "Unknown verify target: ${kind}";;
  esac

  local pass=0 fail=0 skip=0
  local compose_file="${PROJECT_ROOT}/docker/docker-compose.yml"
  run_check() {
    local label="$1"; shift
    if "$@"; then
      ((pass++))
    else
      local status=$?
      if [[ $status -eq 3 ]]; then
        ((skip++))
        warn "${label} check skipped (missing dependency)"
      else
        ((fail++))
        warn "${label} check failed (exit ${status})"
      fi
    fi
  }

  for name in "${check_names[@]}"; do
    case "$name" in
      acceptance)
        run_check "acceptance" \
          env PROJECT_ROOT="$PROJECT_ROOT" GC_COMPOSE_FILE="$compose_file" \
          bash "$verify_root/acceptance.sh" "${api_base}" "${web_url}" "${admin_url}"
        ;;
      openapi)
        local spec=""
        for cand in "$INPUT_DIR/openapi.yaml" "$INPUT_DIR/openapi.yml" "$INPUT_DIR/openapi.json"; do
          [[ -f "$cand" ]] && { spec="$cand"; break; }
        done
        run_check "openapi" bash "$verify_root/check-openapi.sh" "${spec}" ;;
      a11y)
        run_check "a11y" bash "$verify_root/check-a11y.sh" "${web_url}" "${admin_url}"
        ;;
      lighthouse)
        run_check "lighthouse" bash "$verify_root/check-lighthouse.sh" "${web_url}" "${admin_url}"
        ;;
      consent)
        run_check "consent" bash "$verify_root/check-consent.sh" "${web_url}"
        ;;
      program_filters)
        run_check "program-filters" bash "$verify_root/check-program-filters.sh" "${api_base}"
        ;;
    esac
  done

  if (( fail > 0 )); then
    die "Verify failed — pass=${pass} fail=${fail} skip=${skip}"
  fi
  ok "Verify complete — pass=${pass} skip=${skip}"
}

codex_call() {
  local task="${1:?task}"; shift || true
  local prompt_dir="${GC_DIR}/prompts"
  mkdir -p "$prompt_dir"

  local prompt_file=""
  local output_file=""

  if [[ $# -gt 0 && -f "$1" ]]; then
    prompt_file="$1"
    shift || true
  fi

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --prompt) prompt_file="$2"; shift 2;;
      --output) output_file="$2"; shift 2;;
      *) break;;
    esac
  done

  if [[ -z "$prompt_file" ]]; then
    prompt_file="${prompt_dir}/${task}.md"
    if [[ ! -f "$prompt_file" ]]; then
      cat >"$prompt_file" <<'PROMPT'
# Instruction
You are Codex (gpt-5-codex) assisting the gpt-creator pipeline. Apply requested changes deterministically.
PROMPT
    fi
  fi

  if command -v "$CODEX_BIN" >/dev/null 2>&1; then
    info "Codex ${task} → model=${CODEX_MODEL}"
    local fallback_model="${CODEX_FALLBACK_MODEL:-gpt-5-codex}"
    local run_codex_model
    run_codex_model() {
      local model="$1"
      shift || true
      local args=(exec --model "$model")
      if [[ -n "${CODEX_PROFILE:-}" ]]; then
        args+=(--profile "$CODEX_PROFILE")
      fi
      if [[ -n "${PROJECT_ROOT:-}" ]]; then
        args+=(--cd "$PROJECT_ROOT")
      fi
      if [[ -n "${CODEX_REASONING_EFFORT:-}" ]]; then
        args+=(-c "model_reasoning_effort=\"${CODEX_REASONING_EFFORT}\"")
      fi
      args+=(--full-auto --sandbox workspace-write --skip-git-repo-check)
      if [[ -n "$output_file" ]]; then
        mkdir -p "$(dirname "$output_file")"
        args+=(--output-last-message "$output_file")
      fi
      "$CODEX_BIN" "${args[@]}" < "$prompt_file"
      return $?
    }
    if run_codex_model "$CODEX_MODEL"; then
      :
    elif [[ "$CODEX_MODEL" != "$fallback_model" ]]; then
      warn "Codex model ${CODEX_MODEL} failed; retrying with ${fallback_model}."
      run_codex_model "$fallback_model" || warn "Codex invocation returned non-zero."
    else
      warn "Codex invocation returned non-zero."
    fi
  else
    warn "Codex binary (${CODEX_BIN}) not found — skipping ${task}."
  fi
}

gc_apply_codex_changes() {
  local output_file="${1:?output file required}"
  local project_root="${2:?project root required}"
  python3 - <<'PY' "$output_file" "$project_root"
import json
import subprocess
import sys
from pathlib import Path

output_path = Path(sys.argv[1])
project_root = Path(sys.argv[2])

if not output_path.exists():
    print("no-output", flush=True)
    sys.exit(0)

raw = output_path.read_text(encoding='utf-8').strip()
if not raw:
    print("empty-output", flush=True)
    sys.exit(0)

# Remove code fences if present
if '```' in raw:
    cleaned = []
    fenced = False
    for line in raw.splitlines():
        marker = line.strip()
        if marker.startswith('```'):
            fenced = not fenced
            continue
        if not fenced:
            cleaned.append(line)
    raw = '\n'.join(cleaned).strip()

start = raw.find('{')
end = raw.rfind('}')
if start == -1 or end == -1 or end <= start:
    raise SystemExit("JSON not found in Codex output")

fragment = raw[start:end+1]

import re
fragment = re.sub(r'\\"(?=[}\]\n])', r'\\""', fragment)

while True:
    try:
        payload = json.loads(fragment)
        break
    except json.JSONDecodeError as exc:
        if 'Invalid \\escape' in exc.msg:
            fragment = fragment[:exc.pos] + '\\' + fragment[exc.pos:]
            continue
        decoder = json.JSONDecoder(strict=False)
        try:
            payload = decoder.decode(fragment)
            break
        except json.JSONDecodeError:
            raw_dump = output_path.with_suffix(output_path.suffix + '.raw.txt')
            raw_dump.parent.mkdir(parents=True, exist_ok=True)
            raw_dump.write_text(raw, encoding='utf-8')
            rel_dump = raw_dump
            try:
                rel_dump = raw_dump.relative_to(project_root)
            except ValueError:
                rel_dump = raw_dump
            payload = {
                'plan': [],
                'changes': [],
                'commands': [],
                'notes': [
                    f"Codex output could not be parsed as JSON; review {rel_dump}."
                ],
            }
            print(f"RAW {rel_dump}")
            break

changes = payload.get('changes') or []
written = []
patched = []
manual_notes = []

def ensure_within_root(path: Path) -> Path:
    try:
        full = (project_root / path).resolve(strict=False)
        project = project_root.resolve(strict=True)
    except FileNotFoundError:
        project = project_root.resolve()
        full = (project_root / path).resolve(strict=False)
    if not str(full).startswith(str(project)):
        raise ValueError(f"Path {path} escapes project root")
    return full

for change in changes:
    ctype = change.get('type')
    path = change.get('path')
    if not path:
        raise ValueError('Change entry missing path')
    if ctype == 'file':
        content = change.get('content', '')
        dest = ensure_within_root(Path(path))
        dest.parent.mkdir(parents=True, exist_ok=True)
        dest.write_text(content, encoding='utf-8')
        written.append(str(dest.relative_to(project_root)))
    elif ctype == 'patch':
        diff = change.get('diff')
        if not diff:
            raise ValueError(f"Patch change for {path} missing diff")
        proc = subprocess.run(
            ['git', 'apply', '--whitespace=nowarn', '-'],
            input=diff.encode('utf-8'),
            cwd=str(project_root),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=False,
        )
        if proc.returncode != 0:
            git_err = proc.stderr.decode('utf-8')

            three_way = subprocess.run(
                ['git', 'apply', '--3way', '--whitespace=nowarn', '-'],
                input=diff.encode('utf-8'),
                cwd=str(project_root),
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                check=False,
            )

            if three_way.returncode == 0:
                patched.append(path + ' (3way)')
                continue

            git_err += three_way.stderr.decode('utf-8')

            fallback = subprocess.run(
                ['patch', '-p1', '--forward', '--silent'],
                input=diff.encode('utf-8'),
                cwd=str(project_root),
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                check=False,
            )
            if fallback.returncode != 0:
                # check if patch already applied
                already = subprocess.run(
                    ['git', 'apply', '--reverse', '--check', '-'],
                    input=diff.encode('utf-8'),
                    cwd=str(project_root),
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    check=False,
                )
                if already.returncode == 0:
                    patched.append(path + ' (already applied)')
                    continue

                new_content = None
                diff_lines = diff.splitlines()
                if any(line.startswith('--- /dev/null') for line in diff_lines):
                    capture = False
                    content_lines = []
                    for line in diff_lines:
                        if line.startswith('@@'):
                            capture = True
                            continue
                        if not capture:
                            continue
                        if not line or line.startswith('diff --git'):
                            continue
                        if line.startswith('+'):
                            content_lines.append(line[1:])
                        elif line.startswith('-') or line.startswith('---') or line.startswith('+++'):
                            continue
                        elif line.startswith('\\'):
                            continue
                        else:
                            content_lines.append(line)
                    if content_lines:
                        new_content = '\n'.join(content_lines)
                        if not new_content.endswith('\n'):
                            new_content += '\n'
                if new_content is not None:
                    dest = ensure_within_root(Path(path))
                    dest.parent.mkdir(parents=True, exist_ok=True)
                    if dest.exists():
                        existing = dest.read_text(encoding='utf-8')
                        if existing == new_content:
                            patched.append(path + ' (already exists)')
                            continue
                    dest.write_text(new_content, encoding='utf-8')
                    patched.append(path + ' (reconstructed)')
                    continue

                if new_content is None:
                    try:
                        proc_noctx = subprocess.run(
                            ['git', 'apply', '--reject', '--whitespace=nowarn', '-'],
                            input=diff.encode('utf-8'),
                            cwd=str(project_root),
                            stdout=subprocess.PIPE,
                            stderr=subprocess.PIPE,
                            check=False,
                        )
                        if proc_noctx.returncode == 0:
                            patched.append(path + ' (partial apply)')
                            continue
                        else:
                            git_err += proc_noctx.stderr.decode('utf-8')
                    except Exception:
                        pass

                manual_patch = output_path.with_suffix(output_path.suffix + f".{len(manual_notes)+1}.patch")
                manual_patch.write_text(diff, encoding='utf-8')
                relative_manual = manual_patch
                try:
                    relative_manual = manual_patch.relative_to(project_root)
                except ValueError:
                    pass
                manual_notes.append(f"Patch for {path} could not be applied automatically. Review and apply {relative_manual} manually.")
                patched.append(path + ' (manual)')
                sys.stderr.write(git_err)
                sys.stderr.write(fallback.stderr.decode('utf-8'))
                continue
            patched.append(path + ' (patch)')
        else:
            patched.append(path)
    else:
        raise ValueError(f"Unknown change type: {ctype}")

summary = {
    'written': written,
    'patched': patched,
    'commands': payload.get('commands') or [],
    'notes': (payload.get('notes') or []) + manual_notes,
}
print('APPLIED')
for path in written:
    print(f"WRITE {path}")
for path in patched:
    print(f"PATCH {path}")
for cmd in summary['commands']:
    print(f"CMD {cmd}")
for note in summary['notes']:
    print(f"NOTE {note}")
PY
}

cmd_task_convert() {
  local root="" jira="" force=0
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --jira) jira="$(abs_path "$2")"; shift 2;;
      --force) force=1; shift;;
      -h|--help)
        cat <<'EOUSAGE'
Usage: gpt-creator task-convert [--project PATH] [--jira FILE] [--force]

Convert Jira markdown tasks into per-story JSON files under .gpt-creator/staging/plan/tasks.

Options:
  --project PATH  Project root (defaults to current directory)
  --jira FILE     Jira markdown source (defaults to staging/inputs/jira.md)
  --force         Rebuild all story JSON files even if they already exist
EOUSAGE
        return 0
        ;;
      *) break;;
    esac
  done

  ensure_ctx "$root"
  [[ -n "$jira" ]] || jira="${INPUT_DIR}/jira.md"
  [[ -f "$jira" ]] || die "Jira tasks file not found: ${jira}"

  local tasks_dir="${PLAN_DIR}/tasks"
  local stories_dir="${tasks_dir}/stories"
  local manifest="${tasks_dir}/manifest.json"
  local parsed_local="${tasks_dir}/parsed.local.json"
  mkdir -p "$stories_dir"

  info "Parsing Jira backlog → ${parsed_local}"
  gc_parse_jira_tasks "$jira" "$parsed_local"

  local stats
  if ! stats="$(python3 - "$parsed_local" "$stories_dir" "$manifest" "$force" <<'PY'
import hashlib
import json
import re
import sys
import time
from pathlib import Path

parsed_path = Path(sys.argv[1])
stories_dir = Path(sys.argv[2])
manifest_path = Path(sys.argv[3])
force_flag = sys.argv[4] == '1'

stories_dir.mkdir(parents=True, exist_ok=True)

data = json.loads(parsed_path.read_text(encoding='utf-8'))
all_tasks = data.get('tasks', [])
if not isinstance(all_tasks, list):
    all_tasks = []

try:
    prior_manifest = json.loads(manifest_path.read_text(encoding='utf-8'))
except Exception:
    prior_manifest = {}

prior_entries = prior_manifest.get('stories', []) or []
prior_by_key = {entry.get('key'): entry for entry in prior_entries if entry.get('key')}
used_slugs = set(entry.get('slug') for entry in prior_entries if entry.get('slug'))

def slugify(text: str) -> str:
    base = re.sub(r'[^a-z0-9]+', '-', (text or '').lower()).strip('-')
    return base or 'story'

def assign_slug(preferred: str) -> str:
    base = slugify(preferred)
    slug = base
    counter = 2
    while slug in used_slugs:
        slug = f"{base}-{counter}"
        counter += 1
    used_slugs.add(slug)
    return slug

group_map = {}
order = []
for task in all_tasks:
    key = (
        (task.get('story_id') or '').strip(),
        (task.get('story_title') or '').strip(),
        (task.get('epic_id') or '').strip(),
        (task.get('epic_title') or '').strip(),
    )
    if key not in group_map:
        story_id, story_title, epic_id, epic_title = key
        group_map[key] = {
            'story_id': story_id,
            'story_title': story_title,
            'epic_id': epic_id,
            'epic_title': epic_title,
            'tasks': []
        }
        order.append(group_map[key])
    group_map[key]['tasks'].append(task)

converted = 0
skipped = 0
sequence = 0
manifest_entries = []
new_files = set()

for group in order:
    tasks = group['tasks']
    if not tasks:
        continue
    sequence += 1
    story_id = group['story_id']
    story_title = group['story_title']
    epic_id = group['epic_id']
    epic_title = group['epic_title']
    key_str = '|'.join([
        story_id or '',
        story_title or '',
        epic_id or '',
        epic_title or ''
    ])

    preferred_slug_source = story_id or story_title or epic_id or f'story-{sequence}'
    prior_entry = prior_by_key.get(key_str)
    if prior_entry and prior_entry.get('slug'):
        slug = prior_entry['slug']
        used_slugs.add(slug)
    else:
        slug = assign_slug(preferred_slug_source)

    story_path = Path('stories') / f"{slug}.json"
    absolute_path = stories_dir / f"{slug}.json"
    new_files.add(absolute_path.resolve())

    story_payload = {
        'generated_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
        'story': {
            'epic_id': epic_id,
            'epic_title': epic_title,
            'story_id': story_id,
            'story_title': story_title,
            'sequence': sequence,
        },
        'tasks': tasks,
    }

    story_hash = hashlib.sha1(json.dumps(story_payload, sort_keys=True).encode('utf-8')).hexdigest()
    should_write = True
    if not force_flag and prior_entry and prior_entry.get('hash') == story_hash and absolute_path.exists():
        should_write = False
    if should_write:
        absolute_path.parent.mkdir(parents=True, exist_ok=True)
        absolute_path.write_text(json.dumps(story_payload, indent=2) + '\n', encoding='utf-8')
        converted += 1
    else:
        skipped += 1

    manifest_entries.append({
        'sequence': sequence,
        'slug': slug,
        'path': str(story_path),
        'story_id': story_id,
        'story_title': story_title,
        'epic_id': epic_id,
        'epic_title': epic_title,
        'task_count': len(tasks),
        'task_ids': [t.get('id') for t in tasks if t.get('id')],
        'hash': story_hash,
        'key': key_str,
    })

# Remove orphaned story files
for json_file in stories_dir.glob('*.json'):
    if json_file.resolve() not in new_files:
        json_file.unlink(missing_ok=True)

manifest_payload = {
    'generated_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
    'source': str(parsed_path.resolve()),
    'story_count': sequence,
    'stories': manifest_entries,
}
manifest_path.parent.mkdir(parents=True, exist_ok=True)
manifest_path.write_text(json.dumps(manifest_payload, indent=2) + '\n', encoding='utf-8')

print(f"TOTAL {sequence}")
print(f"CONVERTED {converted}")
print(f"SKIPPED {skipped}")
PY
)"; then
    die "Failed to convert Jira tasks into story JSON files"
  fi

  local total=0 converted=0 skipped=0
  while IFS= read -r line; do
    case "$line" in
      TOTAL\ *) total="${line#TOTAL }" ;;
      CONVERTED\ *) converted="${line#CONVERTED }" ;;
      SKIPPED\ *) skipped="${line#SKIPPED }" ;;
    esac
  done <<<"$stats"

  info "Stories: ${total:-0} (converted: ${converted:-0}, skipped: ${skipped:-0})"
  ok "Manifest updated → ${manifest}"
}

gc_write_task_prompt() {
  local story_file="${1:?story json required}"
  local task_index="${2:?task index required}"
  local prompt_path="${3:?prompt path required}"
  local context_tail="${4:-}"
  local model_name="${5:-$CODEX_MODEL}"
  local project_root="${6:-$PROJECT_ROOT}"
  python3 - "$story_file" "$task_index" "$prompt_path" "$context_tail" "$model_name" "$project_root" <<'PY'
import json
import sys
from pathlib import Path

story_path = Path(sys.argv[1])
task_index = int(sys.argv[2])
prompt_path = Path(sys.argv[3])
context_tail_path = Path(sys.argv[4]) if sys.argv[4] else None
model_name = sys.argv[5]
project_root = sys.argv[6]

data = json.loads(story_path.read_text(encoding='utf-8'))
story_meta = data.get('story', {})
tasks = data.get('tasks', [])
if task_index < 0 or task_index >= len(tasks):
    raise SystemExit(2)

task = tasks[task_index]

def clean(value: str) -> str:
    return (value or '').strip()

lines = []
lines.append(f"# You are Codex (model: {model_name})")
lines.append("")
lines.append("You are assisting the Bhavani Yoga delivery team. Implement the task precisely using the repository at: " + (project_root or '.'))
lines.append("")
lines.append("## Story")
epic_id = clean(story_meta.get('epic_id', ''))
epic_title = clean(story_meta.get('epic_title', ''))
if epic_id or epic_title:
    epic_line = "- Epic: " + " — ".join(part for part in [epic_id, epic_title] if part)
    lines.append(epic_line)
story_id = clean(story_meta.get('story_id', ''))
story_title = clean(story_meta.get('story_title', ''))
sequence = story_meta.get('sequence')
if story_id or story_title:
    story_parts = [story_id, story_title]
    lines.append("- Story: " + " — ".join(part for part in story_parts if part))
if sequence:
    lines.append(f"- Story order: {sequence}")

lines.append("")
lines.append("## Task")
task_id = clean(task.get('id', ''))
task_title = clean(task.get('title', ''))
if task_id:
    lines.append(f"- Task ID: {task_id}")
if task_title:
    lines.append(f"- Title: {task_title}")
estimate = clean(task.get('estimate', ''))
if estimate:
    lines.append(f"- Estimate: {estimate}")
assignees = [clean(a) for a in task.get('assignees', []) if clean(a)]
if assignees:
    lines.append("- Assignees: " + ", ".join(assignees))
tags = [clean(t) for t in task.get('tags', []) if clean(t)]
if tags:
    lines.append("- Tags: " + ", ".join(tags))

lines.append("")
lines.append("### Description")
description = task.get('description') or ''
description = description.strip()
if description:
    lines.extend(description.splitlines())
else:
    lines.append("(No additional description provided.)")

acceptance = [clean(item) for item in task.get('acceptance_criteria', []) if clean(item)]
if acceptance:
    lines.append("")
    lines.append("### Acceptance Criteria")
    for item in acceptance:
        lines.append(f"- {item}")

dependencies = [clean(item) for item in task.get('dependencies', []) if clean(item)]
if dependencies:
    lines.append("")
    lines.append("### Dependencies")
    for dep in dependencies:
        lines.append(f"- {dep}")

lines.append("")
lines.append("## Instructions")
lines.append("- Draft a short plan before modifying files.")
lines.append("- Apply changes directly in the repository; commits are not required.")
lines.append("- Respond with structured JSON described below — no markdown fencing or prose outside the JSON.")
lines.append("- Verify acceptance criteria before finishing.")
lines.append("- If blocked, explain why and suggest next steps inside the JSON response.")

lines.append("")
lines.append("## Output JSON schema")
lines.append("Return a single JSON object with keys exactly as follows (omit null/empty collections when not needed):")
lines.append("{")
lines.append("  \"plan\": [\"short step-by-step plan items...\"],")
lines.append("  \"changes\": [")
lines.append("    { \"type\": \"patch\", \"path\": \"relative/file/path\", \"diff\": \"UNIFIED_DIFF\" },")
lines.append("    { \"type\": \"file\", \"path\": \"relative/file/path\", \"content\": \"entire file content\" }")
lines.append("  ],")
lines.append("  \"commands\": [\"optional shell commands to run (e.g., npm install)\"],")
lines.append("  \"notes\": [\"follow-up items or blockers\"]")
lines.append("}")
lines.append("- Use UTF-8, escape newlines as \n inside JSON strings.")
lines.append("- Diff entries must be valid unified diffs (git apply compatible) against the current workspace.")
lines.append("- File entries provide the complete desired file content (for new or fully rewritten files).")
lines.append("- Do not emit markdown fences, commentary, or additional text outside the JSON object.")

if context_tail_path and context_tail_path.exists():
    tail_text = context_tail_path.read_text(encoding='utf-8').splitlines()
    lines.append("")
    lines.append("## Shared Context (truncated)")
    lines.append("")
    lines.extend(tail_text)

prompt_path.parent.mkdir(parents=True, exist_ok=True)
prompt_path.write_text("\n".join(lines) + "\n", encoding='utf-8')

title_preview = task_title.replace('\t', ' ').replace('\n', ' ')
print(f"{task_id}\t{title_preview}")
PY
}

cmd_work_on_tasks() {
  local root="" resume=1 story_filter="" no_verify=0
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --story|--from-story) story_filter="$2"; shift 2;;
      --fresh) resume=0; shift;;
      --no-verify) no_verify=1; shift;;
      -h|--help)
        cat <<'EOHELP'
Usage: gpt-creator work-on-tasks [options]

Execute previously converted story JSON tasks using Codex, with resumable progress.

Options:
  --project PATH   Project root (defaults to current directory)
  --story ID|SLUG  Start from the matching story id or slug (inclusive)
  --fresh          Ignore saved progress and start from the first story
  --no-verify      Skip running gpt-creator verify after tasks complete
EOHELP
        return 0
        ;;
      *) break;;
    esac
  done

  ensure_ctx "$root"
  local tasks_dir="${PLAN_DIR}/tasks"
  local stories_dir="${tasks_dir}/stories"
  local manifest="${tasks_dir}/manifest.json"
  [[ -f "$manifest" ]] || die "Task manifest not found. Run 'gpt-creator task-convert' first."
  [[ -d "$stories_dir" ]] || die "Stories directory missing at ${stories_dir}"

  local state_dir="${PLAN_DIR}/work"
  local runs_dir="${state_dir}/runs"
  local state_file="${state_dir}/state.json"
  mkdir -p "$runs_dir"

  local run_stamp="$(date +%Y%m%d_%H%M%S)"
  local run_dir="${runs_dir}/${run_stamp}"
  mkdir -p "$run_dir"
  local ctx_file="${run_dir}/context.md"
  gc_build_context_file "$ctx_file" "$STAGING_DIR"
  local context_tail="${run_dir}/context_tail.md"
  if ! tail -n 400 "$ctx_file" >"$context_tail" 2>/dev/null; then
    cp "$ctx_file" "$context_tail"
  fi

  info "Work run directory → ${run_dir}"

  local resume_flag=1
  [[ $resume -eq 1 ]] || resume_flag=0

  mapfile -t STORY_PLAN < <(
    python3 - "$manifest" "$state_file" "${story_filter}" "$resume_flag" <<'PY'
import json
import sys
from pathlib import Path

manifest_path = Path(sys.argv[1])
state_path = Path(sys.argv[2])
story_filter = (sys.argv[3] or '').strip().lower()
resume_flag = sys.argv[4] == '1'

manifest = json.loads(manifest_path.read_text(encoding='utf-8'))
stories = manifest.get('stories', []) or []

state = {}
if resume_flag and state_path.exists():
    try:
        state = json.loads(state_path.read_text(encoding='utf-8'))
    except Exception:
        state = {}

stories_state = state.get('stories', {})

def norm(value: str) -> str:
    return (value or '').strip().lower()

start_allowed = not story_filter

for entry in stories:
    seq = entry.get('sequence') or entry.get('index') or 0
    slug = entry.get('slug') or ''
    story_id = entry.get('story_id') or ''
    epic_id = entry.get('epic_id') or ''
    epic_title = entry.get('epic_title') or ''
    story_title = entry.get('story_title') or ''
    total = int(entry.get('task_count') or 0)

    if story_filter and not start_allowed:
        keys = {norm(story_id), norm(slug), norm(epic_id), norm(str(seq))}
        if story_filter in keys:
            start_allowed = True
        else:
            continue

    state_entry = stories_state.get(slug) or {}
    completed = int(state_entry.get('completed_tasks') or 0)
    status = state_entry.get('status') or ''

    if resume_flag and not story_filter and status == 'complete':
        continue

    if resume_flag:
        if total == 0:
            next_task = 0
        elif completed >= total:
            next_task = 0 if story_filter else None
        else:
            next_task = max(completed, 0)
        if next_task is None:
            continue
    else:
        next_task = 0

    story_title_clean = story_title.replace('\t', ' ').replace('\n', ' ').strip()
    epic_title_clean = epic_title.replace('\t', ' ').replace('\n', ' ').strip()

    print("\t".join([
        str(seq or 0),
        entry.get('path') or '',
        slug,
        story_id,
        story_title_clean,
        epic_id,
        epic_title_clean,
        str(total),
        str(next_task),
        str(completed),
        status
    ]))
PY
  )

  if [[ ${#STORY_PLAN[@]} -eq 0 ]]; then
    info "No stories to process (already complete)."
    return 0
  fi

  local work_failed=0

  for story_line in "${STORY_PLAN[@]}"; do
    IFS=$'\t' read -r sequence rel_path slug story_id story_title epic_id epic_title total_tasks next_task completed status <<<"$story_line"

    local total_tasks_int=0
    [[ -n "$total_tasks" ]] && total_tasks_int=$((total_tasks))
    local next_task_int=0
    [[ -n "$next_task" ]] && next_task_int=$((next_task))

    local rel_clean="${rel_path#stories/}"
    local story_file="${stories_dir}/${rel_clean}"
    if [[ ! -f "$story_file" ]]; then
      warn "Story JSON missing: ${story_file}"
      work_failed=1
      continue
    fi

    printf -v story_prefix "%03d" "${sequence:-0}"
    [[ -n "$slug" ]] || slug="story-${story_prefix}"
    local story_run_dir="${run_dir}/story_${story_prefix}_${slug}"
    mkdir -p "${story_run_dir}/prompts" "${story_run_dir}/out"

    info "Story ${story_prefix} (${story_id:-$slug}) — ${story_title:-Unnamed}"

    if (( total_tasks_int == 0 )); then
      info "  No tasks for this story; marking complete."
      gc_update_work_state "$state_file" "$slug" "complete" 0 0 "$run_stamp"
      continue
    fi

    gc_update_work_state "$state_file" "$slug" "in-progress" "$next_task_int" "$total_tasks_int" "$run_stamp"

    local task_index
    for (( task_index = next_task_int; task_index < total_tasks_int; task_index++ )); do
      local task_number
      printf -v task_number "%03d" $((task_index + 1))
      local prompt_path="${story_run_dir}/prompts/task_${task_number}.prompt.md"
      local output_path="${story_run_dir}/out/task_${task_number}.out.md"

      local prompt_meta
      if ! prompt_meta="$(gc_write_task_prompt "$story_file" "$task_index" "$prompt_path" "$context_tail" "$CODEX_MODEL" "$PROJECT_ROOT")"; then
        warn "  Failed to build prompt for task index ${task_index}"
        work_failed=1
        break
      fi

      local task_id="" task_title=""
      IFS=$'\t' read -r task_id task_title <<<"$prompt_meta"
      info "  Task ${task_number}: ${task_id:-no-id} — ${task_title:-(untitled)}"

      local call_name="story-${slug}-task-${task_number}"
      if codex_call "$call_name" --prompt "$prompt_path" --output "$output_path"; then
        local apply_output
        if ! apply_output="$(gc_apply_codex_changes "$output_path" "$PROJECT_ROOT")"; then
          warn "  Failed to apply changes for ${call_name}; inspect ${output_path}."
          work_failed=1
          break
        fi
        if [[ "$apply_output" == "no-output" || "$apply_output" == "empty-output" ]]; then
          warn "  Codex produced no actionable JSON for ${call_name}."
          work_failed=1
          break
        fi
        while IFS= read -r change_line; do
          case "$change_line" in
            APPLIED) info "    Changes applied." ;;
            WRITE\ *) info "    Wrote ${change_line#WRITE }" ;;
            PATCH\ *) info "    Patched ${change_line#PATCH }" ;;
            CMD\ *) info "    Suggested command: ${change_line#CMD }" ;;
            NOTE\ *) warn "    Note: ${change_line#NOTE }" ;;
          esac
        done <<<"$apply_output"
        gc_update_work_state "$state_file" "$slug" "in-progress" $((task_index + 1)) "$total_tasks_int" "$run_stamp"
      else
        warn "  Codex execution failed for ${call_name}; progress saved."
        work_failed=1
        break
      fi
    done

    if (( task_index < total_tasks_int )); then
      warn "Stopping at story ${slug} due to previous error."
      break
    fi

    gc_update_work_state "$state_file" "$slug" "complete" "$total_tasks_int" "$total_tasks_int" "$run_stamp"
  done

  if [[ $work_failed -eq 0 && $no_verify -eq 0 ]]; then
    info "Re-running verify after work run"
    if ! cmd_verify all --project "$PROJECT_ROOT"; then
      warn "Verify command reported failures."
      work_failed=1
    fi
  fi

  if [[ $work_failed -eq 0 ]]; then
    ok "work-on-tasks complete → ${run_dir}"
  else
    warn "work-on-tasks completed with issues — inspect ${run_dir}"
    return 1
  fi
}

cmd_iterate() {
  local root="" jira="" reverify=1
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --jira) jira="$(abs_path "$2")"; shift 2;;
      --no-verify) reverify=0; shift;;
      *) break;;
    esac
  done
  ensure_ctx "$root"
  [[ -n "$jira" ]] || jira="${INPUT_DIR}/jira.md"
  [[ -f "$jira" ]] || die "Jira tasks file not found: ${jira}"

  local tasks_json_local="${PLAN_DIR}/jira-tasks.local.json"
  gc_parse_jira_tasks "$jira" "$tasks_json_local"
  ok "Parsed Jira tasks → ${tasks_json_local}"
  local tasks_json="${tasks_json_local}"

  local codex_parse_prompt="${PLAN_DIR}/iterate-codex-parse.md"
  local codex_raw_json="${PLAN_DIR}/jira-tasks.codex.raw.txt"
  local codex_json="${PLAN_DIR}/jira-tasks.codex.json"
  {
    cat >"$codex_parse_prompt" <<'PROMPT'
# Instruction
You are a structured-data assistant. Convert the following Jira backlog markdown into strict JSON.

## Requirements
- Output **only** valid JSON (no prose, no code fences).
- Structure: { "tasks": [ { "epic_id": str, "epic_title": str, "story_id": str, "story_title": str, "id": str, "title": str, "assignees": [str], "tags": [str], "estimate": str, "description": str, "acceptance_criteria": [str], "dependencies": [str] } ] }.
- Each task begins with a bold identifier such as **T18.5.2**; treat every such block as a separate task and capture its parent story/epic when present.
- Preserve bullet details verbatim inside the description and acceptance criteria lists. Do not repeat metadata (assignee/tags/estimate) inside the description.
- Use empty strings/arrays when information is missing.
- Do not include explanatory text.

## Jira Markdown
PROMPT
    cat "$jira" >>"$codex_parse_prompt"
    cat >>"$codex_parse_prompt" <<'PROMPT'
## End Markdown
PROMPT
  }

  if codex_call "iterate-parse" --prompt "$codex_parse_prompt" --output "$codex_raw_json"; then
    if python3 - <<'PY' "$codex_raw_json" "$codex_json"
import json, pathlib, re, sys
raw_path, out_path = sys.argv[1:3]
text = pathlib.Path(raw_path).read_text().strip()
if not text:
    raise SystemExit(1)
if text.startswith('```'):
    text = re.sub(r'^```[a-zA-Z0-9_-]*\s*', '', text)
    text = re.sub(r'```\s*$', '', text)
data = json.loads(text)
if isinstance(data, list):
    data = {'tasks': data}
elif 'tasks' not in data:
    data = {'tasks': [data]}
pathlib.Path(out_path).write_text(json.dumps(data, indent=2) + '\n')
PY
      then
      ok "Codex parsed Jira tasks → ${codex_json}"
      tasks_json="$codex_json"
    else
      warn "Codex JSON output invalid; falling back to local parser results."
    fi
  else
    warn "Codex parsing step failed; using local parser output."
  fi

  local iterate_dir="${PLAN_DIR}/iterate"
  mkdir -p "$iterate_dir"
  local order_file="${iterate_dir}/tasks-order.txt"

  python3 - <<'PY' "$tasks_json" "$iterate_dir" "$PROJECT_ROOT"
import json, pathlib, sys
source, out_dir, project_root = sys.argv[1:4]
tasks = json.load(open(source)).get('tasks', [])
out = pathlib.Path(out_dir)
out.mkdir(parents=True, exist_ok=True)
index_path = out / 'tasks-order.txt'
with index_path.open('w') as idx:
    for i, task in enumerate(tasks, 1):
        title = (task.get('title') or '').strip() or f'Task {i}'
        task_id = (task.get('id') or '').strip()
        description = (task.get('description') or '').strip() or '(No additional details provided.)'
        estimate = (task.get('estimate') or '').strip()
        tags = ', '.join(task.get('tags') or [])
        assignees = ', '.join(task.get('assignees') or [])
        story_bits = [part for part in [(task.get('story_id') or '').strip(), (task.get('story_title') or '').strip()] if part]
        prompt_path = out / f'task-{i:02d}.md'
        idx.write(str(prompt_path) + '\n')
        lines = []
        if task_id and not title.startswith(task_id):
            lines.append(f"# Task {i}: {task_id} — {title}")
        else:
            lines.append(f"# Task {i}: {title}")
        lines.append('')
        lines.append('## Context')
        lines.append(f'- Working directory: {project_root}')
        if task_id:
            lines.append(f'- Task ID: {task_id}')
        if story_bits:
            lines.append(f"- Story: {' — '.join(story_bits)}")
        if assignees:
            lines.append(f'- Assignees: {assignees}')
        if estimate:
            lines.append(f'- Estimate: {estimate}')
        if tags:
            lines.append(f'- Tags: {tags}')
        lines.append('')
        lines.append('## Description')
        lines.append(description or '(No additional details provided.)')
        lines.append('')
        if task.get('acceptance_criteria'):
            lines.append('## Acceptance Criteria')
            for ac in task['acceptance_criteria']:
                lines.append(f'- {ac}')
            lines.append('')
        if task.get('dependencies'):
            lines.append('## Dependencies')
            for dep in task['dependencies']:
                lines.append(f'- {dep}')
            lines.append('')
        lines.append('')
        lines.append('## Instructions')
        lines.append('- Outline your plan before modifying files.')
        lines.append('- Implement the task in the repository; commits are not required.')
        lines.append('- Show relevant diffs (git snippets) and command results.')
        lines.append('- Verify acceptance criteria for this task.')
        lines.append('- If blocked, explain why and propose next steps.')
        lines.append('')
        lines.append('## Output Format')
        lines.append('- Begin with a heading `Task {i}`.')
        lines.append('- Summarise changes, tests, and outstanding follow-ups.')
        prompt_path.write_text('\n'.join(lines) + '\n')
PY

  if [[ -s "$order_file" ]]; then
    while IFS= read -r prompt_path; do
      [[ -z "$prompt_path" ]] && continue
      local base_name="$(basename "$prompt_path" .md)"
      local output_path="${prompt_path%.md}.output.md"
      info "Running Codex for ${base_name}"
      codex_call "$base_name" --prompt "$prompt_path" --output "$output_path" || warn "Codex task ${base_name} returned non-zero"
    done < "$order_file"
  else
    warn "No Jira tasks to process after parsing."
  fi

  local summary_prompt="${iterate_dir}/summary.md"
  local summary_output="${iterate_dir}/summary.output.md"
  python3 - <<'PY' "$tasks_json" "$order_file" "$summary_prompt"
import json, pathlib, sys
tasks = json.load(open(sys.argv[1])).get('tasks', [])
order_file = pathlib.Path(sys.argv[2])
prompt_path = pathlib.Path(sys.argv[3])
lines = ['# Summary Request', '', 'Summarise the completed Jira work and list follow-up actions.']
lines.append('')
lines.append('## Task Reports')
if order_file.exists():
    for i, prompt in enumerate(order_file.read_text().splitlines(), 1):
        if not prompt:
            continue
        title = tasks[i-1].get('title') if i-1 < len(tasks) else f'Task {i}'
        out_path = pathlib.Path(prompt).with_suffix('.output.md')
        lines.append(f'- Task {i}: {title}')
        if out_path.exists():
            content = out_path.read_text().strip()
            if content:
                snippet = content[:2000]
                lines.append('  ```')
                lines.append(snippet)
                lines.append('  ```')
        else:
            lines.append('  (No output captured)')
else:
    lines.append('- No outputs available.')
lines.append('')
lines.append('## Output Requirements')
lines.append('- Provide an overall summary of work completed.')
lines.append('- List follow-up items or blockers.')
lines.append('- Use markdown headings and bullet lists.')
prompt_path.write_text('\n'.join(lines) + '\n')
PY

  codex_call "iterate-summary" --prompt "$summary_prompt" --output "$summary_output" || warn "Codex summary step returned non-zero"

  if [[ "$reverify" -eq 1 ]]; then
    info "Re-running verify after iteration"
    cmd_verify all --project "$PROJECT_ROOT"
  fi
}

cmd_create_project() {
  local path="${1:-}"; [[ -n "$path" ]] || die "create-project requires a path"
  ensure_ctx "$path"
  mkdir -p "$PROJECT_ROOT"
  info "Project root: ${PROJECT_ROOT}"

  cmd_scan --project "$PROJECT_ROOT"
  cmd_normalize --project "$PROJECT_ROOT"
  cmd_plan --project "$PROJECT_ROOT"
  cmd_generate all --project "$PROJECT_ROOT"
  cmd_db provision --project "$PROJECT_ROOT" || warn "Database provision step reported an error"
  cmd_run up --project "$PROJECT_ROOT" || warn "Stack start reported an error"
  cmd_verify acceptance --project "$PROJECT_ROOT" || warn "Acceptance checks failing — review stack health."
  ok "Project bootstrap complete"
}

usage() {
cat <<EOF
${APP_NAME} v${VERSION}

Usage:
  ${APP_NAME} create-project <path>
  ${APP_NAME} scan [--project <path>]
  ${APP_NAME} normalize [--project <path>]
  ${APP_NAME} plan [--project <path>]
  ${APP_NAME} generate <api|web|admin|db|docker|all> [--project <path>]
  ${APP_NAME} db <provision|import|seed> [--project <path>]
  ${APP_NAME} run <up|down|logs|open> [--project <path>]
  ${APP_NAME} verify <acceptance|nfr|all> [--project <path>] [--api-url API_BASE] [--api-health URL] [--web-url URL] [--admin-url URL]
  ${APP_NAME} task-convert [--project <path>] [--jira <file>] [--force]
  ${APP_NAME} work-on-tasks [--project <path>] [--story ID|SLUG] [--fresh] [--no-verify]
  ${APP_NAME} iterate [--project <path>] [--jira <file>] [--no-verify]
  ${APP_NAME} version
  ${APP_NAME} help

Environment overrides:
  CODEX_BIN, CODEX_MODEL, DOCKER_BIN, MYSQL_BIN, EDITOR_CMD, GC_API_HEALTH_URL, GC_WEB_URL, GC_ADMIN_URL
EOF
}

main() {
  local cmd="${1:-help}"; shift || true
  case "$cmd" in
    help|-h|--help) usage ;;
    version|-v|--version) echo "${APP_NAME} ${VERSION}" ;;
    create-project) cmd_create_project "$@" ;;
    scan)           cmd_scan "$@" ;;
    normalize)      cmd_normalize "$@" ;;
    plan)           cmd_plan "$@" ;;
    generate)       cmd_generate "$@" ;;
    db)             cmd_db "$@" ;;
    run)            cmd_run "$@" ;;
    verify)         cmd_verify "$@" ;;
    task-convert)   cmd_task_convert "$@" ;;
    work-on-tasks)  cmd_work_on_tasks "$@" ;;
    iterate)        cmd_iterate "$@" ;;
    *) die "Unknown command: ${cmd}. See '${APP_NAME} help'" ;;
  esac
}

main "$@"
