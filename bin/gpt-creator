#!/usr/bin/env bash
# gpt-creator — scaffolding & orchestration CLI
# Aligns with Product Definition & Requirements (PDR v0.2)
# Usage: gpt-creator <command> [args]

set -Eeuo pipefail

if [[ -t 1 && $# -eq 0 && -z "${GC_SKIP_TUI_AUTO:-}" ]]; then
  export GC_SKIP_TUI_AUTO=1
  exec "$0" tui
fi

# Crash logging globals
GC_CRASH_LOGGED=0
GC_LAST_ERROR_CMD=""
GC_LAST_ERROR_STATUS=0
GC_FAIL_LOG_DIR=""
GC_INVOCATION=""

GC_LAST_CRASH_LOG=""
GC_MAIN_PID="$$"

if [[ -n "${GC_REPORTS_ON:-}" && "${GC_REPORTS_ON}" != "0" ]]; then
  GC_REPORTS_ON=1
else
  GC_REPORTS_ON=0
fi
GC_REPORTS_IDLE_TIMEOUT="${GC_REPORTS_IDLE_TIMEOUT:-1800}"
GC_REPORTS_CHECK_INTERVAL="${GC_REPORTS_CHECK_INTERVAL:-60}"
GC_REPORTS_INITIALIZED=0
GC_REPORTS_STORE_DIR=""
GC_REPORTS_ACTIVITY_FILE=""
GC_REPORTS_IDLE_SENTINEL=""
GC_REPORTS_WATCHDOG_PID=""
GC_FILTERED_ARGS=()

# Ensure docker compose commands have adequate timeouts unless caller overrides
if [[ -n "${GC_DOCKER_COMPOSE_TIMEOUT:-}" ]]; then
  COMPOSE_HTTP_TIMEOUT="$GC_DOCKER_COMPOSE_TIMEOUT"
  DOCKER_CLIENT_TIMEOUT="$GC_DOCKER_COMPOSE_TIMEOUT"
fi
: "${COMPOSE_HTTP_TIMEOUT:=600}"
: "${DOCKER_CLIENT_TIMEOUT:=$COMPOSE_HTTP_TIMEOUT}"
export COMPOSE_HTTP_TIMEOUT DOCKER_CLIENT_TIMEOUT

: "${GC_DOCKER_HEALTH_TIMEOUT:=10}"
: "${GC_DOCKER_HEALTH_INTERVAL:=1}"
: "${GC_PNPM_VERSION:=10.17.1}"
: "${GC_CODEX_EXEC_TIMEOUT:=0}"

resolve_cli_root() {
  local source="${BASH_SOURCE[0]}"
  while [[ -L "$source" ]]; do
    local dir
    dir="$(cd "$(dirname "$source")" && pwd)"
    source="$(readlink "$source")"
    [[ "$source" != /* ]] && source="$dir/$source"
  done
  local abs_dir
  abs_dir="$(cd "$(dirname "$source")" && pwd)"
  GC_SELF_PATH="${abs_dir}/$(basename "$source")"
  cd "${abs_dir}/.." && pwd
}

cmd_task_convert() {
  warn "'task-convert' is deprecated; use 'create-tasks' instead. Running create-tasks now."
  cmd_create_tasks "$@"
}

CLI_ROOT="$(resolve_cli_root)"
unset -f resolve_cli_root

gc_env_file() { echo "${PROJECT_ROOT:-$PWD}/.env"; }

gc_random_string() {
  python3 - <<'PY'
import secrets, string
alphabet = string.ascii_letters + string.digits
print(''.join(secrets.choice(alphabet) for _ in range(32)))
PY
}

gc_set_env_var() {
  local key="$1" value="$2"
  local env_file="$(gc_env_file)"
  python3 - <<'PY' "$env_file" "$key" "$value"
import pathlib, sys
path = pathlib.Path(sys.argv[1])
key = sys.argv[2]
value = sys.argv[3]
if path.exists():
    raw_lines = path.read_text().splitlines()
else:
    raw_lines = []
lines = []
for line in raw_lines:
    stripped = line.strip()
    if not stripped:
        lines.append('')
        continue
    if stripped.startswith('#') or '=' in line:
        lines.append(line)
# Non key/value lines are dropped
for idx, line in enumerate(lines):
    if line.startswith(f"{key}="):
        lines[idx] = f"{key}={value}"
        break
else:
    lines.append(f"{key}={value}")
path.write_text('\n'.join(lines) + '\n')
PY
}

gc_env_sync_ports() {
  GC_PORT_RESERVATIONS=""
  GC_DB_HOST_PORT="${GC_DB_HOST_PORT:-${DB_HOST_PORT:-${DB_PORT:-3306}}}"
  DB_NAME="${DB_NAME:-$GC_DB_NAME}"
  DB_USER="${DB_USER:-$GC_DB_USER}"
  DB_PASSWORD="${DB_PASSWORD:-$GC_DB_PASSWORD}"
  DB_ROOT_PASSWORD="${DB_ROOT_PASSWORD:-$GC_DB_ROOT_PASSWORD}"
  DB_HOST_PORT="${DB_HOST_PORT:-$GC_DB_HOST_PORT}"
  GC_API_HOST_PORT="${GC_API_HOST_PORT:-${API_HOST_PORT:-3000}}"
  GC_WEB_HOST_PORT="${GC_WEB_HOST_PORT:-${WEB_HOST_PORT:-5173}}"
  GC_ADMIN_HOST_PORT="${GC_ADMIN_HOST_PORT:-${ADMIN_HOST_PORT:-5174}}"
  GC_PROXY_HOST_PORT="${GC_PROXY_HOST_PORT:-${PROXY_HOST_PORT:-8080}}"
  API_HOST_PORT="${API_HOST_PORT:-$GC_API_HOST_PORT}"
  WEB_HOST_PORT="${WEB_HOST_PORT:-$GC_WEB_HOST_PORT}"
  ADMIN_HOST_PORT="${ADMIN_HOST_PORT:-$GC_ADMIN_HOST_PORT}"
  PROXY_HOST_PORT="${PROXY_HOST_PORT:-$GC_PROXY_HOST_PORT}"
  local api_base_default="http://localhost:${GC_API_HOST_PORT}/api/v1"
  GC_API_BASE_URL="${GC_API_BASE_URL:-$api_base_default}"
  VITE_API_BASE="${VITE_API_BASE:-$GC_API_BASE_URL}"
  local expected_health="${GC_API_BASE_URL%/}/health"
  if [[ -z "${GC_API_HEALTH_URL:-}" ]]; then
    GC_API_HEALTH_URL="$expected_health"
    gc_set_env_var GC_API_HEALTH_URL "$GC_API_HEALTH_URL"
  elif [[ "$GC_API_HEALTH_URL" == "http://localhost:${GC_API_HOST_PORT}/health" && "$expected_health" != "$GC_API_HEALTH_URL" ]]; then
    GC_API_HEALTH_URL="$expected_health"
    gc_set_env_var GC_API_HEALTH_URL "$GC_API_HEALTH_URL"
  fi
  local proxy_origin="http://localhost:${GC_PROXY_HOST_PORT}"
  GC_WEB_URL="${GC_WEB_URL:-${proxy_origin}/}"
  GC_ADMIN_URL="${GC_ADMIN_URL:-${proxy_origin}/admin/}"
  gc_reserve_port db "$GC_DB_HOST_PORT"
  gc_reserve_port api "$GC_API_HOST_PORT"
  gc_reserve_port web "$GC_WEB_HOST_PORT"
  gc_reserve_port admin "$GC_ADMIN_HOST_PORT"
  gc_reserve_port proxy "$GC_PROXY_HOST_PORT"

  if gc_reports_enabled; then
    gc_reports_initialize
    gc_reports_touch_activity "$(date +%s)"
  fi
}

gc_sanitize_env_file() {
  local env_file="$1"
  python3 - <<'PY' "$env_file"
import pathlib, re, sys
path = pathlib.Path(sys.argv[1])
if not path.exists():
    raise SystemExit(0)
pattern = re.compile(r"^(?:export\s+)?([A-Za-z_][A-Za-z0-9_]*)=(.*)$")
ansi_re = re.compile(r"\x1b\[[0-9;]*m")
whitespace_re = re.compile(r"\s")
lines = path.read_text().splitlines()
cleaned = []
for line in lines:
    stripped = line.strip()
    if not stripped:
        continue
    if stripped.startswith('#'):
        cleaned.append(line)
        continue
    match = pattern.match(line)
    if not match:
        # drop invalid line
        continue
    key, value = match.groups()
    value = ansi_re.sub('', value).strip()
    if '➜' in value or 'remapping' in value or value.startswith('Port '):
        continue
    if key.endswith('_HOST_PORT') or key in {'DB_HOST_PORT', 'DB_PORT', 'MYSQL_HOST_PORT'}:
        if not value.isdigit():
            continue
    if whitespace_re.search(value) and not (value.startswith('"') and value.endswith('"')) and not (value.startswith("'") and value.endswith("'")):
        # unquoted whitespace breaks sourcing; drop
        continue
    cleaned.append(f"{key}={value}")
path.write_text('\n'.join(cleaned) + ('\n' if cleaned else ''))
PY
}

gc_load_env() {
  local env_file="$(gc_env_file)"
  if [[ -f "$env_file" ]]; then
    gc_sanitize_env_file "$env_file"
    set -a
    # shellcheck disable=SC1090
    source "$env_file"
    set +a
  fi
  GC_DB_NAME="${GC_DB_NAME:-${DB_NAME:-app}}"
  GC_DB_USER="${GC_DB_USER:-${DB_USER:-app}}"
  GC_DB_PASSWORD="${GC_DB_PASSWORD:-${DB_PASSWORD:-app_pass}}"
  GC_DB_ROOT_PASSWORD="${GC_DB_ROOT_PASSWORD:-${DB_ROOT_PASSWORD:-root}}"
  gc_env_sync_ports
}

gc_create_env_if_missing() {
  local env_file="$(gc_env_file)"
  if [[ -f "$env_file" ]]; then
    return
  fi
  local slug
  slug="$(basename "${PROJECT_ROOT:-$PWD}")"
  slug=$(printf '%s' "$slug" | tr -c '[:alnum:]' '_')
  slug=$(printf '%s' "$slug" | tr '[:upper:]' '[:lower:]')
  slug=$(printf '%.12s' "$slug")
  [[ -n "$slug" ]] || slug="app"
  local db_name="${slug}_db"
  local db_user="gc_${slug}_user"
  local db_password="$(gc_random_string)"
  local db_root_password="$(gc_random_string)"
  cat > "$env_file" <<EOF
# gpt-creator environment
DB_NAME=${db_name}
DB_USER=${db_user}
DB_PASSWORD=${db_password}
DB_ROOT_USER=root
DB_ROOT_PASSWORD=${db_root_password}
DB_HOST=127.0.0.1
DB_PORT=3306
DB_HOST_PORT=3306
API_HOST_PORT=3000
WEB_HOST_PORT=5173
ADMIN_HOST_PORT=5174
PROXY_HOST_PORT=8080
DATABASE_URL=mysql://${db_user}:${db_password}@127.0.0.1:3306/${db_name}
VITE_API_BASE=http://localhost:3000/api/v1
# Optional: GitHub issue reporting
GC_GITHUB_REPO=bekirdag/gpt-creator
GC_GITHUB_TOKEN=
# GC_REPORTER=
# GC_REPORT_ASSIGNEE=
EOF
  chmod 600 "$env_file" || true
}

VERSION="0.2.0"
APP_NAME="gpt-creator"

# Defaults (override via env)
CODEX_BIN="${CODEX_BIN:-codex}"
CODEX_MODEL="${CODEX_MODEL:-gpt-5-codex}"
CODEX_FALLBACK_MODEL="${CODEX_FALLBACK_MODEL:-gpt-5-codex}"
CODEX_REASONING_EFFORT="${CODEX_REASONING_EFFORT:-high}"
EDITOR_CMD="${EDITOR_CMD:-code}"
DOCKER_BIN="${DOCKER_BIN:-docker}"
MYSQL_BIN="${MYSQL_BIN:-mysql}"

# Colors (TTY-only)
if [[ -t 1 ]]; then
  c_reset=$'\033[0m'; c_dim=$'\033[2m'; c_bold=$'\033[1m'
  c_red=$'\033[31m'; c_yellow=$'\033[33m'; c_cyan=$'\033[36m'; c_green=$'\033[32m'
else
  c_reset=; c_dim=; c_bold=; c_red=; c_yellow=; c_cyan=; c_green=
fi

ts() { date +"%Y-%m-%dT%H:%M:%S"; }
die() { echo "${c_red}✖${c_reset} $*" >&2; exit 1; }
info(){ echo "${c_cyan}➜${c_reset} $*"; }
ok()  { echo "${c_green}✔${c_reset} $*"; }
warn(){ echo "${c_yellow}!${c_reset} $*"; }

abs_path() {
  python3 - "$1" <<'PY' 2>/dev/null || perl -MCwd=abs_path -e 'print abs_path(shift)."\n"' "$1" || echo "$1"
import os,sys; print(os.path.abspath(sys.argv[1]))
PY
}

to_lower() {
  printf '%s' "$1" | tr '[:upper:]' '[:lower:]'
}

slugify_name() {
  local s="${1:-}"
  s="$(to_lower "$s")"
  s="$(printf '%s' "$s" | tr -cs 'a-z0-9' '-')"
  s="$(printf '%s' "$s" | sed -E 's/-+/-/g; s/^-+//; s/-+$//')"
  printf '%s\n' "${s:-gptcreator}"
}

GC_PORT_RESERVATIONS=""

gc_port_for_service() {
  local service="$1"
  local entry
  for entry in $GC_PORT_RESERVATIONS; do
    local svc="${entry%%:*}"
    if [[ "$svc" == "$service" ]]; then
      printf '%s\n' "${entry#*:}"
      return 0
    fi
  done
  return 1
}

gc_unreserve_port() {
  local service="$1"
  [[ -n "$service" ]] || return 0
  local entry new_list=""
  for entry in $GC_PORT_RESERVATIONS; do
    local svc="${entry%%:*}"
    if [[ "$svc" == "$service" ]]; then
      continue
    fi
    if [[ -z "$new_list" ]]; then
      new_list="$entry"
    else
      new_list+=" $entry"
    fi
  done
  GC_PORT_RESERVATIONS="$new_list"
}

gc_reserve_port() {
  local service="$1" port="$2"
  [[ -n "$service" && -n "$port" ]] || return 0
  gc_unreserve_port "$service"
  if [[ -z "${GC_PORT_RESERVATIONS:-}" ]]; then
    GC_PORT_RESERVATIONS="${service}:${port}"
  else
    GC_PORT_RESERVATIONS+=" ${service}:${port}"
  fi
}

gc_port_is_reserved() {
  local port="$1"
  local entry
  for entry in $GC_PORT_RESERVATIONS; do
    if [[ "${entry#*:}" == "$port" ]]; then
      return 0
    fi
  done
  return 1
}

gc_port_reserved_by_other() {
  local port="$1" service="$2"
  local entry
  for entry in $GC_PORT_RESERVATIONS; do
    local svc="${entry%%:*}"
    local val="${entry#*:}"
    if [[ "$val" == "$port" && "$svc" != "$service" ]]; then
      return 0
    fi
  done
  return 1
}

# Context directories inside project
ensure_ctx() {
  local root="${1:-}"
  if [[ -z "${root}" ]]; then root="${PROJECT_ROOT:-$PWD}"; fi
  PROJECT_ROOT="$(abs_path "$root")"
  GC_DIR="${PROJECT_ROOT}/.gpt-creator"
  STAGING_DIR="${GC_DIR}/staging"
  INPUT_DIR="${STAGING_DIR}/inputs"
  PLAN_DIR="${STAGING_DIR}/plan"
  LOG_DIR="${GC_DIR}/logs"
  ART_DIR="${GC_DIR}/artifacts"
  mkdir -p "$GC_DIR" "$STAGING_DIR" "$INPUT_DIR" "$PLAN_DIR" "$LOG_DIR" "$ART_DIR"
  gc_create_env_if_missing
  gc_load_env
  local base_name
  base_name="$(basename "$PROJECT_ROOT")"
  PROJECT_SLUG="$(slugify_name "${GC_DOCKER_PROJECT_NAME:-$base_name}")"
  GC_DOCKER_PROJECT_NAME="${GC_DOCKER_PROJECT_NAME:-$PROJECT_SLUG}"
  COMPOSE_PROJECT_NAME="${COMPOSE_PROJECT_NAME:-$GC_DOCKER_PROJECT_NAME}"
  GC_FAIL_LOG_DIR="$LOG_DIR"
  export GC_DOCKER_PROJECT_NAME COMPOSE_PROJECT_NAME PROJECT_SLUG

  if gc_reports_enabled; then
    gc_reports_initialize
    gc_reports_touch_activity "$(date +%s)"
  fi
}

gc_project_templates_root() {
  local root="${CLI_ROOT}/project_templates"
  mkdir -p "$root"
  printf '%s\n' "$root"
}

gc_find_primary_rfp() {
  local search_root="${1:-.}"
  find "$search_root" -maxdepth 3 -type f \
    \( -iname 'rfp.md' -o -iname '*rfp*.md' -o -iname '*request*for*proposal*.md' \) \
    | sort | head -n 1
}

gc_bootstrap_state_dir() {
  printf '%s\n' "${PLAN_DIR}/bootstrap"
}

gc_bootstrap_state_file() {
  printf '%s\n' "$(gc_bootstrap_state_dir)/state.json"
}

gc_capture_error_context() {
  local status="${1:-0}"
  local command="${2:-}"
  (( status == 0 )) && return
  GC_LAST_ERROR_STATUS="$status"
  GC_LAST_ERROR_CMD="$command"
}

gc_logs_dir() {
  local dir="${GC_FAIL_LOG_DIR:-}"
  if [[ -z "$dir" ]]; then
    if [[ -n "${PROJECT_ROOT:-}" ]]; then
      dir="${PROJECT_ROOT}/.gpt-creator/logs"
    else
      dir="${PWD}/.gpt-creator/logs"
    fi
  fi
  if ! mkdir -p "$dir" 2>/dev/null; then
    return 1
  fi
  printf '%s\n' "$dir"
}

gc_reports_enabled() {
  (( GC_REPORTS_ON != 0 ))
}

gc_reports_current_user() {
  if [[ -n "${GC_REPORTER:-}" ]]; then
    printf '%s\n' "$GC_REPORTER"
    return 0
  fi
  local name
  name="$(git config user.name 2>/dev/null || true)"
  if [[ -z "$name" ]]; then
    name="${USER:-}"
  fi
  if [[ -z "$name" ]]; then
    name="$(whoami 2>/dev/null || true)"
  fi
  printf '%s\n' "${name:-maintainer}"
}

gc_reports_escape() {
  local s="${1:-}"
  s="${s//\\/\\\\}"
  s="${s//$'\n'/\\n}"
  s="${s//\"/\\\"}"
  printf '%s' "$s"
}

gc_reports_issue_file() {
  local kind="${1:-generic}"
  local dir="${GC_REPORTS_STORE_DIR:-}"
  if [[ -z "$dir" ]]; then
    local base
    base="$(gc_logs_dir)" || return 1
    dir="${base}/issue-reports"
  fi
  if ! mkdir -p "$dir" 2>/dev/null; then
    return 1
  fi
  local stamp random file
  stamp="$(date -u +"%Y%m%dT%H%M%SZ")"
  random="$(printf '%04x%04x' "$RANDOM" "$RANDOM")"
  file="${dir}/${stamp}-${kind}-${random}.yml"
  if [[ -e "$file" ]]; then
    file="${dir}/${stamp}-${kind}-${random}-${RANDOM}.yml"
  fi
  if ! : >"$file" 2>/dev/null; then
    return 1
  fi
  printf '%s\n' "$file"
}

gc_reports_write_issue() {
  local kind="${1:-generic}"
  local summary="${2:-}"
  local definition="${3:-}"
  local priority="${4:-P2-medium}"
  local file
  file="$(gc_reports_issue_file "$kind")" || return 1
  local summary_escaped
  summary_escaped="$(gc_reports_escape "$summary")"
  local reporter
  reporter="$(gc_reports_current_user)"
  local timestamp
  timestamp="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
  {
    printf 'summary: "%s"\n' "$summary_escaped"
    printf 'priority: %s\n' "$priority"
    printf 'issue_definition: |\n'
    if [[ -n "$definition" ]]; then
      while IFS= read -r line || [[ -n "$line" ]]; do
        printf '  %s\n' "$line"
      done <<<"$definition"
    else
      printf '  (no additional details provided)\n'
    fi
    printf 'metadata:\n'
    printf '  type: %s\n' "$kind"
    printf '  timestamp: "%s"\n' "$timestamp"
    printf '  reporter: "%s"\n' "$(gc_reports_escape "$reporter")"
    printf '  command: "%s"\n' "$(gc_reports_escape "${GC_INVOCATION:-$0}")"
    if [[ -n "${GC_LAST_ERROR_STATUS:-}" ]]; then
      printf '  exit_code: %s\n' "${GC_LAST_ERROR_STATUS}"
    fi
    if [[ -n "${GC_LAST_ERROR_CMD:-}" ]]; then
      printf '  last_command: "%s"\n' "$(gc_reports_escape "${GC_LAST_ERROR_CMD}")"
    fi
    printf '  working_dir: "%s"\n' "$(gc_reports_escape "$PWD")"
    printf '  status: open\n'
    printf '  likes: 0\n'
    printf '  comments: 0\n'
  } >"$file"
  printf '%s\n' "$file"
}

gc_reports_sync_github() {
  local report_file="${1:-}"
  local kind="${2:-generic}"
  local summary="${3:-}"
  local definition="${4:-}"
  local priority="${5:-P2-medium}"
  local cli_version="${VERSION:-}"
  local binary_path="${GC_SELF_PATH:-}"

  if [[ -z "$binary_path" || ! -r "$binary_path" ]]; then
    if [[ -n "${CLI_ROOT:-}" && -r "${CLI_ROOT}/bin/${APP_NAME}" ]]; then
      binary_path="${CLI_ROOT}/bin/${APP_NAME}"
    else
      binary_path="$(command -v "${APP_NAME}" 2>/dev/null || true)"
    fi
  fi

  local repo="${GC_GITHUB_REPO:-}"
  local token="${GC_GITHUB_TOKEN:-}"
  if [[ -z "$repo" || -z "$token" ]]; then
    return 0
  fi

  local response
  if ! response="$(
    GC_REPORT_SUMMARY="$summary" \
    GC_REPORT_DEFINITION="$definition" \
    GC_REPORT_PRIORITY="$priority" \
    GC_REPORT_KIND="$kind" \
    GC_REPORT_FILE="$report_file" \
    GC_REPORT_COMMAND="${GC_INVOCATION:-}" \
    GC_REPORT_EXIT="${GC_LAST_ERROR_STATUS:-}" \
    GC_REPORT_LAST_CMD="${GC_LAST_ERROR_CMD:-}" \
    GC_REPORT_WORKDIR="$PWD" \
    GC_REPORT_PROJECT="${PROJECT_ROOT:-$PWD}" \
    GC_REPORTER="${GC_REPORTER:-}" \
    GC_REPORT_VERSION="$cli_version" \
    GC_REPORT_BINARY="${binary_path:-}" \
    python3 - <<'PY' "$repo" "$token"
import json
import os
import sys
import urllib.error
import urllib.request
import hashlib
import pathlib

repo, token = sys.argv[1:3]

summary = (os.environ.get("GC_REPORT_SUMMARY") or "").strip()
definition = (os.environ.get("GC_REPORT_DEFINITION") or "").strip()
priority = (os.environ.get("GC_REPORT_PRIORITY") or "P2-medium").strip() or "P2-medium"
kind = (os.environ.get("GC_REPORT_KIND") or "generic").strip() or "generic"
report_file = os.environ.get("GC_REPORT_FILE", "")
invocation = os.environ.get("GC_REPORT_COMMAND", "")
exit_code = os.environ.get("GC_REPORT_EXIT", "")
last_cmd = os.environ.get("GC_REPORT_LAST_CMD", "")
project_root = os.environ.get("GC_REPORT_PROJECT") or os.getcwd()
workdir = os.environ.get("GC_REPORT_WORKDIR") or os.getcwd()
reporter = os.environ.get("GC_REPORTER", "")
version = (os.environ.get("GC_REPORT_VERSION") or "").strip()
binary_path = (os.environ.get("GC_REPORT_BINARY") or "").strip()

title = summary or f"{kind.capitalize()} report"
title = title.strip()[:120] or "Automated crash report"

body_lines = []
if definition:
    body_lines.append(definition)
else:
    body_lines.append("(no additional details provided)")
body_lines.extend(["", "---", ""])

binary_hash = ""
if binary_path:
    try:
        path = pathlib.Path(binary_path)
        if path.is_file():
            hasher = hashlib.sha256()
            with path.open("rb") as handle:
                for chunk in iter(lambda: handle.read(1024 * 1024), b""):
                    hasher.update(chunk)
            binary_hash = hasher.hexdigest()
    except (OSError, ValueError):
        binary_hash = ""

signature = ""
if version or binary_hash:
    payload_source = f"{version}:{binary_hash}".encode("utf-8")
    signature = hashlib.sha256(payload_source).hexdigest()

metadata = [
    ("Priority", priority),
    ("Report Type", kind),
    ("Reporter", reporter),
    ("Invocation", invocation),
    ("Exit Code", exit_code),
    ("Last Command", last_cmd),
    ("Project Root", project_root),
    ("Working Directory", workdir),
    ("Report File", report_file),
    ("CLI Version", version),
    ("CLI Binary SHA256", binary_hash),
    ("CLI Signature", signature),
]

for label, value in metadata:
    if value:
        body_lines.append(f"- **{label}**: {value}")

watermark = ""
if signature:
    watermark = f"{version or 'unknown'}:{signature}"
elif version:
    watermark = f"{version}:unsigned"
if watermark:
    body_lines.extend(["", f"<!-- gpt-creator:{watermark} -->"])

labels = [
    "auto-report",
    f"kind:{kind}",
    f"priority:{priority}",
]
if version:
    labels.append(f"cli-version:{version.replace(' ', '_')}")

payload = {
    "title": title,
    "body": "\n".join(body_lines),
    "labels": labels,
}

request = urllib.request.Request(
    f"https://api.github.com/repos/{repo}/issues",
    data=json.dumps(payload).encode("utf-8"),
    headers={
        "Authorization": f"Bearer {token}",
        "Accept": "application/vnd.github+json",
        "Content-Type": "application/json",
        "X-GitHub-Api-Version": "2022-11-28",
    },
    method="POST",
)

try:
    with urllib.request.urlopen(request) as resp:
        data = json.loads(resp.read().decode("utf-8"))
except urllib.error.HTTPError as err:
    message = err.read().decode("utf-8", "ignore")
    print(f"HTTP {err.code}: {message}", file=sys.stderr)
    sys.exit(1)
except Exception as exc:
    print(f"GitHub issue request failed: {exc}", file=sys.stderr)
    sys.exit(1)

html_url = data.get("html_url", "")
number = data.get("number")
print(html_url)
print(number if number is not None else "")
PY
  )"; then
    warn "Failed to create GitHub issue for $(basename "$report_file")."
    return 0
  fi

  local issue_url=""
  local issue_number=""
  IFS=$'\n' read -r issue_url issue_number <<<"$response"
  if [[ -n "$issue_url" ]]; then
    gc_reports_set_metadata_field "$report_file" issue_url "\"$issue_url\""
    if [[ -n "$issue_number" ]]; then
      gc_reports_set_metadata_field "$report_file" issue_number "$issue_number"
    fi
    info "GitHub issue created -> ${issue_url}"
  fi
}

gc_reports_record_issue() {
  local kind="${1:-generic}"
  local summary="${2:-}"
  local definition="${3:-}"
  local priority="${4:-P2-medium}"
  local report_file
  report_file="$(gc_reports_write_issue "$kind" "$summary" "$definition" "$priority")" || return 1
  gc_reports_sync_github "$report_file" "$kind" "$summary" "$definition" "$priority"
  printf '%s\n' "$report_file"
}

gc_reports_touch_activity() {
  local timestamp="${1:-$(date +%s)}"
  local command="${2:-}"
  local file="${GC_REPORTS_ACTIVITY_FILE:-}"
  [[ -n "$file" ]] || return 0
  if [[ -n "$command" ]]; then
    command="${command//$'\n'/ }"
    printf '%s\t%s\n' "$timestamp" "$command" >"$file" 2>/dev/null || true
  else
    printf '%s\n' "$timestamp" >"$file" 2>/dev/null || true
  fi
}

gc_reports_activity_trap() {
  gc_reports_touch_activity "$(date +%s)" "$1"
  return 0
}

gc_reports_handle_crash() {
  local status="${1:-1}"
  gc_reports_enabled || return 0
  local summary
  printf -v summary "Crash (exit %s) while running '%s'" "$status" "${GC_INVOCATION:-$0}"
  local log_dir
  if ! log_dir="$(gc_logs_dir)"; then
    log_dir="<unknown>"
  fi
  local -a lines
  lines=("The CLI exited unexpectedly with status ${status}.")
  if [[ -n "${GC_LAST_ERROR_CMD:-}" ]]; then
    lines+=("Last command observed before exit: ${GC_LAST_ERROR_CMD}")
  fi
  if [[ -n "${GC_LAST_CRASH_LOG:-}" ]]; then
    lines+=("Crash log stored at: ${GC_LAST_CRASH_LOG}")
  fi
  lines+=("Inspect logs under ${log_dir} for further diagnostics.")
  local definition=""
  if ((${#lines[@]})); then
    printf -v definition '%s\n' "${lines[@]}"
    definition="${definition%$'\n'}"
  fi
  local path
  if path="$(gc_reports_record_issue "crash" "$summary" "$definition" "P0-critical")"; then
    warn "Issue report recorded for crash → ${path}"
  fi
}

gc_reports_handle_idle() {
  local idle_seconds="${1:-0}"
  local heartbeat="${2:-}"
  local last_command="${3:-}"
  gc_reports_enabled || return 0
  local summary
  printf -v summary "Idle/stall detected after %ss while running '%s'" "$idle_seconds" "${GC_INVOCATION:-$0}"
  local log_dir
  if ! log_dir="$(gc_logs_dir)"; then
    log_dir="<unknown>"
  fi
  if [[ -n "$last_command" ]]; then
    last_command="${last_command//$'\n'/ }"
  fi
  local -a lines
  lines=("No CLI activity recorded for ${idle_seconds} seconds.")
  if [[ -n "$heartbeat" ]]; then
    lines+=("Heartbeat file: ${heartbeat}")
  fi
  if [[ -n "$last_command" ]]; then
    lines+=("Last command observed: ${last_command}")
  fi
  lines+=("Inspect processes and logs under ${log_dir} to verify whether the command stalled.")
  local definition=""
  if ((${#lines[@]})); then
    printf -v definition '%s\n' "${lines[@]}"
    definition="${definition%$'\n'}"
  fi
  local path
  if path="$(gc_reports_record_issue "idle" "$summary" "$definition" "P1-high")"; then
    warn "Issue report recorded for idle stall → ${path}"
  fi
}

gc_reports_watchdog_loop() {
  local timeout="${1:-0}"
  local interval="${2:-0}"
  local activity_file="${3:-}"
  local main_pid="${4:-0}"
  local sentinel="${5:-}"

  (( timeout > 0 )) || return 0
  (( interval > 0 )) || interval="$timeout"
  [[ -n "$activity_file" && -n "$main_pid" ]] || return 0

  while kill -0 "$main_pid" 2>/dev/null; do
    sleep "$interval" || break
    [[ -f "$activity_file" ]] || continue
    local raw
    raw="$(cat "$activity_file" 2>/dev/null)" || continue
    local payload="$raw"
    local last="${payload%%$'\t'*}"
    local activity_command=""
    if [[ "$payload" == *$'\t'* ]]; then
      activity_command="${payload#*$'\t'}"
    fi
    [[ "$last" =~ ^[0-9]+$ ]] || continue
    local now
    now="$(date +%s)"
    local delta=$(( now - last ))
    if (( delta >= timeout )); then
      if [[ -n "$sentinel" ]]; then
        printf '%s\n' "$now" >"$sentinel" 2>/dev/null || true
      fi
      gc_reports_handle_idle "$delta" "$activity_file" "$activity_command"
      break
    fi
  done
}

gc_reports_start_watchdog() {
  local timeout="${1:-0}"
  local interval="${2:-0}"
  local activity="${3:-}"
  local main_pid="${4:-0}"
  local sentinel="${5:-}"

  if (( timeout <= 0 )); then
    return 0
  fi
  if (( interval <= 0 || interval > timeout )); then
    interval="$timeout"
  fi
  if [[ -z "$activity" || -z "$main_pid" ]]; then
    return 0
  fi
  if [[ -n "${GC_REPORTS_WATCHDOG_PID:-}" ]] && kill -0 "$GC_REPORTS_WATCHDOG_PID" 2>/dev/null; then
    return 0
  fi

  gc_reports_watchdog_loop "$timeout" "$interval" "$activity" "$main_pid" "$sentinel" &
  GC_REPORTS_WATCHDOG_PID=$!
}

gc_reports_initialize() {
  gc_reports_enabled || return 0
  if (( GC_REPORTS_INITIALIZED )); then
    return 0
  fi
  local log_dir
  if ! log_dir="$(gc_logs_dir)"; then
    warn "Unable to determine log directory for issue reporting"
    return 0
  fi
  GC_REPORTS_STORE_DIR="${log_dir}/issue-reports"
  if ! mkdir -p "$GC_REPORTS_STORE_DIR" 2>/dev/null; then
    warn "Unable to prepare reports directory at ${GC_REPORTS_STORE_DIR}"
    return 0
  fi
  GC_REPORTS_ACTIVITY_FILE="${GC_REPORTS_STORE_DIR}/heartbeat-${GC_MAIN_PID}.txt"
  GC_REPORTS_IDLE_SENTINEL="${GC_REPORTS_STORE_DIR}/idle-${GC_MAIN_PID}.flag"
  if ! : >"$GC_REPORTS_ACTIVITY_FILE" 2>/dev/null; then
    warn "Unable to initialize heartbeat tracking for issue reporting"
    return 0
  fi
  GC_REPORTS_INITIALIZED=1
  gc_reports_touch_activity "$(date +%s)"
  gc_reports_start_watchdog "$GC_REPORTS_IDLE_TIMEOUT" "$GC_REPORTS_CHECK_INTERVAL" "$GC_REPORTS_ACTIVITY_FILE" "$GC_MAIN_PID" "$GC_REPORTS_IDLE_SENTINEL"
  return 0
}

gc_reports_cleanup() {
  if [[ -n "${GC_REPORTS_WATCHDOG_PID:-}" ]]; then
    kill "$GC_REPORTS_WATCHDOG_PID" 2>/dev/null || true
    wait "$GC_REPORTS_WATCHDOG_PID" 2>/dev/null || true
    GC_REPORTS_WATCHDOG_PID=""
  fi
}

gc_reports_dir() {
  if [[ -n "${GC_REPORTS_STORE_DIR:-}" ]]; then
    printf '%s\n' "$GC_REPORTS_STORE_DIR"
    return 0
  fi
  local base
  base="$(gc_logs_dir)" || return 1
  local dir="${base}/issue-reports"
  if ! mkdir -p "$dir" 2>/dev/null; then
    return 1
  fi
  printf '%s\n' "$dir"
}

gc_reports_set_metadata_field() {
  local report_file="${1:?report file required}"
  local key="${2:?metadata key required}"
  local value="${3:-}"
  python3 - <<'PY' "$report_file" "$key" "$value"
import sys

path, key, value = sys.argv[1:4]

try:
    lines = []
    with open(path, 'r', encoding='utf-8', errors='ignore') as fh:
        lines = fh.read().splitlines()
except FileNotFoundError:
    raise SystemExit(f"report file not found: {path}")

metadata_found = False
metadata_active = False
inserted = False
result = []

for line in lines:
    if line.strip() == "metadata:":
        metadata_found = True
        metadata_active = True
        result.append(line)
        continue
    if metadata_active:
        if line.startswith("  "):
            stripped = line.strip()
            if ":" in stripped:
                current_key = stripped.split(":", 1)[0].strip()
                if current_key == key:
                    result.append(f"  {key}: {value}".rstrip())
                    inserted = True
                    continue
        else:
            if not inserted:
                result.append(f"  {key}: {value}".rstrip())
                inserted = True
            metadata_active = False
    result.append(line)

if metadata_active and not inserted:
    result.append(f"  {key}: {value}".rstrip())
    inserted = True

if not metadata_found:
    result.append("metadata:")
    result.append(f"  {key}: {value}".rstrip())

with open(path, 'w', encoding='utf-8', errors='ignore') as fh:
    fh.write('\n'.join(result).rstrip() + '\n')
PY
}

gc_reports_resolve_slug() {
  local slug="${1:-}"
  [[ -n "$slug" ]] || return 1
  local dir
  dir="$(gc_reports_dir)" || return 1
  local candidate
  for ext in yml yaml; do
    candidate="${dir}/${slug}.${ext}"
    if [[ -f "$candidate" ]]; then
      printf '%s\n' "$candidate"
      return 0
    fi
  done
  candidate="${dir}/${slug}"
  if [[ -f "$candidate" ]]; then
    printf '%s\n' "$candidate"
    return 0
  fi
  local -a matches=()
  while IFS= read -r file; do
    local base
    base="$(basename "$file")"
    base="${base%.*}"
    if [[ "$base" == "$slug"* ]]; then
      matches+=("$file")
    fi
  done < <(find "$dir" -maxdepth 1 -type f \( -name '*.yml' -o -name '*.yaml' \) -print 2>/dev/null)
  local count="${#matches[@]}"
  if (( count == 1 )); then
    printf '%s\n' "${matches[0]}"
    return 0
  fi
  if (( count > 1 )); then
    warn "Multiple reports match slug '${slug}'."
    local entry
    for entry in "${matches[@]}"; do
      warn "  $(basename "$entry")"
    done
    return 2
  fi
  return 1
}

gc_reports_run_work() {
  local slug="${1:?slug required}"
  local branch_hint="${2:-}"
  local push_after="${3:-1}"
  local prompt_only="${4:-0}"
  local assignee_override="${5:-}"

  local report_path
  if ! report_path="$(gc_reports_resolve_slug "$slug")"; then
    warn "No issue report found for slug: ${slug}"
    return 1
  fi

  local branch="${branch_hint:-report/${slug}}"
  local push_flag="$push_after"
  if [[ "$push_flag" != "0" ]]; then
    push_flag=1
  fi
  local prompt_only_flag="$prompt_only"
  if [[ "$prompt_only_flag" != "0" ]]; then
    prompt_only_flag=1
  fi

  local report_dir="${GC_DIR}/reports/${slug}"
  mkdir -p "$report_dir"
  local prompt_path="${report_dir}/work.md"
  local project_root="${PROJECT_ROOT:-$PWD}"

  if ! python3 - <<'PY' "$report_path" "$prompt_path" "$project_root" "$slug" "$branch" "$push_flag"
import sys

report_path, prompt_path, project_root, slug, branch, push_flag = sys.argv[1:6]
push = push_flag == "1"

with open(report_path, 'r', encoding='utf-8', errors='ignore') as fh:
    raw_lines = fh.read().splitlines()

summary = ""
priority = ""
definition_lines = []
metadata = {}
collect_definition = False
metadata_active = False

for line in raw_lines:
    if line.startswith("summary:") and not summary:
        value = line.split(":", 1)[1].strip()
        if value.startswith('"') and value.endswith('"') and len(value) >= 2:
            value = value[1:-1]
        summary = value
    elif line.startswith("priority:") and not priority:
        priority = line.split(":", 1)[1].strip()
    elif line.startswith("issue_definition:"):
        collect_definition = True
        continue
    elif collect_definition:
        if line.startswith("  "):
            definition_lines.append(line[2:])
        else:
            collect_definition = False
    if line.strip() == "metadata:":
        metadata_active = True
        continue
    if metadata_active:
        if line.startswith("  "):
            stripped = line.strip()
            if ":" in stripped:
                k, v = stripped.split(":", 1)
                metadata[k.strip()] = v.strip().strip('"')
        else:
            metadata_active = False

definition_text = "\n".join(definition_lines).strip()
issue_type = metadata.get("type", "unknown")
status = metadata.get("status", "open")
timestamp = metadata.get("timestamp", "")
likes = metadata.get("likes", "0")
comments = metadata.get("comments", "0")
try:
    popularity = int(likes) + int(comments)
except ValueError:
    popularity = 0

summary_for_commit = summary.replace('"', '').strip()
if len(summary_for_commit) > 64:
    summary_for_commit = summary_for_commit[:61] + "..."

instructions = []
instructions.append(f"# Resolve Issue {slug}\n")
instructions.append("## Summary")
instructions.append(f"- Summary: {summary or '(not provided)'}")
instructions.append(f"- Priority: {priority or 'unknown'}")
instructions.append(f"- Type: {issue_type}")
instructions.append(f"- Current Status: {status}")
instructions.append(f"- Popularity Score: {popularity} (likes={likes}, comments={comments})")
if timestamp:
    instructions.append(f"- Reported At: {timestamp}")
instructions.append(f"- Report File: {report_path}")
instructions.append(f"- Working Branch: {branch}")
instructions.append("")
instructions.append("## Issue Definition")
instructions.append("```")
instructions.append(definition_text or "(no issue definition provided)")
instructions.append("```")
instructions.append("")
instructions.append("## Workflow Requirements")
instructions.append(f"1. Checkout the branch `{branch}` (create it if missing).")
instructions.append("2. Investigate and resolve the described issue with deterministic steps.")
instructions.append("3. Run any relevant checks or tests (e.g. `gpt-creator verify acceptance`) to confirm the fix.")
instructions.append(f"4. Stage and commit the changes with a concise message (suggested: `fix: {slug} {summary_for_commit}`).")
if push:
    instructions.append(f"5. Push the branch to origin via `git push origin {branch}`.")
instructions.append("6. Provide a short summary of the fix in the commit message body if additional context is required.")
instructions.append("")
instructions.append("## Notes for Codex")
instructions.append("- Operate deterministically and avoid modifying unrelated files.")
instructions.append("- Do not edit the issue report YAML; the CLI updates metadata automatically.")
instructions.append("- Focus on resolving the root cause and keep diffs as small as possible.")
instructions.append("- If the issue cannot be resolved, leave the repository unchanged and exit with a failure code describing the blocker.")
instructions.append("")
instructions.append("## Repository Context")
instructions.append(f"- Project Root: {project_root}")
instructions.append(f"- Branch: {branch}")
instructions.append("")

with open(prompt_path, 'w', encoding='utf-8', errors='ignore') as fh:
    fh.write("\n".join(instructions).rstrip() + "\n")
PY
  then
    warn "Failed to prepare Codex prompt for report ${slug}"
    return 1
  fi

  info "Prepared Codex prompt → ${prompt_path}"

  local now_utc
  now_utc="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
  local assignee="${assignee_override:-${GC_REPORT_ASSIGNEE:-$(gc_reports_current_user)}}"
  if (( prompt_only_flag )); then
    [[ -n "$assignee" ]] && gc_reports_set_metadata_field "$report_path" assigned "\"$(gc_reports_escape "$assignee")\""
    gc_reports_set_metadata_field "$report_path" branch "\"$(gc_reports_escape "$branch")\""
    gc_reports_set_metadata_field "$report_path" status open
    info "Prompt generated (skipping Codex execution due to --prompt-only)."
    info "Run: ${CODEX_BIN:-codex} exec --model ${CODEX_MODEL} --cd \"${PROJECT_ROOT:-$PWD}\" < ${prompt_path}"
    return 0
  fi

  if [[ -n "$assignee" ]]; then
    gc_reports_set_metadata_field "$report_path" assigned "\"$(gc_reports_escape "$assignee")\""
  fi
  gc_reports_set_metadata_field "$report_path" status in-progress
  gc_reports_set_metadata_field "$report_path" branch "\"$(gc_reports_escape "$branch")\""
  gc_reports_set_metadata_field "$report_path" last_started "\"$now_utc\""

  if codex_call "report-${slug}" --prompt "$prompt_path"; then
    local completed_utc
    completed_utc="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
    gc_reports_set_metadata_field "$report_path" status resolved
    gc_reports_set_metadata_field "$report_path" last_completed "\"$completed_utc\""
    ok "Codex resolved report ${slug}"
    return 0
  else
    local failed_utc
    failed_utc="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
    gc_reports_set_metadata_field "$report_path" status open
    gc_reports_set_metadata_field "$report_path" last_failed "\"$failed_utc\""
    warn "Codex failed to resolve report ${slug}"
    return 1
  fi
}

gc_reports_set_idle_timeout() {
  local value="${1:-}"
  if [[ -z "$value" ]]; then
    die "--reports-idle-timeout requires a value in seconds"
  fi
  if ! [[ "$value" =~ ^[0-9]+$ ]]; then
    die "--reports-idle-timeout expects an integer number of seconds (received: $value)"
  fi
  GC_REPORTS_IDLE_TIMEOUT="$value"
}

gc_reports_extract_global_flags() {
  GC_FILTERED_ARGS=()
  local -a args=("$@")
  local idx=0
  while (( idx < ${#args[@]} )); do
    local arg="${args[idx]}"
    case "$arg" in
      --)
        GC_FILTERED_ARGS+=("${args[@]:idx}")
        return 0
        ;;
      --reports-on)
        GC_REPORTS_ON=1
        ;;
      --reports-off)
        GC_REPORTS_ON=0
        ;;
      --reports-idle-timeout=*)
        gc_reports_set_idle_timeout "${arg#*=}"
        ;;
      --reports-idle-timeout)
        (( idx + 1 < ${#args[@]} )) || die "--reports-idle-timeout requires a value in seconds"
        idx=$((idx + 1))
        gc_reports_set_idle_timeout "${args[idx]}"
        ;;
      *)
        GC_FILTERED_ARGS+=("$arg")
        ;;
    esac
    idx=$((idx + 1))
  done
}

gc_write_crash_log() {
  local status="${1:-1}"
  if (( GC_CRASH_LOGGED )); then
    return
  fi

  local log_dir
  log_dir="$(gc_logs_dir)" || return

  local log_file="${log_dir}/crash.log"
  local timestamp
  timestamp="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
  local invocation="${GC_INVOCATION:-$0}"
  local last_command="${GC_LAST_ERROR_CMD:-}"

  {
    printf 'timestamp=%s\n' "$timestamp"
    printf 'command=%s\n' "$invocation"
    printf 'exit_code=%s\n' "$status"
    if [[ -n "$last_command" ]]; then
      printf 'last_command=%s\n' "$last_command"
    fi
    if [[ -n "${PROJECT_ROOT:-}" ]]; then
      printf 'project_root=%s\n' "$PROJECT_ROOT"
    fi
    printf 'working_dir=%s\n' "$PWD"
    printf -- '---\n'
  } >>"$log_file" 2>/dev/null || return

  GC_LAST_CRASH_LOG="$log_file"
  GC_CRASH_LOGGED=1
}

gc_exit_handler() {
  local status="${1:-0}"
  if (( status != 0 )); then
    if [[ -z "${GC_LAST_ERROR_STATUS:-}" || "${GC_LAST_ERROR_STATUS}" -eq 0 ]]; then
      GC_LAST_ERROR_STATUS="$status"
    fi
    gc_write_crash_log "$status"
    if gc_reports_enabled; then
      gc_reports_handle_crash "$status"
    fi
  fi
  gc_reports_cleanup
}

gc_record_codex_usage() {
  local log_file="${1:-}"
  local task="${2:-}"
  local model="${3:-}"
  local prompt_file="${4:-}"
  local exit_code="${5:-0}"

  [[ -n "$log_file" && -f "$log_file" ]] || return 0

  local usage_dir="${LOG_DIR:-${PROJECT_ROOT:-$PWD}/.gpt-creator/logs}"
  mkdir -p "$usage_dir"
  local usage_file="${usage_dir}/codex-usage.ndjson"
  local timestamp
  timestamp="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"

  local py_output=""
  if py_output="$(
    python3 - <<'PY' "$log_file" "$usage_file" "$timestamp" "$task" "$model" "$prompt_file" "$exit_code"
import hashlib
import json
import pathlib
import re
import sys

log_path = pathlib.Path(sys.argv[1])
usage_path = pathlib.Path(sys.argv[2])
timestamp = sys.argv[3]
task = sys.argv[4] or None
model = sys.argv[5] or None
prompt_file = sys.argv[6] or None
exit_code = int(sys.argv[7])

if log_path.exists():
    raw_text = log_path.read_text(encoding="utf-8", errors="ignore")
else:
    raw_text = ""

fields = {}
def parse_number(text: str) -> int:
    cleaned = re.sub(r"[,_]", "", text.strip())
    if not cleaned:
        raise ValueError("empty")
    return int(cleaned)

def capture(field: str, value: str) -> None:
    if not value:
        return
    try:
        fields[field] = parse_number(value)
    except Exception:
        pass

specific_patterns = [
    ("prompt_tokens", r'prompt[\s_\-]*tokens(?:\s*(?:used|consumed))?["\']?\s*[:=]\s*([\d,._]+)'),
    ("completion_tokens", r'completion[\s_\-]*tokens(?:\s*(?:used|consumed))?["\']?\s*[:=]\s*([\d,._]+)'),
    ("total_tokens", r'total[\s_\-]*tokens(?:\s*(?:used|consumed))?["\']?\s*[:=]\s*([\d,._]+)'),
    ("total_tokens", r'tokens[\s_\-]*used\s*[:=]\s*([\d,._]+)'),
    ("total_tokens", r'tokens[\s_\-]*consumed\s*[:=]\s*([\d,._]+)'),
    ("prompt_tokens", r'prompt\s*=\s*([\d,._]+)'),
    ("completion_tokens", r'completion\s*=\s*([\d,._]+)'),
    ("total_tokens", r'total\s*=\s*([\d,._]+)'),
    ("prompt_tokens", r'input[\s_\-]*tokens?["\']?\s*[:=]\s*([\d,._]+)'),
    ("completion_tokens", r'output[\s_\-]*tokens?["\']?\s*[:=]\s*([\d,._]+)'),
    ("cached_tokens", r'cached[\s_\-]*tokens?["\']?\s*[:=]\s*([\d,._]+)'),
    ("billable_units", r'(?:billable|chargeable)[\s_\-]*(?:tokens|units)?["\']?\s*[:=]\s*([\d,._]+)'),
    ("request_units", r'request[\s_\-]*(?:tokens|units)?["\']?\s*[:=]\s*([\d,._]+)'),
]

for key, pattern in specific_patterns:
    matches = re.findall(pattern, raw_text, flags=re.IGNORECASE)
    if matches:
        capture(key, matches[-1])

general_pattern = re.compile(
    r'\b(?P<label>(?:prompt|completion|total|tokens|input|output|cached|billable|chargeable|request|requests)'
    r'(?:[\s_-]*(?:tokens?|units?|used|consumed))?)\b[^0-9]{0,20}(\d[\d,._]*)',
    re.IGNORECASE,
)

for label, value in general_pattern.findall(raw_text):
    label_lower = label.lower()
    if "prompt" in label_lower or "input" in label_lower:
        capture("prompt_tokens", value)
    elif "completion" in label_lower or "output" in label_lower:
        capture("completion_tokens", value)
    elif "cached" in label_lower:
        capture("cached_tokens", value)
    elif "billable" in label_lower or "chargeable" in label_lower:
        capture("billable_units", value)
    elif "request" in label_lower:
        capture("request_units", value)
    else:
        capture("total_tokens", value)

if "total_tokens" not in fields:
    prompt_val = fields.get("prompt_tokens")
    completion_val = fields.get("completion_tokens")
    if prompt_val is not None or completion_val is not None:
        total = (prompt_val or 0) + (completion_val or 0)
        fields["total_tokens"] = total

record = {
    "timestamp": timestamp,
    "task": task,
    "model": model,
    "prompt_file": prompt_file,
    "exit_code": exit_code,
    "usage_captured": bool(fields),
}

for key in ("prompt_tokens", "completion_tokens", "total_tokens", "cached_tokens", "billable_units", "request_units"):
    if key in fields:
        record[key] = fields[key]

limit_needles = [
    "usage limit",
    "usage-limit",
    "usage cap",
    "usage-cap",
    "quota exceeded",
    "quota has been reached",
    "exceeded your current quota",
    "exceeded your quota",
    "quota reached",
    "credit balance is too low",
    "billing hard limit",
    "hard usage limit",
    "usage credits exhausted",
]

limit_message = None
if raw_text:
    for line in raw_text.splitlines():
        lower = line.lower()
        if not lower.strip():
            continue
        if any(needle in lower for needle in limit_needles):
            limit_message = line.strip()
            break

if limit_message:
    record["limit_detected"] = True
    record["limit_message"] = limit_message

usage_path.parent.mkdir(parents=True, exist_ok=True)
with usage_path.open("a", encoding="utf-8") as fh:
    fh.write(json.dumps(record, sort_keys=True, separators=(",", ":")))
    fh.write("\n")

if limit_message:
    print(f"LIMIT_DETECTED\t{limit_message}")
PY
  )"; then
    if [[ "$py_output" == LIMIT_DETECTED$'\t'* ]]; then
      GC_CODEX_USAGE_LIMIT_REACHED=1
      GC_CODEX_USAGE_LIMIT_MESSAGE="${py_output#LIMIT_DETECTED$'\t'}"
    elif [[ "$py_output" == "LIMIT_DETECTED" ]]; then
      GC_CODEX_USAGE_LIMIT_REACHED=1
      GC_CODEX_USAGE_LIMIT_MESSAGE=""
    fi
    rm -f "$log_file" || true
    return 0
  else
    warn "Failed to record Codex usage for task=${task} model=${model}."
    return 1
  fi
}

gc_bootstrap_reset_state() {
  local file
  file="$(gc_bootstrap_state_file)"
  rm -f "$file"
}

gc_bootstrap_mark_step() {
  local step="${1:?step required}"
  local status="${2:?status required}"
  local file
  file="$(gc_bootstrap_state_file)"
  mkdir -p "$(dirname "$file")"
  python3 - <<'PY' "$file" "$step" "$status"
import json
import sys
import time
from pathlib import Path

path = Path(sys.argv[1])
step = sys.argv[2]
status = sys.argv[3]

if status == 'reset':
    if path.exists():
        try:
            data = json.loads(path.read_text(encoding='utf-8'))
        except Exception:
            data = {}
        steps = data.get('steps', {})
        steps.pop(step, None)
        data['steps'] = steps
        path.write_text(json.dumps(data, indent=2) + '\n', encoding='utf-8')
    sys.exit(0)

try:
    data = json.loads(path.read_text(encoding='utf-8'))
except Exception:
    data = {}

steps = data.setdefault('steps', {})
steps[step] = {
    'status': status,
    'updated_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
}
if status == 'done':
    data['last_completed'] = step
elif status == 'failed':
    data['failed_step'] = step
else:
    data.pop('failed_step', None)

path.write_text(json.dumps(data, indent=2) + '\n', encoding='utf-8')
PY
}

gc_bootstrap_mark_complete() {
  local file
  file="$(gc_bootstrap_state_file)"
  mkdir -p "$(dirname "$file")"
  python3 - <<'PY' "$file"
import json
import sys
import time
from pathlib import Path

path = Path(sys.argv[1])
try:
    data = json.loads(path.read_text(encoding='utf-8'))
except Exception:
    data = {}

data['completed_at'] = time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())
path.write_text(json.dumps(data, indent=2) + '\n', encoding='utf-8')
PY
}

gc_bootstrap_step_is_done() {
  local step="${1:?step required}"
  local file
  file="$(gc_bootstrap_state_file)"
  [[ -f "$file" ]] || return 1
  python3 - "$file" "$step" <<'PY'
import json
import sys
from pathlib import Path

path = Path(sys.argv[1])
step = sys.argv[2]
try:
    data = json.loads(path.read_text(encoding='utf-8'))
except Exception:
    data = {}
steps = data.get('steps') or {}
status = steps.get(step, {}).get('status')
sys.exit(0 if status == 'done' else 1)
PY
}

gc_bootstrap_run_step() {
  local step="${1:?step required}"
  shift
  if gc_bootstrap_step_is_done "$step"; then
    info "Step '${step}' already completed; skipping."
    return 0
  fi
  if "$@"; then
    gc_bootstrap_mark_step "$step" done
    return 0
  else
    gc_bootstrap_mark_step "$step" failed
    return 1
  fi
}

gc_bootstrap_have_rfp() {
  local stage="${STAGING_DIR:-}" input="${INPUT_DIR:-}"
  [[ -n "$stage" && -f "$stage/docs/rfp.md" ]] && return 0
  [[ -n "$stage" && -f "$stage/rfp.md" ]] && return 0
  [[ -n "$input" && -f "$input/rfp.md" ]] && return 0
  return 1
}

gc_auto_project_template() {
  local project_root="${1:?project root required}"
  local templates_root="${2:?templates root required}"
  shift 2
  local -a template_dirs=("$@")
  local count=${#template_dirs[@]}
  (( count )) || return 1
  if (( count == 1 )); then
    printf '%s\n' "${template_dirs[0]}"
    return 0
  fi

  local rfp_path
  rfp_path="$(gc_find_primary_rfp "$project_root")"
  [[ -n "$rfp_path" ]] || return 1

  python3 - "$rfp_path" "${template_dirs[@]}" <<'PY'
import json
import re
import sys
from pathlib import Path

rfp_path = Path(sys.argv[1])
rfp_text = rfp_path.read_text(encoding='utf-8', errors='ignore').lower()
templates = sys.argv[2:]

stopwords = {"the","and","with","from","base","template","project","app","service","system"}

def template_tokens(path: Path):
    tokens = set()
    name = path.name.lower()
    tokens.update(token for token in re.split(r'[^a-z0-9]+', name) if len(token) >= 3)
    tags_file = path / 'tags.txt'
    if tags_file.exists():
        for line in tags_file.read_text(encoding='utf-8', errors='ignore').splitlines():
            for token in re.split(r'[^a-z0-9]+', line.lower()):
                if len(token) >= 3:
                    tokens.add(token)
    template_json = path / 'template.json'
    if template_json.exists():
        try:
            data = json.loads(template_json.read_text(encoding='utf-8', errors='ignore'))
        except Exception:
            data = {}
        for field in ("tags", "keywords", "stack"):
            value = data.get(field)
            if isinstance(value, str):
                for token in re.split(r'[^a-z0-9]+', value.lower()):
                    if len(token) >= 3:
                        tokens.add(token)
            elif isinstance(value, list):
                for entry in value:
                    text = str(entry).lower()
                    for token in re.split(r'[^a-z0-9]+', text):
                        if len(token) >= 3:
                            tokens.add(token)
    return {token for token in tokens if token not in stopwords}

scores = []
for template in templates:
    path = Path(template)
    tokens = template_tokens(path)
    score = 0
    for token in tokens:
        if token and token in rfp_text:
            score += rfp_text.count(token)
    scores.append((score, template))

scores.sort(reverse=True)
if scores and scores[0][0] > 0:
    print(scores[0][1])
PY
}

gc_copy_project_template() {
  local template_dir="${1:?template directory required}"
  local project_root="${2:?project root required}"
  python3 - "$template_dir" "$project_root" <<'PY'
import shutil
import sys
from pathlib import Path

src = Path(sys.argv[1])
dest = Path(sys.argv[2])

for path in src.rglob('*'):
    if path.name in {'.git', '.DS_Store'}:
        continue
    rel = path.relative_to(src)
    target = dest / rel
    if path.is_dir():
        target.mkdir(parents=True, exist_ok=True)
        continue
    target.parent.mkdir(parents=True, exist_ok=True)
    if target.exists():
        print(f"SKIP {rel}")
        continue
    shutil.copy2(path, target)
    print(f"COPY {rel}")
PY
}

gc_apply_project_template() {
  local project_root="${1:?project root required}"
  local template_request="${2:-auto}"
  local templates_root
  templates_root="$(gc_project_templates_root)"

  mapfile -t available_templates < <(find "$templates_root" -mindepth 1 -maxdepth 1 -type d | sort)
  (( ${#available_templates[@]} )) || {
    info "No project templates available under ${templates_root}; continuing without scaffolding."
    return 0
  }

  local chosen=""
  local request_lower="$(to_lower "$template_request")"
  if [[ "$request_lower" == "skip" ]]; then
    info "Skipping project template scaffolding (per flag)."
    return 0
  fi

  if [[ "$request_lower" != "auto" ]]; then
    for tpl in "${available_templates[@]}"; do
      local tpl_name="$(basename "$tpl")"
      if [[ "$(to_lower "$tpl_name")" == "$request_lower" ]]; then
        chosen="$tpl"
        template_request="$tpl_name"
        break
      fi
    done
    if [[ -z "$chosen" ]]; then
      warn "Template '${template_request}' not found under ${templates_root}; available: $(printf '%s ' "${available_templates[@]##*/}")"
      return 1
    fi
  else
    chosen="$(gc_auto_project_template "$project_root" "$templates_root" "${available_templates[@]}")"
    if [[ -z "$chosen" ]]; then
      info "No matching project template determined automatically; continuing without scaffolding."
      return 0
    fi
  fi

  local template_name="$(basename "$chosen")"
  info "Applying project template → ${template_name}"
  if ! gc_copy_project_template "$chosen" "$project_root"; then
    warn "Failed to copy template '${template_name}'"
    return 1
  fi
}

gc_parse_jira_tasks() {
  local jira_file="${1:?jira markdown path required}"
  local out_json="${2:?output json path required}"
  python3 - <<'PY' "$jira_file" "$out_json"
import json
import re
import sys
import time
from pathlib import Path

jira_path, out_path = sys.argv[1:3]
lines = Path(jira_path).read_text(encoding='utf-8').splitlines()

epic_id = ""
epic_title = ""
story_id = ""
story_title = ""
tasks = []
current = None
section = None

dashes = r'[\-\u2012\u2013\u2014\u2015]'


def normalise_list(value: str):
    cleaned = value.replace('+', ',').replace('/', ',').replace('&', ',')
    parts = [part.strip() for part in re.split(r',|;|\\band\\b', cleaned, flags=re.I) if part.strip()]
    seen = set()
    ordered = []
    for item in parts:
        key = item.lower()
        if key not in seen:
            seen.add(key)
            ordered.append(item)
    return ordered


def flush_current():
    global current, tasks, section
    if current is None:
        return
    description_lines = [line.rstrip() for line in current['description_lines'] if line.strip()]
    description = "\n".join(description_lines).strip()
    current['description'] = description
    del current['description_lines']
    tasks.append(current)
    current = None
    section = None


for raw in lines:
    stripped = raw.strip()
    if not stripped:
        if current is not None and section == 'description':
            current['description_lines'].append('')
        continue

    if stripped.lower() in {'**epic**', '**story**', '### **story**'}:
        continue

    epic_heading = re.match(r'^##\s+Epic\s+([A-Za-z0-9_.-]+)\s+' + dashes + r'\s+(.*)$', stripped)
    epic_bold = re.match(r'^\*\*([Ee][A-Za-z0-9_.:-]+)\s*' + dashes + r'\s*(.+?)\*\*$', stripped)
    if epic_heading or epic_bold:
        flush_current()
        if epic_heading:
            epic_id, epic_title = epic_heading.group(1).strip(), epic_heading.group(2).strip()
        else:
            epic_id, epic_title = epic_bold.group(1).strip(), epic_bold.group(2).strip()
        story_id = ""
        story_title = ""
        continue

    story_heading = re.match(r'^###\s+Story\s+([A-Za-z0-9_.-]+)\s+' + dashes + r'\s+(.*)$', stripped)
    story_bold = re.match(r'^\*\*([Ss][A-Za-z0-9_.:-]+)\s*' + dashes + r'\s*(.+?)\*\*$', stripped)
    if story_heading or story_bold:
        flush_current()
        if story_heading:
            story_id, story_title = story_heading.group(1).strip(), story_heading.group(2).strip()
        else:
            story_id, story_title = story_bold.group(1).strip(), story_bold.group(2).strip()
        continue

    task_match = re.match(r'^\*\*([Tt][A-Za-z0-9_.:-]+)\s*' + dashes + r'\s*(.+?)\*\*$', stripped)
    if task_match:
        flush_current()
        task_id, task_title = task_match.groups()
        current = {
            'epic_id': epic_id,
            'epic_title': epic_title,
            'story_id': story_id,
            'story_title': story_title,
            'id': task_id.strip(),
            'title': task_title.strip(),
            'assignees': [],
            'tags': [],
            'estimate': '',
            'description_lines': [],
            'acceptance_criteria': [],
            'dependencies': [],
        }
        section = None
        continue

    if current is None:
        continue

    if '**Description:**' in stripped:
        section = 'description'
        after = stripped.split('**Description:**', 1)[1].strip()
        if after:
            current['description_lines'].append(after)
        continue

    if '**Acceptance Criteria:**' in stripped:
        section = 'ac'
        after = stripped.split('**Acceptance Criteria:**', 1)[1].strip()
        if after:
            current['acceptance_criteria'].append(after)
        continue

    if '**Dependencies:**' in stripped:
        section = 'dependencies'
        after = stripped.split('**Dependencies:**', 1)[1].strip()
        if after:
            current['dependencies'] = normalise_list(after)
        continue

    if section == 'ac':
        if stripped.startswith('*') or stripped.startswith('-'):
            current['acceptance_criteria'].append(stripped.lstrip('*- ').rstrip())
            continue
        else:
            section = None

    if section == 'dependencies':
        if stripped.startswith('*') or stripped.startswith('-'):
            current['dependencies'].extend(normalise_list(stripped.lstrip('*- ')))
            continue
        else:
            section = None

    segments = [seg.strip() for seg in re.split(r'[·•]', stripped) if seg.strip()]
    meta_consumed = False
    for seg in segments:
        plain = seg.replace('**', '')
        lower = plain.lower()
        if lower.startswith('assignee:'):
            value = plain.split(':', 1)[1].strip()
            if value:
                current['assignees'] = normalise_list(value)
                meta_consumed = True
        elif lower.startswith('tags:'):
            value = plain.split(':', 1)[1].strip()
            if value:
                current['tags'] = normalise_list(value)
                meta_consumed = True
        elif lower.startswith('estimate:'):
            value = plain.split(':', 1)[1].strip()
            if value and not current['estimate']:
                current['estimate'] = value
                meta_consumed = True

    if section == 'description' or (not meta_consumed and section is None):
        current['description_lines'].append(raw.rstrip())

flush_current()

payload = {
    'generated_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
    'tasks': tasks
}
Path(out_path).write_text(json.dumps(payload, indent=2) + '\n', encoding='utf-8')
PY
}

gc_build_tasks_db() {
  local tasks_json="${1:?tasks json path required}"
  local db_path="${2:?sqlite db path required}"
  local force_flag="${3:-0}"
  python3 - <<'PY' "$tasks_json" "$db_path" "$force_flag"
import json
import re
import sqlite3
import sys
import time
from collections import OrderedDict
from pathlib import Path

tasks_json_path = Path(sys.argv[1])
db_path = Path(sys.argv[2])
force = sys.argv[3] == '1'

payload = json.loads(tasks_json_path.read_text(encoding='utf-8'))
all_tasks = payload.get('tasks') or []
generated_at = time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())

def slugify(text: str) -> str:
    slug = re.sub(r'[^a-z0-9]+', '-', (text or '').lower()).strip('-')
    return slug or 'item'

def story_key_for(task: dict) -> str:
    return '|'.join([
        (task.get('story_id') or '').strip(),
        (task.get('story_title') or '').strip(),
        (task.get('epic_id') or '').strip(),
        (task.get('epic_title') or '').strip(),
    ])

grouped = OrderedDict()
for task in all_tasks:
    key = story_key_for(task)
    grouped.setdefault(key, {
        'story_id': (task.get('story_id') or '').strip(),
        'story_title': (task.get('story_title') or '').strip(),
        'epic_id': (task.get('epic_id') or '').strip(),
        'epic_title': (task.get('epic_title') or '').strip(),
        'tasks': []
    })
    grouped[key]['tasks'].append(task)

db_path.parent.mkdir(parents=True, exist_ok=True)
conn = sqlite3.connect(str(db_path))
conn.row_factory = sqlite3.Row
cur = conn.cursor()
cur.execute('PRAGMA foreign_keys = ON')
cur.execute('PRAGMA journal_mode = WAL')

def list_to_text(values):
    return ', '.join(str(item).strip() for item in values if str(item).strip()) if values else None

def as_text(value):
    if isinstance(value, list):
        return list_to_text(value)
    if isinstance(value, dict):
        return json.dumps(value, ensure_ascii=False)
    if value is None:
        return None
    text = str(value).strip()
    return text if text else None

def ensure_table():
    cur.execute('''
        CREATE TABLE IF NOT EXISTS metadata (
          key TEXT PRIMARY KEY,
          value TEXT NOT NULL
        )
    ''')
    cur.execute('''
        CREATE TABLE IF NOT EXISTS epics (
          epic_key TEXT PRIMARY KEY,
          epic_id TEXT,
          title TEXT,
          slug TEXT,
          created_at TEXT NOT NULL,
          updated_at TEXT NOT NULL
        )
    ''')
    cur.execute('''
        CREATE TABLE IF NOT EXISTS stories (
          story_slug TEXT PRIMARY KEY,
          story_key TEXT UNIQUE,
          story_id TEXT,
          story_title TEXT,
          epic_key TEXT,
          epic_title TEXT,
          sequence INTEGER,
          status TEXT,
          completed_tasks INTEGER,
          total_tasks INTEGER,
          last_run TEXT,
          updated_at TEXT NOT NULL,
          created_at TEXT NOT NULL,
          FOREIGN KEY(epic_key) REFERENCES epics(epic_key)
        )
    ''')
    cur.execute('''
        CREATE TABLE IF NOT EXISTS tasks (
          id INTEGER PRIMARY KEY AUTOINCREMENT,
          story_slug TEXT NOT NULL,
          position INTEGER NOT NULL,
          task_id TEXT,
          title TEXT,
          description TEXT,
          estimate TEXT,
          assignees_json TEXT,
          tags_json TEXT,
          acceptance_json TEXT,
          dependencies_json TEXT,
          tags_text TEXT,
          story_points TEXT,
          dependencies_text TEXT,
          assignee_text TEXT,
          document_reference TEXT,
          idempotency TEXT,
          rate_limits TEXT,
          rbac TEXT,
          messaging_workflows TEXT,
          performance_targets TEXT,
          observability TEXT,
          acceptance_text TEXT,
          endpoints TEXT,
          sample_create_request TEXT,
          sample_create_response TEXT,
          user_story_ref_id TEXT,
          epic_ref_id TEXT,
          status TEXT NOT NULL DEFAULT 'pending',
          started_at TEXT,
          completed_at TEXT,
          last_run TEXT,
          story_id TEXT,
          story_title TEXT,
          epic_key TEXT,
          epic_title TEXT,
          updated_at TEXT NOT NULL,
          created_at TEXT NOT NULL,
          UNIQUE(story_slug, position),
          FOREIGN KEY(story_slug) REFERENCES stories(story_slug)
        )
    ''')

ensure_table()

def column_exists(table: str, column: str) -> bool:
    cur.execute(f"PRAGMA table_info({table})")
    return any(row['name'] == column for row in cur.fetchall())

def ensure_column(table: str, column: str, definition: str):
    if not column_exists(table, column):
        cur.execute(f"ALTER TABLE {table} ADD COLUMN {column} {definition}")

ensure_column('stories', 'completed_tasks', 'INTEGER')
ensure_column('stories', 'total_tasks', 'INTEGER')
ensure_column('stories', 'status', "TEXT DEFAULT 'pending'")
ensure_column('stories', 'last_run', 'TEXT')
ensure_column('stories', 'epic_title', 'TEXT')

ensure_column('tasks', 'story_id', 'TEXT')
ensure_column('tasks', 'story_title', 'TEXT')
ensure_column('tasks', 'epic_key', 'TEXT')
ensure_column('tasks', 'epic_title', 'TEXT')
ensure_column('tasks', 'status', "TEXT DEFAULT 'pending'")
ensure_column('tasks', 'started_at', 'TEXT')
ensure_column('tasks', 'completed_at', 'TEXT')
ensure_column('tasks', 'last_run', 'TEXT')
ensure_column('tasks', 'tags_text', 'TEXT')
ensure_column('tasks', 'story_points', 'TEXT')
ensure_column('tasks', 'dependencies_text', 'TEXT')
ensure_column('tasks', 'assignee_text', 'TEXT')
ensure_column('tasks', 'document_reference', 'TEXT')
ensure_column('tasks', 'idempotency', 'TEXT')
ensure_column('tasks', 'rate_limits', 'TEXT')
ensure_column('tasks', 'rbac', 'TEXT')
ensure_column('tasks', 'messaging_workflows', 'TEXT')
ensure_column('tasks', 'performance_targets', 'TEXT')
ensure_column('tasks', 'observability', 'TEXT')
ensure_column('tasks', 'acceptance_text', 'TEXT')
ensure_column('tasks', 'endpoints', 'TEXT')
ensure_column('tasks', 'sample_create_request', 'TEXT')
ensure_column('tasks', 'sample_create_response', 'TEXT')
ensure_column('tasks', 'user_story_ref_id', 'TEXT')
ensure_column('tasks', 'epic_ref_id', 'TEXT')

prior_story_slugs = {}
prior_story_state = {}
prior_task_state = {}

if not force:
    try:
        for row in cur.execute('SELECT story_slug, story_key, status, completed_tasks, total_tasks, last_run, updated_at, created_at FROM stories'):
            story_slug = row['story_slug']
            story_key = row['story_key']
            if story_key:
                prior_story_slugs[story_key] = story_slug
            prior_story_state[story_slug] = {
                'status': row['status'] or 'pending',
                'completed_tasks': int(row['completed_tasks'] or 0),
                'total_tasks': int(row['total_tasks'] or 0),
                'last_run': row['last_run'],
                'updated_at': row['updated_at'],
                'created_at': row['created_at'],
            }
    except sqlite3.OperationalError:
        pass

    try:
        for row in cur.execute('SELECT story_slug, position, task_id, status, started_at, completed_at, last_run FROM tasks'):
            base = {
                'status': row['status'] or 'pending',
                'started_at': row['started_at'],
                'completed_at': row['completed_at'],
                'last_run': row['last_run'],
            }
            prior_task_state[('pos', row['story_slug'], row['position'])] = base
            task_id = (row['task_id'] or '').strip().lower()
            if task_id:
                prior_task_state[('id', row['story_slug'], task_id)] = base
    except sqlite3.OperationalError:
        pass

cur.execute('DELETE FROM tasks')
cur.execute('DELETE FROM stories')
cur.execute('DELETE FROM epics')

used_story_slugs = set(prior_story_slugs.values())
used_story_slugs.discard('')

def assign_story_slug(preferred: str, story_key: str) -> str:
    if not preferred:
        preferred = 'story'
    slug = slugify(preferred)
    if story_key in prior_story_slugs:
        return prior_story_slugs[story_key]
    base = slug or 'story'
    slug = base
    i = 2
    while slug in used_story_slugs:
        slug = f"{base}-{i}"
        i += 1
    used_story_slugs.add(slug)
    return slug

epics_inserted = {}
story_count = 0
task_count = 0
restored_stories = 0
restored_tasks = 0

for sequence, (story_key, info) in enumerate(grouped.items(), start=1):
    tasks = info['tasks']
    if not tasks:
        continue
    story_id = info['story_id']
    story_title = info['story_title']
    epic_id = info['epic_id']
    epic_title = info['epic_title']

    preferred_slug_source = story_id or story_title or epic_id or f'story-{sequence}'
    story_slug = assign_story_slug(preferred_slug_source, story_key)
    restored = story_slug in prior_story_state
    if restored:
        restored_stories += 1

    epic_key = (epic_id or epic_title or '').strip()
    if not epic_key:
        epic_key = None
    else:
        epic_slug = slugify(epic_key)
        if epic_key not in epics_inserted:
            cur.execute('''
                INSERT OR REPLACE INTO epics(epic_key, epic_id, title, slug, created_at, updated_at)
                VALUES(?, ?, ?, ?, ?, ?)
            ''', (
                epic_key,
                epic_id or None,
                epic_title or None,
                epic_slug,
                generated_at,
                generated_at,
            ))
            epics_inserted[epic_key] = True

    completed_tasks = 0
    story_status = 'pending'
    story_total = len(tasks)

    for position, task in enumerate(tasks):
        task_id = (task.get('id') or '').strip()
        description = (task.get('description') or '').strip()
        estimate = (task.get('estimate') or '').strip()
        assignees = task.get('assignees') or []
        tags = task.get('tags') or []
        acceptance = task.get('acceptance_criteria') or []
        dependencies = task.get('dependencies') or []

        key_id = ('id', story_slug, task_id.lower()) if task_id else None
        key_pos = ('pos', story_slug, position)
        restore = None
        if key_id and key_id in prior_task_state and not force:
            restore = prior_task_state[key_id]
        elif key_pos in prior_task_state and not force:
            restore = prior_task_state[key_pos]

        status = (restore or {}).get('status') or 'pending'
        started_at = (restore or {}).get('started_at')
        completed_at = (restore or {}).get('completed_at')
        last_run = (restore or {}).get('last_run')

        if status == 'complete':
            completed_tasks += 1

        if restore:
            restored_tasks += 1

        tags_text = list_to_text(tags)
        dependencies_text = list_to_text(dependencies)
        assignee_text = list_to_text(assignees)
        story_points = as_text(task.get('story_points')) or (estimate if estimate else None)
        document_reference = as_text(task.get('document_reference') or task.get('document_ref'))
        idempotency_text = as_text(task.get('idempotency'))
        rate_limits = as_text(task.get('rate_limits'))
        rbac_text = as_text(task.get('rbac') or task.get('rbac_requirements'))
        messaging_workflows = as_text(task.get('messaging_workflows') or task.get('messaging_and_workflows'))
        performance_targets = as_text(task.get('performance_targets'))
        observability = as_text(task.get('observability'))
        acceptance_text = '\n'.join(item.strip() for item in acceptance if item.strip()) if acceptance else None
        if not acceptance_text:
            acceptance_text = as_text(task.get('acceptance_text'))
        endpoints = as_text(task.get('endpoints'))
        sample_create_request = as_text(task.get('sample_create_request') or task.get('sample_request'))
        sample_create_response = as_text(task.get('sample_create_response') or task.get('sample_response'))
        user_story_ref_id = as_text(task.get('user_story_ref_id') or task.get('user_story_reference_id') or story_id)
        epic_ref_id = as_text(task.get('epic_ref_id') or task.get('epic_reference_id') or epic_id)

        cur.execute('''
            INSERT INTO tasks (
              story_slug, position, task_id, title, description, estimate,
              assignees_json, tags_json, acceptance_json, dependencies_json,
              tags_text, story_points, dependencies_text, assignee_text,
              document_reference, idempotency, rate_limits, rbac,
              messaging_workflows, performance_targets, observability,
              acceptance_text, endpoints, sample_create_request, sample_create_response,
              user_story_ref_id, epic_ref_id,
              status, started_at, completed_at, last_run,
              story_id, story_title, epic_key, epic_title,
              updated_at, created_at
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            story_slug,
            position,
            task_id or None,
            (task.get('title') or '').strip() or None,
            description or None,
            estimate or None,
            json.dumps(assignees, ensure_ascii=False),
            json.dumps(tags, ensure_ascii=False),
            json.dumps(acceptance, ensure_ascii=False),
            json.dumps(dependencies, ensure_ascii=False),
            tags_text,
            story_points,
            dependencies_text,
            assignee_text,
            document_reference,
            idempotency_text,
            rate_limits,
            rbac_text,
            messaging_workflows,
            performance_targets,
            observability,
            acceptance_text,
            endpoints,
            sample_create_request,
            sample_create_response,
            user_story_ref_id,
            epic_ref_id,
            status,
            started_at,
            completed_at,
            last_run,
            story_id or None,
            story_title or None,
            epic_key,
            epic_title or None,
            generated_at,
            generated_at,
        ))

    if completed_tasks >= story_total and story_total > 0:
        story_status = 'complete'
    elif completed_tasks > 0:
        story_status = 'in-progress'
    else:
        story_status = 'pending'

    if restored and not force:
        state = prior_story_state.get(story_slug, {})
        if state:
            story_status = state.get('status') or story_status
            restored_completed = int(state.get('completed_tasks') or completed_tasks)
            completed_tasks = max(completed_tasks, restored_completed)
            story_total = state.get('total_tasks') or story_total

    cur.execute('''
        INSERT OR REPLACE INTO stories (
          story_slug, story_key, story_id, story_title,
          epic_key, epic_title, sequence, status,
          completed_tasks, total_tasks, last_run,
          updated_at, created_at
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    ''', (
        story_slug,
        story_key,
        story_id or None,
        story_title or None,
        epic_key,
        epic_title or None,
        sequence,
        story_status,
        completed_tasks,
        story_total,
        (prior_story_state.get(story_slug) or {}).get('last_run') if restored and not force else None,
        generated_at,
        (prior_story_state.get(story_slug) or {}).get('created_at', generated_at) if restored and not force else generated_at,
    ))

    story_count += 1
    task_count += story_total

cur.execute('INSERT OR REPLACE INTO metadata(key, value) VALUES(?, ?)', ('generated_at', generated_at))
cur.execute('INSERT OR REPLACE INTO metadata(key, value) VALUES(?, ?)', ('source', str(tasks_json_path)))

conn.commit()
conn.close()

print(f'STORIES {story_count}')
print(f'TASKS {task_count}')
print(f'RESTORED_STORIES {restored_stories}')
print(f'RESTORED_TASKS {restored_tasks}')
PY
}

gc_build_context_file() {
  local dest_file="${1:?context destination required}"
  local staging_dir="${2:-$GC_STAGING_DIR}"
  local max_lines="${GC_CONTEXT_FILE_LINES:-200}"
  local allow_ui_dump="${GC_CONTEXT_INCLUDE_UI:-0}"
  local -a skip_patterns=()
  if declare -p GC_CONTEXT_SKIP_PATTERNS >/dev/null 2>&1; then
    skip_patterns=("${GC_CONTEXT_SKIP_PATTERNS[@]}")
  fi
  mkdir -p "$(dirname "$dest_file")"
  {
    echo "# Project Context (auto-generated)"
    echo
    shopt -s nullglob
    shopt -s globstar 2>/dev/null || true
    local f
    local -a patterns=(
      "$staging_dir"/pdr.* 
      "$staging_dir"/sds.* 
      "$staging_dir"/openapi.* 
      "$staging_dir"/*.md 
      "$staging_dir"/*.mdx 
      "$staging_dir"/*.adoc 
      "$staging_dir"/*.mmd 
      "$staging_dir"/*.sql 
      "$staging_dir"/*.yml 
      "$staging_dir"/*.yaml
    )
    if [[ "$allow_ui_dump" == "1" ]]; then
      patterns+=("$staging_dir"/*ui*pages*.* "$staging_dir"/*rfp*.*)
    fi
    for f in "${patterns[@]}"; do
      [[ -f "$f" ]] || continue
      local base_name
      base_name="$(basename "$f")"

      local skip_file=0
      if ((${#skip_patterns[@]} > 0)); then
        local pattern
        for pattern in "${skip_patterns[@]}"; do
          [[ -n "$pattern" ]] || continue
          if [[ "$base_name" == $pattern || "$f" == $pattern ]]; then
            skip_file=1
            break
          fi
          # Treat plain patterns without glob characters as substring matches.
          if [[ "$pattern" != *'*'* && "$pattern" != *'?'* ]]; then
            if [[ "$f" == *"$pattern"* ]]; then
              skip_file=1
              break
            fi
          fi
        done
      fi
      (( skip_file )) && continue

      echo ""
      echo "----- FILE: ${base_name} -----"
      local mime=""
      if command -v file >/dev/null 2>&1; then
        mime="$(file -b --mime-type "$f" 2>/dev/null || true)"
      fi
      if [[ -n "$mime" ]]; then
        case "$mime" in
          text/*|application/json|application/vnd.openxmlformats-officedocument*|application/xml|application/xhtml+xml)
            ;;
          *)
            echo "(skipped non-text file; path: $f)"
            continue
            ;;
        esac
      fi
      python3 - "$f" "$max_lines" <<'PY'
import json
import pathlib
import re
import sys

path = pathlib.Path(sys.argv[1])
try:
    limit = int(sys.argv[2])
except Exception:
    limit = 200

try:
    raw = path.read_text(encoding="utf-8", errors="replace")
except Exception as exc:
    print(f"(failed to read text: {exc})")
    raise SystemExit(0)

ext = path.suffix.lower()
css_exts = {".css", ".scss", ".sass", ".less", ".pcss", ".styl"}
markup_exts = {".html", ".htm", ".vue", ".jsx", ".tsx"}

def emit(lines):
    if limit > 0 and len(lines) > limit:
        omitted = len(lines) - limit
        lines = lines[:limit]
        lines.append(f"... ({omitted} line(s) truncated) ...")
    print("\n".join(lines))
    raise SystemExit(0)

if ext in css_exts:
    tokens = re.findall(r"--([a-z0-9_-]+)", raw, re.I)
    unique = sorted({token for token in tokens if token})
    lines = ["CSS variables (first 40):"]
    for token in unique[:40]:
        lines.append(f"- --{token}")
    if len(unique) > 40:
        lines.append(f"... ({len(unique) - 40} additional variables omitted)")
    emit(lines)

if ext in markup_exts:
    clean = re.sub(r"<script[\s\S]*?</script>", "", raw, flags=re.I)
    clean = re.sub(r"<style[\s\S]*?</style>", "", clean, flags=re.I)
    text = re.sub(r"<[^>]+>", " ", clean)
    text = re.sub(r"\s+", " ", text).strip()
    if not text:
        emit(["(markup collapsed to empty after stripping tags)"])
    chunks = [text[i:i + 200] for i in range(0, len(text), 200)]
    lines = ["Markup summary:"] + chunks[:5]
    if len(chunks) > 5:
        lines.append(f"... ({len(chunks) - 5} additional chunks omitted)")
    emit(lines)

try:
    parsed = json.loads(raw)
    preview = json.dumps(parsed, indent=2)[:4000]
    emit(["JSON summary:", preview])
except Exception:
    pass

lines = raw.splitlines()
result = []
max_width = 160
table_run = 0
table_notice = False
sql_insert_run = 0
sql_notice = False

def truncate(line: str) -> str:
    if len(line) <= max_width:
        return line
    return line[:max_width].rstrip() + " …"

for original in lines:
    line = original.rstrip()
    stripped = line.lstrip()

    if not stripped:
        if not result or result[-1] != "":
            result.append("")
        continue

    pipe_count = line.count("|")
    is_table_like = pipe_count >= 6 or (pipe_count >= 3 and "," in line and len(line) > 80)
    if is_table_like:
        table_run += 1
    else:
        table_run = 0
        table_notice = False
    if is_table_like and table_run > 30:
        if not table_notice:
            result.append("... (additional table rows truncated) ...")
            table_notice = True
        continue

    upper = stripped.upper()
    if upper.startswith("INSERT INTO") or upper.startswith("UPDATE "):
        sql_insert_run += 1
    else:
        sql_insert_run = 0
        sql_notice = False
    if sql_insert_run > 40:
        if not sql_notice:
            result.append("... (repetitive SQL statements truncated) ...")
            sql_notice = True
        continue

    line = truncate(line.replace("\t", "  "))
    result.append(line)

emit(result or ["(no textual content captured)"])
PY
    done
    shopt -u globstar 2>/dev/null || true
    shopt -u nullglob || true
  } >"$dest_file"
}

gc_build_context_digest() {
  local context_file="${1:?context file required}"
  local digest_file="${2:?digest destination required}"
  local limit_lines="${3:-400}"
  [[ -f "$context_file" ]] || {
    printf '%s\n' "(warn) context digest skipped; missing source ${context_file}" >&2
    return 1
  }
  python3 - <<'PY' "$context_file" "$digest_file" "$limit_lines"
import hashlib
import math
import pathlib
import sys

context_path = pathlib.Path(sys.argv[1])
dest_path = pathlib.Path(sys.argv[2])
try:
    limit = int(sys.argv[3])
except Exception:
    limit = 400

if limit <= 0:
    dest_path.write_text("", encoding="utf-8")
    raise SystemExit(0)

try:
    raw = context_path.read_text(encoding="utf-8", errors="replace")
except Exception as exc:
    dest_path.write_text(f"(failed to read shared context: {exc})\n", encoding="utf-8")
    raise SystemExit(0)

lines = raw.splitlines()
sections = []
current_name = None
current_lines: list[str] = []

for line in lines:
    if line.startswith("----- FILE: ") and line.endswith(" -----"):
        if current_name is not None:
            sections.append((current_name, current_lines))
        current_name = line[len("----- FILE: "):-len(" -----")].strip() or "unnamed"
        current_lines = []
        continue
    if current_name is not None:
        current_lines.append(line.rstrip())

if current_name is not None:
    sections.append((current_name, current_lines))

output: list[str] = []
output.append("Hint: use --context-mode raw to include the literal tail.")
remaining = max(limit - len(output), 10)

if not sections:
    output.append("(no staged context files indexed)")
    dest_path.write_text("\n".join(output) + "\n", encoding="utf-8")
    raise SystemExit(0)

per_header_cost = 2  # heading + blank separator
remaining_sections = len(sections)

for index, (name, section_lines) in enumerate(sections, 1):
    if remaining <= 0:
        output.append("… (context digest truncated; raise --context-lines or switch to raw)")
        break

    digest_src = "\n".join(section_lines).encode("utf-8", "replace")
    digest = hashlib.sha256(digest_src).hexdigest()[:12]
    heading = f"### {name} (sha256 {digest})"
    output.append(heading)
    remaining -= 1
    if remaining <= 0:
        output.append("… (context digest truncated; raise --context-lines or switch to raw)")
        break

    remaining_sections = len(sections) - index
    # Reserve at least two lines per remaining section for future headings.
    reserved_for_rest = max(0, remaining_sections * per_header_cost)
    available_for_this = max(3, min(12, remaining - reserved_for_rest))

    sample_lines = []
    for raw_line in section_lines:
        stripped = raw_line.strip()
        if not stripped:
            continue
        if stripped.startswith("... (additional "):
            continue
        if stripped.startswith("----- FILE:"):
            continue
        stripped = stripped.replace("\t", "  ")
        if len(stripped) > 160:
            stripped = stripped[:160].rstrip() + " …"
        sample_lines.append(stripped)
        if len(sample_lines) >= available_for_this:
            break

    if not sample_lines:
        sample_lines = ["(no excerpt captured; file may be binary or truncated)"]

    for sample in sample_lines:
        if remaining <= 0:
            output.append("… (context digest truncated; raise --context-lines or switch to raw)")
            break
        output.append(sample)
        remaining -= 1

    if remaining <= 0:
        break

    if index != len(sections):
        output.append("")
        remaining -= 1

dest_path.parent.mkdir(parents=True, exist_ok=True)
dest_path.write_text("\n".join(output).rstrip() + "\n", encoding="utf-8")
PY
}

gc_update_work_state() {
  local db_path="${1:?tasks database path required}"
  local story_slug="${2:?story slug required}"
  local status="${3:?status required}"
  local completed="${4:-0}"
  local total="${5:-0}"
  local run_stamp="${6:-manual}"
  python3 - <<'PY' "$db_path" "$story_slug" "$status" "$completed" "$total" "$run_stamp"
import sqlite3
import sys
import time

db_path, story_slug, status, completed, total, run_stamp = sys.argv[1:7]
story_slug = (story_slug or "").strip()
status_requested = (status or "pending").strip().lower()
completed = int(completed or 0)
total = int(total or 0)
run_stamp = run_stamp or 'manual'
timestamp = time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())

conn = sqlite3.connect(db_path)
conn.row_factory = sqlite3.Row
cur = conn.cursor()

row = cur.execute(
    """
    SELECT
        COUNT(*) AS total_count,
        SUM(CASE WHEN LOWER(COALESCE(status, '')) = 'complete' THEN 1 ELSE 0 END) AS complete_count,
        SUM(CASE WHEN LOWER(COALESCE(status, '')) = 'in-progress' THEN 1 ELSE 0 END) AS in_progress_count
      FROM tasks
     WHERE LOWER(COALESCE(story_slug, '')) = ?
    """,
    (story_slug.lower(),)
).fetchone()

total_count = int(row["total_count"] or 0)
complete_count = int(row["complete_count"] or 0)
in_progress_count = int(row["in_progress_count"] or 0)
pending_count = max(total_count - complete_count - in_progress_count, 0)

if total_count > 0:
    status_final = status_requested
    if complete_count >= total_count:
        status_final = "complete"
    elif complete_count > 0 or in_progress_count > 0:
        if status_final == "complete":
            status_final = "in-progress"
        elif status_final not in {"blocked", "on-hold", "in-progress"}:
            status_final = "in-progress"
    else:
        if status_final not in {"blocked", "on-hold"}:
            status_final = "pending"
    completed_value = complete_count
    total_value = total_count
else:
    status_final = status_requested or "pending"
    completed_value = completed
    total_value = total

cur.execute(
    """
    UPDATE stories
       SET status = ?,
           completed_tasks = ?,
           total_tasks = ?,
           last_run = ?,
           updated_at = ?
     WHERE story_slug = ?
    """,
    (status_final, completed_value, total_value, run_stamp, timestamp, story_slug)
)
if cur.rowcount == 0:
    cur.execute(
        """
        INSERT INTO stories (story_slug, story_key, status, completed_tasks, total_tasks, last_run, created_at, updated_at)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """,
        (story_slug, story_slug, status_final, completed_value, total_value, run_stamp, timestamp, timestamp)
    )

conn.commit()
conn.close()
PY
}

gc_update_task_state() {
  local db_path="${1:?tasks database path required}"
  local story_slug="${2:?story slug required}"
  local position="${3:?task position required}"
  local status="${4:?status required}"
  local run_stamp="${5:-manual}"
  python3 - <<'PY' "$db_path" "$story_slug" "$position" "$status" "$run_stamp"
import sqlite3
import sys
import time

db_path, story_slug, position, status, run_stamp = sys.argv[1:6]
position = int(position)
run_stamp = run_stamp or 'manual'
timestamp = time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())

conn = sqlite3.connect(db_path)
conn.row_factory = sqlite3.Row
cur = conn.cursor()
row = cur.execute(
    'SELECT id, status, started_at, completed_at FROM tasks WHERE story_slug = ? AND position = ?',
    (story_slug, position)
).fetchone()

if row is not None:
    fields = [
        ('status', status),
        ('last_run', run_stamp),
        ('updated_at', timestamp),
    ]

    started_at = row['started_at']
    completed_at = row['completed_at']

    if status == 'in-progress' and not started_at:
        fields.append(('started_at', timestamp))
    elif status == 'complete':
        if not started_at:
            fields.append(('started_at', timestamp))
        fields.append(('completed_at', timestamp))
    elif status == 'pending':
        fields.append(('started_at', None))
        fields.append(('completed_at', None))
    elif status == 'blocked':
        if not started_at:
            fields.append(('started_at', timestamp))

    set_clause = ', '.join(f"{col} = ?" for col, _ in fields)
    params = [value for _, value in fields] + [story_slug, position]
    cur.execute(f'UPDATE tasks SET {set_clause} WHERE story_slug = ? AND position = ?', params)

conn.commit()
conn.close()
PY
}

gc_rewind_backlog_from_task() {
  local db_path="${1:?tasks database path required}"
  local task_ref_raw="${2:?task reference required}"
  local story_hint_raw="${3:-}"
  python3 - "$db_path" "$task_ref_raw" "$story_hint_raw" <<'PY'
import re
import sqlite3
import sys
import time

args = sys.argv[1:]
if len(args) < 2:
    print("Task reference required.", file=sys.stderr)
    sys.exit(1)

db_path, task_ref = args[0], args[1]
story_hint = args[2] if len(args) > 2 else ""
task_ref = (task_ref or "").strip()
if not task_ref:
    print("Task reference required.", file=sys.stderr)
    sys.exit(1)

conn = sqlite3.connect(db_path)
conn.row_factory = sqlite3.Row
cur = conn.cursor()

timestamp = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())

stories = cur.execute(
    "SELECT story_slug, story_key, story_id, story_title, sequence FROM stories ORDER BY sequence ASC, story_slug ASC"
).fetchall()

if not stories:
    print("No stories available in backlog.", file=sys.stderr)
    sys.exit(1)

def clean(value: str) -> str:
    return (value or "").strip()

def norm(value: str) -> str:
    return clean(value).lower()

def slug_norm(value: str) -> str:
    text = norm(value)
    if not text:
        return ""
    return re.sub(r"[^a-z0-9]+", "-", text).strip("-")

story_by_keys = {}
story_order = []
for story in stories:
    slug = clean(story["story_slug"])
    if not slug:
        continue
    story_order.append(story)
    keys = {
        norm(story["story_key"]),
        norm(story["story_id"]),
        norm(slug),
        slug_norm(story["story_key"]),
        slug_norm(story["story_id"]),
        slug_norm(slug),
    }
    for key in keys:
        if key and key not in story_by_keys:
            story_by_keys[key] = story

def resolve_story(token: str):
    token_norm = norm(token)
    if token_norm in story_by_keys:
        return story_by_keys[token_norm]
    token_slug = slug_norm(token)
    if token_slug and token_slug in story_by_keys:
        return story_by_keys[token_slug]
    return None

def locate_by_story_and_position(story_token: str, position_text: str):
    story = resolve_story(story_token)
    if story is None:
        print(f"Story not found for reference '{story_token}'.", file=sys.stderr)
        sys.exit(1)
    try:
        position_input = int(position_text)
    except ValueError:
        print(f"Invalid task position '{position_text}'. Use a positive integer (1-based).", file=sys.stderr)
        sys.exit(1)
    if position_input <= 0:
        print(f"Task position must be at least 1 (got {position_input}).", file=sys.stderr)
        sys.exit(1)
    index = position_input - 1
    row = cur.execute(
        """
        SELECT t.story_slug, t.position, t.task_id, t.title, s.story_title
          FROM tasks t
          LEFT JOIN stories s ON s.story_slug = t.story_slug
         WHERE LOWER(COALESCE(t.story_slug, '')) = ?
           AND t.position = ?
        """,
        (norm(story["story_slug"]), index),
    ).fetchone()
    if row is None:
        print(
            f"No task found for story '{story['story_slug']}' at position {position_input}.",
            file=sys.stderr,
        )
        sys.exit(1)
    return row

def locate_by_task_id(task_id: str):
    token = norm(task_id)
    if not token:
        return None
    row = cur.execute(
        """
        SELECT t.story_slug, t.position, t.task_id, t.title, s.story_title, s.sequence
          FROM tasks t
          LEFT JOIN stories s ON s.story_slug = t.story_slug
         WHERE LOWER(COALESCE(t.task_id, '')) = ?
         ORDER BY s.sequence ASC, t.position ASC
         LIMIT 1
        """,
        (token,),
    ).fetchone()
    return row

target_row = None

if ":" in task_ref:
    story_part, position_part = task_ref.split(":", 1)
    story_part = story_part.strip()
    position_part = position_part.strip()
    if not story_part or not position_part:
        print(
            "Invalid task reference. Expected format STORY:POSITION (e.g. auth-login:3).",
            file=sys.stderr,
        )
        sys.exit(1)
    target_row = locate_by_story_and_position(story_part, position_part)
else:
    target_row = locate_by_task_id(task_ref)
    if target_row is None:
        print(
            "Task reference not found. Use a task id (e.g. TASK-123) or story-slug:position.",
            file=sys.stderr,
        )
        sys.exit(1)

target_story_slug = clean(target_row["story_slug"])
target_position = int(target_row["position"] or 0)
target_task_id = clean(target_row["task_id"])
target_title = clean(target_row["title"]) or "(untitled task)"
target_story_title = clean(target_row["story_title"])

story_hint_clean = clean(story_hint)
if story_hint_clean:
    hint_story = resolve_story(story_hint_clean)
    if hint_story is None:
        print(
            f"Story reference '{story_hint}' not found; expected story '{target_story_slug}'.",
            file=sys.stderr,
        )
        sys.exit(1)
    hint_slug = clean(hint_story["story_slug"])
    if norm(hint_slug) != norm(target_story_slug):
        print(
            f"Story reference '{story_hint}' resolves to '{hint_slug}', which does not match task story '{target_story_slug}'.",
            file=sys.stderr,
        )
        sys.exit(1)

ordered_slugs = [clean(story["story_slug"]) for story in story_order if clean(story["story_slug"])]
try:
    story_index = ordered_slugs.index(target_story_slug)
except ValueError:
    print(f"Story slug '{target_story_slug}' missing from ordered backlog.", file=sys.stderr)
    sys.exit(1)

affected_slugs = ordered_slugs[story_index:]
if not affected_slugs:
    print(f"No stories found from '{target_story_slug}'.", file=sys.stderr)
    sys.exit(1)

cur.execute(
    """
    UPDATE tasks
       SET status = 'pending',
           started_at = NULL,
           completed_at = NULL,
           last_run = NULL,
           updated_at = ?
     WHERE story_slug = ?
       AND position >= ?
    """,
    (timestamp, target_story_slug, target_position),
)

if len(affected_slugs) > 1:
    placeholders = ",".join("?" * (len(affected_slugs) - 1))
    cur.execute(
        f"""
        UPDATE tasks
           SET status = 'pending',
               started_at = NULL,
               completed_at = NULL,
               last_run = NULL,
               updated_at = ?
         WHERE story_slug IN ({placeholders})
        """,
        (timestamp, *affected_slugs[1:]),
    )

placeholders = ",".join("?" * len(affected_slugs))
cur.execute(
    f"""
    UPDATE stories
       SET status = 'pending',
           last_run = NULL,
           updated_at = ?
     WHERE story_slug IN ({placeholders})
    """,
    (timestamp, *affected_slugs),
)

conn.commit()
conn.close()

print("\t".join([
    target_story_slug,
    target_story_title or target_story_slug,
    str(target_position + 1),
    target_task_id,
    target_title.replace("\t", " ").replace("\n", " "),
]))
PY
}

gc_align_task_story_slugs() {
  local db_path="${1:?tasks database path required}"
  python3 - <<'PY' "$db_path"
import sqlite3
import sys
import time
import re
import difflib

db_path = sys.argv[1]
conn = sqlite3.connect(db_path)
conn.row_factory = sqlite3.Row
cur = conn.cursor()

def normalize(value: str) -> str:
    return (value or "").strip()

def slug_norm(value: str) -> str:
    value = normalize(value).lower()
    if not value:
        return ""
    return re.sub(r"[^a-z0-9]+", "-", value).strip("-")

story_norm_map = {}
story_norm_keys = []
for story in cur.execute("SELECT story_slug, story_key, story_id, story_title FROM stories"):
    slug = normalize(story["story_slug"])
    if not slug:
        continue
    canonical = slug
    for candidate in {
        story["story_slug"],
        story["story_key"],
        story["story_id"],
        story["story_title"],
    }:
        norm = slug_norm(candidate)
        if norm and norm not in story_norm_map:
            story_norm_map[norm] = canonical
    lower_slug = canonical.lower()
    if lower_slug not in story_norm_map:
        story_norm_map[lower_slug] = canonical

story_norm_keys = list(story_norm_map.keys())

updates = []
timestamp = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())

for task in cur.execute("SELECT id, story_slug, story_id, story_title FROM tasks"):
    current_slug = normalize(task["story_slug"])
    story_id = normalize(task["story_id"])
    story_title = normalize(task["story_title"])

    target_slug = None
    candidates = [
        story_id,
        current_slug,
        story_title,
    ]

    for candidate in candidates:
        norm = slug_norm(candidate)
        if norm and norm in story_norm_map:
            target_slug = story_norm_map[norm]
            break

    if not target_slug:
        combined = slug_norm(f"{story_id}-{story_title}") or slug_norm(current_slug)
        if combined:
            match = difflib.get_close_matches(combined, story_norm_keys, n=1, cutoff=0.84)
            if match:
                target_slug = story_norm_map[match[0]]

    if target_slug and target_slug != current_slug:
        updates.append((target_slug, timestamp, task["id"]))

if updates:
    cur.executemany("UPDATE tasks SET story_slug = ?, updated_at = ? WHERE id = ?", updates)
    conn.commit()

conn.close()
PY
}

gc_reset_task_progress() {
  local db_path="${1:?tasks database path required}"
  python3 - <<'PY' "$db_path"
import sqlite3
import sys
import time

db_path = sys.argv[1]
timestamp = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())

conn = sqlite3.connect(db_path)
cur = conn.cursor()

cur.execute(
    """
    UPDATE stories
       SET status = 'pending',
           completed_tasks = 0,
           last_run = NULL,
           updated_at = ?
    """,
    (timestamp,),
)
cur.execute(
    """
    UPDATE tasks
       SET status = 'pending',
           started_at = NULL,
           completed_at = NULL,
           last_run = NULL,
           updated_at = ?
    """,
    (timestamp,),
)

conn.commit()
conn.close()
PY
}

gc_sync_story_totals() {
  local db_path="${1:?tasks database path required}"
  python3 - <<'PY' "$db_path"
import sqlite3
import sys
import time

db_path = sys.argv[1]
timestamp = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())

conn = sqlite3.connect(db_path)
conn.row_factory = sqlite3.Row
cur = conn.cursor()

stories = cur.execute("SELECT story_slug, status FROM stories").fetchall()

def normalize(value: str) -> str:
    return (value or "").strip()

def status_for_counts(total: int, completed: int) -> str:
    if total <= 0:
        return "pending"
    if completed >= total:
        return "complete"
    if completed > 0:
        return "in-progress"
    return "pending"

updates = []
for story in stories:
    slug = normalize(story["story_slug"])
    if not slug:
        continue
    slug_lower = slug.lower()
    row = cur.execute(
        """
        SELECT
            COUNT(*) AS total_count,
            SUM(CASE WHEN LOWER(COALESCE(status, '')) = 'complete' THEN 1 ELSE 0 END) AS complete_count
          FROM tasks
         WHERE LOWER(COALESCE(story_slug, '')) = ?
        """,
        (slug_lower,),
    ).fetchone()
    total = int(row["total_count"] or 0)
    completed = int(row["complete_count"] or 0)
    story_status = status_for_counts(total, completed)
    updates.append((total, completed, story_status, timestamp, slug))

if updates:
    cur.executemany(
        """
        UPDATE stories
           SET total_tasks = ?,
               completed_tasks = ?,
               status = ?,
               updated_at = ?
         WHERE story_slug = ?
        """,
        updates,
    )
    conn.commit()

conn.close()
PY
}

gc_fetch_story_task_counts() {
  local db_path="${1:?tasks database path required}"
  local story_slug="${2:?story slug required}"
  python3 - <<'PY' "$db_path" "$story_slug"
import sqlite3
import sys

db_path, story_slug = sys.argv[1:3]
slug = (story_slug or "").strip()
if not slug:
    print("0\t0")
    sys.exit(0)

conn = sqlite3.connect(db_path)
cur = conn.cursor()

slug_lower = slug.lower()
row = cur.execute(
    """
    SELECT
        COUNT(*) AS total_count,
        SUM(CASE WHEN LOWER(COALESCE(status, '')) = 'complete' THEN 1 ELSE 0 END) AS complete_count
      FROM tasks
     WHERE LOWER(COALESCE(story_slug, '')) = ?
    """,
    (slug_lower,),
).fetchone()
conn.close()

total = int(row[0] or 0)
completed = int(row[1] or 0)
print(f"{total}\t{completed}")
PY
}

gc_trim_memory() {
  local phase="${1:-memory-cycle}"
  info "[memory] Reclaiming resources (${phase})"

  if command -v python3 >/dev/null 2>&1; then
    python3 - <<'PY' >/dev/null 2>&1 || true
import gc
gc.collect()
PY
  fi

  if command -v purge >/dev/null 2>&1; then
    purge >/dev/null 2>&1 || true
  elif command -v sync >/dev/null 2>&1; then
    sync || true
  fi

  if command -v docker >/dev/null 2>&1; then
    docker container prune -f >/dev/null 2>&1 || true
    docker image prune -f >/dev/null 2>&1 || true
  fi
}

gc_count_pending_tasks() {
  local db_path="${1:?tasks database path required}"
  python3 - <<'PY' "$db_path"
import sqlite3
import sys

db_path = sys.argv[1]
conn = sqlite3.connect(db_path)
cur = conn.cursor()
row = cur.execute("SELECT COUNT(*) FROM tasks WHERE LOWER(COALESCE(status, 'pending')) != 'complete'").fetchone()
pending = row[0] if row else 0
conn.close()
print(pending)
PY
}

gc_tasks_db_has_rows() {
  local db_path="${1:?tasks database path required}"
  [[ -f "$db_path" ]] || return 1
  python3 - <<'PY' "$db_path"
import sqlite3
import sys

db_path = sys.argv[1]
try:
    conn = sqlite3.connect(db_path)
except sqlite3.DatabaseError:
    sys.exit(1)

stories = 0
tasks = 0
try:
    cur = conn.cursor()
    cur.execute("SELECT COUNT(*) FROM stories")
    row = cur.fetchone()
    stories = int(row[0]) if row and row[0] is not None else 0
    cur.execute("SELECT COUNT(*) FROM tasks")
    row = cur.fetchone()
    tasks = int(row[0]) if row and row[0] is not None else 0
except sqlite3.DatabaseError:
    conn.close()
    sys.exit(1)
finally:
    try:
        conn.close()
    except Exception:
        pass

sys.exit(0 if stories > 0 and tasks > 0 else 1)
PY
}

gc_has_legacy_tasks_json() {
  local json_dir="${PLAN_DIR}/create-jira-tasks/json"
  [[ -f "${json_dir}/epics.json" ]] || return 1
  [[ -d "${json_dir}/stories" ]] || return 1
  [[ -d "${json_dir}/tasks" ]] || return 1
  return 0
}

gc_rebuild_tasks_db_from_json() {
  local force_flag="${1:-0}"
  local json_dir="${PLAN_DIR}/create-jira-tasks/json"
  local epics_json="${json_dir}/epics.json"
  local stories_dir="${json_dir}/stories"
  local tasks_dir="${json_dir}/tasks"
  local refined_dir="${json_dir}/refined"
  [[ -f "$epics_json" ]] || return 1
  [[ -d "$stories_dir" ]] || return 1
  [[ -d "$tasks_dir" ]] || return 1

  local payload="${json_dir}/tasks_payload.json"
  python3 "${CLI_ROOT}/src/lib/create-jira-tasks/to_payload.py" \
    "$epics_json" "$stories_dir" "$tasks_dir" "$refined_dir" "$payload" || return 1

  local tasks_workspace="${PLAN_DIR}/tasks"
  mkdir -p "$tasks_workspace"
  local db_path="${tasks_workspace}/tasks.db"
  python3 "${CLI_ROOT}/src/lib/create-jira-tasks/to_sqlite.py" \
    "$payload" "$db_path" "$force_flag" || return 1

  return 0
}

# docker compose helper (prefers "docker compose" then docker-compose)
docker_compose() {
  local project_name="${GC_DOCKER_PROJECT_NAME:-${COMPOSE_PROJECT_NAME:-}}"
  if [[ -z "$project_name" ]]; then
    local base="$(basename "${PROJECT_ROOT:-$PWD}")"
    project_name="$(slugify_name "$base")"
  fi
  local compose_verbose_flag=""
  if [[ -n "${GC_DOCKER_VERBOSE:-}" ]]; then
    compose_verbose_flag="--verbose"
  fi
  if command -v docker >/dev/null 2>&1 && docker compose version >/dev/null 2>&1; then
    if [[ -n "$compose_verbose_flag" ]]; then
      COMPOSE_PROJECT_NAME="$project_name" docker compose "$compose_verbose_flag" "$@"
    else
      COMPOSE_PROJECT_NAME="$project_name" docker compose "$@"
    fi
  elif command -v docker-compose >/dev/null 2>&1; then
    if [[ -n "$compose_verbose_flag" ]]; then
      docker-compose -p "$project_name" "$compose_verbose_flag" "$@"
    else
      docker-compose -p "$project_name" "$@"
    fi
  else
    die "docker compose is not available. Install Docker Desktop or docker-compose."
  fi
}

gc_compose_port() {
  local compose_file="$1" service="$2" container_port="${3:-}"
  [[ -f "$compose_file" ]] || return 1
  local output=""
  if command -v docker >/dev/null 2>&1 && docker compose version >/dev/null 2>&1; then
    output="$(COMPOSE_PROJECT_NAME="$GC_DOCKER_PROJECT_NAME" docker compose -f "$compose_file" port "$service" "$container_port" 2>/dev/null | head -n1)"
  fi
  if [[ -z "$output" ]] && command -v docker-compose >/dev/null 2>&1; then
    output="$(docker-compose -p "$GC_DOCKER_PROJECT_NAME" -f "$compose_file" port "$service" "$container_port" 2>/dev/null | head -n1)"
  fi
  [[ -n "$output" ]] || return 1
  output="${output##*:}"
  [[ "$output" =~ ^[0-9]+$ ]] || return 1
  printf '%s\n' "$output"
}

gc_container_name() {
  local service="$1"
  printf '%s-%s\n' "${GC_DOCKER_PROJECT_NAME}" "$service"
}

container_exists() {
  local name="$1"
  docker ps -a --format '{{.Names}}' | grep -Fxq "$name"
}

container_state() {
  local name="$1"
  docker inspect -f '{{.State.Status}}' "$name" 2>/dev/null || echo "absent"
}

gc_start_created_containers() {
  local compose_file="$1"
  shift || true
  local -a services=("$@")
  local service container state started any_started=0

  for service in "${services[@]}"; do
    container="$(gc_container_name "$service")"
    if ! container_exists "$container"; then
      continue
    fi
    state="$(container_state "$container")"
    if [[ "$state" == "created" ]]; then
      info "Container ${container} stuck in 'created'; attempting manual start"
      if docker start "$container" >/dev/null 2>&1; then
        started=1
        any_started=1
      else
        warn "Failed to start ${container}; attempting compose start fallback"
        docker_compose -f "$compose_file" start "$service" >/dev/null 2>&1 || true
      fi
    fi
  done

  if (( any_started == 1 )); then
    local wait_seconds=0
    local timeout="${GC_DOCKER_HEALTH_TIMEOUT:-10}"
    local poll_interval="${GC_DOCKER_HEALTH_INTERVAL:-1}"
    (( poll_interval <= 0 )) && poll_interval=1
    while (( wait_seconds < timeout )); do
      local all_ready=1
      for service in "${services[@]}"; do
        container="$(gc_container_name "$service")"
        if ! container_exists "$container"; then
          continue
        fi
        state="$(container_state "$container")"
        case "$state" in
          running|healthy) continue ;;
          exited|dead)
            warn "Container ${container} exited unexpectedly (state=${state}). Check logs."
            all_ready=0
            ;;
          *)
            all_ready=0
            ;;
        esac
      done
      if (( all_ready == 1 )); then
        break
      fi
      sleep "$poll_interval"
      (( wait_seconds += poll_interval )) || true
    done
  fi
}

port_in_use() {
  local port="$1"
  if command -v lsof >/dev/null 2>&1; then
    lsof -nP -iTCP:"$port" -sTCP:LISTEN >/dev/null 2>&1 && return 0
  elif command -v netstat >/dev/null 2>&1; then
    netstat -an 2>/dev/null | grep -E "\.${port} .*LISTEN" >/dev/null && return 0
  fi
  return 1
}

find_free_port() {
  local start="${1:-3306}"
  local port="$start"; local limit=$((start+100))
  while (( port <= limit )); do
    if ! port_in_use "$port" && ! gc_port_is_reserved "$port"; then
      echo "$port"
      return 0
    fi
    ((port++)) || true
  done
  echo "$start"  # fallback
}

gc_pick_port() {
  local label="$1"
  local default="$2"
  shift 2
  local service_key="$(slugify_name "$label")"
  [[ -n "$service_key" ]] || service_key="$label"
  local existing_port=""
  existing_port="$(gc_port_for_service "$service_key" 2>/dev/null || true)"
  gc_unreserve_port "$service_key"
  local port=""
  local env_name value
  for env_name in "$@"; do
    value="${!env_name:-}"
    if [[ -n "$value" ]] && [[ "$value" =~ ^[0-9]+$ ]]; then
      port="$value"
      break
    fi
  done
  [[ -n "$port" ]] || port="$default"
  if [[ ! "$port" =~ ^[0-9]+$ ]] || (( port < 1 || port > 65535 )); then
    port="$default"
  fi
  local original="$port"
  local attempts=0
  local limit=200
  while (( attempts < limit )); do
    if (( port < 1 || port > 65535 )); then
      port="$default"
    fi
    if ! port_in_use "$port" && ! gc_port_reserved_by_other "$port" "$service_key"; then
      break
    fi
    ((port++))
    ((attempts++))
  done
  while gc_port_reserved_by_other "$port" "$service_key"; do
    ((port++))
  done
  if (( attempts >= limit )); then
    warn "Unable to find free port for ${label}; using ${port}" >&2
  elif [[ -n "$original" && "$port" != "$original" ]]; then
    info "Port ${original} in use; remapping ${label} to ${port}" >&2
  elif [[ -z "$existing_port" && "$port" != "$default" ]]; then
    info "Port ${default} in use; remapping ${label} to ${port}" >&2
  fi
  gc_reserve_port "$service_key" "$port"
  echo "$port"
}

wait_for_endpoint() {
  local url="$1" label="$2"
  local max_time="${3:-${GC_DOCKER_HEALTH_TIMEOUT:-10}}"
  local delay="${4:-${GC_DOCKER_HEALTH_INTERVAL:-1}}"
  (( delay <= 0 )) && delay=1
  (( max_time <= 0 )) && max_time=1
  local attempts=$(( (max_time + delay - 1) / delay ))
  (( attempts < 1 )) && attempts=1
  local i=1
  while (( i <= attempts )); do
    if curl -fsS --max-time 2 "$url" >/dev/null 2>&1; then
      ok "${label} ready → ${url}"
      return 0
    fi
    sleep "$delay"
    ((i++)) || true
  done
  warn "${label} not ready after ${max_time}s → ${url}"
  return 1
}

gc_sha256_file() {
  local path="$1"
  [[ -f "$path" ]] || return 1
  python3 - <<'PY' "$path"
import hashlib, pathlib, sys
path = pathlib.Path(sys.argv[1])
data = path.read_bytes()
print(hashlib.sha256(data).hexdigest())
PY
}

gc_host_prepare_pnpm() {
  if command -v pnpm >/dev/null 2>&1; then
    return 0
  fi
  if command -v corepack >/dev/null 2>&1; then
    corepack enable pnpm >/dev/null 2>&1 || true
    local version="${GC_PNPM_VERSION:-10.17.1}"
    corepack prepare "pnpm@${version}" --activate >/dev/null 2>&1 || \
      corepack use "pnpm@${version}" >/dev/null 2>&1 || true
  fi
  command -v pnpm >/dev/null 2>&1
}

gc_refresh_stack_prepare_node_modules() {
  [[ "${GC_SKIP_HOST_PNPM_INSTALL:-0}" == "1" ]] && return 0
  local root="${PROJECT_ROOT:-$PWD}"
  local lock_file="${root}/pnpm-lock.yaml"
  local has_manifest=0
  if [[ -f "${root}/pnpm-workspace.yaml" || -f "${root}/package.json" ]]; then
    has_manifest=1
  fi
  (( has_manifest )) || return 0

  local modules_dir="${root}/node_modules/.pnpm"
  local stamp_file="${root}/node_modules/.pnpm-lock.hash"
  local need_install=0
  local lock_hash=""

  if [[ -f "$lock_file" ]]; then
    lock_hash="$(gc_sha256_file "$lock_file" 2>/dev/null || true)"
  fi

  if [[ ! -d "$modules_dir" ]]; then
    need_install=1
  else
    if [[ -z "$lock_hash" ]]; then
      need_install=1
    else
      local stamp_hash=""
      [[ -f "$stamp_file" ]] && stamp_hash="$(cat "$stamp_file" 2>/dev/null || true)"
      if [[ "$lock_hash" != "$stamp_hash" ]]; then
        need_install=1
      fi
    fi
  fi

  if (( need_install )); then
    if [[ ! -f "$lock_file" ]]; then
      info "pnpm lockfile missing; generating via install"
    fi
    info "Installing workspace dependencies via pnpm (host)"
    if ! gc_host_prepare_pnpm; then
      warn "pnpm is not available on the host; skipping host install (containers may retry)."
      return 0
    fi
    local install_rc=0
    if (cd "$root" && CI=1 PNPM_IGNORE_NODE_VERSION=1 pnpm install --frozen-lockfile --unsafe-perm --prefer-offline --engine-strict=false --reporter=append-only); then
      install_rc=0
    else
      warn "pnpm install --frozen-lockfile failed; retrying without frozen lockfile"
      if (cd "$root" && CI=1 PNPM_IGNORE_NODE_VERSION=1 pnpm install --unsafe-perm --prefer-offline --engine-strict=false --no-frozen-lockfile --reporter=append-only); then
        install_rc=0
      else
        install_rc=1
      fi
    fi
    if (( install_rc == 0 )); then
      lock_hash="$(gc_sha256_file "$lock_file" 2>/dev/null || true)"
      if [[ -n "$lock_hash" ]]; then
        mkdir -p "${root}/node_modules"
        printf '%s' "$lock_hash" > "$stamp_file"
      else
        rm -f "$stamp_file"
      fi
      ok "Host dependencies installed"
    else
      warn "Host pnpm install failed; containers will attempt dependency installation."
    fi
  fi
}

render_template_file() {
  local src="$1" dest="$2"
  local db_name="${GC_DB_NAME:-${DB_NAME:-app}}"
  local db_user="${GC_DB_USER:-${DB_USER:-app}}"
  local db_pass="${GC_DB_PASSWORD:-${DB_PASSWORD:-app_pass}}"
  local db_host_port="${GC_DB_HOST_PORT:-${DB_HOST_PORT:-3306}}"
  local db_root_pass="${GC_DB_ROOT_PASSWORD:-${DB_ROOT_PASSWORD:-root}}"
  local project_slug="${GC_DOCKER_PROJECT_NAME:-${PROJECT_SLUG:-$(slugify_name "$(basename "${PROJECT_ROOT:-$PWD}")")}}"
  local api_host_port="${GC_API_HOST_PORT:-${API_HOST_PORT:-3000}}"
  local web_host_port="${GC_WEB_HOST_PORT:-${WEB_HOST_PORT:-5173}}"
  local admin_host_port="${GC_ADMIN_HOST_PORT:-${ADMIN_HOST_PORT:-5174}}"
  local proxy_host_port="${GC_PROXY_HOST_PORT:-${PROXY_HOST_PORT:-8080}}"
  python3 - <<'PY' "$src" "$dest" "$db_name" "$db_user" "$db_pass" "$db_host_port" "$db_root_pass" "$project_slug" "$api_host_port" "$web_host_port" "$admin_host_port" "$proxy_host_port"
import pathlib, sys
args = sys.argv[1:]
src, dest, db_name, db_user, db_pass, db_host_port = args[:6]
db_root_pass = args[6] if len(args) > 6 else ''
project_slug = args[7] if len(args) > 7 else 'gptcreator'
api_host_port = args[8] if len(args) > 8 else '3000'
web_host_port = args[9] if len(args) > 9 else '5173'
admin_host_port = args[10] if len(args) > 10 else '5174'
proxy_host_port = args[11] if len(args) > 11 else '8080'
text = pathlib.Path(src).read_text()
text = text.replace('{{DB_NAME}}', db_name)
text = text.replace('{{DB_USER}}', db_user)
text = text.replace('{{DB_PASSWORD}}', db_pass)
text = text.replace('{{DB_HOST_PORT}}', db_host_port)
text = text.replace('{{DB_ROOT_PASSWORD}}', db_root_pass)
text = text.replace('{{PROJECT_SLUG}}', project_slug)
text = text.replace('{{API_HOST_PORT}}', api_host_port)
text = text.replace('{{WEB_HOST_PORT}}', web_host_port)
text = text.replace('{{ADMIN_HOST_PORT}}', admin_host_port)
text = text.replace('{{PROXY_HOST_PORT}}', proxy_host_port)
pathlib.Path(dest).write_text(text)
PY
}

gc_render_sql() {
  local src="$1" dest="$2" database="$3" app_user="$4" app_pass="$5"
  python3 - <<'PY' "$src" "$dest" "$database" "$app_user" "$app_pass"
import pathlib, re, sys
src, dest, db_name, app_user, app_pass = sys.argv[1:6]
text = pathlib.Path(src).read_text()

text = text.replace('{{DB_NAME}}', db_name)
text = text.replace('{{DB_USER}}', app_user)
text = text.replace('{{DB_PASSWORD}}', app_pass)

def rewrite_add_column_if_not_exists(sql):
    alter_pattern = re.compile(r'ALTER\s+TABLE\s+(`?)([A-Za-z_][A-Za-z0-9_]*)\1\s+(.*?);', re.IGNORECASE | re.DOTALL)
    parts = []
    last_idx = 0

    def find_clause_end(body, start_idx):
        depth = 0
        i = start_idx
        while i < len(body):
            ch = body[i]
            if ch == '(':
                depth += 1
            elif ch == ')':
                if depth > 0:
                    depth -= 1
            elif ch == ',' and depth == 0:
                return i
            elif ch == ';' and depth == 0:
                return i
            i += 1
        return len(body)

    def split_clauses(body):
        clauses = []
        current = []
        depth = 0
        for ch in body:
            if ch == '(':
                depth += 1
            elif ch == ')':
                if depth > 0:
                    depth -= 1
            if ch == ',' and depth == 0:
                clauses.append(''.join(current))
                current = []
                continue
            current.append(ch)
        tail = ''.join(current)
        if tail.strip():
            clauses.append(tail)
        return clauses

    add_col_pattern = re.compile(r'ADD\s+COLUMN\s+IF\s+NOT\s+EXISTS\s+(`?)([A-Za-z_][A-Za-z0-9_]*)\1\s+', re.IGNORECASE)
    add_key_pattern = re.compile(r'ADD\s+(UNIQUE\s+)?KEY\s+(`?)([A-Za-z_][A-Za-z0-9_]*)\2', re.IGNORECASE)
    add_constraint_pattern = re.compile(r'ADD\s+CONSTRAINT\s+(`?)([A-Za-z_][A-Za-z0-9_]*)\1\s+FOREIGN\s+KEY', re.IGNORECASE)

    for match in alter_pattern.finditer(sql):
        table_name = match.group(2)
        body = match.group(3)
        clauses = split_clauses(body)

        column_additions = []
        index_additions = []
        constraint_additions = []
        leftover_clauses = []

        for clause in clauses:
            clause_stripped = clause.strip()
            if not clause_stripped:
                continue
            col_match = add_col_pattern.match(clause_stripped)
            if col_match:
                definition = clause_stripped[col_match.end():].strip().rstrip(',')
                col_name = col_match.group(2)
                quote = col_match.group(1) or ''
                column_additions.append((col_name, quote, definition))
                continue
            key_match = add_key_pattern.match(clause_stripped)
            if key_match:
                index_name = key_match.group(3)
                index_additions.append((clause_stripped, index_name))
                continue
            constraint_match = add_constraint_pattern.match(clause_stripped)
            if constraint_match:
                constraint_name = constraint_match.group(2)
                constraint_additions.append((clause_stripped, constraint_name))
                continue
            leftover_clauses.append(clause.rstrip())

        if not (column_additions or index_additions or constraint_additions):
            continue

        parts.append(sql[last_idx:match.start()])

        dynamic_sql = []
        for col_name, quote, definition in column_additions:
            column_token = f"{quote}{col_name}{quote}"
            ddl = f"ALTER TABLE `{table_name}` ADD COLUMN {column_token} {definition}".strip()
            ddl_escaped = ddl.replace("'", "''")
            dynamic_sql.append(
                "SET @ddl := (\n"
                "  SELECT IF(\n"
                "    EXISTS(SELECT 1 FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = DATABASE() AND TABLE_NAME = '"
                + table_name + "' AND COLUMN_NAME = '" + col_name + "'),\n"
                "    'DO 0',\n"
                "    '" + ddl_escaped + "'\n"
                "  )\n"
                ");\nPREPARE stmt FROM @ddl;\nEXECUTE stmt;\nDEALLOCATE PREPARE stmt;\n"
            )

        for clause_text, index_name in index_additions:
            ddl = f"ALTER TABLE `{table_name}` {clause_text}".strip()
            ddl_escaped = ddl.replace("'", "''")
            dynamic_sql.append(
                "SET @ddl := (\n"
                "  SELECT IF(\n"
                "    EXISTS(SELECT 1 FROM INFORMATION_SCHEMA.STATISTICS WHERE TABLE_SCHEMA = DATABASE() AND TABLE_NAME = '"
                + table_name + "' AND INDEX_NAME = '" + index_name + "'),\n"
                "    'DO 0',\n"
                "    '" + ddl_escaped + "'\n"
                "  )\n"
                ");\nPREPARE stmt FROM @ddl;\nEXECUTE stmt;\nDEALLOCATE PREPARE stmt;\n"
            )

        for clause_text, constraint_name in constraint_additions:
            ddl = f"ALTER TABLE `{table_name}` {clause_text}".strip()
            ddl_escaped = ddl.replace("'", "''")
            dynamic_sql.append(
                "SET @ddl := (\n"
                "  SELECT IF(\n"
                "    EXISTS(SELECT 1 FROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS WHERE TABLE_SCHEMA = DATABASE() AND TABLE_NAME = '"
                + table_name + "' AND CONSTRAINT_NAME = '" + constraint_name + "'),\n"
                "    'DO 0',\n"
                "    '" + ddl_escaped + "'\n"
                "  )\n"
                ");\nPREPARE stmt FROM @ddl;\nEXECUTE stmt;\nDEALLOCATE PREPARE stmt;\n"
            )

        if leftover_clauses:
            remaining_body = ',\n'.join(leftover_clauses)
            dynamic_sql.append(f"ALTER TABLE `{table_name}`\n{remaining_body}\n;")

        parts.append('\n'.join(dynamic_sql))
        last_idx = match.end()

    parts.append(sql[last_idx:])
    return ''.join(parts)

text = rewrite_add_column_if_not_exists(text)

old_slug_update = """UPDATE instructors
SET slug = LOWER(
  REPLACE(
    REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(
      TRIM(COALESCE(NULLIF(display_name,''), CONCAT(TRIM(first_name),' ',TRIM(last_name)))),
      'ç','c'),'ğ','g'),'ı','i'),'ö','o'),'ş','s'),'ü','u'
    )
  , ' ', '-')
)
WHERE slug IS NULL;"""

new_slug_update = """UPDATE instructors
SET slug = LOWER(
  REPLACE(
    REPLACE(
      REPLACE(
        REPLACE(
          REPLACE(
            REPLACE(
              REPLACE(
                TRIM(COALESCE(NULLIF(display_name,''), CONCAT(TRIM(first_name),' ',TRIM(last_name)))),
                'ç','c'),
              'ğ','g'),
            'ı','i'),
          'ö','o'),
        'ş','s'),
      'ü','u'),
    ' ', '-')
)
WHERE slug IS NULL;"""

text = text.replace(old_slug_update, new_slug_update)

old_unique_block = """ALTER TABLE instructors
  MODIFY slug VARCHAR(120) NOT NULL,
  ADD UNIQUE KEY uq_instructors_slug (slug);"""

new_unique_block = """ALTER TABLE instructors
  MODIFY slug VARCHAR(120) NOT NULL;

SET @ddl := (
  SELECT IF(
    EXISTS(SELECT 1 FROM INFORMATION_SCHEMA.STATISTICS WHERE TABLE_SCHEMA = DATABASE() AND TABLE_NAME = 'instructors' AND INDEX_NAME = 'uq_instructors_slug'),
    'DO 0',
    'ALTER TABLE `instructors` ADD UNIQUE KEY `uq_instructors_slug` (`slug`)' 
  )
);
PREPARE stmt FROM @ddl;
EXECUTE stmt;
DEALLOCATE PREPARE stmt;"""

text = text.replace(old_unique_block, new_unique_block)

text = re.sub(r'^\s*USE\s+`?[^`]+`?;', lambda _m: f"USE `{db_name}`;", text, flags=re.IGNORECASE | re.MULTILINE)
text = re.sub(r'CREATE\s+DATABASE\s+IF\s+NOT\s+EXISTS\s+`[^`]+`', lambda _m: f"CREATE DATABASE IF NOT EXISTS `{db_name}`", text, flags=re.IGNORECASE)
text = re.sub(r'ON\s+`[^`]+`\.\*\s+TO', lambda _m: f"ON `{db_name}`.* TO", text, flags=re.IGNORECASE)
text = re.sub(r"(CREATE\s+USER[^']*')([^']+)(')", lambda m: f"{m.group(1)}{app_user}{m.group(3)}", text, flags=re.IGNORECASE)
text = re.sub(r"(IDENTIFIED\s+BY\s+')([^']+)(')", lambda m: f"{m.group(1)}{app_pass}{m.group(3)}", text, flags=re.IGNORECASE)
text = re.sub(r"(TO\s+')([^']+)('@)", lambda m: f"{m.group(1)}{app_user}{m.group(3)}", text, flags=re.IGNORECASE)

def wrap_add_column(match):
    prefix, name = match.group(1), match.group(2)
    if name.startswith('`') and name.endswith('`'):
        return match.group(0)
    return f"{prefix}`{name}`"

text = re.sub(r"(?i)(ADD\s+COLUMN\s+)([A-Za-z_][A-Za-z0-9_]*)", wrap_add_column, text)

for ident in ("row_number", "field_name", "error_code"):
    pattern = re.compile(rf"(?<![`'])\b{re.escape(ident)}\b(?![`'])", re.IGNORECASE)
    text = pattern.sub(lambda m: f"`{m.group(0)}`", text)

def drop_check_constraint(src_text, name):
    pattern = re.compile(rf"CONSTRAINT\s+{re.escape(name)}\s+CHECK\s*\(", re.IGNORECASE)
    while True:
        match = pattern.search(src_text)
        if not match:
            return src_text
        start = match.start()
        comma_idx = src_text.rfind(',', 0, start)
        if comma_idx == -1:
            comma_idx = start
        depth = 1
        i = match.end()
        while i < len(src_text):
            ch = src_text[i]
            if ch == '(':
                depth += 1
            elif ch == ')':
                depth -= 1
                if depth == 0:
                    i += 1
                    break
            i += 1
        else:
            return src_text
        src_text = src_text[:comma_idx] + src_text[i:]

for constraint in ("ck_seo_target", "ck_legal_rev_publish"):
    text = drop_check_constraint(text, constraint)

pathlib.Path(dest).write_text(text)
PY
}

gc_temp_file() {
  local dir="$1" prefix="$2" suffix="$3"
  python3 - <<'PY' "$dir" "$prefix" "$suffix"
import os, sys, tempfile, pathlib
dir_path, prefix, suffix = sys.argv[1:4]
path = pathlib.Path(dir_path)
path.mkdir(parents=True, exist_ok=True)
fd, temp_path = tempfile.mkstemp(prefix=prefix, suffix=suffix, dir=str(path))
os.close(fd)
print(temp_path)
PY
}

gc_execute_sql() {
  local compose_file="$1" sql_file="$2" database="$3"
  local root_user="$4" root_pass="$5" app_user="$6" app_pass="$7" fallback_init="$8" label="$9"
  local import_ok=0
  [[ -n "$label" ]] || label="operation"
  local container_host="127.0.0.1"

  if [[ -f "$compose_file" ]]; then
    if docker_compose -f "$compose_file" ps >/dev/null 2>&1; then
      info "Using docker-compose db service for ${label}"
      docker_compose -f "$compose_file" up -d db >/dev/null 2>&1 || true
      if [[ -n "$root_pass" ]]; then
        if docker_compose -f "$compose_file" exec -T db mysql -h"${container_host}" -u"${root_user}" -p"${root_pass}" "${database}" < "${sql_file}"; then
          import_ok=1
        else
          warn "Root ${label} failed; retrying as ${app_user}"
        fi
      else
        if docker_compose -f "$compose_file" exec -T db mysql -h"${container_host}" -u"${root_user}" "${database}" < "${sql_file}"; then
          import_ok=1
        fi
      fi
      if [[ "$import_ok" -ne 1 ]]; then
        if docker_compose -f "$compose_file" exec -T db mysql -h"${container_host}" -u"${app_user}" ${app_pass:+-p"${app_pass}"} "${database}" < "${sql_file}"; then
          import_ok=1
        fi
      fi
      if [[ "$import_ok" -ne 1 && -n "$fallback_init" && -f "$fallback_init" ]]; then
        local fallback_output fallback_user fallback_pass
        fallback_output="$(python3 - "$fallback_init" <<'PY'
import re, sys
text = open(sys.argv[1]).read()
user = re.search(r"CREATE USER IF NOT EXISTS '([^']+)'", text)
password = re.search(r"IDENTIFIED BY '([^']+)'", text)
if user and password:
    print(user.group(1))
    print(password.group(1))
PY
)"
        if [[ -n "$fallback_output" ]]; then
          IFS=$'\n' read -r fallback_user fallback_pass _ <<<"$fallback_output"
          unset IFS
          if [[ -n "$fallback_user" && -n "$fallback_pass" ]]; then
            if docker_compose -f "$compose_file" exec -T db mysql -h"${container_host}" -u"${fallback_user}" ${fallback_pass:+-p"${fallback_pass}"} "${database}" < "${sql_file}"; then
              import_ok=1
            fi
          fi
        fi
      fi
      if [[ "$import_ok" -eq 1 ]]; then
        return 0
      fi
    fi
  fi

  local host="${DB_HOST:-127.0.0.1}"
  local port="${DB_HOST_PORT:-${GC_DB_HOST_PORT:-${DB_PORT:-3306}}}"
  if ${MYSQL_BIN} -h "$host" -P "$port" -u "$root_user" ${root_pass:+-p"${root_pass}"} "$database" < "$sql_file"; then
    return 0
  fi

  if ${MYSQL_BIN} -h "$host" -P "$port" -u "$app_user" ${app_pass:+-p"${app_pass}"} "$database" < "$sql_file"; then
    return 0
  fi

  return 1
}

gc_refresh_stack_collect_sql() {
  local root="$1"
  python3 - <<'PY' "$root"
import os
import re
import shlex
import sys
from pathlib import Path

root = os.path.abspath(sys.argv[1])

candidate_dirs = []
seen_dirs = set()
for rel in [
    os.path.join('.gpt-creator', 'staging', 'sql'),
    os.path.join('.gpt-creator', 'staging'),
    os.path.join('staging', 'sql'),
    os.path.join('staging'),
    os.path.join('db'),
    os.path.join('database'),
    os.path.join('sql'),
    os.path.join('data', 'sql'),
    os.path.join('data'),
    '.',
]:
    path = os.path.abspath(os.path.join(root, rel))
    if os.path.isdir(path) and path not in seen_dirs:
        candidate_dirs.append(path)
        seen_dirs.add(path)

ignore_dirs = {
    '.git', '.hg', '.svn', '.tox', '.pytest_cache', '.idea', '.vscode',
    '__pycache__', 'node_modules', 'vendor', 'dist', 'build', 'tmp', 'temp'
}

entries = []
seen_files = set()

for base in candidate_dirs:
    for dirpath, dirnames, filenames in os.walk(base):
        dirnames[:] = [d for d in dirnames if d not in ignore_dirs]
        for fname in filenames:
            if not fname.lower().endswith('.sql'):
                continue
            full = os.path.abspath(os.path.join(dirpath, fname))
            if full in seen_files:
                continue
            seen_files.add(full)
            rel_path = os.path.relpath(full, root)
            base_name = os.path.basename(full)
            rel_norm = rel_path.replace('\\', '/')
            if rel_norm.startswith('.gpt-creator/staging/') and (base_name.startswith('import-') or base_name.startswith('seed-')):
                continue
            lower = fname.lower()
            dir_lower = dirpath.lower()
            label = 'schema'
            if 'init' in lower or 'init' in dir_lower:
                label = 'init'
            elif any(token in lower or token in dir_lower for token in ('seed', 'fixture', 'sample', 'data-seed', 'seed-data')):
                label = 'seed'
            elif any(token in lower for token in ('dump', 'schema', 'structure', 'backup', 'snapshot')):
                label = 'schema'
            try:
                mtime = os.path.getmtime(full)
            except OSError:
                mtime = 0
            entries.append((label, mtime, full))

order_map = {'init': 0, 'schema': 1, 'seed': 2}
entries.sort(key=lambda item: (order_map.get(item[0], 3), item[1], item[2]))

init_list = [path for label, _, path in entries if label == 'init']
schema_list = [path for label, _, path in entries if label == 'schema']
seed_list = [path for label, _, path in entries if label == 'seed']
all_list = [path for _, _, path in entries]

db_create_re = re.compile(r"CREATE\s+DATABASE\s+(?:IF\s+NOT\s+EXISTS\s+)?(?P<name>`[^`]+`|\"[^\"]+\"|'[^']+'|[A-Za-z0-9_]+)", re.IGNORECASE)
use_re = re.compile(r"\bUSE\s+(?P<name>`[^`]+`|\"[^\"]+\"|'[^']+'|[A-Za-z0-9_]+)", re.IGNORECASE)
create_user_re = re.compile(r"CREATE\s+USER\s+(?:IF\s+NOT\s+EXISTS\s+)?'(?P<user>[^']+)'(?:\s*@\s*'(?P<host>[^']*)')?\s+IDENTIFIED(?:\s+WITH\s+[A-Za-z0-9_]+)?\s+BY\s+'(?P<pw>[^']+)'", re.IGNORECASE)
alter_user_re = re.compile(r"ALTER\s+USER\s+'(?P<user>[^']+)'(?:\s*@\s*'(?P<host>[^']*)')?\s+IDENTIFIED(?:\s+WITH\s+[A-Za-z0-9_]+)?\s+BY\s+'(?P<pw>[^']+)'", re.IGNORECASE)
grant_re = re.compile(r"GRANT\s+.+?\s+ON\s+(?P<db>`[^`]+`|\"[^\"]+\"|'[^']+'|[A-Za-z0-9_]+(?:\\.[^\s;]+)?)\s+TO\s+'(?P<user>[^']+)'", re.IGNORECASE)

def normalise_identifier(token: str) -> str:
    token = token.strip().rstrip(';').strip()
    if token.endswith('.*'):
        token = token[:-2]
    if '.' in token:
        token = token.split('.', 1)[0]
    if token and token[0] in "`\"'" and token[-1] == token[0]:
        token = token[1:-1]
    return token.strip()

db_name = ''
app_user = ''
app_password = ''
user_host = ''

for label, _, path in entries:
    if db_name and app_user and app_password:
        break
    try:
        text = Path(path).read_text(encoding='utf-8', errors='ignore')
    except Exception:
        continue
    if not db_name:
        match = db_create_re.search(text)
        if match:
            db_name = normalise_identifier(match.group('name'))
    if not db_name:
        match = use_re.search(text)
        if match:
            db_name = normalise_identifier(match.group('name'))
    if not app_user or not app_password:
        match = create_user_re.search(text) or alter_user_re.search(text)
        if match:
            app_user = match.group('user')
            app_password = match.group('pw')
            host = match.group('host') if match.group('host') is not None else ''
            user_host = host or '%'
    if app_user and not db_name:
        for m in grant_re.finditer(text):
            if m.group('user') == app_user:
                candidate = normalise_identifier(m.group('db'))
                if candidate and candidate != '*':
                    db_name = candidate
                    break

def emit_array(name, values):
    if values:
        joined = ' '.join(shlex.quote(v) for v in values)
        print(f"{name}=({joined})")
    else:
        print(f"{name}=()")

def emit_value(name, value):
    quoted = shlex.quote(value) if value else "''"
    print(f"{name}={quoted}")

emit_array('refresh_sql_init_files', init_list)
emit_array('refresh_sql_schema_files', schema_list)
emit_array('refresh_sql_seed_files', seed_list)
emit_array('refresh_sql_all_files', all_list)
emit_value('refresh_sql_default_db_name', db_name)
emit_value('refresh_sql_default_db_user', app_user)
emit_value('refresh_sql_default_db_password', app_password)
emit_value('refresh_sql_default_user_host', user_host or '%')
PY
}

gc_refresh_stack_exec_mysql() {
  local container_id="$1" sql_file="$2" user="$3" password="$4" database="$5"
  local port="${6:-3306}"

  [[ -n "$container_id" ]] || return 1
  [[ -f "$sql_file" ]] || return 1
  local -a cmd=(docker exec -i "$container_id" mysql --protocol=TCP -h 127.0.0.1 -P "$port" "-u${user}")
  if [[ -n "$password" ]]; then
    cmd+=("-p${password}")
  fi
  if [[ -n "$database" ]]; then
    cmd+=("$database")
  fi
  if ! "${cmd[@]}" <"$sql_file"; then
    return 1
  fi
  return 0
}

gc_refresh_stack_exec_inline_sql() {
  local container_id="$1" user="$2" password="$3" database="$4"
  local port="${5:-3306}"
  local sql_content
  sql_content="$(cat)"
  local -a cmd=(docker exec -i "$container_id" mysql --protocol=TCP -h 127.0.0.1 -P "$port" "-u${user}")
  if [[ -n "$password" ]]; then
    cmd+=("-p${password}")
  fi
  if [[ -n "$database" ]]; then
    cmd+=("$database")
  fi
  if ! printf "%s" "$sql_content" | "${cmd[@]}"; then
    return 1
  fi
  return 0
}

gc_refresh_stack_inspect_containers() {
  local compose_file="${1:?compose file required}"
  local -a container_ids=()
  mapfile -t container_ids < <(docker_compose -f "$compose_file" ps --all -q 2>/dev/null | awk 'NF')
  if (( ${#container_ids[@]} == 0 )); then
    mapfile -t container_ids < <(docker_compose -f "$compose_file" ps -q 2>/dev/null | awk 'NF')
  fi
  if (( ${#container_ids[@]} == 0 )); then
    printf '%s\n' "No containers found for project ${GC_DOCKER_PROJECT_NAME}."
    return 1
  fi

  local inspect_json=""
  if ! inspect_json="$(docker inspect "${container_ids[@]}" 2>/dev/null)"; then
    printf '%s\n' "Failed to inspect Docker containers for project ${GC_DOCKER_PROJECT_NAME}."
    return 1
  fi

  python3 - <<'PY' <<<"${inspect_json}"
import json
import sys

try:
    data = json.load(sys.stdin)
except Exception as exc:
    print(f"Unable to parse docker inspect output: {exc}")
    sys.exit(1)

if not isinstance(data, list):
    data = [data]

if not data:
    print("No container state data returned by docker inspect.")
    sys.exit(1)

pending = []
failures = []
healthy = []

for entry in data:
    name = (entry.get("Name") or "").lstrip("/")
    labels = entry.get("Config", {}).get("Labels", {}) or {}
    service = labels.get("com.docker.compose.service") or name
    state = entry.get("State") or {}
    status = (state.get("Status") or "").lower()
    health = (state.get("Health", {}).get("Status") or "").lower()
    exit_code = state.get("ExitCode")

    detail = f"{service}: status={status or 'unknown'}"
    if health:
        detail += f", health={health}"
    if exit_code not in (None, 0):
        detail += f", exit_code={exit_code}"

    if status == "running":
        if health in ("", "healthy"):
            healthy.append(detail)
        elif health == "starting":
            pending.append(detail)
        else:
            failures.append(detail)
    elif status in ("created", "starting"):
        pending.append(detail)
    else:
        failures.append(detail)

if failures:
    print("Container failures detected:")
    for line in failures:
        print(f"  - {line}")
    sys.exit(1)

if pending:
    print("Containers still starting:")
    for line in pending:
        print(f"  - {line}")
    sys.exit(2)

print("All containers running and healthy:")
for line in healthy:
    print(f"  - {line}")
sys.exit(0)
PY
}

gc_refresh_stack_wait_for_containers() {
  local compose_file="${1:?compose file required}"
  local timeout="${2:-${GC_DOCKER_HEALTH_TIMEOUT:-10}}"
  local interval="${3:-${GC_DOCKER_HEALTH_INTERVAL:-1}}"
  (( interval <= 0 )) && interval=1
  local elapsed=0
  local output rc

  while (( elapsed <= timeout )); do
    output="$(gc_refresh_stack_inspect_containers "$compose_file")"
    rc=$?
    if (( rc == 0 )); then
      while IFS= read -r line; do
        [[ -z "$line" ]] && continue
        info "$line"
      done <<<"$output"
      return 0
    elif (( rc == 2 )); then
      while IFS= read -r line; do
        [[ -z "$line" ]] && continue
        info "$line"
      done <<<"$output"
      sleep "$interval"
      (( elapsed += interval ))
      continue
    else
      while IFS= read -r line; do
        [[ -z "$line" ]] && continue
        warn "$line"
      done <<<"$output"
      return 1
    fi
  done

  warn "Timed out after ${timeout}s waiting for containers to report healthy state."
  output="$(gc_refresh_stack_inspect_containers "$compose_file")"
  rc=$?
  local log_fn=warn
  if (( rc == 0 )); then
    log_fn=info
  fi
  while IFS= read -r line; do
    [[ -z "$line" ]] && continue
    "$log_fn" "$line"
  done <<<"$output"
  (( rc == 0 )) || return 1
  return 0
}

# ---------- Scan helpers ----------
has_pattern() { LC_ALL=C grep -E -i -m 1 -q -- "$1" "$2" 2>/dev/null; }
classify_file() {
  local path="$1"
  local name="${path##*/}"
  local lower="$(to_lower "$name")"
  local path_norm="$(to_lower "$path")"
  local ext="${lower##*.}"
  local type="" conf=0

  case "$ext" in
    md)
      if [[ "$lower" == *pdr* ]]; then type="pdr"; conf=0.95
      elif [[ "$lower" == *sds* ]]; then type="sds"; conf=0.92
      elif [[ "$lower" == *rfp* ]]; then type="rfp"; conf=0.9
      elif [[ "$lower" == *jira* ]]; then type="jira"; conf=0.88
      elif [[ "$lower" == *ui*pages* || "$lower" == *website*ui*pages* ]]; then type="ui_pages"; conf=0.85
      elif has_pattern '\bJIRA\b|Issue Key' "$path"; then type="jira"; conf=0.6
      fi
      ;;
    yml|yaml|json)
      if has_pattern '^[[:space:]]*openapi[[:space:]]*:[[:space:]]*3' "$path" || has_pattern '"openapi"[[:space:]]*:' "$path" || has_pattern '"swagger"[[:space:]]*:' "$path"; then
        type="openapi"; conf=0.94
      fi
      ;;
    sql)
      type="sql"; conf=0.65
      if has_pattern 'CREATE[[:space:]]+TABLE' "$path"; then conf=0.8; fi
      ;;
    mmd)
      type="mermaid"; conf=0.7
      ;;
    html)
      local is_html=0
      if [[ "$path_norm" == *"page_samples"* || "$path_norm" == *"page-samples"* ]]; then
        is_html=1
      elif echo "$lower" | grep -Eq '(abo|auth|prg|evt|ctn)[0-9]+\.html'; then
        is_html=1
      fi
      if [[ $is_html -eq 1 ]]; then
        type="page_sample_html"; conf=0.7
      fi
      ;;
    css)
      local is_css=0
      if [[ "$path_norm" == *"page_samples"* || "$path_norm" == *"samples"* ]]; then
        is_css=1
      elif [[ "$lower" == *style.css ]]; then
        is_css=1
      fi
      if [[ $is_css -eq 1 ]]; then
        type="page_sample_css"; conf=0.6
      fi
      ;;
  esac

  if [[ -n "$type" ]]; then
    printf '%s|%.2f\n' "$type" "$conf"
  fi
}

cmd_scan() {
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"

  info "Scanning ${PROJECT_ROOT} for project artifacts…"
  local manifest="${GC_DIR}/scan.tsv"
  local tmp="${manifest}.tmp"
  printf "type\tconfidence\tpath\n" > "$tmp"

  while IFS= read -r -d '' f; do
    local hit
    hit="$(classify_file "$f")" || true
    if [[ -n "$hit" ]]; then
      local type conf
      IFS='|' read -r type conf <<<"$hit"
      printf "%s\t%.2f\t%s\n" "$type" "$conf" "$f" >> "$tmp"
    fi
  done < <(find "$PROJECT_ROOT" \
      \( -name '.git' -o -name 'node_modules' -o -name 'dist' -o -name 'build' -o -name '.venv' -o -name '.gpt-creator' \) -prune -o -type f -print0)

  mv "$tmp" "$manifest"

  local scan_json="${STAGING_DIR}/scan.json"
  python3 - <<'PY' "$manifest" "$PROJECT_ROOT" "$scan_json"
import csv, json, sys, time, pathlib
manifest, root, out = sys.argv[1:4]
rows = []
with open(manifest, newline='') as fh:
    reader = csv.DictReader(fh, delimiter='\t')
    for row in reader:
        if not row['type']:
            continue
        rows.append({
            "type": row['type'],
            "confidence": float(row['confidence'] or 0),
            "path": str(pathlib.Path(row['path']).resolve())
        })
scan = {
    "project_root": str(pathlib.Path(root).resolve()),
    "generated_at": time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
    "artifacts": rows
}
pathlib.Path(out).parent.mkdir(parents=True, exist_ok=True)
with open(out, 'w') as fh:
    json.dump(scan, fh, indent=2)
print(out)
PY
  ok "Scan manifest → ${scan_json}"
}

cmd_normalize() {
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"

  local scan_json="${STAGING_DIR}/scan.json"
  if [[ ! -f "$scan_json" ]]; then
    warn "No scan.json found, running scan first."
    cmd_scan --project "$PROJECT_ROOT"
  fi

  scan_json="${STAGING_DIR}/scan.json"
  python3 - <<'PY' "$scan_json" "$INPUT_DIR" "$PLAN_DIR"
import json, sys, shutil, pathlib, time
scan_path, input_dir, plan_dir = sys.argv[1:4]
input_dir = pathlib.Path(input_dir)
plan_dir = pathlib.Path(plan_dir)
input_dir.mkdir(parents=True, exist_ok=True)
plan_dir.mkdir(parents=True, exist_ok=True)

data = json.load(open(scan_path))
project_root = pathlib.Path(data.get('project_root', '.')).resolve()
artifacts = data.get('artifacts', [])

unique_types = {"pdr", "sds", "rfp", "jira", "ui_pages", "openapi"}
unique = {}
for entry in artifacts:
    t = entry.get('type')
    if t in unique_types:
        if t not in unique or entry['confidence'] > unique[t]['confidence']:
            unique[t] = entry

multi_map = {
    "sql": pathlib.Path('sql'),
    "mermaid": pathlib.Path('mermaid'),
    "page_sample_html": pathlib.Path('page_samples'),
    "page_sample_css": pathlib.Path('page_samples')
}
collected = {key: [] for key in multi_map}
for entry in artifacts:
    t = entry.get('type')
    if t in multi_map:
        collected[t].append(entry)

provenance = []

def copy_file(src_path, rel_dest, entry):
    src = pathlib.Path(src_path)
    dest = input_dir / rel_dest
    dest.parent.mkdir(parents=True, exist_ok=True)
    shutil.copy2(src, dest)
    provenance.append({
        "type": entry.get('type'),
        "source": str(src),
        "destination": str(dest.relative_to(input_dir)),
        "confidence": entry.get('confidence', 0)
    })

name_map = {
    "pdr": pathlib.Path('pdr.md'),
    "sds": pathlib.Path('sds.md'),
    "rfp": pathlib.Path('rfp.md'),
    "jira": pathlib.Path('jira.md'),
    "ui_pages": pathlib.Path('ui-pages.md')
}

for t, entry in unique.items():
    src = entry['path']
    if t == 'openapi':
        suffix = pathlib.Path(src).suffix.lower()
        if suffix in {'.yaml', '.yml'}:
            rel = pathlib.Path('openapi.yaml')
        elif suffix == '.json':
            rel = pathlib.Path('openapi.json')
        else:
            rel = pathlib.Path('openapi.src')
    else:
        rel = name_map[t]
    copy_file(src, rel, entry)

for t, entries in collected.items():
    dest_root = multi_map[t]
    for entry in entries:
        src = pathlib.Path(entry['path'])
        try:
            rel = src.resolve().relative_to(project_root)
        except ValueError:
            rel = pathlib.Path(src.name)
        rel = dest_root / rel
        copy_file(src, rel, entry)

# discovery.yaml (summary)
import io
from textwrap import indent
summary = io.StringIO()
summary.write('generated_at: %s\n' % time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()))
summary.write('project_root: %s\n' % project_root)
summary.write('artifacts:\n')
for entry in artifacts:
    summary.write('  - type: %s\n' % entry.get('type'))
    summary.write('    confidence: %.2f\n' % entry.get('confidence', 0))
    summary.write('    path: %s\n' % entry.get('path'))
(input_dir / '..' / 'discovery.yaml').resolve().write_text(summary.getvalue())

prov = {
    'generated_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
    'entries': provenance
}
(plan_dir / 'provenance.json').write_text(json.dumps(prov, indent=2))
PY

  ok "Normalized inputs → ${INPUT_DIR}"
}

cmd_plan() {
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"

  local openapi=""
  for cand in "$INPUT_DIR/openapi.yaml" "$INPUT_DIR/openapi.yml" "$INPUT_DIR/openapi.json" "$INPUT_DIR/openapi.src"; do
    [[ -f "$cand" ]] && { openapi="$cand"; break; }
  done
  local sql_dir="$INPUT_DIR/sql"

  python3 - <<'PY' "$openapi" "$sql_dir" "$PLAN_DIR"
import json, os, re, sys, time, pathlib
from collections import OrderedDict
openapi_path, sql_dir, plan_dir = sys.argv[1:4]
plan_dir = pathlib.Path(plan_dir)
plan_dir.mkdir(parents=True, exist_ok=True)

routes = []
schemas = []
openapi_loaded = False
if openapi_path:
    try:
        text = pathlib.Path(openapi_path).read_text()
        if openapi_path.endswith('.json'):
            data = json.loads(text)
            openapi_loaded = True
        else:
            try:
                import yaml  # type: ignore
                data = yaml.safe_load(text)  # type: ignore
                openapi_loaded = True
            except Exception:
                data = None
        if openapi_loaded and isinstance(data, dict):
            for path, methods in (data.get('paths') or {}).items():
                if isinstance(methods, dict):
                    for method, body in methods.items():
                        if not isinstance(body, dict):
                            continue
                        routes.append({
                            'method': method.upper(),
                            'path': path,
                            'summary': body.get('summary') or ''
                        })
            schemas = list((data.get('components') or {}).get('schemas') or {})
        else:
            raise ValueError('fallback parser')
    except Exception:
        routes = []
        schemas = []
        text = pathlib.Path(openapi_path).read_text() if openapi_path else ''
        current_path = None
        for line in text.splitlines():
            if re.match(r'^\s*/[^\s]+:\s*$', line):
                current_path = line.strip().rstrip(':')
                continue
            if current_path:
                m = re.match(r'^\s{2,}(get|post|put|patch|delete|options|head):\s*$', line, re.I)
                if m:
                    routes.append({'method': m.group(1).upper(), 'path': current_path, 'summary': ''})
                    continue
                if re.match(r'^\S', line):
                    current_path = None

        in_components = False
        in_schemas = False
        for line in text.splitlines():
            stripped = line.strip()
            if not stripped:
                continue
            if re.match(r'^components:\s*$', stripped):
                in_components = True
                in_schemas = False
                continue
            if in_components and re.match(r'^schemas:\s*$', stripped):
                in_schemas = True
                continue
            indent = len(line) - len(line.lstrip(' '))
            if in_schemas:
                if indent <= 2 and not stripped.startswith('#') and not stripped.startswith('schemas:'):
                    in_schemas = False
                    continue
                if indent == 4 and re.match(r'^[A-Za-z0-9_.-]+:\s*$', stripped):
                    name = stripped.split(':', 1)[0]
                    schemas.append(name)

sql_tables = []
sql_dir_path = pathlib.Path(sql_dir)
if sql_dir and sql_dir_path.is_dir():
    for sql_file in sql_dir_path.rglob('*.sql'):
        try:
            text = sql_file.read_text()
        except Exception:
            continue
        for m in re.finditer(r'CREATE\s+TABLE\s+`?([A-Za-z0-9_]+)`?', text, flags=re.IGNORECASE):
            sql_tables.append(m.group(1))

schema_set = {s.lower() for s in schemas}
table_set = {t.lower() for t in sql_tables}
only_in_openapi = sorted(schema_set - table_set)
only_in_sql = sorted(table_set - schema_set)

routes_path = plan_dir / 'routes.md'
entities_path = plan_dir / 'entities.md'
tasks_path = plan_dir / 'tasks.json'
plan_todo = plan_dir / 'PLAN_TODO.md'

def write_routes():
    lines = ['# Routes', '']
    if routes:
        for item in sorted(routes, key=lambda r: (r['path'], r['method'])):
            summary = f" — {item['summary']}" if item.get('summary') else ''
            lines.append(f"- `{item['method']} {item['path']}`{summary}")
    else:
        lines.append('No routes detected — ensure openapi.yaml is present in staging/inputs.')
    routes_path.write_text('\n'.join(lines) + '\n')


def write_entities():
    lines = ['# Entities', '']
    lines.append('## OpenAPI Schemas')
    if schemas:
        for name in sorted(schemas):
            lines.append(f'- {name}')
    else:
        lines.append('- (none found)')
    lines.append('')
    lines.append('## SQL Tables')
    if sql_tables:
        for name in sorted(sql_tables):
            lines.append(f'- {name}')
    else:
        lines.append('- (none found)')
    lines.append('')
    lines.append('## Detected deltas')
    if only_in_openapi:
        lines.append('- Only in OpenAPI: ' + ', '.join(only_in_openapi))
    if only_in_sql:
        lines.append('- Only in SQL: ' + ', '.join(only_in_sql))
    if not only_in_openapi and not only_in_sql:
        lines.append('- None (schemas and tables aligned on name)')
    entities_path.write_text('\n'.join(lines) + '\n')


def write_tasks():
    tasks = []
    if only_in_openapi:
        tasks.append({
            'id': 'align-openapi-sql',
            'title': 'Align OpenAPI schemas with SQL tables',
            'details': f"Create tables or update schemas for: {', '.join(only_in_openapi)}"
        })
    if only_in_sql:
        tasks.append({
            'id': 'document-sql-gap',
            'title': 'Document SQL tables missing from OpenAPI',
            'details': f"Expose or document SQL tables not covered by API: {', '.join(only_in_sql)}"
        })
    tasks_path.write_text(json.dumps({'generated_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()), 'tasks': tasks}, indent=2))


def write_plan_todo():
    lines = [
        '# Build Plan',
        '',
        '- Validate discovery outputs under `staging/inputs`.',
        '- Review `routes.md` & `entities.md` for coverage and deltas.',
        '- Implement generation steps for API, DB, Web, Admin, Docker.',
        '- Run `gpt-creator generate all --project <path>` if not already executed.',
        '- Bring the stack up with `gpt-creator run up` and smoke test.',
        '- Execute `gpt-creator verify all` to satisfy acceptance & NFR gates.',
        '- Iterate on Jira tasks using `gpt-creator iterate` until checks pass.'
    ]
    plan_todo.write_text('\n'.join(lines) + '\n')

write_routes()
write_entities()
write_tasks()
write_plan_todo()
PY

  ok "Plan artifacts created under ${PLAN_DIR}"
}

copy_template_tree() {
  local src="$1" dest="$2"
  [[ -d "$src" ]] || die "Template directory not found: $src"
  find "$src" -type d ! -name '.DS_Store' | while IFS= read -r dir; do
    local rel="${dir#$src}"
    mkdir -p "$dest/$rel"
  done
  find "$src" -type f | while IFS= read -r file; do
    local base="$(basename "$file")"
    [[ "$base" == '.DS_Store' ]] && continue
    local rel="${file#$src/}"
    local target="$dest/$rel"
    if [[ "$target" == *.tmpl ]]; then
      target="${target%.tmpl}"
      mkdir -p "$(dirname "$target")"
      render_template_file "$file" "$target"
    else
      mkdir -p "$(dirname "$target")"
      cp "$file" "$target"
    fi
  done
}

cmd_generate() {
  local facet="${1:-}"; shift || true
  [[ -n "$facet" ]] || die "generate requires a facet: api|web|admin|db|docker|all"
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"
  local templates="$CLI_ROOT/templates"

  case "$facet" in
    api)
      local out="$PROJECT_ROOT/apps/api"
      mkdir -p "$out"
      copy_template_tree "$templates/api/nestjs" "$out"
      ok "API scaffolded → ${out}"
      ;;
    web)
      local out="$PROJECT_ROOT/apps/web"
      mkdir -p "$out"
      copy_template_tree "$templates/web/vue3" "$out"
      ok "Web scaffolded → ${out}"
      ;;
    admin)
      local out="$PROJECT_ROOT/apps/admin"
      mkdir -p "$out"
      copy_template_tree "$templates/admin/vue3" "$out"
      ok "Admin scaffolded → ${out}"
      ;;
    db)
      local out="$PROJECT_ROOT/db"
      mkdir -p "$out"
      copy_template_tree "$templates/db/mysql" "$out"
      ok "DB artifacts scaffolded → ${out}"
      ;;
    docker)
      local out="$PROJECT_ROOT/docker"
      mkdir -p "$out"
      local preferred="${GC_DB_HOST_PORT:-${DB_HOST_PORT:-${MYSQL_HOST_PORT:-3306}}}"
      gc_unreserve_port db
      if port_in_use "$preferred"; then
        local next; next="$(find_free_port "$preferred")"
        if [[ "$next" != "$preferred" ]]; then
          info "Port $preferred in use; remapping MySQL to $next"
          preferred="$next"
        fi
      fi
      GC_DB_HOST_PORT="$preferred"
      DB_HOST_PORT="$GC_DB_HOST_PORT"
      MYSQL_HOST_PORT="$GC_DB_HOST_PORT"
      gc_reserve_port db "$GC_DB_HOST_PORT"
      gc_set_env_var DB_HOST_PORT "$GC_DB_HOST_PORT"
      gc_set_env_var MYSQL_HOST_PORT "$GC_DB_HOST_PORT"
      gc_set_env_var GC_DB_HOST_PORT "$GC_DB_HOST_PORT"
      local api_host_port="$(gc_pick_port "API" 3000 GC_API_HOST_PORT API_HOST_PORT)"
      GC_API_HOST_PORT="$api_host_port"
      API_HOST_PORT="$GC_API_HOST_PORT"
      gc_set_env_var API_HOST_PORT "$API_HOST_PORT"
      gc_set_env_var GC_API_HOST_PORT "$GC_API_HOST_PORT"
      gc_reserve_port api "$GC_API_HOST_PORT"
      local web_host_port="$(gc_pick_port "Web" 5173 GC_WEB_HOST_PORT WEB_HOST_PORT)"
      GC_WEB_HOST_PORT="$web_host_port"
      WEB_HOST_PORT="$GC_WEB_HOST_PORT"
      gc_set_env_var WEB_HOST_PORT "$WEB_HOST_PORT"
      gc_set_env_var GC_WEB_HOST_PORT "$GC_WEB_HOST_PORT"
      gc_reserve_port web "$GC_WEB_HOST_PORT"
      local admin_host_port="$(gc_pick_port "Admin" 5174 GC_ADMIN_HOST_PORT ADMIN_HOST_PORT)"
      GC_ADMIN_HOST_PORT="$admin_host_port"
      ADMIN_HOST_PORT="$GC_ADMIN_HOST_PORT"
      gc_set_env_var ADMIN_HOST_PORT "$ADMIN_HOST_PORT"
      gc_set_env_var GC_ADMIN_HOST_PORT "$GC_ADMIN_HOST_PORT"
      gc_reserve_port admin "$GC_ADMIN_HOST_PORT"
      local proxy_host_port="$(gc_pick_port "Proxy" 8080 GC_PROXY_HOST_PORT PROXY_HOST_PORT)"
      GC_PROXY_HOST_PORT="$proxy_host_port"
      PROXY_HOST_PORT="$GC_PROXY_HOST_PORT"
      gc_set_env_var PROXY_HOST_PORT "$PROXY_HOST_PORT"
      gc_set_env_var GC_PROXY_HOST_PORT "$GC_PROXY_HOST_PORT"
      gc_reserve_port proxy "$GC_PROXY_HOST_PORT"
      local local_url="mysql://${GC_DB_USER}:${GC_DB_PASSWORD}@127.0.0.1:${GC_DB_HOST_PORT}/${GC_DB_NAME}"
      gc_set_env_var DATABASE_URL "$local_url"
      local api_base_url="http://localhost:${GC_API_HOST_PORT}/api/v1"
      gc_set_env_var GC_API_BASE_URL "$api_base_url"
      gc_set_env_var VITE_API_BASE "$api_base_url"
      local api_health_url="${api_base_url%/}/health"
      gc_set_env_var GC_API_HEALTH_URL "$api_health_url"
      local proxy_base="http://localhost:${GC_PROXY_HOST_PORT}"
      gc_set_env_var GC_WEB_URL "${proxy_base}/"
      gc_set_env_var GC_ADMIN_URL "${proxy_base}/admin/"
      gc_load_env
      copy_template_tree "$templates/docker" "$out"
      if [[ -f "$out/pnpm-entry.sh" ]]; then
        chmod +x "$out/pnpm-entry.sh" || true
      fi
      ok "Docker assets scaffolded → ${out}"
      ;;
    all)
      for f in api db web admin docker; do
        cmd_generate "$f" --project "$PROJECT_ROOT"
      done
      return 0
      ;;
    *) die "Unknown facet: ${facet}";;
  esac
}

cmd_db() {
  local action="${1:-}"; shift || true
  [[ -n "$action" ]] || die "db requires: provision|import|seed"
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"
  local compose_file="$PROJECT_ROOT/docker/docker-compose.yml"

  case "$action" in
    provision)
      [[ -f "$compose_file" ]] || die "Compose file not found at ${compose_file}; run 'gpt-creator generate docker'."
      info "Starting database service via docker compose"
      docker_compose -f "$compose_file" up -d db
      ok "MySQL container provisioned"
      ;;
    import)
      local sql_file
      sql_file="$(find "$INPUT_DIR/sql" -maxdepth 2 -type f -name '*.sql' | head -n1 || true)"
      [[ -n "$sql_file" ]] || die "No staged SQL found under ${INPUT_DIR}/sql"
      info "Importing SQL from ${sql_file}"
      local database="${DB_NAME:-$GC_DB_NAME}"
      local root_user="${DB_ROOT_USER:-root}"
      local root_pass="${DB_ROOT_PASSWORD:-${GC_DB_ROOT_PASSWORD:-}}"
      local app_user="${DB_USER:-$GC_DB_USER}"
      local app_pass="${DB_PASSWORD:-$GC_DB_PASSWORD}"
      local cleanup_files=()
      trap 'for f in "${cleanup_files[@]}"; do [[ -n "$f" && -f "$f" ]] && rm -f "$f"; done; trap - RETURN' RETURN
      local rendered_sql
      rendered_sql="$(gc_temp_file "$STAGING_DIR" "import-" ".sql")"
      cleanup_files+=("$rendered_sql")
      gc_render_sql "$sql_file" "$rendered_sql" "$database" "$app_user" "$app_pass"
      local init_sql="${INPUT_DIR}/sql/db/init.sql"
      if gc_execute_sql "$compose_file" "$rendered_sql" "$database" "$root_user" "$root_pass" "$app_user" "$app_pass" "$init_sql" "import"; then
        ok "Database import finished"
      else
        die "Database import failed"
      fi
      ;;
    seed)
      local seed_file="${PROJECT_ROOT}/db/seed.sql"
      [[ -f "$seed_file" ]] || die "Seed file not found: ${seed_file}"
      info "Seeding database from ${seed_file}"
      local database="${DB_NAME:-$GC_DB_NAME}"
      local root_user="${DB_ROOT_USER:-root}"
      local root_pass="${DB_ROOT_PASSWORD:-${GC_DB_ROOT_PASSWORD:-}}"
      local app_user="${DB_USER:-$GC_DB_USER}"
      local app_pass="${DB_PASSWORD:-$GC_DB_PASSWORD}"
      local cleanup_files=()
      trap 'for f in "${cleanup_files[@]}"; do [[ -n "$f" && -f "$f" ]] && rm -f "$f"; done; trap - RETURN' RETURN
      local rendered_seed
      rendered_seed="$(gc_temp_file "$STAGING_DIR" "seed-" ".sql")"
      cleanup_files+=("$rendered_seed")
      gc_render_sql "$seed_file" "$rendered_seed" "$database" "$app_user" "$app_pass"
      local fallback_init="${PROJECT_ROOT}/db/init.sql"
      if [[ ! -f "$fallback_init" ]]; then
        fallback_init="${INPUT_DIR}/sql/db/init.sql"
      fi
      if gc_execute_sql "$compose_file" "$rendered_seed" "$database" "$root_user" "$root_pass" "$app_user" "$app_pass" "$fallback_init" "seed"; then
        ok "Database seed applied"
      else
        die "Database seed failed"
      fi
      ;;
    *) die "Unknown db action: ${action}";;
  esac
}

cmd_run() {
  local action="${1:-}"; shift || true
  [[ -n "$action" ]] || die "run requires: up|down|logs|open"
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"
  local compose_file="$PROJECT_ROOT/docker/docker-compose.yml"

  case "$action" in
    up)
      [[ -f "$compose_file" ]] || die "Compose file not found at ${compose_file}; generate docker assets first."
      gc_refresh_stack_prepare_node_modules
      docker_compose -f "$compose_file" up -d
      ok "Stack is starting (check docker compose ps)"
      local api_base="${GC_API_BASE_URL:-http://localhost:3000/api/v1}"
      local web_url="${GC_WEB_URL:-http://localhost:8080/}"
      local admin_url="${GC_ADMIN_URL:-http://localhost:8080/admin/}"
      local health_timeout="${GC_DOCKER_HEALTH_TIMEOUT:-10}"
      local health_interval="${GC_DOCKER_HEALTH_INTERVAL:-1}"
      wait_for_endpoint "${api_base%/}/health" "API /health" "$health_timeout" "$health_interval" || true
      local web_ping="${web_url%/}/__vite_ping"
      if ! wait_for_endpoint "$web_ping" "Web (vite ping)" "$health_timeout" "$health_interval"; then
        wait_for_endpoint "${web_url%/}/" "Web" "$health_timeout" "$health_interval" || true
      fi
      local admin_ping="${admin_url%/}/__vite_ping"
      if ! wait_for_endpoint "$admin_ping" "Admin (vite ping)" "$health_timeout" "$health_interval"; then
        wait_for_endpoint "${admin_url%/}/" "Admin" "$health_timeout" "$health_interval" || true
      fi
      ;;
    down)
      [[ -f "$compose_file" ]] || die "Compose file not found at ${compose_file}"
      docker_compose -f "$compose_file" down
      ok "Stack shut down"
      ;;
    logs)
      [[ -f "$compose_file" ]] || die "Compose file not found at ${compose_file}"
      docker_compose -f "$compose_file" logs -f
      ;;
    open)
      if command -v open >/dev/null 2>&1; then
        open "http://localhost:8080" || open "http://localhost:5173" || true
      else
        ${EDITOR_CMD} "$PROJECT_ROOT" || true
      fi
      ;;
    *) die "Unknown run action: ${action}";;
  esac
}

cmd_refresh_stack() {
  local root="" compose_override="" sql_override="" seed_override=""
  local skip_import=0 skip_seed=0
  local only_services="" skip_services=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --compose) compose_override="$(abs_path "$2")"; shift 2;;
      --sql) sql_override="$(abs_path "$2")"; shift 2;;
      --seed) seed_override="$(abs_path "$2")"; shift 2;;
      --no-import) skip_import=1; shift;;
      --no-seed) skip_seed=1; shift;;
      --only-services) only_services="${2:-}"; shift 2;;
      --skip-services) skip_services="${2:-}"; shift 2;;
      -h|--help)
        cat <<'EOHELP'
Usage: gpt-creator refresh-stack [options]

Tear down, rebuild, and prime the local Docker stack (schema + seeds).

Options:
  --project PATH   Project root (defaults to current directory)
  --compose FILE   Override docker-compose file
  --sql FILE       Explicit SQL dump to import (auto-discovered if omitted)
  --seed FILE      Seed SQL file to apply after import
  --only-services LIST  Comma/space separated subset of services to start (e.g. "web,api")
  --skip-services LIST  Comma/space separated services to skip when starting (e.g. "db,admin")
  --no-import      Skip schema import step
  --no-seed        Skip seeding step
  -h, --help       Show this help
EOHELP
        return 0
        ;;
      *) break;;
    esac
  done

  ensure_ctx "$root"

  local -a refresh_sql_init_files=() refresh_sql_schema_files=() refresh_sql_seed_files=() refresh_sql_all_files=()
  local refresh_sql_default_db_name="" refresh_sql_default_db_user="" refresh_sql_default_db_password="" refresh_sql_default_user_host=""
  eval "$(gc_refresh_stack_collect_sql "$PROJECT_ROOT")"

  if [[ -n "$sql_override" ]]; then
    refresh_sql_schema_files=("$sql_override")
  fi
  if [[ -n "$seed_override" ]]; then
    refresh_sql_seed_files=("$seed_override")
  fi

  local env_updated=0
  if [[ -n "$refresh_sql_default_db_name" && "$refresh_sql_default_db_name" != "$GC_DB_NAME" ]]; then
    gc_set_env_var DB_NAME "$refresh_sql_default_db_name"
    gc_set_env_var GC_DB_NAME "$refresh_sql_default_db_name"
    env_updated=1
  fi
  if [[ -n "$refresh_sql_default_db_user" && "$refresh_sql_default_db_user" != "$GC_DB_USER" ]]; then
    gc_set_env_var DB_USER "$refresh_sql_default_db_user"
    gc_set_env_var GC_DB_USER "$refresh_sql_default_db_user"
    env_updated=1
  fi
  if [[ -n "$refresh_sql_default_db_password" && "$refresh_sql_default_db_password" != "$GC_DB_PASSWORD" ]]; then
    gc_set_env_var DB_PASSWORD "$refresh_sql_default_db_password"
    gc_set_env_var GC_DB_PASSWORD "$refresh_sql_default_db_password"
    env_updated=1
  fi
  if (( env_updated )); then
    gc_load_env
    local host_port="${GC_DB_HOST_PORT:-${DB_HOST_PORT:-3306}}"
    local database_url="mysql://${GC_DB_USER}:${GC_DB_PASSWORD}@127.0.0.1:${host_port}/${GC_DB_NAME}"
    gc_set_env_var DATABASE_URL "$database_url"
  fi

  info "Using database '${GC_DB_NAME}' with user '${GC_DB_USER}'"

  local compose_file="$compose_override"
  if [[ -n "$compose_file" ]]; then
    compose_file="$(abs_path "$compose_file")"
  else
    info "Rendering docker assets from templates"
    if ! cmd_generate docker --project "$PROJECT_ROOT"; then
      die "Failed to generate docker assets"
    fi
    if [[ -f "${PROJECT_ROOT}/docker/compose.yaml" ]]; then
      compose_file="${PROJECT_ROOT}/docker/compose.yaml"
    elif [[ -f "${PROJECT_ROOT}/docker/docker-compose.yml" ]]; then
      compose_file="${PROJECT_ROOT}/docker/docker-compose.yml"
    elif [[ -f "${PROJECT_ROOT}/docker-compose.yml" ]]; then
      compose_file="${PROJECT_ROOT}/docker-compose.yml"
    else
      die "Compose file not found after generation. Expected docker/compose.yaml or docker-compose.yml"
    fi
  fi

  info "Refreshing Docker stack for ${GC_DOCKER_PROJECT_NAME}"

  info "Stopping existing containers (removing volumes)"
  docker_compose -f "$compose_file" down -v --remove-orphans || true

  local slug="$GC_DOCKER_PROJECT_NAME"
  local -a stale_containers=(
    "${slug}-db"
    "${slug}-api"
    "${slug}-web"
    "${slug}-admin"
    "${slug}-proxy"
    "${slug}_db"
    "${slug}_api"
    "${slug}_web"
    "${slug}_admin"
    "${slug}_proxy"
  )
  local container
  for container in "${stale_containers[@]}"; do
    if docker ps -a --format '{{.Names}}' | grep -Fxq "$container"; then
      info "Removing leftover container ${container}"
      docker rm -f "$container" >/dev/null 2>&1 || true
    fi
  done

  if (( ${#refresh_sql_all_files[@]} > 0 )); then
    info "Discovered SQL assets:"
    local listed
    for listed in "${refresh_sql_all_files[@]}"; do
      if [[ "$listed" == "$PROJECT_ROOT/"* ]]; then
        info "  - ${listed#$PROJECT_ROOT/}"
      else
        info "  - ${listed}"
      fi
    done
  else
    info "No SQL assets discovered automatically."
  fi

  local -a all_services=(db api web admin proxy)
  local -a services_to_start=()
  if [[ -n "$only_services" ]]; then
    local normalized_only="${only_services//,/ }"
    read -r -a services_to_start <<< "$normalized_only"
  else
    services_to_start=("${all_services[@]}")
  fi
  if [[ -n "$skip_services" ]]; then
    local -a skip_list=()
    local normalized_skip="${skip_services//,/ }"
    read -r -a skip_list <<< "$normalized_skip"
    if (( ${#skip_list[@]} > 0 )); then
      local -a filtered=()
      local svc skip_flag skip_item
      for svc in "${services_to_start[@]}"; do
        skip_flag=0
        for skip_item in "${skip_list[@]}"; do
          [[ -z "$skip_item" ]] && continue
          if [[ "$svc" == "$skip_item" ]]; then
            skip_flag=1
            break
          fi
        done
        if (( skip_flag == 0 )); then
          filtered+=("$svc")
        fi
      done
      services_to_start=("${filtered[@]}")
    fi
  fi
  if (( ${#services_to_start[@]} > 0 )); then
    # Deduplicate and drop empties
    local -a deduped=()
    local svc seen_services=""
    for svc in "${services_to_start[@]}"; do
      [[ -z "$svc" ]] && continue
      case " $seen_services " in
        *" $svc "*) continue ;;
      esac
      deduped+=("$svc")
      seen_services+=" $svc"
    done
    services_to_start=("${deduped[@]}")
  fi

  if (( ${#services_to_start[@]} == 0 )); then
    warn "No services selected to start; skipping docker compose up."
  else
    info "Building and starting containers (${services_to_start[*]})"
    GC_DOCKER_VERBOSE="${GC_DOCKER_VERBOSE:-1}"
    gc_refresh_stack_prepare_node_modules
    docker_compose -f "$compose_file" up -d --build "${services_to_start[@]}"
    gc_start_created_containers "$compose_file" "${services_to_start[@]}"
  fi

  local db_requested=0
  local svc
  for svc in "${services_to_start[@]}"; do
    if [[ "$svc" == "db" ]]; then
      db_requested=1
      break
    fi
  done

  local db_container=""
  if (( db_requested )); then
    db_container="$(docker_compose -f "$compose_file" ps -q db || true)"
    if [[ -n "$db_container" ]]; then
      info "Waiting for MySQL to be ready…"
      local waited=0
      local mysql_timeout="${GC_DOCKER_HEALTH_TIMEOUT:-10}"
      local sleep_interval="${GC_DOCKER_HEALTH_INTERVAL:-1}"
      (( sleep_interval <= 0 )) && sleep_interval=1
      while (( waited < mysql_timeout )); do
        if docker exec -i "$db_container" sh -lc 'mysqladmin ping -h 127.0.0.1 --silent' >/dev/null 2>&1; then
          info "MySQL is ready."
          break
        fi
        sleep "$sleep_interval"
        ((waited += sleep_interval)) || true
      done
      if (( waited >= mysql_timeout )); then
        warn "MySQL readiness timeout after ${mysql_timeout}s (continuing)."
      fi
    else
      warn "Database container did not start; SQL import will be skipped."
    fi
  else
    info "Database service excluded from start; skipping readiness wait."
  fi

  docker_compose -f "$compose_file" ps

  local db_port="3306"
  local root_user="${DB_ROOT_USER:-root}"
  local root_pass="${DB_ROOT_PASSWORD:-${GC_DB_ROOT_PASSWORD:-}}"
  local app_user="${DB_USER:-$GC_DB_USER}"
  local app_pass="${DB_PASSWORD:-$GC_DB_PASSWORD}"
  local db_name="${DB_NAME:-$GC_DB_NAME}"
  local app_host="${refresh_sql_default_user_host:-%}"

  local import_rc=0 seed_rc=0
  local schema_attempted=0 seed_attempted=0

  if [[ -z "$db_container" ]]; then
    if (( skip_import == 0 )); then
      import_rc=1
    fi
    if (( skip_seed == 0 )); then
      seed_rc=1
    fi
  else
    if (( skip_import == 0 || skip_seed == 0 )); then
      local ensure_sql
      ensure_sql="$(python3 - <<'PY' "$db_name" "$app_user" "$app_pass" "$app_host"
import sys

db, user, password, host = sys.argv[1:5]
if not host:
    host = '%'
if not db:
    db = 'app'
if not user:
    user = 'app'

def quote_identifier(name: str) -> str:
    return '`' + name.replace('`', '``') + '`'

def quote_string(value: str) -> str:
    return "'" + value.replace("'", "''") + "'"

statements = [
    f"CREATE DATABASE IF NOT EXISTS {quote_identifier(db)} CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;",
    f"CREATE USER IF NOT EXISTS {quote_string(user)}@{quote_string(host)} IDENTIFIED BY {quote_string(password)};",
    f"GRANT ALL PRIVILEGES ON {quote_identifier(db)}.* TO {quote_string(user)}@{quote_string(host)};",
    "FLUSH PRIVILEGES;",
]
print("\n".join(statements))
PY
)"
      if ! gc_refresh_stack_exec_inline_sql "$db_container" "$root_user" "$root_pass" "" "$db_port" <<<"$ensure_sql"; then
        warn "Failed to ensure database or user; continuing with imports."
      else
        info "Ensured database ${db_name} and user ${app_user}"
      fi
    fi

    if (( skip_import == 0 )) && (( ${#refresh_sql_init_files[@]} + ${#refresh_sql_schema_files[@]} == 0 )); then
      info "No schema SQL files found; skipping import."
      skip_import=1
    fi
    if (( skip_seed == 0 )) && (( ${#refresh_sql_seed_files[@]} == 0 )); then
      info "No seed SQL files found; skipping seeding."
      skip_seed=1
    fi

    if (( skip_import == 0 )); then
      local file display
      for file in "${refresh_sql_init_files[@]}"; do
        [[ -f "$file" ]] || { warn "Init SQL not found: $file"; import_rc=1; continue; }
        display="$file"
        [[ "$display" == "$PROJECT_ROOT/"* ]] && display="${display#$PROJECT_ROOT/}"
        info "Applying init SQL: ${display}"
        ((schema_attempted++))
        if ! gc_refresh_stack_exec_mysql "$db_container" "$file" "$root_user" "$root_pass" "" "$db_port"; then
          warn "Init SQL failed as ${root_user}; retrying as ${app_user}"
          if ! gc_refresh_stack_exec_mysql "$db_container" "$file" "$app_user" "$app_pass" "" "$db_port"; then
            warn "Init SQL failed: ${display}"
            import_rc=1
            continue
          fi
        fi
      done

      for file in "${refresh_sql_schema_files[@]}"; do
        [[ -f "$file" ]] || { warn "Schema SQL not found: $file"; import_rc=1; continue; }
        display="$file"
        [[ "$display" == "$PROJECT_ROOT/"* ]] && display="${display#$PROJECT_ROOT/}"
        info "Importing schema SQL: ${display}"
        ((schema_attempted++))
        if ! gc_refresh_stack_exec_mysql "$db_container" "$file" "$root_user" "$root_pass" "$db_name" "$db_port"; then
          warn "Schema import failed as ${root_user}; retrying as ${app_user}"
          if ! gc_refresh_stack_exec_mysql "$db_container" "$file" "$app_user" "$app_pass" "$db_name" "$db_port"; then
            warn "Schema SQL failed: ${display}"
            import_rc=1
            continue
          fi
        fi
      done
    else
      info "Skipping schema import (--no-import)"
    fi

    if (( skip_seed == 0 )); then
      local seed_file seed_display
      for seed_file in "${refresh_sql_seed_files[@]}"; do
        [[ -f "$seed_file" ]] || { warn "Seed SQL not found: $seed_file"; seed_rc=1; continue; }
        seed_display="$seed_file"
        [[ "$seed_display" == "$PROJECT_ROOT/"* ]] && seed_display="${seed_display#$PROJECT_ROOT/}"
        info "Applying seed SQL: ${seed_display}"
        ((seed_attempted++))
        if ! gc_refresh_stack_exec_mysql "$db_container" "$seed_file" "$root_user" "$root_pass" "$db_name" "$db_port"; then
          warn "Seed import failed as ${root_user}; retrying as ${app_user}"
          if ! gc_refresh_stack_exec_mysql "$db_container" "$seed_file" "$app_user" "$app_pass" "$db_name" "$db_port"; then
            warn "Seed SQL failed: ${seed_display}"
            seed_rc=1
            continue
          fi
        fi
      done
    else
      info "Skipping seeding (--no-seed)"
    fi
  fi

  info "Verifying Docker service health"
  local stack_health_rc=0
  if gc_refresh_stack_wait_for_containers "$compose_file" "${GC_DOCKER_HEALTH_TIMEOUT:-10}" "${GC_DOCKER_HEALTH_INTERVAL:-1}"; then
    ok "Docker services healthy"
  else
    stack_health_rc=1
    warn "Docker services reported issues; inspect compose logs for details."
  fi

  local status=0
  if (( import_rc != 0 )); then
    status=1
  elif (( skip_import == 0 && schema_attempted > 0 )); then
    ok "Database schema imported"
  fi
  if (( seed_rc != 0 )); then
    status=1
  elif (( skip_seed == 0 && seed_attempted > 0 )); then
    ok "Database seeds applied"
  fi
  if (( stack_health_rc != 0 )); then
    status=1
  fi

  if (( status == 0 )); then
    ok "Stack refreshed successfully"
  else
    warn "Stack refresh completed with issues; inspect logs above."
  fi
  return $status
}


cmd_verify() {
  local kind="${1:-all}"; shift || true
  local root=""
  local api_base="${GC_API_BASE_URL:-http://localhost:3000/api/v1}"
  local api_health="${GC_API_HEALTH_URL:-}"
  local web_url="${GC_WEB_URL:-http://localhost:8080/}"
  local admin_url="${GC_ADMIN_URL:-http://localhost:8080/admin/}"
  local api_base_override=0 api_health_override=0 web_url_override=0 admin_url_override=0

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --api-url) api_base="$2"; api_base_override=1; shift 2;;
      --api-health) api_health="$2"; api_health_override=1; shift 2;;
      --web-url) web_url="$2"; web_url_override=1; shift 2;;
      --admin-url) admin_url="$2"; admin_url_override=1; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"

  kind="$(printf '%s' "$kind" | tr '[:upper:]' '[:lower:]')"

  local compose_file="${PROJECT_ROOT}/docker/docker-compose.yml"
  local ports_updated=0
  if [[ -f "$compose_file" ]]; then
    local detected
    if detected="$(gc_compose_port "$compose_file" api 3000)"; then
      if [[ -n "$detected" && "$detected" != "$GC_API_HOST_PORT" ]]; then
        GC_API_HOST_PORT="$detected"; API_HOST_PORT="$detected"; ports_updated=1
      fi
    fi
    if detected="$(gc_compose_port "$compose_file" web 5173)"; then
      if [[ -n "$detected" && "$detected" != "$GC_WEB_HOST_PORT" ]]; then
        GC_WEB_HOST_PORT="$detected"; WEB_HOST_PORT="$detected"; ports_updated=1
      fi
    fi
    if detected="$(gc_compose_port "$compose_file" admin 5173)"; then
      if [[ -n "$detected" && "$detected" != "$GC_ADMIN_HOST_PORT" ]]; then
        GC_ADMIN_HOST_PORT="$detected"; ADMIN_HOST_PORT="$detected"; ports_updated=1
      fi
    fi
    if detected="$(gc_compose_port "$compose_file" proxy 80)"; then
      if [[ -n "$detected" && "$detected" != "$GC_PROXY_HOST_PORT" ]]; then
        GC_PROXY_HOST_PORT="$detected"; PROXY_HOST_PORT="$detected"; ports_updated=1
      fi
    fi
  fi
  (( ports_updated )) && gc_env_sync_ports

  if (( api_base_override == 0 )); then
    api_base="${GC_API_BASE_URL:-$api_base}"
  fi
  if (( web_url_override == 0 )); then
    web_url="${GC_WEB_URL:-$web_url}"
  fi
  if (( admin_url_override == 0 )); then
    admin_url="${GC_ADMIN_URL:-$admin_url}"
  fi
  if (( api_health_override == 0 )); then
    api_health="${GC_API_HEALTH_URL:-$api_health}"
  fi

  local trimmed_base="${api_base%/}"
  api_health="${api_health:-${trimmed_base}/health}"

  local verify_root="$CLI_ROOT/verify"
  [[ -d "$verify_root" ]] || die "verify scripts directory missing at ${verify_root}"

  case "$kind" in
    program_filters) kind="program-filters" ;;
    program-filters|acceptance|openapi|a11y|lighthouse|consent|nfr|all) ;;
    *) die "Unknown verify target: ${kind}";;
  esac

  local -a check_names
  case "$kind" in
    acceptance) check_names=(acceptance) ;;
    openapi|a11y|lighthouse|consent|program-filters)
      check_names=("$kind")
      ;;
    nfr)
      check_names=(openapi a11y lighthouse consent program-filters)
      ;;
    all)
      check_names=(acceptance openapi a11y lighthouse consent program-filters)
      ;;
  esac

  local summary_dir="${PROJECT_ROOT}/.gpt-creator/staging/verify"
  local logs_dir="${summary_dir}/logs"
  mkdir -p "$summary_dir" "$logs_dir"

  local summary_path="${summary_dir}/summary.json"
  local python_available=0
  local python_bin=""
  if command -v python3 >/dev/null 2>&1; then
    python_available=1
    python_bin="$(command -v python3)"
  fi
  local check_order="acceptance,openapi,lighthouse,a11y,consent,program-filters"

  local pass=0 fail=0 skip=0

  update_verify_summary() {
    [[ "$python_available" -eq 1 ]] || return 0
    local name="$1"
    local status="$2"
    local label="$3"
    local message="$4"
    local log_path="$5"
    local report_path="$6"
    local score="$7"
    local duration="$8"
    local run_kind="$9"
    local timestamp="${10}"
    local event=""
    event="$(
      CHECK_NAME="$name" \
      CHECK_STATUS="$status" \
      CHECK_LABEL="$label" \
      CHECK_MESSAGE="$message" \
      CHECK_LOG="$log_path" \
      CHECK_REPORT="$report_path" \
      CHECK_SCORE="$score" \
      CHECK_DURATION="$duration" \
      CHECK_RUN_KIND="$run_kind" \
      CHECK_TIMESTAMP="$timestamp" \
      CHECK_ORDER="$check_order" \
      PROJECT_ROOT="$PROJECT_ROOT" \
      "$python_bin" - "$summary_path" <<'PY'
import json, os, sys, datetime
path = sys.argv[1]
root = os.environ.get("PROJECT_ROOT", "")
name = os.environ.get("CHECK_NAME", "")
if not name:
    sys.exit(0)
label = os.environ.get("CHECK_LABEL", name.title())
status = os.environ.get("CHECK_STATUS", "unknown")
message = os.environ.get("CHECK_MESSAGE", "")
log_path = os.environ.get("CHECK_LOG", "")
report_path = os.environ.get("CHECK_REPORT", "")
score_raw = os.environ.get("CHECK_SCORE", "")
duration_raw = os.environ.get("CHECK_DURATION", "")
run_kind = os.environ.get("CHECK_RUN_KIND", "")
timestamp = os.environ.get("CHECK_TIMESTAMP", datetime.datetime.utcnow().replace(microsecond=0).isoformat() + "Z")
order_raw = os.environ.get("CHECK_ORDER", "")

def relify(path_value):
    if not path_value:
        return ""
    if not os.path.isabs(path_value) and root:
        abs_path = os.path.normpath(os.path.join(root, path_value))
    else:
        abs_path = os.path.normpath(path_value)
    if root:
        try:
            rel = os.path.relpath(abs_path, root)
        except Exception:
            rel = abs_path
    else:
        rel = abs_path
    return rel.replace(os.sep, "/")

score = None
if score_raw:
    try:
        score = float(score_raw)
    except Exception:
        score = None

duration = None
if duration_raw:
    try:
        duration = float(duration_raw)
    except Exception:
        duration = None

try:
    with open(path, "r", encoding="utf-8") as fh:
        data = json.load(fh)
except Exception:
    data = {}

checks = data.get("checks")
if not isinstance(checks, dict):
    checks = {}

entry = checks.get(name, {})
entry["name"] = name
entry["label"] = label
entry["status"] = status
if message:
    entry["message"] = message
elif "message" in entry:
    del entry["message"]

log_rel = relify(log_path)
if log_rel:
    entry["log"] = log_rel
elif "log" in entry:
    del entry["log"]

report_rel = relify(report_path)
if report_rel:
    entry["report"] = report_rel
elif "report" in entry:
    del entry["report"]

if score is not None:
    entry["score"] = score
elif "score" in entry:
    del entry["score"]

if duration is not None:
    entry["duration_seconds"] = duration
elif "duration_seconds" in entry:
    del entry["duration_seconds"]

entry["updated"] = timestamp
if run_kind:
    entry["run_kind"] = run_kind

checks[name] = entry
data["checks"] = checks
data["last_updated"] = timestamp
if run_kind:
    data["last_run_kind"] = run_kind

stats = {"passed": 0, "failed": 0, "skipped": 0, "total": 0}
for chk in checks.values():
    status_value = str(chk.get("status", "")).lower()
    if status_value in ("pass", "passed", "ok", "success"):
        stats["passed"] += 1
    elif status_value in ("skip", "skipped"):
        stats["skipped"] += 1
    else:
        stats["failed"] += 1
    stats["total"] += 1
data["stats"] = stats

order = [part.strip() for part in order_raw.split(",") if part.strip()]
if order:
    merged = []
    seen = set()
    for item in order:
        if item not in seen:
            merged.append(item); seen.add(item)
    for item in checks.keys():
        if item not in seen:
            merged.append(item); seen.add(item)
    data["order"] = merged
else:
    existing = data.get("order")
    merged = []
    seen = set()
    if isinstance(existing, list):
        for item in existing:
            if isinstance(item, str) and item not in seen:
                merged.append(item); seen.add(item)
    for item in checks.keys():
        if item not in seen:
            merged.append(item); seen.add(item)
    data["order"] = merged

with open(path, "w", encoding="utf-8") as fh:
    json.dump(data, fh, indent=2)

event = {
    "name": name,
    "label": label,
    "status": status,
    "message": message,
    "log": entry.get("log", ""),
    "report": entry.get("report", ""),
    "score": entry.get("score"),
    "updated": timestamp,
    "run_kind": run_kind,
    "stats": stats,
    "duration_seconds": entry.get("duration_seconds"),
}
print(json.dumps(event))
PY
    )" || event=""
    if [[ -n "$event" ]]; then
      printf '::verify::%s\n' "$event"
    fi
  }

  run_check() {
    local name="$1"; shift
    local label="$1"; shift
    local -a cmd=("$@")
    local timestamp="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
    local stamp="$(date -u +"%Y%m%d-%H%M%S")"
    local log_file="${logs_dir}/${stamp}-${name}.log"
    SECONDS=0
    set +e
    "${cmd[@]}" 2>&1 | tee "$log_file"
    local exit_status=${PIPESTATUS[0]}
    set -e
    local duration="$SECONDS"
    local status message
    case "$exit_status" in
      0)
        status="pass"
        message="${label} checks passed."
        ((pass++))
        ;;
      3)
        status="skip"
        message="${label} check skipped (missing dependency)."
        ((skip++))
        warn "${label} check skipped (missing dependency)"
        ;;
      *)
        status="fail"
        message="${label} check failed (exit ${exit_status})."
        ((fail++))
        warn "${label} check failed (exit ${exit_status})"
        ;;
    esac
    cp -f "$log_file" "${logs_dir}/${name}-latest.log" 2>/dev/null || true
    local log_rel="${log_file#$PROJECT_ROOT/}"
    log_rel="${log_rel#./}"
    update_verify_summary "$name" "$status" "$label" "$message" "$log_rel" "" "" "$duration" "$kind" "$timestamp"
    return 0
  }

  find_openapi_candidate() {
    local spec=""
    for cand in "$INPUT_DIR/openapi.yaml" "$INPUT_DIR/openapi.yml" "$INPUT_DIR/openapi.json"; do
      if [[ -f "$cand" ]]; then
        spec="$cand"
        break
      fi
    done
    printf '%s' "$spec"
  }

  for name in "${check_names[@]}"; do
    case "$name" in
      acceptance)
        run_check "acceptance" "Acceptance" \
          env PROJECT_ROOT="$PROJECT_ROOT" GC_COMPOSE_FILE="$compose_file" \
          bash "$verify_root/acceptance.sh" "${api_base}" "${web_url}" "${admin_url}" "${api_health}"
        ;;
      openapi)
        run_check "openapi" "OpenAPI" \
          bash "$verify_root/check-openapi.sh" "$(find_openapi_candidate)"
        ;;
      a11y)
        run_check "a11y" "Accessibility" \
          bash "$verify_root/check-a11y.sh" "${web_url}" "${admin_url}"
        ;;
      lighthouse)
        run_check "lighthouse" "Lighthouse" \
          bash "$verify_root/check-lighthouse.sh" "${web_url}" "${admin_url}"
        ;;
      consent)
        run_check "consent" "Consent" \
          bash "$verify_root/check-consent.sh" "${web_url}"
        ;;
      program-filters)
        run_check "program-filters" "Program Filters" \
          bash "$verify_root/check-program-filters.sh" "${api_base}"
        ;;
    esac
  done

  if (( fail > 0 )); then
    die "Verify failed — pass=${pass} fail=${fail} skip=${skip}"
  fi
  ok "Verify complete — pass=${pass} skip=${skip}"
}

gc_exec_with_timeout() {
  local timeout="${1:-0}"
  local stdin_file="${2:-}"
  local log_file="${3:-}"
  shift 3 || true
  local -a cmd=("$@")

  if (( ${#cmd[@]} == 0 )); then
    return 1
  fi

  if (( timeout <= 0 )); then
    if [[ -n "$log_file" ]]; then
      if [[ -n "$stdin_file" ]]; then
        if "${cmd[@]}" <"$stdin_file" 2>&1 | tee "$log_file"; then
          return 0
        else
          local -a pipeline_status=("${PIPESTATUS[@]}")
          local rc=0
          for status in "${pipeline_status[@]}"; do
            if (( status != 0 )); then
              rc="$status"
            fi
          done
          (( rc == 0 )) && rc=1
          return "$rc"
        fi
      else
        if "${cmd[@]}" 2>&1 | tee "$log_file"; then
          return 0
        else
          local -a pipeline_status=("${PIPESTATUS[@]}")
          local rc=0
          for status in "${pipeline_status[@]}"; do
            if (( status != 0 )); then
              rc="$status"
            fi
          done
          (( rc == 0 )) && rc=1
          return "$rc"
        fi
      fi
    else
      if [[ -n "$stdin_file" ]]; then
        "${cmd[@]}" <"$stdin_file"
      else
        "${cmd[@]}"
      fi
      return "$?"
    fi
  fi

  python3 - "$timeout" "$stdin_file" "$log_file" "${cmd[@]}" <<'PY'
import os
import select
import signal
import subprocess
import sys
import time

def to_int(value: str) -> int:
    try:
        return max(0, int(value))
    except (TypeError, ValueError):
        return 0

timeout = to_int(sys.argv[1] if len(sys.argv) > 1 else "0")
stdin_path = sys.argv[2] if len(sys.argv) > 2 else ""
log_path = sys.argv[3] if len(sys.argv) > 3 else ""
cmd = sys.argv[4:]

if not cmd:
    sys.exit(1)

stdin = None
if stdin_path:
    try:
        stdin = open(stdin_path, "rb")
    except FileNotFoundError:
        print(f"{stdin_path}: No such file or directory", file=sys.stderr, flush=True)
        sys.exit(1)

log = None
if log_path:
    log_dir = os.path.dirname(log_path)
    if log_dir:
        os.makedirs(log_dir, exist_ok=True)
    log = open(log_path, "ab", buffering=0)

def write_out(data: bytes) -> None:
    if not data:
        return
    try:
        os.write(sys.stdout.fileno(), data)
    except OSError:
        pass
    if log:
        try:
            log.write(data)
        except OSError:
            pass

preexec = os.setsid if hasattr(os, "setsid") else None

try:
    proc = subprocess.Popen(
        cmd,
        stdin=stdin,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        bufsize=0,
        preexec_fn=preexec,
    )
except FileNotFoundError:
    write_out(f"{cmd[0]} not found\n".encode())
    if log:
        log.close()
    sys.exit(127)

start = time.monotonic()
last_activity = start
timed_out = False

if not proc.stdout:
    if stdin:
        stdin.close()
    if log:
        log.close()
    sys.exit(1)

fd = proc.stdout.fileno()

try:
    while True:
        if timeout:
            now = time.monotonic()
            idle_for = now - last_activity
            if idle_for >= timeout:
                timed_out = True
                break
            remaining = timeout - idle_for
            wait_time = remaining if remaining < 0.2 else 0.2
        else:
            wait_time = 0.2
        ready, _, _ = select.select([fd], [], [], wait_time)
        if ready:
            chunk = os.read(fd, 8192)
            if chunk:
                write_out(chunk)
                last_activity = time.monotonic()
            else:
                break
        else:
            if proc.poll() is not None:
                break
    if not timed_out:
        while True:
            chunk = os.read(fd, 8192)
            if not chunk:
                break
            write_out(chunk)
except KeyboardInterrupt:
    proc.terminate()
    raise
finally:
    if stdin:
        stdin.close()

proc.stdout.close()

if timed_out:
    try:
        if preexec:
            os.killpg(proc.pid, signal.SIGTERM)
        else:
            proc.terminate()
    except ProcessLookupError:
        pass
    try:
        proc.wait(5)
    except subprocess.TimeoutExpired:
        try:
            if preexec:
                os.killpg(proc.pid, signal.SIGKILL)
            else:
                proc.kill()
        except ProcessLookupError:
            pass
        proc.wait()
    write_out(f"\n[gc] Command produced no output for {timeout} seconds\n".encode())
    if log:
        log.flush()
        log.close()
    sys.exit(124)

exit_code = proc.poll()
if exit_code is None:
    exit_code = proc.wait()

if log:
    log.flush()
    log.close()

sys.exit(exit_code)
PY
  return "$?"
}

codex_call() {
  local task="${1:?task}"; shift || true
  local prompt_dir="${GC_DIR}/prompts"
  mkdir -p "$prompt_dir"

  local prompt_file=""
  local output_file=""

  if [[ $# -gt 0 && -f "$1" ]]; then
    prompt_file="$1"
    shift || true
  fi

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --prompt) prompt_file="$2"; shift 2;;
      --output) output_file="$2"; shift 2;;
      *) break;;
    esac
  done

  if [[ -z "$prompt_file" ]]; then
    prompt_file="${prompt_dir}/${task}.md"
    if [[ ! -f "$prompt_file" ]]; then
      cat >"$prompt_file" <<'PROMPT'
# Instruction
You are Codex (gpt-5-codex) assisting the gpt-creator pipeline. Apply requested changes deterministically.
PROMPT
    fi
  fi

  if [[ "${GC_CODEX_USAGE_LIMIT_REACHED:-0}" == "1" ]]; then
    warn "Codex usage limit previously reached; skipping ${task}."
    return 95
  fi

  if command -v "$CODEX_BIN" >/dev/null 2>&1; then
    info "Codex ${task} → model=${CODEX_MODEL}"
    local fallback_model="${CODEX_FALLBACK_MODEL:-gpt-5-codex}"
    local run_codex_model
    run_codex_model() {
      local model="$1"
      shift || true
      local args=(exec --model "$model")
      if [[ -n "${CODEX_PROFILE:-}" ]]; then
        args+=(--profile "$CODEX_PROFILE")
      fi
      if [[ -n "${PROJECT_ROOT:-}" ]]; then
        args+=(--cd "$PROJECT_ROOT")
      fi
      if [[ -n "${CODEX_REASONING_EFFORT:-}" ]]; then
        args+=(-c "model_reasoning_effort=\"${CODEX_REASONING_EFFORT}\"")
      fi
      args+=(--full-auto --sandbox workspace-write --skip-git-repo-check)
      if [[ -n "$output_file" ]]; then
        mkdir -p "$(dirname "$output_file")"
        args+=(--output-last-message "$output_file")
      fi
      local usage_dir="${LOG_DIR:-${PROJECT_ROOT:-$PWD}/.gpt-creator/logs}"
      mkdir -p "$usage_dir"
      local task_slug
      task_slug="$(printf '%s' "$task" | tr '[:upper:]' '[:lower:]')"
      task_slug="$(printf '%s' "$task_slug" | tr -c 'a-z0-9' '_')"
      [[ -n "$task_slug" ]] || task_slug="codex"
      local model_slug
      model_slug="$(printf '%s' "$model" | tr '[:upper:]' '[:lower:]')"
      model_slug="$(printf '%s' "$model_slug" | tr -c 'a-z0-9' '_')"
      [[ -n "$model_slug" ]] || model_slug="model"
      local codex_log=""
      if ! codex_log="$(mktemp "${usage_dir}/codex-${task_slug}-${model_slug}.XXXXXX.log" 2>/dev/null)"; then
        codex_log="$(mktemp 2>/dev/null)" || codex_log=""
      fi
      local exec_timeout=0
      if [[ "${GC_CODEX_EXEC_TIMEOUT:-}" =~ ^[0-9]+$ ]]; then
        exec_timeout=$((GC_CODEX_EXEC_TIMEOUT))
      fi
      if [[ -n "$codex_log" ]]; then
        local cmd_status=0
        if gc_exec_with_timeout "$exec_timeout" "$prompt_file" "$codex_log" "$CODEX_BIN" "${args[@]}"; then
          cmd_status=0
        else
          cmd_status=$?
        fi
        gc_record_codex_usage "$codex_log" "$task" "$model" "$prompt_file" "$cmd_status"
        if [[ "${GC_CODEX_USAGE_LIMIT_REACHED:-0}" == "1" ]]; then
          return 95
        fi
        return "$cmd_status"
      else
        local cmd_status=0
        if gc_exec_with_timeout "$exec_timeout" "$prompt_file" "" "$CODEX_BIN" "${args[@]}"; then
          cmd_status=0
        else
          cmd_status=$?
        fi
        if [[ "${GC_CODEX_USAGE_LIMIT_REACHED:-0}" == "1" ]]; then
          return 95
        fi
        return "$cmd_status"
      fi
    }
    if run_codex_model "$CODEX_MODEL"; then
      return 0
    fi
    local primary_status="$?"
    if [[ "${GC_CODEX_USAGE_LIMIT_REACHED:-0}" == "1" ]]; then
      return "$primary_status"
    fi
    if [[ "$CODEX_MODEL" != "$fallback_model" ]]; then
      warn "Codex model ${CODEX_MODEL} failed; retrying with ${fallback_model}."
      if run_codex_model "$fallback_model"; then
        return 0
      fi
      primary_status="$?"
      if [[ "${GC_CODEX_USAGE_LIMIT_REACHED:-0}" == "1" ]]; then
        return "$primary_status"
      fi
      warn "Codex invocation returned non-zero."
      return "$primary_status"
    fi
    warn "Codex invocation returned non-zero."
    return "$primary_status"
  else
    warn "Codex binary (${CODEX_BIN}) not found — skipping ${task}."
  fi
}

ensure_go_runtime() {
  local min_major=1
  local min_minor=21
  local target_version="1.22.4"
  local requested_bin="${GO_BIN:-}"
  local resolved_bin=""

  if [[ -n "$requested_bin" ]]; then
    if command -v "$requested_bin" >/dev/null 2>&1; then
      resolved_bin="$(command -v "$requested_bin")"
    else
      warn "GO_BIN is set to '${requested_bin}' but that binary was not found on PATH."
    fi
  fi

  if [[ -z "$resolved_bin" ]] && command -v go >/dev/null 2>&1; then
    resolved_bin="$(command -v go)"
  fi

  if [[ -n "$resolved_bin" ]]; then
    local raw_version
    raw_version="$("$resolved_bin" version 2>/dev/null | awk '{print $3}')"
    local version="${raw_version#go}"
    version="${version%%[^0-9.]*}"
    local IFS=.
    read -r version_major version_minor _ <<<"${version}"
    version_major="${version_major:-0}"
    version_minor="${version_minor:-0}"
    if (( 10#$version_major > min_major )) || { (( 10#$version_major == min_major )) && (( 10#$version_minor >= min_minor )); }; then
      GC_GO_BIN="$resolved_bin"
      export GO_BIN="$GC_GO_BIN"
      return
    fi
    warn "Found Go ${version:-unknown} at ${resolved_bin}, but need ≥ ${min_major}.${min_minor}."
  fi

  local os_name arch_name go_os go_arch
  os_name="$(uname -s)"
  arch_name="$(uname -m)"
  case "$os_name" in
    Linux) go_os="linux" ;;
    Darwin) go_os="darwin" ;;
    *)
      die "Unsupported OS (${os_name}) for automatic Go installation. Install Go ${min_major}.${min_minor}+ and set GO_BIN."
      ;;
  esac

  case "$arch_name" in
    x86_64|amd64) go_arch="amd64" ;;
    arm64|aarch64) go_arch="arm64" ;;
    *)
      die "Unsupported architecture (${arch_name}) for automatic Go installation. Install Go ${min_major}.${min_minor}+ and set GO_BIN."
      ;;
  esac

  local base_dir
  if [[ -n "${PLAN_DIR:-}" ]]; then
    base_dir="$PLAN_DIR"
  else
    base_dir="${XDG_CACHE_HOME:-$HOME/.cache}/gpt-creator"
    mkdir -p "$base_dir"
  fi
  local runtime_root="${base_dir}/.runtime/go"
  local archive_basename="go${target_version}.${go_os}-${go_arch}"
  local target_dir="${runtime_root}/${archive_basename}"
  local go_root="${target_dir}/go"
  local go_binary="${go_root}/bin/go"

  mkdir -p "$runtime_root"

  if [[ ! -x "$go_binary" ]]; then
    info "Installing Go ${target_version} (${go_os}-${go_arch})"
    local url="https://go.dev/dl/${archive_basename}.tar.gz"
    local tmp_archive
    tmp_archive="$(mktemp "${runtime_root}/go-${target_version}.XXXXXX")"
    if ! curl -fsSL "$url" -o "$tmp_archive"; then
      rm -f "$tmp_archive"
      die "Failed to download Go toolchain from ${url}"
    fi
    rm -rf "$target_dir"
    mkdir -p "$target_dir"
    if ! tar -xzf "$tmp_archive" -C "$target_dir"; then
      rm -f "$tmp_archive"
      die "Failed to extract Go toolchain archive (${url})"
    fi
    rm -f "$tmp_archive"
  fi

  if [[ ! -x "$go_binary" ]]; then
    die "Go toolchain installation incomplete; expected executable at ${go_binary}"
  fi

  local gopath="${target_dir}/gopath"
  local cache_dir="${target_dir}/cache"
  mkdir -p "${gopath}/bin" "${gopath}/pkg/mod" "${cache_dir}"
  export PATH="${go_root}/bin:${PATH}"
  export GOROOT="${go_root}"
  export GOPATH="${gopath}"
  export GOMODCACHE="${gopath}/pkg/mod"
  export GOCACHE="${cache_dir}"
  GC_GO_BIN="$go_binary"
  export GO_BIN="$GC_GO_BIN"
  ok "Go runtime pinned to ${target_version}"
}

ensure_node_runtime() {
  local root_dir="${1:-$PROJECT_ROOT}"
  local target_version="20.10.0"
  local runtime_root="${PLAN_DIR}/.runtime"
  local archive_dir="node-v${target_version}"

  # If current node already satisfies v20.x with sufficient minor, skip.
  if command -v node >/dev/null 2>&1; then
    local current
    current="$(node -v 2>/dev/null | sed 's/^v//')"
    if [[ "$current" == 20.* ]]; then
      local minor="${current#20.}"
      minor="${minor%%.*}"
      local patch="${current#20.${minor}.}"
      [[ -z "$patch" ]] && patch=0
      if (( 10#${minor:-0} > 10 )) || { (( 10#${minor:-0} == 10 )) && (( 10#${patch:-0} >= 0 )); }; then
        return
      fi
    fi
  fi

  mkdir -p "$runtime_root"

  local os_name="$(uname -s)"
  local arch_name="$(uname -m)"
  local os_tag="" arch_tag="" ext="tar.xz"

  case "$os_name" in
    Darwin)
      os_tag="darwin"
      ext="tar.gz"
      ;;
    Linux)
      os_tag="linux"
      ;;
    *)
      warn "Unsupported OS (${os_name}) for automatic Node runtime; continuing with system Node."
      return
      ;;
  esac

  case "$arch_name" in
    x86_64|amd64)
      arch_tag="x64"
      ;;
    arm64|aarch64)
      arch_tag="arm64"
      ;;
    *)
      warn "Unsupported CPU architecture (${arch_name}) for automatic Node runtime; continuing with system Node."
      return
      ;;
  esac

  local archive_name="node-v${target_version}-${os_tag}-${arch_tag}"
  local target_dir="${runtime_root}/${archive_name}"

  if [[ ! -d "$target_dir" ]]; then
    local url="https://nodejs.org/dist/v${target_version}/${archive_name}.${ext}"
    info "Downloading Node.js ${target_version} (${os_tag}-${arch_tag})"
    local tmp_archive
    tmp_archive="$(mktemp "${runtime_root}/node-${target_version}.XXXXXX")"
    if ! curl -fsSL "$url" -o "$tmp_archive"; then
      warn "Failed to download Node.js runtime from ${url}; continuing with system Node."
      rm -f "$tmp_archive"
      return
    fi
    mkdir -p "$runtime_root"
    if [[ "$ext" == "tar.gz" ]]; then
      tar -xzf "$tmp_archive" -C "$runtime_root"
    else
      tar -xJf "$tmp_archive" -C "$runtime_root"
    fi
    rm -f "$tmp_archive"
  fi

  if [[ ! -d "$target_dir" ]]; then
    warn "Node runtime directory ${target_dir} missing after download; continuing with system Node."
    return
  fi

  export PATH="${target_dir}/bin:${PATH}"
  export NODE_HOME="$target_dir"
  export npm_config_cache="${runtime_root}/npm-cache"
  export PNPM_HOME="${runtime_root}/pnpm-home"
  mkdir -p "$npm_config_cache" "$PNPM_HOME"
  export PATH="${PNPM_HOME}:${PATH}"
  export GC_NODE_RUNTIME="${target_dir}"
  ok "Node.js runtime pinned to ${target_version}"
}

ensure_node_dependencies() {
  local root_dir="${1:-$PROJECT_ROOT}"
  local sentinel="${PLAN_DIR}/.deps-installed"

  ensure_node_runtime "$root_dir"

  if [[ -f "$sentinel" ]]; then
    return
  fi

  local installer_desc=""
  local -a installer_cmd=()

  if [[ -f "$root_dir/pnpm-lock.yaml" || -f "$root_dir/pnpm-workspace.yaml" ]]; then
    if command -v pnpm >/dev/null 2>&1; then
      installer_desc="pnpm workspace"
      installer_cmd=(pnpm install --frozen-lockfile)
    fi
  fi

  if [[ ${#installer_cmd[@]} -eq 0 && -f "$root_dir/package.json" ]]; then
    if command -v pnpm >/dev/null 2>&1; then
      installer_desc="pnpm"
      installer_cmd=(pnpm install)
    elif command -v npm >/dev/null 2>&1; then
      installer_desc="npm"
      installer_cmd=(npm install)
    fi
  fi

  if [[ ${#installer_cmd[@]} -eq 0 ]]; then
    return
  fi

  local log_file="/tmp/gc_deps_install.log"

  info "Ensuring Node.js dependencies via ${installer_desc}"
  local install_status=1
  if [[ ${installer_cmd[0]} == pnpm ]]; then
    if (cd "$root_dir" && CI=1 PNPM_IGNORE_NODE_VERSION=1 "${installer_cmd[@]}" >"$log_file" 2>&1); then
      install_status=0
    fi
  else
    if (cd "$root_dir" && "${installer_cmd[@]}" >"$log_file" 2>&1); then
      install_status=0
    fi
  fi

  if (( install_status == 0 )); then
    mkdir -p "$(dirname "$sentinel")"
    touch "$sentinel"
    ok "Dependencies installed"
  else
    warn "Dependency installation via ${installer_desc} failed; inspect ${log_file}. Continuing anyway."
  fi
}

gc_apply_codex_changes() {
  local output_file="${1:?output file required}"
  local project_root="${2:?project root required}"
  python3 - <<'PY' "$output_file" "$project_root"
import json
import subprocess
import sys
from pathlib import Path

output_path = Path(sys.argv[1])
project_root = Path(sys.argv[2])

if not output_path.exists():
    print("no-output", flush=True)
    sys.exit(0)

raw = output_path.read_text(encoding='utf-8').strip()
if not raw:
    print("empty-output", flush=True)
    sys.exit(0)

# Remove code fences if present
if '```' in raw:
    cleaned = []
    fenced = False
    for line in raw.splitlines():
        marker = line.strip()
        if marker.startswith('```'):
            fenced = not fenced
            continue
        if not fenced:
            cleaned.append(line)
    raw = '\n'.join(cleaned).strip()

start = raw.find('{')
end = raw.rfind('}')
if start == -1 or end == -1 or end <= start:
    raise SystemExit("JSON not found in Codex output")

fragment = raw[start:end+1]

import re
fragment = re.sub(r'\\"(?=[}\]\n])', r'\\""', fragment)

while True:
    try:
        payload = json.loads(fragment)
        break
    except json.JSONDecodeError as exc:
        if 'Invalid \\escape' in exc.msg:
            fragment = fragment[:exc.pos] + '\\' + fragment[exc.pos:]
            continue
        decoder = json.JSONDecoder(strict=False)
        try:
            payload = decoder.decode(fragment)
            break
        except json.JSONDecodeError:
            raw_dump = output_path.with_suffix(output_path.suffix + '.raw.txt')
            raw_dump.parent.mkdir(parents=True, exist_ok=True)
            raw_dump.write_text(raw, encoding='utf-8')
            rel_dump = raw_dump
            try:
                rel_dump = raw_dump.relative_to(project_root)
            except ValueError:
                rel_dump = raw_dump
            payload = {
                'plan': [],
                'changes': [],
                'commands': [],
                'notes': [
                    f"Codex output could not be parsed as JSON; review {rel_dump}."
                ],
            }
            print('STATUS parse-error')
            print(f"RAW {rel_dump}")
            break

changes = payload.get('changes') or []
written = []
patched = []
manual_notes = []

def rewrite_patch_paths(diff_text: str) -> str:
    mapping = {
        'api/': 'apps/api/',
        'web/': 'apps/web/',
        'admin/': 'apps/admin/',
        'site/': 'apps/web/',
    }

    def rewrite_path(path: str) -> str:
        for old, new in mapping.items():
            if path.startswith(old) and not path.startswith(new):
                return new + path[len(old):]
        return path

    lines = diff_text.splitlines()
    rewritten = []
    for line in lines:
        if line.startswith('diff --git a/'):
            parts = line.split()
            if len(parts) >= 4:
                a_path = parts[2][2:]
                b_path = parts[3][2:]
                new_a = rewrite_path(a_path)
                new_b = rewrite_path(b_path)
                if new_a != a_path or new_b != b_path:
                    line = f"diff --git a/{new_a} b/{new_b}"
        elif line.startswith('--- a/'):
            path = line[6:]
            new_path = rewrite_path(path)
            if new_path != path:
                line = f"--- a/{new_path}"
        elif line.startswith('+++ b/'):
            path = line[6:]
            new_path = rewrite_path(path)
            if new_path != path:
                line = f"+++ b/{new_path}"
        rewritten.append(line)
    return '\n'.join(rewritten)

def ensure_diff_headers(diff_text: str, path: str) -> str:
    if 'diff --git ' in diff_text:
        return diff_text

    lines = diff_text.splitlines()
    header = [
        f'diff --git a/{path} b/{path}',
        f'--- a/{path}',
        f'+++ b/{path}',
    ]
    return '\n'.join(header + lines)

def ensure_within_root(path: Path) -> Path:
    try:
        full = (project_root / path).resolve(strict=False)
        project = project_root.resolve(strict=True)
    except FileNotFoundError:
        project = project_root.resolve()
        full = (project_root / path).resolve(strict=False)
    if not str(full).startswith(str(project)):
        raise ValueError(f"Path {path} escapes project root")
    return full

for change in changes:
    ctype = change.get('type')
    path = change.get('path')
    if not path:
        raise ValueError('Change entry missing path')
    if ctype == 'file':
        content = change.get('content', '')
        dest = ensure_within_root(Path(path))
        dest.parent.mkdir(parents=True, exist_ok=True)
        if dest.exists() and dest.is_dir():
            try:
                rel_path = str(dest.relative_to(project_root))
            except ValueError:
                rel_path = str(dest)
            manual_notes.append(
                f"Skipped writing {rel_path} because that path already exists as a directory."
            )
            continue
        dest.write_text(content, encoding='utf-8')
        written.append(str(dest.relative_to(project_root)))
    elif ctype == 'patch':
        diff = change.get('diff')
        if not diff:
            raise ValueError(f"Patch change for {path} missing diff")
        diff = rewrite_patch_paths(diff)
        diff = ensure_diff_headers(diff, path)
        if not diff.endswith('\n'):
            diff += '\n'
        proc = subprocess.run(
            ['git', 'apply', '--whitespace=nowarn', '-'],
            input=diff.encode('utf-8'),
            cwd=str(project_root),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=False,
        )
        if proc.returncode != 0:
            git_err = proc.stderr.decode('utf-8')

            three_way = subprocess.run(
                ['git', 'apply', '--3way', '--whitespace=nowarn', '-'],
                input=diff.encode('utf-8'),
                cwd=str(project_root),
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                check=False,
            )

            if three_way.returncode == 0:
                patched.append(path + ' (3way)')
                continue

            git_err += three_way.stderr.decode('utf-8')

            fallback = subprocess.run(
                ['patch', '-p1', '--forward', '--silent'],
                input=diff.encode('utf-8'),
                cwd=str(project_root),
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                check=False,
            )
            if fallback.returncode != 0:
                # check if patch already applied
                already = subprocess.run(
                    ['git', 'apply', '--reverse', '--check', '-'],
                    input=diff.encode('utf-8'),
                    cwd=str(project_root),
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    check=False,
                )
                if already.returncode == 0:
                    patched.append(path + ' (already applied)')
                    continue

                new_content = None
                diff_lines = diff.splitlines()
                multi_file = sum(1 for line in diff_lines if line.startswith('diff --git ')) > 1
                if not multi_file and any(line.startswith('--- /dev/null') for line in diff_lines):
                    capture = False
                    content_lines = []
                    for line in diff_lines:
                        if line.startswith('@@'):
                            capture = True
                            continue
                        if not capture:
                            continue
                        if not line or line.startswith('diff --git'):
                            continue
                        if line.startswith('+'):
                            content_lines.append(line[1:])
                        elif line.startswith('-') or line.startswith('---') or line.startswith('+++'):
                            continue
                        elif line.startswith('\\'):
                            continue
                        else:
                            content_lines.append(line)
                    if content_lines:
                        new_content = '\n'.join(content_lines)
                        if not new_content.endswith('\n'):
                            new_content += '\n'
                if new_content is not None:
                    dest = ensure_within_root(Path(path))
                    if dest.exists() and dest.is_dir():
                        new_content = None
                    else:
                        dest.parent.mkdir(parents=True, exist_ok=True)
                        if dest.exists():
                            existing = dest.read_text(encoding='utf-8')
                            if existing == new_content:
                                patched.append(path + ' (already exists)')
                                continue
                        dest.write_text(new_content, encoding='utf-8')
                        patched.append(path + ' (reconstructed)')
                        continue

                if new_content is None:
                    try:
                        proc_noctx = subprocess.run(
                            ['git', 'apply', '--reject', '--whitespace=nowarn', '-'],
                            input=diff.encode('utf-8'),
                            cwd=str(project_root),
                            stdout=subprocess.PIPE,
                            stderr=subprocess.PIPE,
                            check=False,
                        )
                        if proc_noctx.returncode == 0:
                            patched.append(path + ' (partial apply)')
                            continue
                        else:
                            git_err += proc_noctx.stderr.decode('utf-8')
                    except Exception:
                        pass

                manual_patch = output_path.with_suffix(output_path.suffix + f".{len(manual_notes)+1}.patch")
                manual_patch.write_text(diff, encoding='utf-8')
                relative_manual = manual_patch
                try:
                    relative_manual = manual_patch.relative_to(project_root)
                except ValueError:
                    pass
                manual_notes.append(f"Patch for {path} could not be applied automatically. Review and apply {relative_manual} manually.")
                patched.append(path + ' (manual)')
                sys.stderr.write(git_err)
                sys.stderr.write(fallback.stderr.decode('utf-8'))
                continue
            patched.append(path + ' (patch)')
        else:
            patched.append(path)
    else:
        raise ValueError(f"Unknown change type: {ctype}")

summary = {
    'written': written,
    'patched': patched,
    'commands': payload.get('commands') or [],
    'notes': (payload.get('notes') or []) + manual_notes,
}
print('APPLIED')
for path in written:
    print(f"WRITE {path}")
for path in patched:
    print(f"PATCH {path}")
for cmd in summary['commands']:
    print(f"CMD {cmd}")
for note in summary['notes']:
    print(f"NOTE {note}")
PY
}

cmd_create_jira_tasks() {
  local args=()
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project)
        args+=(--project "$(abs_path "$2")")
        shift 2
        ;;
      --model)
        args+=(--model "$2")
        shift 2
        ;;
      --force|--dry-run)
        args+=("$1")
        shift
        ;;
      -h|--help)
        # Let the dedicated CLI script handle help output.
        args+=("$1")
        shift
        ;;
      *)
        args+=("$1")
        shift
        ;;
    esac
  done

  local shell_bin="${BASH:-bash}"
  "$shell_bin" "$CLI_ROOT/src/cli/create-jira-tasks.sh" "${args[@]}"
}

cmd_create_tasks() {
  local root="" jira="" force=0
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --jira) jira="$(abs_path "$2")"; shift 2;;
      --force) force=1; shift;;
      -h|--help)
        cat <<'EOUSAGE'
Usage: gpt-creator create-tasks [--project PATH] [--jira FILE] [--force]

Convert Jira markdown tasks into a project-scoped SQLite database stored under .gpt-creator/staging/plan/tasks.

Options:
  --project PATH  Project root (defaults to current directory)
  --jira FILE     Jira markdown source (defaults to staging/inputs/jira.md)
  --force         Rebuild the tasks database without restoring prior progress metadata
EOUSAGE
        return 0
        ;;
      *) break;;
    esac
  done

  ensure_ctx "$root"
  [[ -n "$jira" ]] || jira="${INPUT_DIR}/jira.md"
  [[ -f "$jira" ]] || die "Jira tasks file not found: ${jira}"

  local tasks_dir="${PLAN_DIR}/tasks"
  local parsed_local="${tasks_dir}/parsed.local.json"
  local tasks_db="${tasks_dir}/tasks.db"
  mkdir -p "$tasks_dir"

  info "Parsing Jira backlog → ${parsed_local}"
  gc_parse_jira_tasks "$jira" "$parsed_local"

  info "Building tasks database → ${tasks_db}"
  local db_stats
  if ! db_stats="$(gc_build_tasks_db "$parsed_local" "$tasks_db" "$force")"; then
    die "Failed to build Jira tasks SQLite database"
  fi

  local story_count=0 task_count=0 restored_stories=0 restored_tasks=0
  while IFS= read -r line; do
    case "$line" in
      STORIES\ *) story_count="${line#STORIES }" ;;
      TASKS\ *) task_count="${line#TASKS }" ;;
      RESTORED_STORIES\ *) restored_stories="${line#RESTORED_STORIES }" ;;
      RESTORED_TASKS\ *) restored_tasks="${line#RESTORED_TASKS }" ;;
    esac
  done <<<"$db_stats"

  info "Stories: ${story_count:-0} (restored: ${restored_stories:-0})"
  info "Tasks: ${task_count:-0} (restored statuses: ${restored_tasks:-0})"
  ok "Tasks database updated → ${tasks_db}"

  local legacy_manifest="${tasks_dir}/manifest.json"
  local legacy_stories="${tasks_dir}/stories"
  if [[ -f "$legacy_manifest" || -d "$legacy_stories" ]]; then
    warn "Legacy task manifest/JSON artifacts detected under ${tasks_dir}; they are no longer used now that tasks live in SQLite. Remove them when convenient to avoid confusion."
  fi
}

cmd_backlog() {
  local root="" type_arg="" item_children="" show_progress=0 task_details=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project|--root)
        root="$(abs_path "$2")"
        shift 2
        ;;
      --type)
        type_arg="$2"
        shift 2
        ;;
      --item-children)
        item_children="$2"
        shift 2
        ;;
      --progress)
        show_progress=1
        shift
        ;;
      --task-details)
        task_details="$2"
        shift 2
        ;;
      -h|--help)
        cat <<'EOUSAGE'
Usage: gpt-creator backlog [--project PATH|--root PATH]
                           [--type epics|stories]
                           [--item-children ID]
                           [--progress]
                           [--task-details ID]

Inspect the backlog stored in .gpt-creator/staging/plan/tasks/tasks.db without an interactive prompt.

  --type epics|stories     List backlog summaries (defaults to 'epics' when no other flag is provided).
  --item-children ID       Show the direct children of the epic/story identified by ID (slug, key, or ID).
  --progress               Print an overall progress bar summarising task completion.
  --task-details ID        Show a detailed view for the matching task ID (case-insensitive).

Pass any combination of the flags above; each requested view is printed sequentially.
EOUSAGE
        return 0
        ;;
      *)
        die "Unknown argument for backlog: $1"
        ;;
    esac
  done

  if [[ -z "$type_arg" && -z "$item_children" && "$show_progress" -eq 0 && -z "$task_details" ]]; then
    type_arg="epics"
  fi

  ensure_ctx "$root"
  local tasks_db="${PLAN_DIR}/tasks/tasks.db"
  if [[ ! -f "$tasks_db" ]]; then
    die "Tasks database not found at ${tasks_db}. Run 'gpt-creator create-tasks' first."
  fi

  python3 - <<'PY' "$tasks_db" "${type_arg:-}" "${item_children:-}" "$show_progress" "${task_details:-}"
import sqlite3
import sys
import textwrap

db_path, type_arg, item_children, progress_flag, task_details = sys.argv[1:6]

conn = sqlite3.connect(db_path)
conn.row_factory = sqlite3.Row

def pluralize(value, singular, plural=None):
    try:
        count = int(value or 0)
    except Exception:
        count = 0
    if count == 1:
        return f"{count} {singular}"
    return f"{count} {plural or singular + 's'}"

def empty_counts():
    return {
        "stories": 0,
        "stories_complete": 0,
        "stories_in_progress": 0,
        "stories_pending": 0,
        "tasks": 0,
        "tasks_complete": 0,
        "tasks_in_progress": 0,
        "tasks_pending": 0,
    }

def fetch_stories():
    query = """
        SELECT story_slug, story_id, story_title, epic_key, epic_title,
               status, completed_tasks, total_tasks, sequence
        FROM stories
        ORDER BY COALESCE(sequence, 0), story_title COLLATE NOCASE
    """
    return [dict(row) for row in conn.execute(query)]

def fetch_epics():
    query = """
        SELECT epic_key, epic_id, title, slug
        FROM epics
        ORDER BY title COLLATE NOCASE
    """
    return [dict(row) for row in conn.execute(query)]

def fetch_task_counts():
    query = """
        SELECT story_slug,
               COUNT(*) AS total,
               SUM(CASE WHEN LOWER(COALESCE(status, '')) = 'complete' THEN 1 ELSE 0 END) AS completed,
               SUM(CASE WHEN LOWER(COALESCE(status, '')) = 'in-progress' THEN 1 ELSE 0 END) AS in_progress
        FROM tasks
        GROUP BY story_slug
    """
    counts = {}
    for row in conn.execute(query):
        counts[row["story_slug"]] = {
            "total": row["total"] or 0,
            "completed": row["completed"] or 0,
            "in_progress": row["in_progress"] or 0,
        }
    return counts

def fetch_tasks_for_story(slug):
    query = """
        SELECT position, task_id, title, status, estimate
        FROM tasks
        WHERE story_slug = ?
        ORDER BY position
    """
    return [dict(row) for row in conn.execute(query, (slug,))]

UNASSIGNED_KEY = "__unassigned__"
UNASSIGNED_LABEL = "Unassigned backlog"

def canonical_epic_descriptor(epic_id=None, epic_slug=None, epic_title=None):
    epic_id = (epic_id or "").strip()
    epic_slug = (epic_slug or "").strip()
    epic_title = (epic_title or "").strip()
    for candidate in (epic_slug, epic_id, epic_title):
        if candidate:
            norm = candidate.lower()
            display_title = epic_title or epic_slug or epic_id
            return norm, epic_id, epic_slug, display_title
    return UNASSIGNED_KEY, "", "", UNASSIGNED_LABEL

def derive_pseudo_epic(identifier):
    if not identifier:
        return None
    clean = identifier.replace("/", "-").replace("_", "-")
    parts = [part for part in clean.split("-") if part]
    if len(parts) >= 2:
        return "-".join(parts[:2]).upper()
    if len(parts) == 1:
        return parts[0].upper()
    return None

def determine_epic_for_story(story):
    norm, epic_id, epic_slug, epic_title = canonical_epic_descriptor(
        story.get("epic_id"),
        story.get("epic_key"),
        story.get("epic_title"),
    )
    if norm != UNASSIGNED_KEY:
        return norm, epic_id, epic_slug or epic_id.lower(), epic_title

    for candidate in (story.get("story_id"), story.get("story_slug")):
        pseudo = derive_pseudo_epic((candidate or "").strip())
        if pseudo:
            return pseudo.lower(), pseudo, pseudo.lower(), pseudo
    return norm, epic_id, epic_slug, epic_title

def format_epic_label(epic_id, epic_slug, epic_title):
    epic_title = (epic_title or "").strip()
    epic_id = (epic_id or "").strip()
    epic_slug = (epic_slug or "").strip()
    if epic_title and epic_title != UNASSIGNED_LABEL:
        if epic_id and epic_id.lower() not in epic_title.lower():
            return f"{epic_title} [{epic_id}]"
        return epic_title
    if epic_id:
        return epic_id
    if epic_slug:
        return epic_slug
    return UNASSIGNED_LABEL

def summarise_epics(stories, task_counts):
    summary = {}
    for story in stories:
        norm_key, epic_id, epic_slug, epic_title = determine_epic_for_story(story)
        entry = summary.setdefault(
            norm_key,
            {
                "counts": empty_counts(),
                "epic_id": epic_id,
                "epic_slug": epic_slug,
                "epic_title": epic_title,
            },
        )
        if epic_id and not entry["epic_id"]:
            entry["epic_id"] = epic_id
        if epic_slug and not entry["epic_slug"]:
            entry["epic_slug"] = epic_slug
        story_epic_title = (story.get("epic_title") or "").strip()
        if story_epic_title and entry["epic_title"] in (UNASSIGNED_LABEL, "", None):
            entry["epic_title"] = story_epic_title

        counts = entry["counts"]
        counts["stories"] += 1
        status = (story.get("status") or "pending").strip().lower()
        if status == "complete":
            counts["stories_complete"] += 1
        elif status in {"in-progress", "in progress"}:
            counts["stories_in_progress"] += 1
        else:
            counts["stories_pending"] += 1

        counts_dict = task_counts.get(story["story_slug"], {})
        total = counts_dict.get("total", story.get("total_tasks") or 0) or 0
        completed = counts_dict.get("completed", story.get("completed_tasks") or 0) or 0
        in_progress = counts_dict.get("in_progress", 0) or 0
        pending = max(total - completed - in_progress, 0)

        counts["tasks"] += total
        counts["tasks_complete"] += completed
        counts["tasks_in_progress"] += in_progress
        counts["tasks_pending"] += pending
    return summary

def build_epic_entries(epics, stories, summary):
    epics_by_norm = {}
    for epic in epics:
        norm, epic_id, epic_slug, epic_title = canonical_epic_descriptor(
            epic.get("epic_id"),
            epic.get("epic_key") or epic.get("slug"),
            epic.get("title"),
        )
        epics_by_norm[norm] = {
            "epic_id": epic_id,
            "slug": (epic.get("slug") or epic.get("epic_key") or epic_slug or "").strip(),
            "title": epic_title or format_epic_label(epic_id, epic_slug, epic.get("title")),
            "raw": dict(epic),
        }

    stories_by_norm = {}
    for story in stories:
        norm, _, _, _ = determine_epic_for_story(story)
        stories_by_norm.setdefault(norm, []).append(story)

    entries = []
    all_keys = set(summary.keys()) | set(epics_by_norm.keys())
    if not all_keys:
        all_keys.add(UNASSIGNED_KEY)

    for norm in all_keys:
        meta = summary.get(norm, {})
        epic_info = epics_by_norm.get(norm, {})
        counts = meta.get("counts") or empty_counts()
        epic_id = meta.get("epic_id") or epic_info.get("epic_id") or ""
        epic_slug = meta.get("epic_slug") or epic_info.get("slug") or ""
        epic_title = meta.get("epic_title") or epic_info.get("title") or UNASSIGNED_LABEL
        label = format_epic_label(epic_id, epic_slug, epic_title)
        entries.append(
            {
                "key": None if norm == UNASSIGNED_KEY else norm,
                "label": label,
                "counts": counts,
                "stories": stories_by_norm.get(norm, []),
                "epic": {
                    "epic_id": epic_id,
                    "slug": epic_slug,
                    "title": epic_title,
                },
            }
        )

    entries.sort(key=lambda item: (item["key"] is None, item["label"].lower()))
    return entries

def print_table(headers, rows):
    if not rows:
        print("No records found.")
        return
    widths = [len(str(h)) for h in headers]
    for row in rows:
        for idx, cell in enumerate(row):
            widths[idx] = max(widths[idx], len(str(cell)))

    def fmt(row):
        return "  ".join(str(cell).ljust(widths[idx]) for idx, cell in enumerate(row))

    print(fmt(list(headers)))
    print("  ".join("-" * w for w in widths))
    for row in rows:
        print(fmt(row))

stories = fetch_stories()
epics = fetch_epics()
task_counts = fetch_task_counts()
summary = summarise_epics(stories, task_counts)
entries = build_epic_entries(epics, stories, summary)

stories_by_slug = {}
stories_by_id = {}
for story in stories:
    slug = (story.get("story_slug") or "").strip().lower()
    if slug:
        stories_by_slug[slug] = story
    sid = (story.get("story_id") or "").strip().lower()
    if sid:
        stories_by_id[sid] = story

epic_lookup = {}
for entry in entries:
    epic = entry.get("epic") or {}
    for candidate in (epic.get("slug"), epic.get("epic_id"), epic.get("title"), entry["label"]):
        if candidate and str(candidate).strip():
            epic_lookup[str(candidate).strip().lower()] = entry
    if entry["key"] is None:
        for alias in ("unassigned", "none", "no-epic", "noepic"):
            epic_lookup[alias] = entry

def print_epics_table():
    if not entries:
        print("No epics found in the backlog.")
        return
    headers = ["Epic ID", "Slug", "Title", "Stories", "Tasks", "Progress"]
    rows = []
    for entry in entries:
        counts = entry.get("counts") or empty_counts()
        epic = entry.get("epic") or {}
        epic_id = (epic.get("epic_id") or "").strip() or "-"
        slug = (epic.get("slug") or "").strip() or "-"
        title = entry["label"]
        stories_desc = pluralize(counts["stories"], "story", "stories")
        story_bits = []
        if counts["stories_complete"]:
            story_bits.append(f"{counts['stories_complete']} complete")
        if counts["stories_in_progress"]:
            story_bits.append(f"{counts['stories_in_progress']} in-progress")
        if counts["stories_pending"]:
            story_bits.append(f"{counts['stories_pending']} pending")
        if story_bits:
            stories_desc += f" ({', '.join(story_bits)})"

        tasks_desc = pluralize(counts["tasks"], "task")
        task_bits = []
        if counts["tasks_complete"]:
            task_bits.append(f"{counts['tasks_complete']} complete")
        if counts["tasks_in_progress"]:
            task_bits.append(f"{counts['tasks_in_progress']} in-progress")
        if counts["tasks_pending"]:
            task_bits.append(f"{counts['tasks_pending']} pending")
        if task_bits:
            tasks_desc += f" ({', '.join(task_bits)})"

        total_tasks = counts["tasks"] or 0
        progress = 0.0
        if total_tasks:
            progress = (counts["tasks_complete"] / total_tasks) * 100
        rows.append([
            epic_id,
            slug,
            title,
            stories_desc,
            tasks_desc,
            f"{progress:5.1f}%",
        ])
    print_table(headers, rows)

def print_story_children(entry, identifier):
    stories_for_epic = entry.get("stories") or []
    if not stories_for_epic:
        print(f"No stories found for epic '{identifier}'.")
        return
    headers = ["Story Slug", "Title", "Status", "Epic", "Tasks", "Progress"]
    rows = []
    for story in sorted(stories_for_epic, key=lambda s: (s.get("sequence") or 0, (s.get("story_title") or "").lower())):
        slug = (story.get("story_slug") or "").strip()
        title = (story.get("story_title") or story.get("story_id") or slug or "Story").strip()
        epic_title = (story.get("epic_title") or entry["label"]).strip()
        counts = task_counts.get(story.get("story_slug"), {})
        total = counts.get("total", story.get("total_tasks") or 0) or 0
        complete = counts.get("completed", story.get("completed_tasks") or 0) or 0
        in_progress = counts.get("in_progress", 0) or 0
        pending = max(total - complete - in_progress, 0)
        status, progress, tasks_desc = compute_story_metrics(total, complete, in_progress, pending, story.get("status"))
        rows.append([
            slug or (story.get("story_id") or "-"),
            title,
            status,
            epic_title,
            tasks_desc,
            f"{progress:5.1f}%",
        ])
    print_table(headers, rows)

def truncate(text, width=60):
    text = (text or "").strip()
    if not text:
        return "-"
    if len(text) <= width:
        return text
    return text[: width - 3] + "..."

def print_task_children(story):
    slug = story.get("story_slug")
    tasks = fetch_tasks_for_story(slug)
    if not tasks:
        print(f"No tasks found for story '{slug or story.get('story_id') or story.get('story_title')}'.")
        return
    headers = ["#", "Task ID", "Title", "Status", "Estimate"]
    rows = []
    for task in tasks:
        position = task.get("position")
        index = str((position if position is not None else 0) + 1)
        task_id = (task.get("task_id") or "").strip() or "-"
        title = truncate(task.get("title"), width=80)
        status = (task.get("status") or "pending").strip().lower().replace("_", "-")
        estimate = (task.get("estimate") or "").strip() or "-"
        rows.append([index, task_id, title, status, estimate])
    print_table(headers, rows)

def compute_story_metrics(total, complete, in_progress, pending, status_field):
    status_field = (status_field or "pending").strip().lower().replace("_", "-")
    if total > 0:
        if complete >= total and in_progress == 0 and pending == 0:
            status = "complete"
            progress = 100.0
        elif complete > 0 or in_progress > 0:
            status = "in-progress"
            progress = (complete / total) * 100
        else:
            status = "pending"
            progress = 0.0
    else:
        status = status_field or "pending"
        progress = 100.0 if status == "complete" else 0.0

    if total > 0:
        tasks_desc = f"{complete}/{total} complete"
        if in_progress:
            tasks_desc += f", {in_progress} in-progress"
        if pending and status != "complete":
            tasks_desc += f", {pending} pending"
    else:
        tasks_desc = "0 tasks"

    return status, progress, tasks_desc

def show_item_children(identifier):
    if not identifier:
        return
    ident = identifier.strip().lower()
    entry = epic_lookup.get(ident)
    if entry:
        label = entry["label"]
        epic = entry.get("epic") or {}
        epic_ident = (epic.get("epic_id") or epic.get("slug") or entry["label"] or "unassigned").strip()
        print(f"Stories for epic: {label} ({epic_ident})")
        print_story_children(entry, identifier)
        return

    if ident in stories_by_slug:
        story = stories_by_slug[ident]
    elif ident in stories_by_id:
        story = stories_by_id[ident]
    else:
        story = None

    if story:
        title = story.get("story_title") or story.get("story_id") or story.get("story_slug")
        print(f"Tasks for story: {title} ({story.get('story_slug')})")
        print_task_children(story)
        return

    print(f"No epic or story found for identifier '{identifier}'.", file=sys.stderr)
    sys.exit(1)

def print_stories_overview():
    if not stories:
        print("No stories found in the backlog.")
        return
    headers = ["Story Slug", "Story ID", "Title", "Epic", "Status", "Tasks", "Progress"]
    rows = []
    for story in sorted(
        stories,
        key=lambda s: (
            (s.get("epic_title") or "").lower(),
            s.get("sequence") or 0,
            (s.get("story_title") or "").lower(),
        ),
    ):
        slug = (story.get("story_slug") or "").strip()
        story_id = (story.get("story_id") or "").strip()
        title = (story.get("story_title") or story_id or slug or "Story").strip()
        epic_title = (story.get("epic_title") or "Unassigned").strip()
        counts = task_counts.get(story.get("story_slug"), {})
        total = counts.get("total", story.get("total_tasks") or 0) or 0
        complete = counts.get("completed", story.get("completed_tasks") or 0) or 0
        in_progress = counts.get("in_progress", 0) or 0
        pending = max(total - complete - in_progress, 0)
        status, progress, tasks_desc = compute_story_metrics(total, complete, in_progress, pending, story.get("status"))
        rows.append([
            slug or "-",
            story_id or "-",
            title,
            epic_title,
            status,
            tasks_desc,
            f"{progress:5.1f}%",
        ])
    print_table(headers, rows)

def print_task_details(task_identifier):
    if not task_identifier:
        return
    ident = task_identifier.strip().lower()
    query = """
        SELECT *
        FROM tasks
        WHERE LOWER(COALESCE(task_id, '')) = ?
           OR CAST(id AS TEXT) = ?
    """
    row = conn.execute(query, (ident, ident)).fetchone()
    if row is None:
        print(f"No task found for identifier '{task_identifier}'.", file=sys.stderr)
        sys.exit(1)

    print("Task details")
    print("------------")

    def emit(label, value):
        text = value if isinstance(value, str) else ("" if value is None else str(value))
        if isinstance(text, str):
            text = text.strip()
        print(f"{label}: {text if text else '-'}")

    emit("Task ID", row["task_id"])
    emit("Story Slug", row["story_slug"])
    emit("Story Title", row["story_title"])
    emit("Epic", row["epic_title"] or row["epic_key"])
    emit("Status", row["status"])
    emit("Estimate", row["estimate"])
    emit("Assignees", row["assignee_text"])
    emit("Tags", row["tags_text"])
    emit("Dependencies", row["dependencies_text"])
    emit("Story Points", row["story_points"])
    emit("Document Reference", row["document_reference"])
    emit("Idempotency", row["idempotency"])
    emit("Rate Limits", row["rate_limits"])
    emit("RBAC", row["rbac"])
    emit("Messaging/Workflows", row["messaging_workflows"])
    emit("Performance Targets", row["performance_targets"])
    emit("Observability", row["observability"])
    emit("Endpoints", row["endpoints"])
    emit("Sample Create Request", row["sample_create_request"])
    emit("Sample Create Response", row["sample_create_response"])
    emit("Acceptance Criteria", row["acceptance_text"])
    emit("User Story Ref", row["user_story_ref_id"])
    emit("Epic Ref", row["epic_ref_id"])
    emit("Started At", row["started_at"])
    emit("Completed At", row["completed_at"])
    emit("Last Run", row["last_run"])
    emit("Created At", row["created_at"])
    emit("Updated At", row["updated_at"])

def print_progress():
    row = conn.execute(
        """
        SELECT
          COUNT(*) AS total,
          SUM(CASE WHEN LOWER(COALESCE(status, '')) = 'complete' THEN 1 ELSE 0 END) AS complete,
          SUM(CASE WHEN LOWER(COALESCE(status, '')) = 'in-progress' THEN 1 ELSE 0 END) AS in_progress
        FROM tasks
        """
    ).fetchone()
    total = row["total"] or 0
    complete = row["complete"] or 0
    in_progress = row["in_progress"] or 0
    pending = max(total - complete - in_progress, 0)
    percent = (complete / total * 100) if total else 0.0
    bar_length = 30
    filled_units = int(round((percent / 100) * bar_length))
    filled_units = min(bar_length, max(0, filled_units))
    bar = "#" * filled_units + "-" * (bar_length - filled_units)
    print("Overall backlog progress")
    print(f"Tasks complete: {complete}/{total} ({percent:0.1f}%)")
    print(f"In-progress: {in_progress}, Pending: {pending}")
    print(f"[{bar}]")

try:
    printed = False
    if type_arg:
        t = type_arg.strip().lower()
        if t == "epics":
            print_epics_table()
            printed = True
        elif t == "stories":
            print_stories_overview()
            printed = True
        else:
            print(f"Unsupported backlog type: {type_arg}", file=sys.stderr)
            sys.exit(1)
        if printed and (item_children or progress_flag == "1" or task_details):
            print()
    if item_children:
        show_item_children(item_children)
        printed = True
        if progress_flag == "1" or task_details:
            print()
    if progress_flag == "1":
        print_progress()
        printed = True
        if task_details:
            print()
    if task_details:
        print_task_details(task_details)
finally:
    conn.close()
PY
}

cmd_migrate_tasks_json() {
  local root="" force=0
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --force) force=1; shift;;
      -h|--help)
        cat <<'EOUSAGE'
Usage: gpt-creator migrate-tasks [--project PATH] [--force]

Populate the tasks SQLite database from the JSON outputs produced by
`gpt-creator create-jira-tasks` (under plan/create-jira-tasks/json).

Options:
  --project PATH  Project root (defaults to current directory)
  --force         Rebuild the database without restoring prior task status metadata
EOUSAGE
        return 0
        ;;
      *) break;;
    esac
  done

  ensure_ctx "$root"

  local pipeline_dir="${PLAN_DIR}/create-jira-tasks"
  local json_dir="${pipeline_dir}/json"
  local epics_json="${json_dir}/epics.json"
  local stories_dir="${json_dir}/stories"
  local tasks_dir="${json_dir}/tasks"

  [[ -f "$epics_json" ]] || die "Epics JSON not found: ${epics_json}"
  [[ -d "$stories_dir" ]] || die "Stories JSON directory not found: ${stories_dir}"
  [[ -d "$tasks_dir" ]] || die "Tasks JSON directory not found: ${tasks_dir}"

  local payload="${json_dir}/tasks_payload.json"
  local tasks_workspace="${PLAN_DIR}/tasks"
  mkdir -p "$tasks_workspace"
  local db_path="${tasks_workspace}/tasks.db"

  info "Building tasks payload from JSON → ${payload}"
  info "Updating tasks database → ${db_path}"

  if ! gc_rebuild_tasks_db_from_json "$force"; then
    die "Failed to rebuild tasks database from JSON payload"
  fi

  ok "Tasks database updated → ${db_path}"
}

cmd_refine_tasks() {
  local root="" story_filter="" model_override="" dry_run=0 force=0
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --story) story_filter="$2"; shift 2;;
      --model) model_override="$2"; shift 2;;
      --dry-run) dry_run=1; shift;;
      --force) force=1; shift;;
      -h|--help)
        cat <<'EOUSAGE'
Usage: gpt-creator refine-tasks [--project PATH] [--story SLUG] [--model NAME] [--dry-run] [--force]

Refine tasks stored in the SQLite backlog using Codex, updating each task
record and the refined JSON artifacts as soon as a task is enriched.

Options:
  --project PATH  Project root (defaults to current directory)
  --story SLUG    Limit refinement to a single story slug (optional)
  --model NAME    Override Codex model (defaults to CODEX_MODEL/GC defaults)
  --dry-run       Build prompts without invoking Codex
  --force         Reset refinement progress and reprocess every task
EOUSAGE
        return 0
        ;;
      *) break;;
    esac
  done

  ensure_ctx "$root"

  local tasks_db="${PLAN_DIR}/tasks/tasks.db"
  [[ -f "$tasks_db" ]] || die "Tasks database not found: ${tasks_db}"

  local pipeline_dir="${PLAN_DIR}/create-jira-tasks"
  local json_tasks_dir="${pipeline_dir}/json/tasks"
  [[ -d "$json_tasks_dir" ]] || die "Tasks JSON directory not found: ${json_tasks_dir}"

  local have_refined
  have_refined=$(python3 - <<'PY' "$tasks_db"
import sqlite3, sys

path = sys.argv[1]
conn = sqlite3.connect(path)
cur = conn.cursor()
cur.execute("PRAGMA table_info(tasks)")
cols = {row[1] for row in cur.fetchall()}
added = False
if "refined" not in cols:
    cur.execute("ALTER TABLE tasks ADD COLUMN refined INTEGER DEFAULT 0")
    added = True
if "refined_at" not in cols:
    cur.execute("ALTER TABLE tasks ADD COLUMN refined_at TEXT")
    added = True
if added:
    conn.commit()
cur.execute("PRAGMA table_info(tasks)")
cols = {row[1] for row in cur.fetchall()}
conn.close()
print("1" if "refined" in cols else "0")
PY
  )

  local summary
  summary=$(python3 - <<'PY' "$tasks_db" "$story_filter"
import sqlite3, sys

db_path = sys.argv[1]
filters_raw = sys.argv[2] if len(sys.argv) > 2 else ''
filters = {value.strip().lower() for value in filters_raw.split(',') if value.strip()}

conn = sqlite3.connect(db_path)
cur = conn.cursor()

cur.execute("PRAGMA table_info(tasks)")
cols = {row[1] for row in cur.fetchall()}
have_refined = 'refined' in cols

story_map = {
    (row[0] or '').strip(): (row[1] or '').strip().lower()
    for row in cur.execute("SELECT story_slug, story_id FROM stories")
}

def story_in_scope(slug: str) -> bool:
    lower = slug.lower()
    if not filters:
        return True
    if lower in filters:
        return True
    story_id_lower = story_map.get(slug, '')
    if story_id_lower in filters:
        return True
    return False

if have_refined:
    rows = cur.execute("SELECT story_slug, COALESCE(refined, 0) FROM tasks").fetchall()
else:
    rows = [(slug, 0) for (slug,) in cur.execute("SELECT story_slug FROM tasks").fetchall()]
conn.close()

total_tasks = 0
refined_tasks = 0
stories_total = set()
stories_pending = set()

for slug, refined in rows:
    slug = (slug or '').strip()
    if not slug or not story_in_scope(slug):
        continue
    stories_total.add(slug)
    total_tasks += 1
    try:
        refined_value = int(refined)
    except Exception:
        refined_value = 0
    if refined_value:
        refined_tasks += 1
    else:
        stories_pending.add(slug)

pending_tasks = total_tasks - refined_tasks
print(total_tasks, refined_tasks, pending_tasks, len(stories_total), len(stories_pending))
PY
  ) || die "Failed to summarise tasks backlog"

  local total_tasks refined_tasks pending_tasks total_stories pending_stories
  read -r total_tasks refined_tasks pending_tasks total_stories pending_stories <<<"$summary"

  if (( force )); then
    python3 - <<'PY' "$tasks_db"
import sqlite3, sys
conn = sqlite3.connect(sys.argv[1])
cur = conn.cursor()
cur.execute("UPDATE tasks SET refined = 0, refined_at = NULL")
conn.commit()
conn.close()
PY
    refined_tasks=0
    pending_tasks=$total_tasks
  fi

  info "Backlog summary → tasks: total=${total_tasks}, refined=${refined_tasks}, pending=${pending_tasks}; stories: total=${total_stories}, pending=${pending_stories}${story_filter:+ (filter='${story_filter}')}."

  local codex_cmd="${CODEX_BIN:-${CODEX_CMD:-codex}}"
  if (( dry_run == 0 )) && ! command -v "$codex_cmd" >/dev/null 2>&1; then
    warn "Codex CLI '$codex_cmd' not found; switching to --dry-run."
    dry_run=1
  fi

  local model_name="${model_override:-${CODEX_MODEL:-$GC_DEFAULT_MODEL}}"

  # shellcheck source=src/lib/create-jira-tasks/pipeline.sh
  source "${CLI_ROOT}/src/lib/create-jira-tasks/pipeline.sh"

  local force_flag=0
  local skip_refine=0
  local dry_flag="$dry_run"
  cjt::init "$PROJECT_ROOT" "$model_name" "$force_flag" "$skip_refine" "$dry_flag"
  CJT_DOC_FILES=()
  cjt::build_context_files

  CJT_SYNC_DB=1
  CJT_TASKS_DB_PATH="$tasks_db"
  CJT_IGNORE_REFINE_STATE=1
  CJT_REFINE_FORCE=$force
  CJT_HAVE_REFINED_COLUMN="$have_refined"
  CJT_REFINE_TOTAL_TASKS="$total_tasks"
  CJT_REFINE_REFINED_TASKS="$refined_tasks"
  CJT_REFINE_PENDING_TASKS="$pending_tasks"
  CJT_REFINE_TOTAL_STORIES="$total_stories"
  CJT_REFINE_PENDING_STORIES="$pending_stories"
  if [[ -n "$story_filter" ]]; then
    CJT_ONLY_STORY_SLUG="$story_filter"
  fi

  cjt::refine_tasks
  ok "Task refinement complete"
}

gc_write_task_prompt() {
  local db_path="${1:?tasks db path required}"
  local story_slug="${2:?story slug required}"
  local task_index="${3:?task index required}"
  local prompt_path="${4:?prompt path required}"
  local context_tail="${5:-}"
  local model_name="${6:-$CODEX_MODEL}"
  local project_root="${7:-$PROJECT_ROOT}"
  local staging_dir="${8:-$GC_STAGING_DIR}"
  python3 - "$db_path" "$story_slug" "$task_index" "$prompt_path" "$context_tail" "$model_name" "$project_root" "$staging_dir" <<'PY'
import json
import os
import re
import sqlite3
import sys
from pathlib import Path

DB_PATH, STORY_SLUG, TASK_INDEX, PROMPT_PATH, CONTEXT_TAIL_PATH, MODEL_NAME, PROJECT_ROOT, STAGING_DIR = sys.argv[1:9]
TASK_INDEX = int(TASK_INDEX)

conn = sqlite3.connect(DB_PATH)
conn.row_factory = sqlite3.Row
cur = conn.cursor()

story_row = cur.execute(
    'SELECT story_id, story_title, epic_key, epic_title, sequence FROM stories WHERE story_slug = ?',
    (STORY_SLUG,)
).fetchone()
if story_row is None:
    raise SystemExit(f"Story slug not found: {STORY_SLUG}")

task_rows = cur.execute(
    'SELECT task_id, title, description, estimate, assignees_json, tags_json, acceptance_json, dependencies_json, '
    'tags_text, story_points, dependencies_text, assignee_text, document_reference, idempotency, rate_limits, rbac, '
    'messaging_workflows, performance_targets, observability, acceptance_text, endpoints, sample_create_request, '
    'sample_create_response, user_story_ref_id, epic_ref_id '
    'FROM tasks WHERE story_slug = ? ORDER BY position ASC',
    (STORY_SLUG,)
).fetchall()
conn.close()

if TASK_INDEX < 0 or TASK_INDEX >= len(task_rows):
    raise SystemExit(2)

task = task_rows[TASK_INDEX]

def parse_json_list(value):
    if not value:
        return []
    try:
        parsed = json.loads(value)
        if isinstance(parsed, list):
            return [str(item).strip() for item in parsed if str(item).strip()]
    except Exception:
        pass
    return []

def clean(value: str) -> str:
    return (value or '').strip()


def project_display_name(root: str) -> str:
    if not root:
        return "Project"
    try:
        name = Path(root).name.strip()
    except Exception:
        name = ""
    if not name:
        return "Project"
    tokens = [token for token in re.split(r'[^A-Za-z0-9]+', name) if token]
    if not tokens:
        return "Project"
    words = []
    for token in tokens:
        if len(token) <= 3:
            words.append(token.upper())
        elif token.isupper():
            words.append(token)
        else:
            words.append(token.capitalize())
    return ' '.join(words) or "Project"

assignees = parse_json_list(task['assignees_json'])
tags = parse_json_list(task['tags_json'])
acceptance = parse_json_list(task['acceptance_json'])
dependencies = parse_json_list(task['dependencies_json'])

description = clean(task['description'])
if description:
    description_lines = description.splitlines()
else:
    description_lines = []

tags_text = clean(task['tags_text'])
story_points = clean(task['story_points'])
dependencies_text = clean(task['dependencies_text'])
assignee_text = clean(task['assignee_text'])
document_reference = clean(task['document_reference'])
idempotency_text = clean(task['idempotency'])
rate_limits = clean(task['rate_limits'])
rbac_text = clean(task['rbac'])
messaging_workflows = clean(task['messaging_workflows'])
performance_targets = clean(task['performance_targets'])
observability_text = clean(task['observability'])
acceptance_text_extra = (task['acceptance_text'] or '').strip() if task['acceptance_text'] else ''
endpoints_text = (task['endpoints'] or '').strip() if task['endpoints'] else ''
sample_create_request = (task['sample_create_request'] or '').strip() if task['sample_create_request'] else ''
sample_create_response = (task['sample_create_response'] or '').strip() if task['sample_create_response'] else ''
user_story_ref_id = clean(task['user_story_ref_id'])
epic_ref_id = clean(task['epic_ref_id'])

project_display = project_display_name(PROJECT_ROOT)
repo_path = PROJECT_ROOT or '.'

sample_limit_env = os.getenv("GC_PROMPT_SAMPLE_LINES", "").strip()
try:
    sample_limit = int(sample_limit_env) if sample_limit_env else 80
except ValueError:
    sample_limit = 80
if sample_limit < 0:
    sample_limit = 0

compact_mode = os.getenv("GC_PROMPT_COMPACT", "").strip().lower() not in {"", "0", "false"}

lines = []
lines.append(f"# You are Codex (model: {MODEL_NAME})")
lines.append("")
lines.append(f"You are assisting the {project_display} delivery team. Implement the task precisely using the repository at: {repo_path}")
lines.append("")
lines.append("## Story")

epic_id = clean(story_row['epic_key'])
epic_title = clean(story_row['epic_title'])
story_id = clean(story_row['story_id'])
story_title = clean(story_row['story_title'])
sequence = story_row['sequence']

if compact_mode:
    story_label = story_id or STORY_SLUG
    if story_label and story_title:
        summary = f"- {story_label} — {story_title}"
    elif story_label:
        summary = f"- {story_label}"
    elif story_title:
        summary = f"- {story_title}"
    else:
        summary = "- Story details unavailable"
    extras = []
    if epic_id or epic_title:
        epic_bits = [bit for bit in [epic_id, epic_title] if bit]
        extras.append("epic " + " — ".join(epic_bits))
    if sequence:
        extras.append(f"order {sequence}")
    if extras:
        summary += f" ({'; '.join(extras)})"
    lines.append(summary)
else:
    if epic_id or epic_title:
        parts = [part for part in [epic_id, epic_title] if part]
        lines.append("- Epic: " + " — ".join(parts))
    if story_id or story_title:
        parts = [part for part in [story_id, story_title] if part]
        lines.append("- Story: " + " — ".join(parts))
    if sequence:
        lines.append(f"- Story order: {sequence}")

lines.append("")
lines.append("## Task")
task_id = clean(task['task_id'])
task_title = clean(task['title'])
estimate = clean(task['estimate'])

if compact_mode:
    task_label = task_id or f"Task {TASK_INDEX + 1}"
    summary = f"- {task_label}"
    if task_title:
        summary += f" — {task_title}"
    lines.append(summary)
    meta_bits = []
    if estimate:
        meta_bits.append(f"estimate {estimate}")
    if story_points and story_points != estimate:
        meta_bits.append(f"story points {story_points}")
    if assignees or assignee_text:
        assigned = ", ".join(assignees) if assignees else assignee_text
        meta_bits.append(f"assignees {assigned}")
    if tags:
        tags_summary = ", ".join(tags[:3])
        if len(tags) > 3:
            tags_summary += "…"
        meta_bits.append(f"tags {tags_summary}")
    elif tags_text:
        meta_bits.append(f"tags {tags_text}")
    if document_reference:
        meta_bits.append(f"doc {document_reference}")
    if rate_limits:
        meta_bits.append(f"rate limits {rate_limits}")
    if meta_bits:
        lines.append(f"- Details: {'; '.join(meta_bits)}")
else:
    if task_id:
        lines.append(f"- Task ID: {task_id}")
    if task_title:
        lines.append(f"- Title: {task_title}")
    if estimate:
        lines.append(f"- Estimate: {estimate}")
    if assignees:
        lines.append("- Assignees: " + ", ".join(assignees))
    elif assignee_text:
        lines.append(f"- Assignee: {assignee_text}")
    if tags:
        lines.append("- Tags: " + ", ".join(tags))
    elif tags_text:
        lines.append(f"- Tags: {tags_text}")
    if story_points and story_points != estimate:
        lines.append(f"- Story points: {story_points}")
    elif story_points and not estimate:
        lines.append(f"- Story points: {story_points}")
    if document_reference:
        lines.append(f"- Document reference: {document_reference}")
    if idempotency_text:
        lines.append(f"- Idempotency: {idempotency_text}")
    if rate_limits:
        lines.append(f"- Rate limits: {rate_limits}")
    if rbac_text:
        lines.append(f"- RBAC: {rbac_text}")
    if messaging_workflows:
        lines.append(f"- Messaging & workflows: {messaging_workflows}")
    if performance_targets:
        lines.append(f"- Performance targets: {performance_targets}")
    if observability_text:
        lines.append(f"- Observability: {observability_text}")
    if user_story_ref_id and user_story_ref_id.lower() != story_id.lower():
        lines.append(f"- User story reference ID: {user_story_ref_id}")
    if epic_ref_id and epic_ref_id.lower() != epic_id.lower():
        lines.append(f"- Epic reference ID: {epic_ref_id}")

lines.append("")
lines.append("### Description")
if description_lines:
    lines.extend(description_lines)
else:
    lines.append("(No additional description provided.)")

if acceptance:
    lines.append("")
    lines.append("### Acceptance Criteria")
    for item in acceptance:
        lines.append(f"- {item}")
elif acceptance_text_extra:
    lines.append("")
    lines.append("### Acceptance Criteria")
    lines.extend(acceptance_text_extra.splitlines())

if dependencies:
    lines.append("")
    lines.append("### Dependencies")
    for dep in dependencies:
        lines.append(f"- {dep}")
elif dependencies_text:
    lines.append("")
    lines.append("### Dependencies")
    lines.extend(dependencies_text.splitlines())

if endpoints_text:
    lines.append("")
    lines.append("### Endpoints")
    lines.extend(endpoints_text.splitlines())

doc_snippets_enabled = os.getenv("GC_PROMPT_DOC_SNIPPETS", "").strip().lower() not in {"", "0", "false"}
staging_root = Path(STAGING_DIR).resolve() if STAGING_DIR else None
project_root_path = Path(PROJECT_ROOT).resolve() if PROJECT_ROOT else None

def _split_items(raw: str):
    if not raw:
        return []
    items = re.split(r'[\n;,]+', raw)
    return [item.strip() for item in items if item and item.strip()]

def _collect_candidate_files(ref: str):
    candidates = []
    if not ref:
        return candidates
    ref_stripped = ref.strip()
    ref_lower = ref_stripped.lower()

    def add_path(candidate: Path):
        try:
            resolved = candidate.resolve()
        except Exception:
            resolved = candidate
        if resolved.is_file():
            if resolved not in candidates:
                candidates.append(resolved)

    # Direct path attempts relative to project or staging roots
    for base in filter(None, [project_root_path, staging_root]):
        candidate = base / ref_stripped
        if candidate.is_file():
            add_path(candidate)
    # If ref looks like filename only, search staging dir for matches
    if staging_root and ("." in ref_stripped or "/" not in ref_stripped):
        for match in staging_root.rglob(ref_stripped):
            add_path(match)

    keyword_map = {
        "sds": ["sds.*"],
        "pdr": ["pdr.*"],
        "openapi": ["openapi.*"],
        "swagger": ["openapi.*"],
        "erd": ["*.mmd"],
        "mermaid": ["*.mmd"],
        "schema": ["*.sql", "*.yaml", "*.yml"],
    }
    if staging_root:
        for keyword, patterns in keyword_map.items():
            if keyword in ref_lower:
                for pattern in patterns:
                    for match in staging_root.glob(pattern):
                        add_path(match)

    return candidates

def _extract_snippet(path: Path, term: str, limit: int):
    if limit <= 0:
        return ([], False)
    try:
        text = path.read_text(encoding="utf-8", errors="replace")
    except Exception as exc:
        return ([f"(failed to read {path.name}: {exc})"], False)
    lines_local = text.splitlines()
    if not lines_local:
        return ([], False)
    search_terms: list[str] = []
    if term:
        search_terms.append(term.strip())
        search_terms.extend(
            [token for token in re.split(r'[^a-z0-9/_\-.]+', term.lower()) if len(token) >= 3]
        )

    match_index = None
    for needle in search_terms:
        if not needle:
            continue
        needle_lower = needle.lower()
        for idx, line in enumerate(lines_local):
            if needle_lower in line.lower():
                match_index = idx
                break
        if match_index is not None:
            break

    if match_index is None:
        start = 0
    else:
        start = max(0, match_index - max(5, limit // 2))
    end = min(len(lines_local), start + limit)
    snippet = lines_local[start:end]
    truncated = end < len(lines_local)
    if match_index is not None and start > 0:
        snippet.insert(0, "... (preceding lines omitted)")
    if truncated:
        snippet.append("... (additional content truncated)")
    return (snippet, truncated)

def _minify_payload(value: str) -> str:
    if not value:
        return ""
    raw = value.strip()
    if not raw:
        return ""
    try:
        parsed = json.loads(raw)
    except Exception:
        return raw
    try:
        return json.dumps(parsed, separators=(",", ":"), ensure_ascii=False)
    except Exception:
        return raw

def _chunk_text(text: str, width: int = 160) -> list[str]:
    if not text:
        return []
    return [text[i:i + width] for i in range(0, len(text), width)]

def _normalise_space(text: str) -> str:
    return re.sub(r"\s+", " ", text or "").strip()

def _condense_snippet(snippet_lines, term, max_chars=420):
    core = " ".join(line.strip() for line in snippet_lines if line.strip())
    if not core:
        return ""
    core = _normalise_space(core)
    if term:
        lowered = core.lower()
        idx = lowered.find(term.lower())
        if idx > 0:
            start = max(0, idx - 180)
            core = core[start:]
    sentences = re.split(r'(?<=[.!?])\s+', core)
    assembled: list[str] = []
    total = 0
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
        assembled.append(sentence)
        total += len(sentence)
        if total >= max_chars:
            break
    summary = " ".join(assembled) if assembled else core
    summary = summary.strip()
    if len(summary) > max_chars:
        summary = summary[:max_chars].rstrip() + "…"
    return summary

def append_sample_section(title: str, value: str):
    if not value:
        return
    lines.append("")
    heading = f"### {title}"
    payload = _minify_payload(value)
    if sample_limit <= 0:
        digest_src = payload.encode("utf-8", "replace")
        digest = hashlib.sha256(digest_src).hexdigest()[:12]
        preview = payload[:120]
        if payload and len(payload) > 120:
            preview = preview.rstrip() + "…"
        lines.append(f"{heading} (digest — pass --sample-lines N to view payload)")
        if preview:
            preview_clean = preview.replace("\n", " ").strip()
            lines.append(f"- preview: `{preview_clean}`")
        source_lines = len(value.splitlines()) or 1
        lines.append(f"- original lines: {source_lines}; minified chars: {len(payload)}")
        lines.append(f"- sha256: {digest}")
        return

    sample_chunks = _chunk_text(payload)
    truncated = 0
    if sample_limit and len(sample_chunks) > sample_limit:
        truncated = len(sample_chunks) - sample_limit
        sample_chunks = sample_chunks[:sample_limit]
        heading = f"{heading} (first {sample_limit} chunk{'s' if sample_limit != 1 else ''} of minified payload)"

    lines.append(heading)
    if sample_chunks:
        lines.extend(sample_chunks)
    else:
        lines.append("(payload empty after normalisation)")
    if truncated:
        lines.append(f"... ({truncated} additional chunk{'s' if truncated != 1 else ''} truncated)")
        lines.append("... (raise --sample-lines to include more of the payload)")

if sample_create_request:
    append_sample_section("Sample Create Request", sample_create_request)

if sample_create_response:
    append_sample_section("Sample Create Response", sample_create_response)

doc_snippet_limit = 24 if sample_limit == 0 else max(12, min(30, sample_limit * 2))
doc_snippets = []
if doc_snippets_enabled and (staging_root or project_root_path):
    def _record_snippet(title: str, snippet_lines: list[str], term_label: str) -> None:
        if not snippet_lines:
            return
        digest = hashlib.sha256("\n".join(snippet_lines).encode("utf-8", "replace")).hexdigest()[:12]
        if digest in seen_digests:
            return
        summary = _condense_snippet(snippet_lines, term_label)
        if not summary:
            return
        seen_digests.add(digest)
        doc_snippets.append({
            "title": title,
            "summary": summary,
            "hash": digest,
        })

    seen_pairs = set()
    seen_digests = set()
    references = _split_items(document_reference)
    endpoints_list = _split_items(endpoints_text)
    max_snippets = 3

    for reference in references:
        for candidate in _collect_candidate_files(reference):
            key = (str(candidate), reference.lower())
            if key in seen_pairs:
                continue
            snippet_lines, _ = _extract_snippet(candidate, reference, doc_snippet_limit)
            _record_snippet(f"{candidate.name} — {reference}", snippet_lines, reference)
            seen_pairs.add(key)
            if len(doc_snippets) >= max_snippets:
                break
        if len(doc_snippets) >= max_snippets:
            break

    if staging_root and len(doc_snippets) < max_snippets and endpoints_list:
        openapi_files = list(staging_root.glob("openapi.*"))
        for endpoint in endpoints_list:
            for candidate in openapi_files:
                key = (str(candidate), f"endpoint:{endpoint.lower()}")
                if key in seen_pairs:
                    continue
                snippet_lines, found = _extract_snippet(candidate, endpoint, doc_snippet_limit)
                label = f"{candidate.name} — endpoint {endpoint}"
                if not found:
                    label += " (approximate match)"
                _record_snippet(label, snippet_lines, endpoint)
                seen_pairs.add(key)
                if len(doc_snippets) >= max_snippets:
                    break
            if len(doc_snippets) >= max_snippets:
                break

if doc_snippets:
    lines.append("")
    lines.append("## Referenced Docs")
    for snippet in doc_snippets:
        lines.append(f"### {snippet['title']} (sha256 {snippet['hash']})")
        lines.append(f"- {snippet['summary']}")
        lines.append("")

lines.append("")

lines.append("## Instructions")
if compact_mode:
    lines.append('- Respond with JSON only: {"plan":[], "changes":[], "commands":[], "notes":[]}.')
    lines.append("- Keep plan bullets brief; record blockers or follow-ups in `notes`.")
    lines.append("- Apply repository changes directly and confirm acceptance criteria (note gaps in `notes`).")
    lines.append("- Prefer pnpm for scripts; mention commands that cannot run because of network limits.")

    lines.append("")
    lines.append("## Change Format")
    lines.append("- Use unified diffs or full file bodies inside the `changes` array.")
    lines.append("- Omit keys with no content; no markdown fences or extra prose.")
else:
    lines.append("- Draft a short plan before modifying files.")
    lines.append("- Apply changes directly in the repository; commits are not required.")
    lines.append("- Respond with structured JSON described below — no markdown fencing or prose outside the JSON.")
    lines.append("- Verify acceptance criteria before finishing.")
    lines.append("- If blocked, explain why and suggest next steps inside the JSON response.")
    lines.append("- Prefer pnpm for install/build scripts; avoid npm/yarn unless explicitly required.")
    lines.append("- Assume limited network access; note any commands that cannot run for that reason instead of failing silently.")

    lines.append("")
    lines.append("## Output JSON schema")
    lines.append("Return a single JSON object with keys exactly as follows (omit null/empty collections when not needed):")
    lines.append("{")
    lines.append("  \"plan\": [\"short step-by-step plan items...\"],")
    lines.append("  \"changes\": [")
    lines.append("    { \"type\": \"patch\", \"path\": \"relative/file/path\", \"diff\": \"UNIFIED_DIFF\" },")
    lines.append("    { \"type\": \"file\", \"path\": \"relative/file/path\", \"content\": \"entire file content\" }")
    lines.append("  ],")
    lines.append("  \"commands\": [\"optional shell commands to run (e.g., pnpm install)\"],")
    lines.append("  \"notes\": [\"follow-up items or blockers\"]")
    lines.append("}")
    lines.append("- Use UTF-8, escape newlines as \\n inside JSON strings.")
    lines.append("- Diff entries must be valid unified diffs (git apply compatible) against the current workspace.")
    lines.append("- File entries provide the complete desired file content (for new or fully rewritten files).")
    lines.append("- Do not emit markdown fences, commentary, or additional text outside the JSON object.")

if CONTEXT_TAIL_PATH:
    context_path = Path(CONTEXT_TAIL_PATH)
    if context_path.exists():
        tail_text = context_path.read_text(encoding='utf-8').splitlines()
        tail_mode = os.getenv("GC_CONTEXT_TAIL_MODE", "digest").strip().lower()
        tail_limit = os.getenv("GC_CONTEXT_TAIL_LIMIT", "").strip()
        if tail_mode == "digest":
            heading = "## Shared Context Digest"
        elif tail_mode == "raw":
            heading = "## Shared Context Tail"
            if tail_limit and tail_limit.isdigit():
                heading += f" (last {int(tail_limit)} line{'s' if int(tail_limit) != 1 else ''})"
        else:
            heading = "## Shared Context"
        lines.append("")
        lines.append(heading)
        lines.append("")
        lines.extend(tail_text)

prompt_path = Path(PROMPT_PATH)
prompt_path.parent.mkdir(parents=True, exist_ok=True)
prompt_path.write_text("\n".join(lines) + "\n", encoding='utf-8')

print(f"{task_id}\t{task_title}")
PY
}

# Return success (0) when note language implies manual follow-up is required.
gc_note_requires_followup() {
  local note_text="${1:-}"
  local note_lower="${note_text,,}"

  [[ -n "$note_lower" ]] || return 1

  if [[ "$note_lower" == *"parse-error"* ]]; then
    return 0
  fi

  if [[ "$note_lower" == *"no manual"* || "$note_lower" == *"no manual steps"* || "$note_lower" == *"no manual verification"* || "$note_lower" == *"no manual review"* || "$note_lower" == *"no review needed"* || "$note_lower" == *"no review required"* || "$note_lower" == *"no action required"* || "$note_lower" == *"no follow-up"* || "$note_lower" == *"no follow up"* ]]; then
    return 1
  fi

  if [[ "$note_lower" == *"optional"* || "$note_lower" == *"recommended"* || "$note_lower" == *"informational"* || "$note_lower" == *"for reference"* ]]; then
    if [[ "$note_lower" != *"requires"* && "$note_lower" != *"required"* && "$note_lower" != *"must"* && "$note_lower" != *"need"* && "$note_lower" != *"needs"* && "$note_lower" != *"needed"* && "$note_lower" != *"blocked"* && "$note_lower" != *"blocker"* && "$note_lower" != *"pending"* ]]; then
      return 1
    fi
  fi

  if [[ "$note_lower" == *"error"* || "$note_lower" == *"failure"* || "$note_lower" == *"failed"* ]]; then
    if [[ "$note_lower" == *"no error"* || "$note_lower" == *"no errors"* || "$note_lower" == *"without error"* || "$note_lower" == *"error free"* || "$note_lower" == *"error-free"* || "$note_lower" == *"errors resolved"* || "$note_lower" == *"errors addressed"* || "$note_lower" == *"failure resolved"* || "$note_lower" == *"failure addressed"* ]]; then
      :
    else
      return 0
    fi
  fi

  local contains_manual=0
  if [[ "$note_lower" == *"manual"* || "$note_lower" == *"manually"* ]]; then
    contains_manual=1
  fi

  if (( contains_manual )); then
    if [[ "$note_lower" == *"manual steps optional"* || "$note_lower" == *"manual testing optional"* || "$note_lower" == *"manual qa optional"* || "$note_lower" == *"manual testing recommended"* || "$note_lower" == *"manual qa recommended"* || "$note_lower" == *"manual review optional"* || "$note_lower" == *"manual verification optional"* ]]; then
      contains_manual=0
    fi
  fi

  if (( contains_manual )); then
    local -a manual_triggers=(
      "requires"
      "required"
      "require"
      "must"
      "need"
      "needs"
      "needed"
      "manual follow-up"
      "manual follow up"
      "manual followup"
      "manual verification"
      "manual review"
      "manual steps"
      "manual patch"
      "manual fix"
      "manual merge"
      "manual deploy"
      "manual migration"
      "manual intervention"
      "manual action"
      "apply manually"
      "manually apply"
      "manually patch"
      "manually merge"
      "manually verify"
      "could not"
      "can't"
      "cannot"
      "unable"
      "failed"
      "failure"
      "todo"
      "tbd"
      "pending"
      "block"
      "blocked"
      "follow-up"
      "follow up"
      "followup"
    )
    for trigger in "${manual_triggers[@]}"; do
      if [[ "$note_lower" == *"$trigger"* ]]; then
        return 0
      fi
    done
  fi

  if [[ "$note_lower" == *"review"* ]]; then
    if [[ "$note_lower" == *"no review"* || "$note_lower" == *"reviewed"* ]]; then
      return 1
    fi
    local -a review_triggers=(
      "needs review"
      "need review"
      "required review"
      "requires review"
      "review required"
      "pending review"
      "awaiting review"
      "please review"
      "for review"
      "manual review"
      "review manually"
      "review and apply"
      "review this change"
    )
    for trigger in "${review_triggers[@]}"; do
      if [[ "$note_lower" == *"$trigger"* ]]; then
        return 0
      fi
    done
  fi

  return 1
}

cmd_work_on_tasks() {
  local root="" resume=1 story_filter="" start_task_ref="" no_verify=0 keep_artifacts=0 memory_cycle=0 force_reset=0
  local batch_size=0 sleep_between=0 context_lines=400 context_file_lines=200 prompt_compact=1 sample_lines=0 doc_snippets=0
  local context_mode="digest"
  local -a context_mode_flags=()
  local -a context_mode_choice_flag=()
  local -a context_file_flag=()
  local -a context_skip_flags=()
  local -a context_skip_patterns=()
  local -a prompt_mode_flag=()
  local -a sample_lines_flag=()
  local -a doc_snippets_flag=()

  local codex_timeout_default=600
  local codex_timeout_value="${GC_CODEX_EXEC_TIMEOUT:-}"
  if [[ -z "$codex_timeout_value" ]]; then
    GC_CODEX_EXEC_TIMEOUT="$codex_timeout_default"
  elif [[ "$codex_timeout_value" =~ ^[0-9]+$ ]]; then
    if (( codex_timeout_value <= 0 )); then
      warn "GC_CODEX_EXEC_TIMEOUT (idle timeout) must be greater than zero; defaulting to ${codex_timeout_default}s."
      GC_CODEX_EXEC_TIMEOUT="$codex_timeout_default"
    fi
  else
    warn "GC_CODEX_EXEC_TIMEOUT ('${codex_timeout_value}') is not numeric; defaulting idle timeout to ${codex_timeout_default}s."
    GC_CODEX_EXEC_TIMEOUT="$codex_timeout_default"
  fi
  local codex_timeout_seconds="${GC_CODEX_EXEC_TIMEOUT}"

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --story|--from-story) story_filter="$2"; shift 2;;
      --from-task|--fresh-from|--task)
        start_task_ref="${2:-}"
        [[ -n "$start_task_ref" ]] || die "--from-task requires a task id or story:position reference"
        shift 2
        ;;
      --fresh) resume=0; shift;;
      --force)
        resume=0
        force_reset=1
        shift
        ;;
      --no-verify) no_verify=1; shift;;
      --keep-artifacts) keep_artifacts=1; shift;;
      --memory-cycle) memory_cycle=1; shift;;
      --batch-size) batch_size="${2:-0}"; shift 2;;
      --sleep-between) sleep_between="${2:-0}"; shift 2;;
      --context-lines)
        context_lines="${2:-}"
        context_mode_flags=(--context-lines "$context_lines")
        shift 2
        ;;
      --context-none)
        context_lines=0
        context_mode_flags=(--context-none)
        shift
        ;;
      --context-mode)
        context_mode="${2:-}"
        context_mode_choice_flag=(--context-mode "$context_mode")
        shift 2
        ;;
      --context-file-lines)
        context_file_lines="${2:-}"
        context_file_flag=(--context-file-lines "$context_file_lines")
        shift 2
        ;;
      --context-skip)
        context_skip_patterns+=("$2")
        context_skip_flags+=(--context-skip "$2")
        shift
        ;;
      --prompt-compact)
        prompt_compact=1
        prompt_mode_flag=(--prompt-compact)
        shift
        ;;
      --prompt-expanded)
        prompt_compact=0
        prompt_mode_flag=(--prompt-expanded)
        shift
        ;;
      --context-doc-snippets|--doc-snippets)
        doc_snippets=1
        doc_snippets_flag=(--context-doc-snippets)
        shift
        ;;
      --sample-lines)
        sample_lines="${2:-}"
        sample_lines_flag=(--sample-lines "$sample_lines")
        shift 2
        ;;
      --)
        shift
        break
        ;;
      -h|--help)
        cat <<'EOHELP'
Usage: gpt-creator work-on-tasks [options]

Execute tasks from the project SQLite backlog using Codex, with resumable progress.

Options:
  --project PATH       Project root (defaults to current directory)
  --story ID|SLUG      Start from the matching story id or slug (inclusive)
  --from-task REF      Resume from task REF (task id or story-slug:position) and continue forward
  --fresh              Ignore saved progress and start from the first story
  --no-verify          Skip running gpt-creator verify after tasks complete
  --keep-artifacts     Retain Codex prompt/output files for each task (default cleans up)
  --memory-cycle       Process one task at a time, prune caches, and auto-resume to limit memory usage
  --batch-size NUM     Process at most NUM tasks this run, then pause (default: unlimited)
  --sleep-between SEC  Sleep SEC seconds between tasks (default: 0)
  --context-lines NUM  Include the last NUM lines of shared context in each prompt (default: 400)
  --context-none       Skip attaching shared context to task prompts
  --context-mode MODE  Choose 'digest' (default) to summarise context or 'raw' for the literal tail
  --context-file-lines NUM
                        Limit each shared-context file to NUM lines before summarising (default: 200)
  --context-skip GLOB  Ignore matching files when building shared context (repeatable)
  --prompt-compact     Use the compact instruction/schema block (default setting)
  --prompt-expanded    Restore the legacy verbose instruction/schema block
  --context-doc-snippets
                        Pull scoped excerpts for referenced docs/endpoints when available
  --sample-lines NUM   Include at most NUM chunks of minified sample payloads (default: 0; increase to view raw content)
EOHELP
        return 0
        ;;
      *)
        die "Unknown work-on-tasks option: ${1}"
        ;;
    esac
  done

  if [[ $# -gt 0 ]]; then
    die "Unexpected argument for work-on-tasks: ${1}"
  fi

  [[ "$batch_size" =~ ^[0-9]+$ ]] || die "Invalid --batch-size value: ${batch_size}"
  [[ "$sleep_between" =~ ^[0-9]+$ ]] || die "Invalid --sleep-between value: ${sleep_between}"
  [[ "$context_lines" =~ ^[0-9]+$ ]] || die "Invalid --context-lines value: ${context_lines}"
  [[ "$context_file_lines" =~ ^[0-9]+$ ]] || die "Invalid --context-file-lines value: ${context_file_lines}"
  [[ "$sample_lines" =~ ^-?[0-9]+$ ]] || die "Invalid --sample-lines value: ${sample_lines}"
  batch_size=$((batch_size))
  sleep_between=$((sleep_between))
  context_lines=$((context_lines))
  context_file_lines=$((context_file_lines))
  sample_lines=$((sample_lines))
  (( sample_lines >= 0 )) || die "--sample-lines must be zero or positive (got ${sample_lines})"
  context_mode="${context_mode,,}"
  case "$context_mode" in
    digest|raw) ;;
    *) die "Invalid --context-mode value: ${context_mode}" ;;
  esac

  if [[ -n "$start_task_ref" && $resume -eq 0 ]]; then
    info "--fresh ignored when --from-task is provided; resuming from the specified task instead."
    resume=1
  fi

  if (( memory_cycle )); then
    if (( batch_size == 0 || batch_size > 1 )); then
      info "Memory-cycle enabled; forcing --batch-size 1 for iterative runs."
    fi
    batch_size=1
  fi

  ensure_ctx "$root"
  local tasks_dir="${PLAN_DIR}/tasks"
  local tasks_db="${tasks_dir}/tasks.db"
  mkdir -p "$tasks_dir"

  if ! gc_tasks_db_has_rows "$tasks_db"; then
    die "Task database missing or empty. Run 'gpt-creator create-tasks' (or create-jira-tasks + migrate-tasks) before work-on-tasks."
  fi

  gc_align_task_story_slugs "$tasks_db"
  gc_sync_story_totals "$tasks_db"

  if (( force_reset )); then
    info "Resetting backlog progress to pending (--force)."
    gc_reset_task_progress "$tasks_db"
    gc_sync_story_totals "$tasks_db"
  fi

  local start_task_story_slug="" start_task_story_title="" start_task_position="" start_task_id="" start_task_title=""
  if [[ -n "$start_task_ref" ]]; then
    info "Rewinding backlog starting from task reference '${start_task_ref}'."
    local rewind_info=""
    local original_story_filter="$story_filter"
    if ! rewind_info="$(gc_rewind_backlog_from_task "$tasks_db" "$start_task_ref" "$original_story_filter")"; then
      die "Unable to rewind backlog from task reference '${start_task_ref}'."
    fi
    IFS=$'\t' read -r start_task_story_slug start_task_story_title start_task_position start_task_id start_task_title <<<"$rewind_info"
    if [[ -z "$start_task_story_slug" || -z "$start_task_position" ]]; then
      die "Invalid response while rewinding backlog; aborting."
    fi
    if [[ -n "$original_story_filter" && "${original_story_filter,,}" != "${start_task_story_slug,,}" ]]; then
      info "Normalizing story filter '${original_story_filter}' to story slug '${start_task_story_slug}'."
    fi
    story_filter="$start_task_story_slug"
    gc_sync_story_totals "$tasks_db"
    local story_display="$start_task_story_slug"
    if [[ -n "$start_task_story_title" && "${start_task_story_title,,}" != "${start_task_story_slug,,}" ]]; then
      story_display+=" — ${start_task_story_title}"
    fi
    info "Starting from task ${start_task_position} (${start_task_id:-no-id}) in story ${story_display}."
    if [[ -n "$start_task_title" ]]; then
      info "  ${start_task_title}"
    fi
  fi

  ensure_node_dependencies "$PROJECT_ROOT"

  if (( prompt_compact )); then
    export GC_PROMPT_COMPACT=1
  else
    unset GC_PROMPT_COMPACT
  fi
  if (( doc_snippets )); then
    export GC_PROMPT_DOC_SNIPPETS=1
  else
    unset GC_PROMPT_DOC_SNIPPETS
  fi

  local state_dir="${PLAN_DIR}/work"
  local runs_dir="${state_dir}/runs"
  mkdir -p "$runs_dir"

  GC_CONTEXT_FILE_LINES="$context_file_lines"
  if ((${#context_skip_patterns[@]} > 0)); then
    GC_CONTEXT_SKIP_PATTERNS=("${context_skip_patterns[@]}")
  else
    unset GC_CONTEXT_SKIP_PATTERNS
  fi

  export GC_PROMPT_SAMPLE_LINES="$sample_lines"

  local run_stamp="$(date +%Y%m%d_%H%M%S)"
  local run_dir="${runs_dir}/${run_stamp}"
  mkdir -p "$run_dir"
  local ctx_file="${run_dir}/context.md"
  gc_build_context_file "$ctx_file" "$STAGING_DIR"
  local context_tail=""
  export GC_CONTEXT_TAIL_LIMIT="$context_lines"
  export GC_CONTEXT_TAIL_MODE="$context_mode"
  if (( context_lines > 0 )); then
    case "$context_mode" in
      digest)
        context_tail="${run_dir}/context_digest.md"
        if ! gc_build_context_digest "$ctx_file" "$context_tail" "$context_lines"; then
          warn "Failed to build context digest; falling back to raw tail."
          context_mode="raw"
          export GC_CONTEXT_TAIL_MODE="raw"
        fi
        if [[ "$context_mode" == "raw" ]]; then
          context_tail="${run_dir}/context_tail.md"
          if ! tail -n "$context_lines" "$ctx_file" >"$context_tail" 2>/dev/null; then
            cp "$ctx_file" "$context_tail"
          fi
        fi
        ;;
      raw)
        context_tail="${run_dir}/context_tail.md"
        if ! tail -n "$context_lines" "$ctx_file" >"$context_tail" 2>/dev/null; then
          cp "$ctx_file" "$context_tail"
        fi
        ;;
    esac
  fi

  info "Work run directory → ${run_dir}"

  local resume_flag=1
  [[ $resume -eq 1 ]] || resume_flag=0

  local work_failed=0
  local any_changes=0
  local manual_followups=0
  local usage_limit_triggered=0
  local batch_limit_reached=0

  local processed_total=0
  local processed_any_total=0
  local remaining_tasks=0
  local memory_cycle_single=0

  while :; do
    local iteration_processed_any=0
    local iteration_processed=0
    local continue_current_run=0
    local pending_tasks=0

    local effective_batch_size="$batch_size"
    if (( memory_cycle )); then
      if (( memory_cycle_single )); then
        effective_batch_size=1
      else
        effective_batch_size="$batch_size"
      fi
    fi

    while IFS=$'\t' read -r sequence slug story_id story_title epic_id epic_title total_tasks next_task completed status; do
      if [[ -z "${sequence}${slug}${story_id}${story_title}${epic_id}${epic_title}" ]]; then
        continue
      fi

      iteration_processed_any=1

    : "${total_tasks:=0}"; : "${next_task:=0}"
    local total_tasks_int=0
    if [[ "$total_tasks" =~ ^[0-9]+$ ]]; then
      total_tasks_int=$((total_tasks))
    fi
    local next_task_int=0
    if [[ "$next_task" =~ ^[0-9]+$ ]]; then
      next_task_int=$((next_task))
    fi

    printf -v story_prefix "%03d" "${sequence:-0}"
    [[ -n "$slug" ]] || slug="story-${story_prefix}"
    local story_run_dir="${run_dir}/story_${story_prefix}_${slug}"
    mkdir -p "${story_run_dir}/prompts" "${story_run_dir}/out" "${story_run_dir}/reports"
    local report_dir="${story_run_dir}/reports"

    info "Story ${story_prefix} (${story_id:-$slug}) — ${story_title:-Unnamed}"

    if (( total_tasks_int == 0 )); then
      local stats=""
      if stats="$(gc_fetch_story_task_counts "$tasks_db" "$slug" 2>/dev/null)"; then
        local actual_total=0 actual_completed=0
        IFS=$'\t' read -r actual_total actual_completed <<<"$stats"
        if [[ "$actual_total" =~ ^[0-9]+$ ]] && (( actual_total > 0 )); then
          info "  Found ${actual_total} task(s) in backlog despite zero metadata; synchronising."
          total_tasks_int=$actual_total
          completed="${actual_completed}"
          if [[ "$completed" =~ ^[0-9]+$ ]]; then
            (( completed > actual_total )) && completed="$actual_total"
            if (( next_task_int < completed )); then
              next_task_int=$completed
            fi
          fi
          gc_update_work_state "$tasks_db" "$slug" "pending" "$completed" "$total_tasks_int" "$run_stamp"
        fi
      fi
    fi

    if (( total_tasks_int == 0 )); then
      info "  No tasks for this story; marking complete."
      gc_update_work_state "$tasks_db" "$slug" "complete" 0 0 "$run_stamp"
      if (( keep_artifacts == 0 )); then
        rmdir "${story_run_dir}/prompts" 2>/dev/null || true
        rmdir "${story_run_dir}/out" 2>/dev/null || true
      fi
      continue
    fi

    gc_update_work_state "$tasks_db" "$slug" "in-progress" "$next_task_int" "$total_tasks_int" "$run_stamp"

    local task_index
    local story_failed=0
    for (( task_index = next_task_int; task_index < total_tasks_int; task_index++ )); do
      if (( effective_batch_size > 0 && iteration_processed >= effective_batch_size )); then
        batch_limit_reached=1
        break
      fi
      local task_number
      printf -v task_number "%03d" $((task_index + 1))
      local prompt_path="${story_run_dir}/prompts/task_${task_number}.prompt.md"
      local output_path="${story_run_dir}/out/task_${task_number}.out.md"

      local prompt_meta
      if ! prompt_meta="$(gc_write_task_prompt "$tasks_db" "$slug" "$task_index" "$prompt_path" "$context_tail" "$CODEX_MODEL" "$PROJECT_ROOT" "$STAGING_DIR")"; then
        warn "  Failed to build prompt for task index ${task_index}"
        work_failed=1
        break
      fi

      local task_id="" task_title=""
      IFS=$'\t' read -r task_id task_title <<<"$prompt_meta"
      info "  → Working on task ${task_number} (${task_id:-no-id})"
      info "    ${task_title:-(untitled)}"

      local call_name="story-${slug}-task-${task_number}"
      local codex_ok=0
      local attempt=0
      local max_attempts=2
      local prompt_augmented=0
      local keep_output=$keep_artifacts
      local parse_error_final=0
      local break_after_update=0
      local task_result_status="in-progress"
      local task_needs_review=0
      local -a task_notes=()
      local -a task_written_paths=()
      local -a task_patched_paths=()
      local -a task_commands=()
      local task_changes_applied=0
      local apply_status="pending"
      local task_report_path="${report_dir}/task_${task_number}.log"
      gc_update_task_state "$tasks_db" "$slug" "$task_index" "in-progress" "$run_stamp"

      while (( attempt < max_attempts )); do
        (( ++attempt ))
        if codex_call "$call_name" --prompt "$prompt_path" --output "$output_path"; then
          if [[ ! -s "$output_path" ]]; then
            warn "  Codex produced no output for ${call_name}; manual review required."
            task_needs_review=1
            manual_followups=1
            keep_output=1
            task_result_status="on-hold"
            task_notes+=("Codex produced no output; manual review required.")
            codex_ok=1
            break
          fi
          local apply_output
          if ! apply_output="$(gc_apply_codex_changes "$output_path" "$PROJECT_ROOT")"; then
            warn "  Failed to apply changes for ${call_name}; manual review required (see ${output_path})."
            task_needs_review=1
            manual_followups=1
            keep_output=1
            task_result_status="on-hold"
            task_notes+=("Codex changes could not be applied automatically; review ${output_path}.")
            codex_ok=1
            break
          fi
          if [[ "$apply_output" == "no-output" || "$apply_output" == "empty-output" ]]; then
            warn "  Codex produced no actionable JSON for ${call_name}; manual review required."
            task_needs_review=1
            manual_followups=1
            keep_output=1
            task_result_status="on-hold"
            task_notes+=("Codex response lacked actionable changes.")
            codex_ok=1
            break
          fi
          apply_status="ok"
          while IFS= read -r change_line; do
            case "$change_line" in
              STATUS\ *)
                apply_status="${change_line#STATUS }"
                ;;
              APPLIED)
                info "    Changes applied."
                ;;
              WRITE\ *)
                local written_path="${change_line#WRITE }"
                info "    Wrote ${written_path}"
                any_changes=1
                task_changes_applied=1
                task_written_paths+=("$written_path")
                ;;
              PATCH\ *)
                local patched_path="${change_line#PATCH }"
                info "    Patched ${patched_path}"
                any_changes=1
                task_changes_applied=1
                task_patched_paths+=("$patched_path")
                ;;
              CMD\ *)
                local suggested_cmd="${change_line#CMD }"
                info "    Suggested command: ${suggested_cmd}"
                task_commands+=("$suggested_cmd")
                ;;
              NOTE\ *)
                local note_text="${change_line#NOTE }"
                warn "    Note: ${note_text}"
                task_notes+=("$note_text")
                if gc_note_requires_followup "$note_text"; then
                  task_needs_review=1
                fi
                ;;
            esac
          done <<<"$apply_output"

          if [[ "$apply_status" == "parse-error" ]]; then
            if (( attempt < max_attempts )); then
              warn "  Codex returned invalid JSON; retrying (attempt $((attempt + 1)) of ${max_attempts})."
              if (( prompt_augmented == 0 )); then
                cat >>"$prompt_path" <<'REM'

## Reminder
- Output a single JSON object exactly as described above; do not include any explanatory text outside the JSON.
- If no changes are required, return the JSON with an empty `changes` array and clear notes explaining why.
- Diff entries must remain valid unified diffs.
REM
                prompt_augmented=1
              fi
              continue
            else
              warn "  Codex output was invalid JSON after retry; manual review required (see ${output_path})."
              task_needs_review=1
              manual_followups=1
              keep_output=1
              task_result_status="on-hold"
              task_notes+=("Codex output remained invalid JSON after retries; inspect ${output_path}.")
              codex_ok=1
              parse_error_final=1
              break_after_update=1
            fi
          fi

          if (( keep_output == 0 )); then
            rm -f "$prompt_path" "$output_path"
          fi
          codex_ok=1
        else
          local codex_status=$?
          if [[ "${GC_CODEX_USAGE_LIMIT_REACHED:-0}" == "1" ]]; then
            warn "  Codex usage limit reached for ${call_name}; halting further tasks."
            if [[ -n "${GC_CODEX_USAGE_LIMIT_MESSAGE:-}" ]]; then
              warn "    ${GC_CODEX_USAGE_LIMIT_MESSAGE}"
            fi
            task_result_status="blocked"
            task_needs_review=1
            manual_followups=1
            keep_output=1
            task_notes+=("Codex usage limit reached; wait for quota reset before rerunning work-on-tasks.")
            codex_ok=1
            usage_limit_triggered=1
            work_failed=1
            break
          elif (( codex_status == 124 )); then
            warn "  Codex was idle for ${codex_timeout_seconds}s during ${call_name}."
            if (( attempt < max_attempts )); then
              info "    Retrying task ${task_number} after timeout (attempt $((attempt + 1)) of ${max_attempts})."
              continue
            fi
            task_result_status="on-hold"
            task_needs_review=1
            manual_followups=1
            keep_output=1
            task_notes+=("Codex produced no output for ${codex_timeout_seconds}s; rerun work-on-tasks to continue from this task.")
            codex_ok=1
            break_after_update=1
            work_failed=1
            break
          fi
          warn "  Codex execution failed for ${call_name}; progress saved."
          task_result_status="blocked"
          task_needs_review=1
          manual_followups=1
          keep_output=1
          task_notes+=("Codex execution failed; no automated changes were applied.")
          codex_ok=1
          break
        fi
        break
      done

      if (( task_needs_review )); then
        manual_followups=1
        if [[ "$task_result_status" != "blocked" ]]; then
          task_result_status="on-hold"
        fi
      fi

      if (( codex_ok == 0 )); then
        task_result_status="blocked"
        task_notes+=("Codex execution did not complete; no changes were applied.")
      fi

      if [[ "$task_result_status" == "in-progress" ]]; then
        task_result_status="complete"
      fi

      local story_status_hint="in-progress"
      case "$task_result_status" in
        blocked) story_status_hint="blocked" ;;
        on-hold) story_status_hint="on-hold" ;;
      esac

      local completed_hint="$task_index"
      if [[ "$task_result_status" == "complete" ]]; then
        completed_hint=$((task_index + 1))
      fi

      gc_update_task_state "$tasks_db" "$slug" "$task_index" "$task_result_status" "$run_stamp"
      gc_update_work_state "$tasks_db" "$slug" "$story_status_hint" "$completed_hint" "$total_tasks_int" "$run_stamp"

      local timestamp_utc
      timestamp_utc="$(date -u +%Y-%m-%dT%H:%M:%SZ)"

      local prompt_entry="$prompt_path"
      local output_entry="$output_path"
      local report_entry="$task_report_path"
      local project_prefix="${PROJECT_ROOT}/"
      if [[ -n "$PROJECT_ROOT" ]]; then
        if [[ "$prompt_entry" == "$project_prefix"* ]]; then
          prompt_entry="${prompt_entry#$project_prefix}"
        fi
        if [[ "$output_entry" == "$project_prefix"* ]]; then
          output_entry="${output_entry#$project_prefix}"
        fi
        if [[ "$report_entry" == "$project_prefix"* ]]; then
          report_entry="${report_entry#$project_prefix}"
        fi
      fi
      if [[ ! -f "$prompt_path" ]]; then
        if (( keep_artifacts == 0 )); then
          prompt_entry="(discarded)"
        else
          prompt_entry="(missing)"
        fi
      fi
      if [[ ! -f "$output_path" ]]; then
        if (( keep_output == 0 )); then
          output_entry="(discarded)"
        else
          output_entry="(missing)"
        fi
      fi

      local changes_flag="false"
      if (( task_changes_applied > 0 )); then
        changes_flag="true"
      fi

      {
        printf 'task_number: %s\n' "$task_number"
        printf 'task_id: %s\n' "${task_id:-}"
        printf 'task_title: %s\n' "${task_title//$'\n'/ }"
        printf 'story_slug: %s\n' "$slug"
        printf 'status: %s\n' "$task_result_status"
        printf 'timestamp: %s\n' "$timestamp_utc"
        printf 'attempts: %s\n' "$attempt"
        printf 'apply_status: %s\n' "$apply_status"
        printf 'changes_applied: %s\n' "$changes_flag"
        printf 'prompt_path: %s\n' "$prompt_entry"
        printf 'output_path: %s\n' "$output_entry"
        if ((${#task_written_paths[@]} > 0)); then
          printf 'written:\n'
          for path in "${task_written_paths[@]}"; do
            printf '  - %s\n' "$path"
          done
        fi
        if ((${#task_patched_paths[@]} > 0)); then
          printf 'patched:\n'
          for path in "${task_patched_paths[@]}"; do
            printf '  - %s\n' "$path"
          done
        fi
        if ((${#task_commands[@]} > 0)); then
          printf 'commands:\n'
          for cmd in "${task_commands[@]}"; do
            printf '  - %s\n' "$cmd"
          done
        fi
        printf 'notes:\n'
        if ((${#task_notes[@]} > 0)); then
          for note in "${task_notes[@]}"; do
            printf '  - %s\n' "${note//$'\n'/ }"
          done
        else
          printf '  - (none)\n'
        fi
      } >"$task_report_path"

      case "$task_result_status" in
        complete)
          info "  ✓ Task ${task_number} (${task_id:-no-id}) completed with status: ${task_result_status}"
          ;;
        on-hold)
          warn "  Task ${task_number} (${task_id:-no-id}) marked ${task_result_status}; review ${report_entry}."
          ;;
        blocked)
          warn "  Task ${task_number} (${task_id:-no-id}) blocked; see ${report_entry}."
          ;;
        *)
          info "  Task ${task_number} (${task_id:-no-id}) finished with status: ${task_result_status}"
          ;;
      esac

      (( ++processed_total ))
      (( ++iteration_processed ))

      if [[ "$task_result_status" == "blocked" ]]; then
        story_failed=1
        break
      fi

      if (( break_after_update )); then
        continue
      fi

      if (( sleep_between > 0 )); then
        sleep "$sleep_between"
      fi

    done

    if (( batch_limit_reached )); then
      break
    fi

    if (( story_failed )); then
      warn "Stopping at story ${slug} due to previous error."
      break
    fi

    gc_update_work_state "$tasks_db" "$slug" "complete" "$total_tasks_int" "$total_tasks_int" "$run_stamp"
    if (( keep_artifacts == 0 )); then
      rmdir "${story_run_dir}/prompts" 2>/dev/null || true
      rmdir "${story_run_dir}/out" 2>/dev/null || true
    fi
  done < <(
    python3 - "$tasks_db" "${story_filter}" "$resume_flag" <<'PY'
import sqlite3
import sys
import re

DB_PATH = sys.argv[1]
story_filter = (sys.argv[2] or '').strip().lower()
resume_flag = sys.argv[3] == "1"

conn = sqlite3.connect(DB_PATH)
conn.row_factory = sqlite3.Row
cur = conn.cursor()

stories = cur.execute('SELECT story_slug, story_id, story_title, epic_key, epic_title, sequence, status FROM stories ORDER BY sequence ASC, story_slug ASC').fetchall()

def norm(value):
    return (value or "").strip().lower()

def normalize(value):
    return (value or "").strip()

def slug_norm(value):
    value = norm(value)
    if not value:
        return ""
    return re.sub(r'[^a-z0-9]+', '-', value).strip('-')

start_allowed = not story_filter

for story in stories:
    slug = normalize(story["story_slug"])
    sequence = story["sequence"] or 0
    story_id = normalize(story["story_id"])
    epic_key = normalize(story["epic_key"])
    epic_title = normalize(story["epic_title"])
    story_title = normalize(story["story_title"])

    story_title_clean = story_title.replace('\t', ' ').replace('\n', ' ')
    epic_title_clean = epic_title.replace('\t', ' ').replace('\n', ' ')
    epic_key_clean = epic_key.replace('\t', ' ').replace('\n', ' ')
    story_id_clean = story_id.replace('\t', ' ').replace('\n', ' ')

    if story_filter and not start_allowed:
        keys = {norm(story_id), norm(slug), norm(epic_key), norm(str(sequence))}
        if story_filter in keys:
            start_allowed = True
        else:
            continue

    task_rows = []
    slug_lower = norm(slug)
    if slug_lower:
        task_rows = cur.execute(
            'SELECT position, status FROM tasks WHERE LOWER(COALESCE(story_slug, "")) = ? ORDER BY position ASC',
            (slug_lower,),
        ).fetchall()

    if not task_rows and story_id:
        story_id_lower = norm(story_id)
        if story_id_lower:
            rows = cur.execute(
                'SELECT id, position, status, story_slug FROM tasks WHERE LOWER(COALESCE(story_id, "")) = ? ORDER BY position ASC',
                (story_id_lower,),
            ).fetchall()
            if rows:
                task_rows = [(row["position"], row["status"]) for row in rows]
                if slug_lower:
                    cur.execute(
                        'UPDATE tasks SET story_slug = ? WHERE LOWER(COALESCE(story_id, "")) = ?',
                        (slug, story_id_lower),
                    )
                    conn.commit()

    if not task_rows and slug_lower:
        slug_key = slug_norm(slug)
        if slug_key:
            rows = cur.execute(
                'SELECT position, status FROM tasks WHERE LOWER(COALESCE(story_slug, "")) = ? ORDER BY position ASC',
                (slug_key,),
            ).fetchall()
            task_rows = rows

    total = len(task_rows)
    completed = 0
    next_index = 0
    for row in task_rows:
        status = (row[1] or "").strip().lower()
        if status == "complete":
            completed += 1
            continue
        next_index = row[0] or 0
        break
    else:
        next_index = total

    current_status = (story["status"] or "").strip()

    if resume_flag and not story_filter and current_status.lower() == "complete":
        continue

    if resume_flag:
        if total == 0:
            next_index = 0
        elif completed >= total:
            if story_filter:
                next_index = total
            else:
                continue
    else:
        next_index = 0 if total > 0 else 0

    print("	".join([
        str(sequence),
        slug,
        story_id_clean,
        story_title_clean,
        epic_key_clean,
        epic_title_clean,
        str(total),
        str(next_index),
        str(completed),
        current_status,
    ]))

conn.close()
PY
  )

    if (( iteration_processed_any )); then
      processed_any_total=1
    else
      if (( processed_any_total == 0 )); then
        info "No stories to process (already complete)."
      fi
      break
    fi

    if (( batch_limit_reached )); then
      remaining_tasks="$(gc_count_pending_tasks "$tasks_db" || echo 0)"
      [[ "$remaining_tasks" =~ ^[0-9]+$ ]] || remaining_tasks=0
      break
    fi

    if (( memory_cycle )); then
      pending_tasks="$(gc_count_pending_tasks "$tasks_db" || echo 0)"
      [[ "$pending_tasks" =~ ^[0-9]+$ ]] || pending_tasks=0
      remaining_tasks="$pending_tasks"
      if (( work_failed == 0 )); then
        if (( iteration_processed > 0 )) && (( pending_tasks > 0 )); then
          gc_trim_memory "memory-cycle"
          info "Memory-cycle paused after ${iteration_processed} task(s); ${pending_tasks} pending."
          memory_cycle_single=1
          continue_current_run=1
        elif (( pending_tasks == 0 )); then
          gc_trim_memory "memory-cycle-final"
        else
          gc_trim_memory "memory-cycle"
        fi
      else
        gc_trim_memory "memory-cycle-error"
      fi
    fi

    if (( continue_current_run == 0 )); then
      remaining_tasks="$(gc_count_pending_tasks "$tasks_db" || echo 0)"
      [[ "$remaining_tasks" =~ ^[0-9]+$ ]] || remaining_tasks=0
      if (( work_failed == 0 && memory_cycle == 0 && batch_limit_reached == 0 && effective_batch_size == 0 && iteration_processed > 0 && remaining_tasks > 0 )); then
        if [[ -n "$story_filter" ]]; then
          info "Remaining tasks detected beyond filtered story; rerun with a broader filter to continue."
        else
          info "${remaining_tasks} task(s) remain; continuing work-on-tasks automatically."
          continue_current_run=1
        fi
      fi
    fi

    if (( continue_current_run )); then
      continue
    fi

    break
  done

  if (( processed_any_total == 0 )); then
    return 0
  fi

  if [[ $work_failed -eq 0 && $batch_limit_reached -eq 0 && $no_verify -eq 0 && $remaining_tasks -eq 0 ]]; then
    if (( any_changes == 0 )); then
      info "No repository changes detected; skipping verify."
    else
      info "Re-running verify after work run"
      if ! cmd_verify all --project "$PROJECT_ROOT"; then
        warn "Verify command reported failures."
        work_failed=1
      fi
    fi
  fi

  if (( batch_limit_reached )); then
    info "Batch size limit hit after ${processed_total} task(s); rerun to continue from the next pending task."
  fi

  if (( usage_limit_triggered )); then
    warn "Codex usage limit detected; halt further work until additional quota is available."
  fi

  if [[ $work_failed -eq 0 ]]; then
    if (( batch_limit_reached )); then
      ok "work-on-tasks paused → ${run_dir}"
    else
      ok "work-on-tasks complete → ${run_dir}"
    fi
    if (( manual_followups )); then
      warn "Manual review needed for some tasks — see notes above and preserved output artifacts."
    fi
  else
    warn "work-on-tasks completed with issues — inspect ${run_dir}"
    return 1
  fi
}


cmd_iterate() {
  warn "'iterate' is deprecated; prefer 'gpt-creator create-tasks' followed by 'gpt-creator work-on-tasks'."

  local root="" jira="" reverify=1
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --jira) jira="$(abs_path "$2")"; shift 2;;
      --no-verify) reverify=0; shift;;
      *) break;;
    esac
  done
  ensure_ctx "$root"
  [[ -n "$jira" ]] || jira="${INPUT_DIR}/jira.md"
  [[ -f "$jira" ]] || die "Jira tasks file not found: ${jira}"

  local tasks_json_local="${PLAN_DIR}/jira-tasks.local.json"
  gc_parse_jira_tasks "$jira" "$tasks_json_local"
  ok "Parsed Jira tasks → ${tasks_json_local}"
  local tasks_json="${tasks_json_local}"

  local codex_parse_prompt="${PLAN_DIR}/iterate-codex-parse.md"
  local codex_raw_json="${PLAN_DIR}/jira-tasks.codex.raw.txt"
  local codex_json="${PLAN_DIR}/jira-tasks.codex.json"
  {
    cat >"$codex_parse_prompt" <<'PROMPT'
# Instruction
You are a structured-data assistant. Convert the following Jira backlog markdown into strict JSON.

## Requirements
- Output **only** valid JSON (no prose, no code fences).
- Structure: { "tasks": [ { "epic_id": str, "epic_title": str, "story_id": str, "story_title": str, "id": str, "title": str, "assignees": [str], "tags": [str], "estimate": str, "description": str, "acceptance_criteria": [str], "dependencies": [str] } ] }.
- Each task begins with a bold identifier such as **T18.5.2**; treat every such block as a separate task and capture its parent story/epic when present.
- Preserve bullet details verbatim inside the description and acceptance criteria lists. Do not repeat metadata (assignee/tags/estimate) inside the description.
- Use empty strings/arrays when information is missing.
- Do not include explanatory text.

## Jira Markdown
PROMPT
    cat "$jira" >>"$codex_parse_prompt"
    cat >>"$codex_parse_prompt" <<'PROMPT'
## End Markdown
PROMPT
  }

  if codex_call "iterate-parse" --prompt "$codex_parse_prompt" --output "$codex_raw_json"; then
    if python3 - <<'PY' "$codex_raw_json" "$codex_json"
import json, pathlib, re, sys
raw_path, out_path = sys.argv[1:3]
text = pathlib.Path(raw_path).read_text().strip()
if not text:
    raise SystemExit(1)
if text.startswith('```'):
    text = re.sub(r'^```[a-zA-Z0-9_-]*\s*', '', text)
    text = re.sub(r'```\s*$', '', text)
data = json.loads(text)
if isinstance(data, list):
    data = {'tasks': data}
elif 'tasks' not in data:
    data = {'tasks': [data]}
pathlib.Path(out_path).write_text(json.dumps(data, indent=2) + '\n')
PY
      then
      ok "Codex parsed Jira tasks → ${codex_json}"
      tasks_json="$codex_json"
    else
      warn "Codex JSON output invalid; falling back to local parser results."
    fi
  else
    warn "Codex parsing step failed; using local parser output."
  fi

  local iterate_dir="${PLAN_DIR}/iterate"
  mkdir -p "$iterate_dir"
  local order_file="${iterate_dir}/tasks-order.txt"

  python3 - <<'PY' "$tasks_json" "$iterate_dir" "$PROJECT_ROOT"
import json, pathlib, sys
source, out_dir, project_root = sys.argv[1:4]
tasks = json.load(open(source)).get('tasks', [])
out = pathlib.Path(out_dir)
out.mkdir(parents=True, exist_ok=True)
index_path = out / 'tasks-order.txt'
with index_path.open('w') as idx:
    for i, task in enumerate(tasks, 1):
        title = (task.get('title') or '').strip() or f'Task {i}'
        task_id = (task.get('id') or '').strip()
        description = (task.get('description') or '').strip() or '(No additional details provided.)'
        estimate = (task.get('estimate') or '').strip()
        tags = ', '.join(task.get('tags') or [])
        assignees = ', '.join(task.get('assignees') or [])
        story_bits = [part for part in [(task.get('story_id') or '').strip(), (task.get('story_title') or '').strip()] if part]
        prompt_path = out / f'task-{i:02d}.md'
        idx.write(str(prompt_path) + '\n')
        lines = []
        if task_id and not title.startswith(task_id):
            lines.append(f"# Task {i}: {task_id} — {title}")
        else:
            lines.append(f"# Task {i}: {title}")
        lines.append('')
        lines.append('## Context')
        lines.append(f'- Working directory: {project_root}')
        if task_id:
            lines.append(f'- Task ID: {task_id}')
        if story_bits:
            lines.append(f"- Story: {' — '.join(story_bits)}")
        if assignees:
            lines.append(f'- Assignees: {assignees}')
        if estimate:
            lines.append(f'- Estimate: {estimate}')
        if tags:
            lines.append(f'- Tags: {tags}')
        lines.append('')
        lines.append('## Description')
        lines.append(description or '(No additional details provided.)')
        lines.append('')
        if task.get('acceptance_criteria'):
            lines.append('## Acceptance Criteria')
            for ac in task['acceptance_criteria']:
                lines.append(f'- {ac}')
            lines.append('')
        if task.get('dependencies'):
            lines.append('## Dependencies')
            for dep in task['dependencies']:
                lines.append(f'- {dep}')
            lines.append('')
        lines.append('')
        lines.append('## Instructions')
        lines.append('- Outline your plan before modifying files.')
        lines.append('- Implement the task in the repository; commits are not required.')
        lines.append('- Show relevant diffs (git snippets) and command results.')
        lines.append('- Verify acceptance criteria for this task.')
        lines.append('- If blocked, explain why and propose next steps.')
        lines.append('')
        lines.append('## Output Format')
        lines.append('- Begin with a heading `Task {i}`.')
        lines.append('- Summarise changes, tests, and outstanding follow-ups.')
        prompt_path.write_text('\n'.join(lines) + '\n')
PY

  if [[ -s "$order_file" ]]; then
    while IFS= read -r prompt_path; do
      [[ -z "$prompt_path" ]] && continue
      local base_name="$(basename "$prompt_path" .md)"
      local output_path="${prompt_path%.md}.output.md"
      info "Running Codex for ${base_name}"
      codex_call "$base_name" --prompt "$prompt_path" --output "$output_path" || warn "Codex task ${base_name} returned non-zero"
    done < "$order_file"
  else
    warn "No Jira tasks to process after parsing."
  fi

  local summary_prompt="${iterate_dir}/summary.md"
  local summary_output="${iterate_dir}/summary.output.md"
  python3 - <<'PY' "$tasks_json" "$order_file" "$summary_prompt"
import json, pathlib, sys
tasks = json.load(open(sys.argv[1])).get('tasks', [])
order_file = pathlib.Path(sys.argv[2])
prompt_path = pathlib.Path(sys.argv[3])
lines = ['# Summary Request', '', 'Summarise the completed Jira work and list follow-up actions.']
lines.append('')
lines.append('## Task Reports')
if order_file.exists():
    for i, prompt in enumerate(order_file.read_text().splitlines(), 1):
        if not prompt:
            continue
        title = tasks[i-1].get('title') if i-1 < len(tasks) else f'Task {i}'
        out_path = pathlib.Path(prompt).with_suffix('.output.md')
        lines.append(f'- Task {i}: {title}')
        if out_path.exists():
            content = out_path.read_text().strip()
            if content:
                snippet = content[:2000]
                lines.append('  ```')
                lines.append(snippet)
                lines.append('  ```')
        else:
            lines.append('  (No output captured)')
else:
    lines.append('- No outputs available.')
lines.append('')
lines.append('## Output Requirements')
lines.append('- Provide an overall summary of work completed.')
lines.append('- List follow-up items or blockers.')
lines.append('- Use markdown headings and bullet lists.')
prompt_path.write_text('\n'.join(lines) + '\n')
PY

  codex_call "iterate-summary" --prompt "$summary_prompt" --output "$summary_output" || warn "Codex summary step returned non-zero"

  if [[ "$reverify" -eq 1 ]]; then
    info "Re-running verify after iteration"
    cmd_verify all --project "$PROJECT_ROOT"
  fi
}

cmd_estimate() {
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project)
        root="$(abs_path "$2")"
        shift 2
        ;;
      -h|--help)
        cat <<'USAGE'
Usage: gpt-creator estimate [--project PATH]

Estimate how long it will take to finish the remaining tasks based on story points (15 SP/hour).
USAGE
        return 0
        ;;
      *)
        die "Unknown estimate option: $1"
        ;;
    esac
  done

  ensure_ctx "$root"
  local tasks_db="${PLAN_DIR}/tasks/tasks.db"
  if [[ ! -f "$tasks_db" ]]; then
    die "Tasks database not found at ${tasks_db}. Run 'gpt-creator create-tasks' first."
  fi

  python3 - <<'PY' "$tasks_db"
import math
import re
import sqlite3
import sys

db_path = sys.argv[1]
rate = 15.0

conn = sqlite3.connect(db_path)
conn.row_factory = sqlite3.Row
cur = conn.cursor()
rows = cur.execute("SELECT story_points, status FROM tasks").fetchall()
conn.close()

def parse_points(raw):
    if raw is None:
        return 0.0
    if isinstance(raw, (int, float)):
        return float(raw)
    text = str(raw).strip()
    if not text:
        return 0.0
    normalized = text.lower().replace(",", ".")
    match = re.search(r"-?\d+(?:\.\d+)?", normalized)
    if not match:
        return 0.0
    try:
        return float(match.group(0))
    except ValueError:
        return 0.0

done_statuses = {"complete", "completed", "done"}
total_points = 0.0
remaining_tasks = 0

for row in rows:
    status = (row["status"] or "").strip().lower()
    if status in done_statuses:
        continue
    remaining_tasks += 1
    points = parse_points(row["story_points"])
    total_points += max(points, 0.0)

if remaining_tasks == 0:
    print("All tasks are complete. No remaining story points.")
    raise SystemExit(0)

total_minutes = math.ceil((total_points / rate) * 60) if total_points > 0 else 0
days, rem_minutes = divmod(total_minutes, 1440)
hours, minutes = divmod(rem_minutes, 60)

parts = []
if days:
    parts.append(f"{days}d")
if hours:
    parts.append(f"{hours}h")
if minutes or not parts:
    parts.append(f"{minutes}m")

def fmt_points(value: float) -> str:
    if math.isclose(value, round(value), rel_tol=1e-9, abs_tol=1e-9):
        return str(int(round(value)))
    return f"{value:.2f}".rstrip("0").rstrip(".")

estimate = " ".join(parts)
print(f"Remaining tasks: {remaining_tasks}")
print(f"Remaining story points: {fmt_points(total_points)}")
print(f"Estimated completion time @15 SP/hour: {estimate}")
PY
}


cmd_tokens() {
  local root="" details=0 json_output=0
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project)
        root="$(abs_path "$2")"
        shift 2
        ;;
      --details)
        details=1
        shift
        ;;
      --json)
        json_output=1
        shift
        ;;
      -h|--help)
        cat <<'USAGE'
Usage: gpt-creator tokens [--project PATH] [--details] [--json]

Print aggregated Codex token usage captured under .gpt-creator/logs/codex-usage.ndjson.
USAGE
        return 0
        ;;
      *)
        die "Unknown tokens option: $1"
        ;;
    esac
  done

  local project_root=""
  if [[ -n "$root" ]]; then
    project_root="$root"
  elif [[ -n "${PROJECT_ROOT:-}" ]]; then
    project_root="$PROJECT_ROOT"
  else
    project_root="$PWD"
  fi

  local usage_file="${project_root}/.gpt-creator/logs/codex-usage.ndjson"
  if [[ ! -f "$usage_file" ]]; then
    warn "No Codex usage data found at ${usage_file}. Run a codex-enabled command first."
    return 1
  fi

  python3 - <<'PY' "$usage_file" "$details" "$json_output"
import json
import pathlib
import sys
from datetime import datetime

usage_path = pathlib.Path(sys.argv[1])
details = sys.argv[2] == "1"
json_mode = sys.argv[3] == "1"

def as_int(value):
    if isinstance(value, bool):
        return None
    if isinstance(value, int):
        return value
    if isinstance(value, float):
        return int(value)
    if isinstance(value, str):
        stripped = value.strip()
        if stripped.isdigit():
            return int(stripped)
    return None

def parse_timestamp(value):
    if not isinstance(value, str) or not value:
        return None
    cleaned = value
    if cleaned.endswith("Z"):
        cleaned = cleaned[:-1] + "+00:00"
    try:
        return datetime.fromisoformat(cleaned)
    except ValueError:
        return None

def isoformat(dt):
    if dt is None:
        return ""
    text = dt.isoformat()
    if text.endswith("+00:00"):
        text = text[:-6] + "Z"
    return text

raw_lines = usage_path.read_text(encoding="utf-8", errors="ignore").splitlines()
records = []
captured_count = 0
for line in raw_lines:
    line = line.strip()
    if not line:
        continue
    try:
        payload = json.loads(line)
    except Exception:
        continue
    if not isinstance(payload, dict):
        continue
    records.append(payload)
    if payload.get("usage_captured"):
        captured_count += 1

if not records:
    print("No usage entries recorded.")
    sys.exit(0)

fields = ("prompt_tokens", "completion_tokens", "total_tokens", "cached_tokens", "billable_units", "request_units")
totals = {field: 0 for field in fields}
counts = {field: 0 for field in fields}
for entry in records:
    for field in fields:
        value = as_int(entry.get(field))
        if value is not None:
            totals[field] += value
            counts[field] += 1

timestamps = [ts for ts in (parse_timestamp(rec.get("timestamp")) for rec in records) if ts is not None]
first_ts = isoformat(min(timestamps)) if timestamps else ""
last_ts = isoformat(max(timestamps)) if timestamps else ""

summary = {
    "entries": len(records),
    "captured_entries": captured_count,
    "totals": {field: totals[field] for field in fields if counts[field]},
}
if first_ts:
    summary["first_timestamp"] = first_ts
if last_ts:
    summary["last_timestamp"] = last_ts

def clamp(text, limit):
    text = text or ""
    if len(text) <= limit:
        return text
    return text[: max(0, limit - 3)] + "..."

def fmt_int(value):
    if value is None:
        return "-"
    return f"{value:,}"

def fmt_exit_code(value):
    parsed = as_int(value)
    if parsed is None:
        return "-"
    return str(parsed)

sorted_records = sorted(records, key=lambda rec: rec.get("timestamp") or "")

if json_mode:
    if details:
        rows = []
        for rec in sorted_records:
            rows.append({
                "timestamp": rec.get("timestamp"),
                "task": rec.get("task"),
                "model": rec.get("model"),
                "prompt_tokens": as_int(rec.get("prompt_tokens")),
                "completion_tokens": as_int(rec.get("completion_tokens")),
                "total_tokens": as_int(rec.get("total_tokens")),
                "cached_tokens": as_int(rec.get("cached_tokens")),
                "billable_units": as_int(rec.get("billable_units")),
                "request_units": as_int(rec.get("request_units")),
                "exit_code": as_int(rec.get("exit_code")),
                "usage_captured": bool(rec.get("usage_captured")),
            })
        summary["rows"] = rows
    print(json.dumps(summary, indent=2))
    sys.exit(0)

print(f"Codex usage file: {usage_path}")
print(f"Entries: {summary['entries']} (captured={summary['captured_entries']})")
if first_ts and last_ts:
    print(f"Range: {first_ts} → {last_ts}")

label_map = {
    "prompt_tokens": "Prompt tokens",
    "completion_tokens": "Completion tokens",
    "total_tokens": "Total tokens",
    "cached_tokens": "Cached tokens",
    "billable_units": "Billable units",
    "request_units": "Request units",
}

for field, label in label_map.items():
    if field in summary["totals"]:
        print(f"{label}: {summary['totals'][field]:,}")

if not details:
    sys.exit(0)

headers = ["timestamp", "task", "model", "total", "prompt", "completion", "cached", "billable", "request", "exit", "captured"]
rows = []
for rec in sorted_records:
    rows.append([
        rec.get("timestamp") or "",
        clamp(rec.get("task") or "", 32),
        clamp(rec.get("model") or "", 24),
        fmt_int(as_int(rec.get("total_tokens"))),
        fmt_int(as_int(rec.get("prompt_tokens"))),
        fmt_int(as_int(rec.get("completion_tokens"))),
        fmt_int(as_int(rec.get("cached_tokens"))),
        fmt_int(as_int(rec.get("billable_units"))),
        fmt_int(as_int(rec.get("request_units"))),
        fmt_exit_code(rec.get("exit_code")),
        "yes" if rec.get("usage_captured") else "no",
    ])

widths = []
for index, header in enumerate(headers):
    column_values = [len(header)] + [len(row[index]) for row in rows]
    widths.append(max(column_values))

print()
header_line = "  ".join(header.ljust(widths[i]) for i, header in enumerate(headers))
separator = "  ".join("-" * widths[i] for i in range(len(headers)))
print(header_line)
print(separator)
for row in rows:
    print("  ".join(row[i].ljust(widths[i]) for i in range(len(headers))))
PY
}


cmd_reports() {
  local root=""
  local mode="list"
  local slug=""
  local open_editor=0
  local work_branch=""
  local push_after=1
  local prompt_only=0
  local reporter_filter=""
  local close_invalid=0
  local close_comment="Authenticity failed (automated by gpt-creator reports audit)."
  local close_comment_set=0
  local include_closed=0
  local audit_limit=""
  local audit_limit_set=0
  local digests_path=""
  local digests_path_set=0
  local invalid_label=""
  local invalid_label_set=0
  local allow_pairs=()

  while [[ $# -gt 0 ]]; do
    case "$1" in
      list)
        mode="list"
        shift
        ;;
      backlog)
        mode="backlog"
        shift
        ;;
      auto)
        mode="auto"
        shift
        ;;
      audit)
        mode="audit"
        shift
        ;;
      work)
        mode="work"
        shift
        if [[ $# -eq 0 ]]; then
          die "reports work requires a slug identifier"
        fi
        slug="$1"
        shift
        ;;
      show)
        mode="show"
        shift
        if [[ $# -eq 0 ]]; then
          die "reports show requires a slug identifier"
        fi
        slug="$1"
        shift
        ;;
      --project)
        root="$(abs_path "$2")"
        shift 2
        ;;
      --open)
        open_editor=1
        shift
        ;;
      --branch)
        work_branch="$2"
        shift 2
        ;;
      --no-push)
        push_after=0
        shift
        ;;
      --push)
        push_after=1
        shift
        ;;
      --prompt-only)
        prompt_only=1
        shift
        ;;
      --reporter)
        reporter_filter="$2"
        shift 2
        ;;
      --close-invalid)
        close_invalid=1
        shift
        ;;
      --no-close-invalid)
        close_invalid=0
        shift
        ;;
      --comment)
        close_comment="${2:?--comment requires text}"
        close_comment_set=1
        shift 2
        ;;
      --include-closed)
        include_closed=1
        shift
        ;;
      --limit)
        audit_limit="${2:?--limit requires a positive integer}"
        audit_limit_set=1
        shift 2
        ;;
      --digests)
        digests_path="${2:?--digests requires a file path}"
        digests_path_set=1
        shift 2
        ;;
      --allow)
        allow_pairs+=("${2:?--allow requires VERSION=SHA256}")
        shift 2
        ;;
      --label-invalid)
        invalid_label="${2:?--label-invalid requires a value}"
        invalid_label_set=1
        shift 2
        ;;
      --no-label-invalid)
        invalid_label=""
        invalid_label_set=1
        shift
        ;;
      -h|--help)
        cat <<'USAGE'
Usage:
  gpt-creator reports [--project PATH]
  gpt-creator reports list [--project PATH]
  gpt-creator reports backlog [--project PATH]
  gpt-creator reports auto [--project PATH] [--reporter NAME] [--no-push] [--prompt-only]
  gpt-creator reports work [--project PATH] [--branch NAME] [--no-push] [--prompt-only] <slug>
  gpt-creator reports [--project PATH] [--open] <slug>
  gpt-creator reports show [--project PATH] [--open] <slug>
  gpt-creator reports audit [--close-invalid] [--include-closed] [--limit N]
                               [--digests FILE] [--allow VERSION=SHA256]
                               [--label-invalid NAME|--no-label-invalid]
                               [--comment TEXT]

List captured crash/stall reports, show the open backlog, automatically resolve matching issues, assign Codex to resolve a single report, display a specific entry, or audit GitHub auto-reports for authenticity.
USAGE
        return 0
        ;;
      *)
        if [[ -z "$slug" && "$mode" == "list" ]]; then
          mode="show"
          slug="$1"
          shift
        else
          die "Unknown reports argument: $1"
        fi
        ;;
    esac
  done

  if [[ "$mode" == "show" && -z "$slug" ]]; then
    die "reports show requires a slug identifier"
  fi

  if [[ "$mode" == "work" && -z "$slug" ]]; then
    die "reports work requires a slug identifier"
  fi

  if [[ "$mode" == "work" && "$open_editor" -ne 0 ]]; then
    die "--open cannot be combined with reports work"
  fi

  if [[ "$mode" == "auto" && "$open_editor" -ne 0 ]]; then
    die "--open cannot be combined with reports auto"
  fi

  if [[ "$mode" == "auto" && -n "$work_branch" ]]; then
    die "--branch is not supported for reports auto"
  fi

  if [[ "$mode" != "work" && "$mode" != "auto" ]]; then
    if [[ -n "$work_branch" || "$prompt_only" -ne 0 || "$push_after" -ne 1 ]]; then
      die "reports options --branch/--no-push/--prompt-only are only valid with 'work' or 'auto'"
    fi
  fi

  if [[ "$mode" != "auto" && -n "$reporter_filter" ]]; then
    die "--reporter is only supported with reports auto"
  fi

  if [[ -n "$audit_limit" ]]; then
    if [[ ! "$audit_limit" =~ ^[0-9]+$ ]]; then
      die "--limit expects a non-negative integer"
    fi
  fi

  if [[ "$mode" != "audit" ]]; then
    if (( close_invalid != 0 || include_closed != 0 || close_comment_set != 0 || audit_limit_set != 0 || digests_path_set != 0 || invalid_label_set != 0 || ${#allow_pairs[@]} != 0 )); then
      die "reports options --close-invalid/--include-closed/--limit/--digests/--allow/--label-invalid/--comment are only valid with 'audit'"
    fi
  fi

  local pair_entry
  for pair_entry in "${allow_pairs[@]}"; do
    if [[ ! "$pair_entry" =~ ^[^=]+=[0-9a-fA-F]{64}$ ]]; then
      die "--allow expects VERSION=SHA256 (got '${pair_entry}')"
    fi
  done

  if [[ "$mode" != "audit" ]]; then
    if [[ -n "$root" ]]; then
      ensure_ctx "$root"
    else
      ensure_ctx "${PROJECT_ROOT:-$PWD}"
    fi
  elif [[ -n "$root" ]]; then
    ensure_ctx "$root"
  fi

  local reports_dir=""
  if [[ "$mode" != "audit" ]]; then
    reports_dir="$(gc_reports_dir)" || die "Unable to access issue report directory"
  fi

  case "$mode" in
    audit)
      local repo="${GC_GITHUB_REPO:-}"
      local token="${GC_GITHUB_TOKEN:-}"
      if [[ -z "$repo" || -z "$token" ]]; then
        die "reports audit requires GC_GITHUB_REPO and GC_GITHUB_TOKEN to be set"
      fi
      local state="open"
      if (( include_closed )); then
        state="all"
      fi
      if [[ -z "$digests_path" ]]; then
        if [[ -n "${GC_REPORT_DIGESTS_PATH:-}" ]]; then
          digests_path="${GC_REPORT_DIGESTS_PATH}"
        elif [[ -n "${CLI_ROOT:-}" && -f "${CLI_ROOT}/config/release-digests.json" ]]; then
          digests_path="${CLI_ROOT}/config/release-digests.json"
        fi
      fi
      local allow_join=""
      if ((${#allow_pairs[@]} > 0)); then
        allow_join="$(printf '%s\n' "${allow_pairs[@]}")"
      fi
      python3 - <<'PY' "$repo" "$token" "$state" "$close_invalid" "$close_comment" "$audit_limit" "$digests_path" "$invalid_label" "$allow_join"
import json
import os
import sys
import urllib.error
import urllib.parse
import urllib.request
import hashlib
from textwrap import indent

repo, token, state, close_flag, close_comment, limit_value, digests_path, invalid_label, allow_payload = sys.argv[1:10]
close_invalid = close_flag == "1"
limit = None
if limit_value:
    try:
        parsed_limit = int(limit_value)
        if parsed_limit > 0:
            limit = parsed_limit
    except ValueError:
        limit = None

session_headers = {
    "Authorization": f"Bearer {token}",
    "Accept": "application/vnd.github+json",
    "Content-Type": "application/json",
    "X-GitHub-Api-Version": "2022-11-28",
    "User-Agent": "gpt-creator-reports-audit",
}

def github_request(url, method="GET", payload=None):
    data = None
    if payload is not None:
        data = json.dumps(payload).encode("utf-8")
    request = urllib.request.Request(url, data=data, headers=session_headers, method=method)
    with urllib.request.urlopen(request) as resp:
        if resp.status == 204:
            return None
        body = resp.read().decode("utf-8")
        if not body:
            return None
        return json.loads(body)

def load_allowlist(path, inline_pairs):
    mapping = {}
    inline_pairs = [line.strip() for line in inline_pairs.splitlines() if line.strip()]
    for entry in inline_pairs:
        if "=" not in entry:
            continue
        version, digest = entry.split("=", 1)
        version = version.strip()
        digest = digest.strip().lower()
        if not version or len(digest) != 64:
            continue
        mapping.setdefault(version, set()).add(digest)
    if not path:
        return mapping
    try:
        with open(path, "r", encoding="utf-8") as fh:
            data = json.load(fh)
    except FileNotFoundError:
        return mapping
    except Exception as exc:
        print(f"Failed to read digest allowlist '{path}': {exc}", file=sys.stderr)
        return mapping
    if isinstance(data, dict):
        iterable = data.items()
    elif isinstance(data, list):
        iterable = []
        for item in data:
            if isinstance(item, dict):
                version = str(item.get("version") or "").strip()
                if not version:
                    continue
                sha_values = []
                sha_field = item.get("sha256")
                if isinstance(sha_field, str):
                    sha_values = [sha_field]
                elif isinstance(sha_field, list):
                    sha_values = [val for val in sha_field if isinstance(val, str)]
                elif isinstance(sha_field, dict):
                    sha_values = []
                    for val in sha_field.values():
                        if isinstance(val, str):
                            sha_values.append(val)
                        elif isinstance(val, list):
                            sha_values.extend(x for x in val if isinstance(x, str))
                for digest in sha_values:
                    mapping.setdefault(version, set()).add(digest.lower())
            elif isinstance(item, str):
                if "=" in item:
                    version, digest = item.split("=", 1)
                    mapping.setdefault(version.strip(), set()).add(digest.strip().lower())
        return mapping
    else:
        return mapping
    for version, values in iterable:
        if isinstance(values, str):
            mapping.setdefault(str(version), set()).add(values.lower())
        elif isinstance(values, list):
            mapping.setdefault(str(version), set()).update(val.lower() for val in values if isinstance(val, str))
        elif isinstance(values, dict):
            for val in values.values():
                if isinstance(val, str):
                    mapping.setdefault(str(version), set()).add(val.lower())
                elif isinstance(val, list):
                    mapping.setdefault(str(version), set()).update(v.lower() for v in val if isinstance(v, str))
    return mapping

digest_allowlist = load_allowlist(digests_path, allow_payload or "")

def fetch_issues():
    collected = []
    page = 1
    per_page = 100
    while True:
        url = f"https://api.github.com/repos/{repo}/issues?state={urllib.parse.quote(state)}&labels=auto-report&per_page={per_page}&page={page}"
        try:
            items = github_request(url)
        except urllib.error.HTTPError as err:
            text = err.read().decode("utf-8", "ignore")
            print(f"GitHub API error while fetching issues: {err.code} {text}", file=sys.stderr)
            return collected
        except Exception as exc:
            print(f"Failed to fetch GitHub issues: {exc}", file=sys.stderr)
            return collected
        if not items:
            break
        for issue in items:
            if "pull_request" in issue:
                continue
            collected.append(issue)
            if limit is not None and len(collected) >= limit:
                return collected
        if len(items) < per_page:
            break
        page += 1
    return collected

def parse_issue(issue):
    body = issue.get("body") or ""
    metadata = {}
    for line in body.splitlines():
        stripped = line.strip()
        if stripped.startswith("- **") and "**:" in stripped:
            head, value = stripped.split("**:", 1)
            label = head.replace("- **", "").strip()
            metadata[label] = value.strip()
    watermark_token = ""
    start = body.find("<!--")
    while start != -1:
        end = body.find("-->", start + 4)
        if end == -1:
            break
        comment = body[start + 4:end].strip()
        if comment.lower().startswith("gpt-creator:"):
            watermark_token = comment.split(":", 1)[1].strip()
            break
        start = body.find("<!--", end + 3)
    version = metadata.get("CLI Version", "").strip()
    binary_hash = metadata.get("CLI Binary SHA256", "").strip().lower()
    signature = metadata.get("CLI Signature", "").strip().lower()
    expected_signature = ""
    signature_valid = False
    watermark_valid = False
    reasons = []
    if version or binary_hash:
        expected_signature = hashlib.sha256(f"{version}:{binary_hash}".encode("utf-8")).hexdigest()
    if not binary_hash or len(binary_hash) != 64:
        reasons.append("missing CLI binary hash")
    if not version:
        reasons.append("missing CLI version")
    if signature and expected_signature:
        if signature == expected_signature:
            signature_valid = True
        else:
            reasons.append("signature mismatch")
    else:
        reasons.append("missing CLI signature")
    if watermark_token:
        watermark_expected = f"{version or 'unknown'}:{expected_signature}" if expected_signature else ""
        if watermark_expected and watermark_token.lower() == watermark_expected.lower():
            watermark_valid = True
        elif version and signature_valid and watermark_token.lower() == f"{version.lower()}:unsigned":
            watermark_valid = False
            reasons.append("watermark unsigned but signature present")
        else:
            reasons.append("watermark mismatch")
    else:
        reasons.append("missing watermark")
    digest_allowed = True
    if digest_allowlist:
        allowed_hashes = digest_allowlist.get(version, set())
        if allowed_hashes:
            if binary_hash not in allowed_hashes:
                digest_allowed = False
                reasons.append("binary hash not in allowlist")
        else:
            digest_allowed = False
            reasons.append("version not present in allowlist")
    valid = signature_valid and watermark_valid and digest_allowed
    return {
        "number": issue.get("number"),
        "title": issue.get("title") or "",
        "url": issue.get("html_url") or "",
        "state": issue.get("state"),
        "version": version,
        "binary_hash": binary_hash,
        "signature": signature,
        "expected_signature": expected_signature,
        "signature_valid": signature_valid,
        "watermark": watermark_token,
        "watermark_valid": watermark_valid,
        "digest_allowed": digest_allowed,
        "valid": valid,
        "reasons": reasons,
        "metadata": metadata,
    }

def ensure_label(issue_number):
    if not invalid_label:
        return
    try:
        issue = github_request(f"https://api.github.com/repos/{repo}/issues/{issue_number}")
    except Exception:
        return
    if not issue:
        return
    labels = issue.get("labels") or []
    label_names = {lbl["name"] if isinstance(lbl, dict) else str(lbl) for lbl in labels}
    if invalid_label in label_names:
        return
    label_names.add(invalid_label)
    payload = {"labels": sorted(label_names)}
    try:
        github_request(f"https://api.github.com/repos/{repo}/issues/{issue_number}", method="PATCH", payload=payload)
    except Exception:
        pass

def close_issue(item):
    number = item["number"]
    if not close_invalid:
        return
    if str(item.get("state")) == "closed":
        return
    try:
        github_request(
            f"https://api.github.com/repos/{repo}/issues/{number}/comments",
            method="POST",
            payload={"body": close_comment},
        )
    except Exception as exc:
        print(f"Failed to comment on issue #{number}: {exc}", file=sys.stderr)
    try:
        github_request(
            f"https://api.github.com/repos/{repo}/issues/{number}",
            method="PATCH",
            payload={"state": "closed"},
        )
    except Exception as exc:
        print(f"Failed to close issue #{number}: {exc}", file=sys.stderr)
    else:
        ensure_label(number)
        print(f"Closed issue #{number} ({item['title']}) as authenticity failed.")

issues = fetch_issues()
if not issues:
    print("No GitHub issues labelled 'auto-report' found.")
    sys.exit(0)

parsed = [parse_issue(issue) for issue in issues]
valid_count = sum(1 for item in parsed if item["valid"])
invalid_items = [item for item in parsed if not item["valid"]]

print(f"Audited {len(parsed)} auto-report issue(s) [{state}].")
print(f" - Valid:   {valid_count}")
print(f" - Invalid: {len(invalid_items)}")
if digest_allowlist:
    print(f" - Allowlist source: {digests_path or 'inline overrides only'}")

def summarize(item):
    header = f"#{item['number']} [{item['state']}] {item['title']}"
    verdict = "VALID" if item["valid"] else "INVALID"
    print(f"\n{header}\nResult: {verdict}")
    print(f"URL: {item['url']}")
    print(f"Version: {item['version'] or '(missing)'}")
    print(f"Binary SHA256: {item['binary_hash'] or '(missing)'}")
    print(f"Signature: {item['signature'] or '(missing)'}")
    print(f"Watermark: {item['watermark'] or '(missing)'}")
    print(f"Signature OK: {'yes' if item['signature_valid'] else 'no'}")
    print(f"Watermark OK: {'yes' if item['watermark_valid'] else 'no'}")
    if digest_allowlist:
        print(f"Allowlist OK: {'yes' if item['digest_allowed'] else 'no'}")
    if item["reasons"]:
        detail = "\n".join(f"- {reason}" for reason in item["reasons"])
        print("Notes:\n" + indent(detail, "  "))

for item in parsed:
    summarize(item)
    if not item["valid"]:
        close_issue(item)

PY
      return
      ;;
    list|backlog)
      if [[ -z "$(find "$reports_dir" -maxdepth 1 -type f \( -name '*.yml' -o -name '*.yaml' \) -print -quit 2>/dev/null)" ]]; then
        info "No issue reports recorded yet."
        return 0
      fi
      python3 - <<'PY' "$reports_dir" "$mode"
import datetime
import os
import sys

dir_path = sys.argv[1]
mode = sys.argv[2]
entries = []

for name in os.listdir(dir_path):
    lower = name.lower()
    if not (lower.endswith(".yml") or lower.endswith(".yaml")):
        continue
    path = os.path.join(dir_path, name)
    if not os.path.isfile(path):
        continue
    try:
        stat = os.stat(path)
    except OSError:
        continue
    slug = os.path.splitext(name)[0]
    summary = ""
    priority = ""
    timestamp = ""
    issue_type = ""
    status = ""
    likes = 0
    comments = 0
    reporter = ""
    metadata = False
    try:
        with open(path, "r", encoding="utf-8", errors="replace") as fh:
            for line in fh:
                stripped = line.strip()
                if stripped.startswith("summary:") and not summary:
                    value = stripped.split(":", 1)[1].strip()
                    if value.startswith('"') and value.endswith('"') and len(value) >= 2:
                        value = value[1:-1]
                    summary = value
                elif stripped.startswith("priority:") and not priority:
                    priority = stripped.split(":", 1)[1].strip()
                elif stripped.startswith("type:") and not issue_type:
                    issue_type = stripped.split(":", 1)[1].strip()
                elif stripped.startswith("timestamp:") and not timestamp:
                    timestamp = stripped.split(":", 1)[1].strip().strip('"')
                if stripped == "metadata:":
                    metadata = True
                    continue
                if metadata:
                    if not line.startswith("  "):
                        metadata = False
                    else:
                        meta = line.strip()
                        if meta.startswith("timestamp:") and not timestamp:
                            timestamp = meta.split(":", 1)[1].strip().strip('"')
                        elif meta.startswith("status:") and not status:
                            status = meta.split(":", 1)[1].strip()
                        elif meta.startswith("likes:") and likes == 0:
                            try:
                                likes = int(meta.split(":", 1)[1].strip())
                            except ValueError:
                                likes = 0
                        elif meta.startswith("comments:") and comments == 0:
                            try:
                                comments = int(meta.split(":", 1)[1].strip())
                            except ValueError:
                                comments = 0
                        elif meta.startswith("reporter:") and not reporter:
                            reporter = meta.split(":", 1)[1].strip().strip('"')
    except OSError:
        continue
    popularity = likes + comments
    entries.append((stat.st_mtime, slug, summary, priority, timestamp, issue_type, status, popularity, likes, comments, reporter))

entries.sort(key=lambda item: item[0], reverse=True)

printed = False
for mtime, slug, summary, priority, timestamp, issue_type, status, popularity, likes, comments, reporter in entries:
    if mode == "backlog" and status and status.lower() not in ("open", "new", "todo"):
        continue
    summary = summary or "(no summary)"
    priority = priority or "unknown"
    issue_type = issue_type or "unknown"
    status_display = status or "open"
    display_time = timestamp or datetime.datetime.utcfromtimestamp(mtime).strftime("%Y-%m-%dT%H:%M:%SZ")
    print(f"[{slug}] {display_time} {priority} ({issue_type}) status={status_display} pop={popularity} (likes={likes}, comments={comments})")
    if reporter:
        print(f"  reporter={reporter}")
    print(f"  {summary}")
    print()
    printed = True

if mode == "backlog" and not printed:
    print("No open reports found.")
PY
      ;;
    auto)
      local reporter="${reporter_filter:-$(gc_reports_current_user)}"
      info "Auto-processing reports for reporter: ${reporter}"
      local auto_slugs=()
      while IFS= read -r slug_entry; do
        [[ -n "$slug_entry" ]] && auto_slugs+=("$slug_entry")
      done < <(python3 - <<'PY' "$reports_dir" "$reporter"
import os
import sys

dir_path = sys.argv[1]
reporter_filter = sys.argv[2].strip()
entries = []

for name in os.listdir(dir_path):
    lower = name.lower()
    if not (lower.endswith('.yml') or lower.endswith('.yaml')):
        continue
    path = os.path.join(dir_path, name)
    if not os.path.isfile(path):
        continue
    try:
        stat = os.stat(path)
    except OSError:
        continue
    slug = os.path.splitext(name)[0]
    metadata = {}
    metadata_active = False
    try:
        with open(path, 'r', encoding='utf-8', errors='replace') as fh:
            for line in fh:
                stripped = line.strip()
                if stripped == 'metadata:':
                    metadata_active = True
                    continue
                if metadata_active:
                    if line.startswith('  '):
                        if ':' in stripped:
                            k, v = stripped.split(':', 1)
                            metadata[k.strip()] = v.strip().strip('"')
                    else:
                        metadata_active = False
    except OSError:
        continue
    reporter = metadata.get('reporter', '')
    status = metadata.get('status', 'open').lower()
    if reporter_filter and reporter.lower() != reporter_filter.lower():
        continue
    if status not in ('open', 'new', 'todo'):
        continue
    entries.append((stat.st_mtime, slug))

entries.sort(key=lambda item: item[0], reverse=True)

for _, slug in entries:
    print(slug)
PY
)
      if ((${#auto_slugs[@]} == 0)); then
        info "No matching reports for reporter '${reporter}'."
        return 0
      fi
      local slug_entry
      for slug_entry in "${auto_slugs[@]}"; do
        info "Resolving report ${slug_entry}"
        if ! gc_reports_run_work "$slug_entry" "" "$push_after" "$prompt_only" "$reporter"; then
          die "Codex failed while processing report ${slug_entry}"
        fi
      done
      ;;
    show)
      local report_path=""
      if ! report_path="$(gc_reports_resolve_slug "$slug")"; then
        die "No issue report found for slug: ${slug}"
      fi
      info "Report file: ${report_path}"
      printf '\n'
      cat "$report_path"
      printf '\n'
      if (( open_editor )); then
        local editor_cmd="${EDITOR_CMD:-${EDITOR:-vi}}"
        if [[ -z "$editor_cmd" ]]; then
          warn "EDITOR_CMD not set; skipping --open"
        else
          info "Opening report in ${editor_cmd}"
          if ! bash -lc "${editor_cmd} \"${report_path}\""; then
            warn "Failed to launch editor command: ${editor_cmd}"
          fi
        fi
      fi
      ;;
    work)
      if ! gc_reports_run_work "$slug" "$work_branch" "$push_after" "$prompt_only" ""; then
        die "Codex failed to resolve report ${slug}"
      fi
      ;;
  esac
}

cmd_create_project() {
  local template_request="auto"
  local path=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --template)
        template_request="${2:?--template requires a value (template name or 'auto')}"
        shift 2
        ;;
      --skip-template)
        template_request="skip"
        shift
        ;;
      -h|--help)
        cat <<'USAGE'
Usage: gpt-creator create-project [--template NAME|auto|skip] [--skip-template] <path>

Create a new project root, optionally scaffold it from a project template, then
run the full build pipeline (scan → normalize → plan → generate → db → run → verify).
USAGE
        return 0
        ;;
      *)
        if [[ -z "$path" ]]; then
          path="$1"
        else
          die "Unexpected argument: $1"
        fi
        shift
        ;;
    esac
  done

  [[ -n "$path" ]] || die "create-project requires a path"

  local project_root
  project_root="$(abs_path "$path")"
  mkdir -p "$project_root"

  if ! gc_apply_project_template "$project_root" "$template_request"; then
    warn "Project template application reported issues; continuing with base scaffolding."
  fi

  ensure_ctx "$project_root"
  info "Project root: ${PROJECT_ROOT}"

  cmd_scan --project "$PROJECT_ROOT"
  cmd_normalize --project "$PROJECT_ROOT"
  cmd_plan --project "$PROJECT_ROOT"
  cmd_generate all --project "$PROJECT_ROOT"
  cmd_db provision --project "$PROJECT_ROOT" || warn "Database provision step reported an error"
  cmd_run up --project "$PROJECT_ROOT" || warn "Stack start reported an error"
  cmd_verify acceptance --project "$PROJECT_ROOT" || warn "Acceptance checks failing — review stack health."
  ok "Project bootstrap complete"
}

cmd_bootstrap() {
  local template_request="auto"
  local path=""
  local fresh=0
  local rfp_path=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --template)
        template_request="${2:?--template requires a value (template name or 'auto')}"
        shift 2
        ;;
      --skip-template)
        template_request="skip"
        shift
        ;;
      --rfp)
        rfp_path="${2:?--rfp requires a file path}"
        shift 2
        ;;
      --fresh)
        fresh=1
        shift
        ;;
      --fresh)
        fresh=1
        shift
        ;;
      -h|--help)
        cat <<'USAGE'
Usage: gpt-creator bootstrap [--template NAME|auto|skip] [--skip-template] [--rfp FILE] [--fresh] <path>

Generate a full project from an RFP in one step by running:
  • create-pdr → create-sds → create-db-dump → create-jira-tasks → scan/normalize/plan/generate/db/run/verify
Templates from project_templates/ are applied first (auto-selected by default).
USAGE
        return 0
        ;;
      *)
        if [[ -z "$path" ]]; then
          path="$1"
        else
          die "Unexpected argument: $1"
        fi
        shift
        ;;
    esac
  done

  [[ -n "$path" ]] || die "bootstrap requires a path"

  if [[ -n "$rfp_path" ]]; then
    [[ -f "$rfp_path" ]] || die "RFP file not found: ${rfp_path}"
  fi

  local project_root
  project_root="$(abs_path "$path")"
  mkdir -p "$project_root"

  ensure_ctx "$project_root"
  info "Project root: ${PROJECT_ROOT}"

  if (( fresh )); then
    gc_bootstrap_reset_state
  fi

  mkdir -p "$(gc_bootstrap_state_dir)"

  if gc_bootstrap_step_is_done template; then
    info "Step 'template' already completed; skipping."
  else
    if gc_apply_project_template "$PROJECT_ROOT" "$template_request"; then
      gc_bootstrap_mark_step template done
    else
      gc_bootstrap_mark_step template failed
      die "Project template application failed"
    fi
  fi

  if [[ -n "$rfp_path" ]]; then
    local staged_rfp="${INPUT_DIR}/rfp.md"
    local staged_docs_rfp="${STAGING_DIR}/docs/rfp.md"
    if gc_bootstrap_step_is_done stage-rfp; then
      if [[ ! -f "$staged_rfp" || ! -f "$staged_docs_rfp" ]]; then
        info "Restaging RFP (previous artifacts missing)."
        gc_bootstrap_mark_step stage-rfp reset
      fi
    fi
    if ! gc_bootstrap_step_is_done stage-rfp; then
      mkdir -p "${INPUT_DIR}"
      cp "$rfp_path" "$staged_rfp"
      mkdir -p "${STAGING_DIR}/docs"
      cp "$rfp_path" "${STAGING_DIR}/docs/rfp.md"
      gc_bootstrap_mark_step stage-rfp done
      info "Staged RFP → ${staged_rfp}"
    else
      info "Step 'stage-rfp' already completed; skipping."
    fi
  fi

  info "[1/10] Scanning documentation"
  if ! gc_bootstrap_run_step scan cmd_scan --project "$PROJECT_ROOT"; then
    die "Bootstrap halted during scan"
  fi

  info "[2/10] Normalizing documentation"
  if ! gc_bootstrap_run_step normalize cmd_normalize --project "$PROJECT_ROOT"; then
    die "Bootstrap halted during normalize"
  fi

  info "[3/10] Generating Product Requirements Document"
  if gc_bootstrap_step_is_done create-pdr; then
    info "Step 'create-pdr' already completed; skipping."
  else
    if ! gc_bootstrap_have_rfp; then
      warn "No RFP found in staging; skipping create-pdr. Provide --rfp or add .gpt-creator/staging/docs/rfp.md to enable this step."
      gc_bootstrap_mark_step create-pdr done
    elif bash "$CLI_ROOT/src/cli/create-pdr.sh" --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step create-pdr done
    else
      gc_bootstrap_mark_step create-pdr failed
      die "create-pdr failed"
    fi
  fi

  info "[4/10] Generating System Design Specification"
  if gc_bootstrap_step_is_done create-sds; then
    info "Step 'create-sds' already completed; skipping."
  else
    if bash "$CLI_ROOT/src/cli/create-sds.sh" --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step create-sds done
    else
      gc_bootstrap_mark_step create-sds failed
      die "create-sds failed"
    fi
  fi

  info "[5/11] Generating database schema & seed dumps"
  if gc_bootstrap_step_is_done create-db-dump; then
    info "Step 'create-db-dump' already completed; skipping."
  else
    if bash "$CLI_ROOT/src/cli/create-db-dump.sh" --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step create-db-dump done
    else
      gc_bootstrap_mark_step create-db-dump failed
      die "create-db-dump failed"
    fi
  fi

  info "[6/11] Mining Jira tasks"
  if ! gc_bootstrap_step_is_done create-jira-tasks; then
    if cmd_create_jira_tasks --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step create-jira-tasks done
    else
      gc_bootstrap_mark_step create-jira-tasks failed
      die "create-jira-tasks failed"
    fi
  else
    info "Step 'create-jira-tasks' already completed; skipping."
  fi

  info "[7/11] Planning build"
  if ! gc_bootstrap_step_is_done plan; then
    if cmd_plan --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step plan done
    else
      gc_bootstrap_mark_step plan failed
      die "plan step failed"
    fi
  else
    info "Step 'plan' already completed; skipping."
  fi

  if ! gc_bootstrap_step_is_done generate; then
    info "[8/11] Generating stack code"
    if cmd_generate all --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step generate done
    else
      gc_bootstrap_mark_step generate failed
      die "generate step failed"
    fi
  else
    info "Step 'generate' already completed; skipping."
  fi

  info "[9/11] Provisioning infrastructure"
  if ! gc_bootstrap_step_is_done db-provision; then
    if cmd_db provision --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step db-provision done
    else
      gc_bootstrap_mark_step db-provision failed
      die "Database provision failed"
    fi
  else
    info "Step 'db-provision' already completed; skipping."
  fi

  info "[10/11] Starting stack"
  if ! gc_bootstrap_step_is_done run-up; then
    if cmd_run up --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step run-up done
    else
      gc_bootstrap_mark_step run-up failed
      die "Stack start failed"
    fi
  else
    info "Step 'run-up' already completed; skipping."
  fi

  if ! gc_bootstrap_step_is_done verify; then
    if cmd_verify acceptance --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step verify done
    else
      gc_bootstrap_mark_step verify failed
      die "Acceptance verification failed"
    fi
  else
    info "Step 'verify' already completed; skipping."
  fi

  info "[11/11] Verifying acceptance"
  gc_bootstrap_mark_complete
  ok "Bootstrap complete — code, docs, and backlog generated"
}

cmd_update() {
  local force=0
  local repo_url="${GC_UPDATE_REPO_URL:-https://github.com/bekirdag/gpt-creator.git}"
  local prefix="/usr/local"
  local tmpdir=""

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --force)
        force=1
        shift
        ;;
      -h|--help)
        cat <<EOF
Usage: ${APP_NAME} update [--force]

Fetches the latest gpt-creator sources and reinstalls the CLI.
EOF
        return 0
        ;;
      *)
        die "Unknown argument for update: $1"
        ;;
    esac
  done

  if ! command -v git >/dev/null 2>&1; then
    die "'git' is required for ${APP_NAME} update"
  fi

  tmpdir="$(mktemp -d "${TMPDIR:-/tmp}/gpt-creator-update.XXXXXX")" || die "Unable to create temporary directory"

  info "Cloning repository: $repo_url"
  if ! git clone "$repo_url" "$tmpdir"; then
    rm -rf "$tmpdir"
    die "Failed to clone repository from ${repo_url}"
  fi

  info "Fetching latest changes"
  if ! git -C "$tmpdir" pull --ff-only; then
    rm -rf "$tmpdir"
    die "Failed to update repository in $tmpdir"
  fi

  local install_script="$tmpdir/scripts/install.sh"
  if [[ ! -x "$install_script" ]]; then
    rm -rf "$tmpdir"
    die "Installer not found at $install_script"
  fi

  local -a install_cmd=("$install_script" --prefix "$prefix")
  if (( force )); then
    install_cmd+=("--force")
  fi

  info "Running installer: ${install_cmd[*]}"
  if ! "${install_cmd[@]}"; then
    rm -rf "$tmpdir"
    die "Install script failed"
  fi

  rm -rf "$tmpdir"
  ok "gpt-creator updated successfully"
}

cmd_tui() {
  ensure_go_runtime
  local go_bin="${GC_GO_BIN:-${GO_BIN:-go}}"
  if ! command -v "$go_bin" >/dev/null 2>&1; then
    die "Go 1.21+ is required to run the gpt-creator TUI. Automatic setup failed; install Go manually and set GO_BIN."
  fi
  local tui_dir="${CLI_ROOT}/tui"
  if [[ ! -d "$tui_dir" ]]; then
    die "TUI sources not found at ${tui_dir}"
  fi
  local skip_tidy="${GC_SKIP_TUI_TIDY:-}"
  info "Launching gpt-creator TUI (preview)"
  (
    cd "$tui_dir"
    if [[ -z "$skip_tidy" ]]; then
      info "Ensuring TUI Go modules are tidy"
      if ! "$go_bin" mod tidy >/dev/null 2>&1; then
        warn "'go mod tidy' reported issues; retrying with output"
        if ! "$go_bin" mod tidy; then
          die "Failed to tidy Go modules required for the TUI"
        fi
      fi
    fi
    "$go_bin" run . "$@"
  )
}

usage() {
cat <<EOF
${APP_NAME} v${VERSION}

Usage:
  ${APP_NAME} [--reports-on|--reports-off] [--reports-idle-timeout SECONDS] <command> [args]
  ${APP_NAME} create-project [--template NAME|auto|skip] [--skip-template] <path>
  ${APP_NAME} bootstrap [--template NAME|auto|skip] [--skip-template] [--rfp FILE] [--fresh] <path>
  ${APP_NAME} scan [--project <path>]
  ${APP_NAME} normalize [--project <path>]
  ${APP_NAME} plan [--project <path>]
  ${APP_NAME} generate <api|web|admin|db|docker|all> [--project <path>]
  ${APP_NAME} db <provision|import|seed> [--project <path>]
  ${APP_NAME} run <up|down|logs|open> [--project <path>]
  ${APP_NAME} refresh-stack [options]
  ${APP_NAME} verify <acceptance|nfr|all> [--project <path>] [--api-url API_BASE] [--api-health URL] [--web-url URL] [--admin-url URL]
  ${APP_NAME} create-sds [--project <path>] [--model NAME] [--dry-run] [--force]
  ${APP_NAME} create-pdr [--project <path>] [--model NAME] [--dry-run] [--force]
  ${APP_NAME} create-jira-tasks [--project <path>] [--model NAME] [--force] [--dry-run]
  ${APP_NAME} create-db-dump [--project <path>] [--model NAME] [--dry-run] [--force]
  ${APP_NAME} migrate-tasks [--project <path>] [--force]
  ${APP_NAME} refine-tasks [--project <path>] [--story SLUG] [--model NAME] [--dry-run]
  ${APP_NAME} create-tasks [--project <path>] [--jira <file>] [--force]
  ${APP_NAME} backlog [--project <path>] [--type epics|stories] [--item-children ID] [--progress] [--task-details ID]
  ${APP_NAME} estimate [--project <path>]
  ${APP_NAME} tokens [--project <path>] [--details] [--json]
  ${APP_NAME} reports [--project <path>] [list|backlog|show|work] [--branch NAME] [--no-push] [--prompt-only] [--open] [slug]
  ${APP_NAME} work-on-tasks [--project <path>] [--story ID|SLUG] [--fresh] [--no-verify] [--keep-artifacts]
  ${APP_NAME} iterate [--project <path>] [--jira <file>] [--no-verify] (deprecated)
  ${APP_NAME} tui
  ${APP_NAME} update [--force]
  ${APP_NAME} version
  ${APP_NAME} help

Global flags:
  --reports-on               Enable automatic crash/stall reports for the current invocation
  --reports-off              Disable automatic crash/stall reports (overrides GC_REPORTS_ON=1)
  --reports-idle-timeout <s> Override idle detection threshold in seconds (default: ${GC_REPORTS_IDLE_TIMEOUT})

Environment overrides:
  CODEX_BIN, CODEX_MODEL, DOCKER_BIN, MYSQL_BIN, EDITOR_CMD, GC_API_HEALTH_URL, GC_WEB_URL, GC_ADMIN_URL
EOF
}

main() {
  local cmd="${1:-help}"; shift || true
  local shell_bin="${BASH:-bash}"
  case "$cmd" in
    help|-h|--help) usage ;;
    version|-v|--version) echo "${APP_NAME} ${VERSION}" ;;
    create-project) cmd_create_project "$@" ;;
    bootstrap)      cmd_bootstrap "$@" ;;
    scan)           cmd_scan "$@" ;;
    normalize)      cmd_normalize "$@" ;;
    plan)           cmd_plan "$@" ;;
    generate)       cmd_generate "$@" ;;
    db)             cmd_db "$@" ;;
    run)            cmd_run "$@" ;;
    refresh-stack)  cmd_refresh_stack "$@" ;;
    verify)         cmd_verify "$@" ;;
    migrate-tasks)  cmd_migrate_tasks_json "$@" ;;
    refine-tasks)   cmd_refine_tasks "$@" ;;
    create-sds)     "$shell_bin" "$CLI_ROOT/src/cli/create-sds.sh" "$@" ;;
    create-pdr)     "$shell_bin" "$CLI_ROOT/src/cli/create-pdr.sh" "$@" ;;
    create-jira-tasks) "$CLI_ROOT/src/cli/create-jira-tasks.sh" "$@" ;;
    create-db-dump) "$shell_bin" "$CLI_ROOT/src/cli/create-db-dump.sh" "$@" ;;
    create-tasks)   cmd_create_tasks "$@" ;;
    backlog)        cmd_backlog "$@" ;;
    estimate)       cmd_estimate "$@" ;;
    tokens)         cmd_tokens "$@" ;;
    reports)        cmd_reports "$@" ;;
    task-convert)   cmd_task_convert "$@" ;;
    work-on-tasks)  cmd_work_on_tasks "$@" ;;
    iterate)        cmd_iterate "$@" ;;
    tui)            cmd_tui "$@" ;;
    update)         cmd_update "$@" ;;
    *) die "Unknown command: ${cmd}. See '${APP_NAME} help'" ;;
  esac
}

GC_ORIGINAL_ARGS=("$@")
gc_reports_extract_global_flags "$@"
set -- "${GC_FILTERED_ARGS[@]}"

GC_INVOCATION="$0"
if ((${#GC_ORIGINAL_ARGS[@]})); then
  GC_INVOCATION+=" $(printf '%q ' "${GC_ORIGINAL_ARGS[@]}")"
  GC_INVOCATION="${GC_INVOCATION% }"
fi

if gc_reports_enabled; then
  trap 'gc_reports_activity_trap "$BASH_COMMAND"' DEBUG
fi

trap 'gc_capture_error_context $? "$BASH_COMMAND"' ERR
trap 'gc_exit_handler $?' EXIT

main "$@"
