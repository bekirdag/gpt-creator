#!/usr/bin/env bash
# gpt-creator — scaffolding & orchestration CLI
# Aligns with Product Definition & Requirements (PDR v0.2)
# Usage: gpt-creator <command> [args]

set -Eeuo pipefail

if [[ -t 1 && $# -eq 0 && -z "${GC_SKIP_TUI_AUTO:-}" ]]; then
  export GC_SKIP_TUI_AUTO=1
  exec "$0" tui
fi

# Crash logging globals
GC_CRASH_LOGGED=0
GC_LAST_ERROR_CMD=""
GC_LAST_ERROR_STATUS=0
GC_FAIL_LOG_DIR=""
GC_INVOCATION=""

GC_LAST_CRASH_LOG=""
GC_MAIN_PID="$$"

if [[ -n "${GC_REPORTS_ON:-}" && "${GC_REPORTS_ON}" != "0" ]]; then
  GC_REPORTS_ON=1
else
  GC_REPORTS_ON=0
fi
GC_REPORTS_IDLE_TIMEOUT="${GC_REPORTS_IDLE_TIMEOUT:-1800}"
GC_REPORTS_CHECK_INTERVAL="${GC_REPORTS_CHECK_INTERVAL:-60}"
GC_REPORTS_INITIALIZED=0
GC_REPORTS_STORE_DIR=""
GC_REPORTS_ACTIVITY_FILE=""
GC_REPORTS_IDLE_SENTINEL=""
GC_REPORTS_WATCHDOG_PID=""
GC_FILTERED_ARGS=()

# Ensure docker compose commands have adequate timeouts unless caller overrides
if [[ -n "${GC_DOCKER_COMPOSE_TIMEOUT:-}" ]]; then
  COMPOSE_HTTP_TIMEOUT="$GC_DOCKER_COMPOSE_TIMEOUT"
  DOCKER_CLIENT_TIMEOUT="$GC_DOCKER_COMPOSE_TIMEOUT"
fi
: "${COMPOSE_HTTP_TIMEOUT:=600}"
: "${DOCKER_CLIENT_TIMEOUT:=$COMPOSE_HTTP_TIMEOUT}"
export COMPOSE_HTTP_TIMEOUT DOCKER_CLIENT_TIMEOUT

: "${GC_DOCKER_HEALTH_TIMEOUT:=10}"
: "${GC_DOCKER_HEALTH_INTERVAL:=1}"
: "${GC_PNPM_VERSION:=10.17.1}"
GC_CODEX_EXEC_TIMEOUT_INITIAL="${GC_CODEX_EXEC_TIMEOUT-}"
: "${GC_CODEX_EXEC_TIMEOUT:=0}"
: "${GC_CODEX_EXEC_MAX_DURATION:=900}"
: "${GC_CODEX_MAX_TURNS:=180}"
: "${GC_CODEX_MAX_TOKENS_PER_TASK:=0}"
export GC_CODEX_EXEC_MAX_DURATION
export GC_CODEX_MAX_TURNS

resolve_cli_root() {
  local source="${BASH_SOURCE[0]}"
  while [[ -L "$source" ]]; do
    local dir
    dir="$(cd "$(dirname "$source")" && pwd)"
    source="$(readlink "$source")"
    [[ "$source" != /* ]] && source="$dir/$source"
  done
  local abs_dir
  abs_dir="$(cd "$(dirname "$source")" && pwd)"
  GC_SELF_PATH="${abs_dir}/$(basename "$source")"
  cd "${abs_dir}/.." && pwd
}

cmd_task_convert() {
  warn "'task-convert' is deprecated; use 'create-tasks' instead. Running create-tasks now."
  cmd_create_tasks "$@"
}

CLI_ROOT="$(resolve_cli_root)"
unset -f resolve_cli_root

gc_configure_tmpdir() {
  local base="${1:-${PROJECT_ROOT:-$PWD}}"
  local dir="${base}/.gpt-creator/tmp"
  if mkdir -p "$dir" 2>/dev/null; then
    GC_TMP_DIR="$dir"
    TMPDIR="$dir"
    export GC_TMP_DIR TMPDIR
  else
    if [[ -n "${GC_TMP_DIR:-}" ]]; then
      :
    elif [[ -n "${TMPDIR:-}" && -d "${TMPDIR}" ]]; then
      GC_TMP_DIR="$TMPDIR"
      export GC_TMP_DIR
    fi
  fi
}

gc_configure_tmpdir

if [[ -z "${GC_COMMAND_FAILURE_CACHE:-}" ]]; then
  if [[ -n "${GC_TMP_DIR:-}" ]]; then
    GC_COMMAND_FAILURE_CACHE="${GC_TMP_DIR}/command-failures.json"
  else
    GC_COMMAND_FAILURE_CACHE="${TMPDIR:-/tmp}/command-failures.json"
  fi
fi
export GC_COMMAND_FAILURE_CACHE
GC_COMMAND_FAILURE_WARN_DIGESTS=""

if [[ -z "${GC_COMMAND_STREAM_CACHE:-}" ]]; then
  if [[ -n "${GC_TMP_DIR:-}" ]]; then
    GC_COMMAND_STREAM_CACHE="${GC_TMP_DIR}/command-stream.json"
  else
    GC_COMMAND_STREAM_CACHE="${TMPDIR:-/tmp}/command-stream.json"
  fi
fi
export GC_COMMAND_STREAM_CACHE
GC_COMMAND_STREAM_WARN_DIGESTS=""

if [[ -z "${GC_COMMAND_FILE_CACHE:-}" ]]; then
  if [[ -n "${GC_TMP_DIR:-}" ]]; then
    GC_COMMAND_FILE_CACHE="${GC_TMP_DIR}/command-file-cache.json"
  else
    GC_COMMAND_FILE_CACHE="${TMPDIR:-/tmp}/command-file-cache.json"
  fi
fi
export GC_COMMAND_FILE_CACHE
GC_COMMAND_FILE_WARN_DIGESTS=""

if [[ -z "${GC_COMMAND_SCAN_CACHE:-}" ]]; then
  if [[ -n "${GC_TMP_DIR:-}" ]]; then
    GC_COMMAND_SCAN_CACHE="${GC_TMP_DIR}/command-scan.json"
  else
    GC_COMMAND_SCAN_CACHE="${TMPDIR:-/tmp}/command-scan.json"
  fi
fi
export GC_COMMAND_SCAN_CACHE
GC_COMMAND_SCAN_WARN_DIGESTS=""
GC_COMMAND_GUARD_WARN_DIGESTS=""

gc_env_file() { echo "${PROJECT_ROOT:-$PWD}/.env"; }

gc_random_string() {
  python3 - <<'PY'
import secrets, string
alphabet = string.ascii_letters + string.digits
print(''.join(secrets.choice(alphabet) for _ in range(32)))
PY
}

gc_write_env_var() {
  local target="$1" key="$2" value="$3"
  python3 - <<'PY' "$target" "$key" "$value"
import pathlib, sys
path = pathlib.Path(sys.argv[1])
key = sys.argv[2]
value = sys.argv[3]
if path.exists():
    raw_lines = path.read_text().splitlines()
else:
    raw_lines = []
lines = []
for line in raw_lines:
    stripped = line.strip()
    if not stripped:
        lines.append('')
        continue
    if stripped.startswith('#') or '=' in line:
        lines.append(line)
for idx, line in enumerate(lines):
    if line.startswith(f"{key}="):
        lines[idx] = f"{key}={value}"
        break
else:
    lines.append(f"{key}={value}")
path.write_text('\n'.join(lines) + '\n')
PY
}

gc_set_env_var() {
  local key="$1" value="$2"
  local env_file="$(gc_env_file)"
  gc_write_env_var "$env_file" "$key" "$value"
}

gc_remove_env_var() {
  local target="$1" key="$2"
  [[ -f "$target" ]] || return 0
  python3 - <<'PY' "$target" "$key"
import pathlib, sys
path = pathlib.Path(sys.argv[1])
key = sys.argv[2]
if not path.exists():
    raise SystemExit(0)
lines = path.read_text().splitlines()
filtered = []
for line in lines:
    stripped = line.strip()
    if stripped.startswith('#'):
        filtered.append(line)
        continue
    if line.startswith(f"{key}=") or line.startswith(f"export {key}="):
        continue
    filtered.append(line)
path.write_text('\n'.join(filtered) + ('\n' if filtered else ''))
PY
}

gc_decode_base64() {
  python3 - <<'PY' "$1"
import base64, sys
try:
    sys.stdout.write(base64.b64decode(sys.argv[1]).decode('utf-8'))
except Exception:
    pass
PY
}

cmd_show_file() {
  local project="${PROJECT_ROOT:-$PWD}"
  local target_path=""
  local range_spec="" head_lines="" tail_lines="" refresh=0 diff_mode=0
  local max_lines="${GC_SHOW_FILE_MAX_LINES:-400}"

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project)
        project="$(abs_path "$2")"
        shift 2
        ;;
      --range)
        range_spec="$2"
        shift 2
        ;;
      --head)
        head_lines="$2"
        shift 2
        ;;
      --tail)
        tail_lines="$2"
        shift 2
        ;;
      --max-lines)
        max_lines="$2"
        shift 2
        ;;
      --refresh|--force)
        refresh=1
        shift
        ;;
      --diff)
        diff_mode=1
        shift
        ;;
      -h|--help)
        cat <<'EOHELP'
Usage: gpt-creator show-file [options] PATH

Display a cached snippet of PATH without repeatedly streaming large files.

Options:
  --project DIR       Project root (defaults to current PROJECT_ROOT)
  --range A:B         Show inclusive line range A..B (1-based)
  --head N            Show first N lines
  --tail N            Show last N lines
  --max-lines N       Default line count when no range/head/tail is provided (default: 400)
  --diff              Show a unified diff against the cached snapshot (if any)
  --refresh           Re-read the file and update the cache even if unchanged
EOHELP
        return 0
        ;;
      --)
        shift
        break
        ;;
      -*)
        die "Unknown show-file option: ${1}"
        ;;
      *)
        if [[ -z "$target_path" ]]; then
          target_path="$1"
          shift
        else
          break
        fi
        ;;
    esac
  done

  [[ -n "$target_path" ]] || die "show-file requires a path argument"
  local project_abs
  project_abs="$(abs_path "$project")"
  local resolved_path
  resolved_path="$(abs_path "$target_path")"
  [[ -f "$resolved_path" ]] || die "File not found: ${target_path}"

  [[ -z "$max_lines" || "$max_lines" =~ ^[0-9]+$ ]] || die "--max-lines must be numeric"
  [[ -z "$head_lines" || "$head_lines" =~ ^[0-9]+$ ]] || die "--head value must be numeric"
  [[ -z "$tail_lines" || "$tail_lines" =~ ^[0-9]+$ ]] || die "--tail value must be numeric"

  local cache_dir="${GC_TMP_DIR:-${project_abs}/.gpt-creator/tmp}/view-cache"
  mkdir -p "$cache_dir"

  local rel_path="$resolved_path"
  if [[ "$resolved_path" == "$project_abs"* ]]; then
    rel_path="${resolved_path#$project_abs/}"
  fi

  GC_SHOW_FILE_PROJECT="$project_abs" \
  GC_SHOW_FILE_PATH="$resolved_path" \
  GC_SHOW_FILE_REL="$rel_path" \
  GC_SHOW_FILE_RANGE="$range_spec" \
  GC_SHOW_FILE_HEAD="$head_lines" \
  GC_SHOW_FILE_TAIL="$tail_lines" \
  GC_SHOW_FILE_MAX_LINES="$max_lines" \
  GC_SHOW_FILE_REFRESH="$refresh" \
  GC_SHOW_FILE_DIFF="$diff_mode" \
  GC_SHOW_FILE_CACHE_DIR="$cache_dir" \
  python3 <<'PY'
import difflib
import hashlib
import json
import os
import sys
from datetime import datetime, timezone
from pathlib import Path

project_root = Path(os.environ.get("GC_SHOW_FILE_PROJECT", "") or ".").resolve()
path = Path(os.environ.get("GC_SHOW_FILE_PATH", "")).resolve()
rel_path = os.environ.get("GC_SHOW_FILE_REL") or str(path)
range_spec = (os.environ.get("GC_SHOW_FILE_RANGE") or "").strip()
head_lines = (os.environ.get("GC_SHOW_FILE_HEAD") or "").strip()
tail_lines = (os.environ.get("GC_SHOW_FILE_TAIL") or "").strip()
max_lines_raw = (os.environ.get("GC_SHOW_FILE_MAX_LINES") or "").strip()
refresh = os.environ.get("GC_SHOW_FILE_REFRESH") == "1"
diff_mode = os.environ.get("GC_SHOW_FILE_DIFF") == "1"
cache_dir = Path(os.environ.get("GC_SHOW_FILE_CACHE_DIR") or "").resolve()

if not path.exists():
    print(f"File not found: {rel_path}", file=sys.stderr)
    sys.exit(1)

try:
    max_lines = int(max_lines_raw or 400)
except Exception:
    max_lines = 400
if max_lines <= 0:
    max_lines = 400

mode_descriptor = []
if range_spec:
    mode_descriptor.append(f"range:{range_spec}")
if head_lines:
    mode_descriptor.append(f"head:{head_lines}")
if tail_lines:
    mode_descriptor.append(f"tail:{tail_lines}")
if not mode_descriptor:
    mode_descriptor.append(f"default:{max_lines}")
mode_string = "|".join(mode_descriptor)

cache_key_raw = f"{path}::{mode_string}"
cache_key = hashlib.sha256(cache_key_raw.encode("utf-8", "replace")).hexdigest()
cache_path = cache_dir / f"{cache_key}.json"

def load_lines(file_path: Path):
    text = file_path.read_text(encoding="utf-8", errors="replace")
    lines = text.splitlines()
    return lines, len(lines)

def select_snippet(lines, total):
    if total == 0:
        return [], 1, 0

    if range_spec:
        raw = range_spec
        if ":" in raw:
            start_s, end_s = raw.split(":", 1)
        else:
            start_s, end_s = raw, ""
        try:
            start_line = int(start_s) if start_s.strip() else 1
        except Exception:
            start_line = 1
        try:
            end_line = int(end_s) if end_s.strip() else total
        except Exception:
            end_line = total
        if start_line < 1:
            start_line = 1
        if end_line < start_line:
            end_line = start_line
        if end_line > total:
            end_line = total
        snippet = lines[start_line - 1: end_line]
        return snippet, start_line, end_line

    if head_lines:
        try:
            count = int(head_lines)
        except Exception:
            count = max_lines
        if count <= 0:
            count = max_lines
        snippet = lines[:count]
        end_line = min(total, count)
        return snippet, 1, end_line

    if tail_lines:
        try:
            count = int(tail_lines)
        except Exception:
            count = max_lines
        if count <= 0:
            count = max_lines
        snippet = lines[-count:] if count < total else lines[:]
        start_line = total - len(snippet) + 1
        return snippet, start_line, total

    snippet = lines[:min(total, max_lines)]
    end_line = len(snippet)
    return snippet, 1, end_line

def write_cache(payload):
    cache_dir.mkdir(parents=True, exist_ok=True)
    cache_path.write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")

def format_header(start_line, end_line, total, digest, note=""):
    details = f"{rel_path} (lines {start_line}-{end_line} of {total}; sha256 {digest})"
    if note:
        details += f" — {note}"
    return f"## {details}"

file_stat = path.stat()
existing = None
if cache_path.exists():
    try:
        existing = json.loads(cache_path.read_text(encoding="utf-8"))
    except Exception:
        existing = None

if existing and not refresh and not diff_mode:
    if (existing.get("mtime_ns") == file_stat.st_mtime_ns and
            existing.get("size") == file_stat.st_size):
        start_line = existing.get("start_line", 1)
        end_line = existing.get("end_line", start_line - 1)
        total_lines = existing.get("total_lines", "?")
        digest = existing.get("snippet_hash", "unknown")
        updated_at = existing.get("updated_at", "")
        note = f"cached snapshot {updated_at}" if updated_at else "cached snapshot"
        print(f"(cached) {rel_path} lines {start_line}-{end_line} of {total_lines} (sha256 {digest}); use --refresh to re-display or --diff to compare.")
        sys.exit(0)

lines, total_lines = load_lines(path)
snippet_lines, start_line, end_line = select_snippet(lines, total_lines)
snippet_text = "\n".join(snippet_lines)
snippet_hash = hashlib.sha256(snippet_text.encode("utf-8", "replace")).hexdigest()[:12]
timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ")

cache_payload = {
    "path": str(path),
    "relative": rel_path,
    "mode": mode_string,
    "start_line": start_line,
    "end_line": end_line,
    "total_lines": total_lines,
    "snippet_hash": snippet_hash,
    "snippet": snippet_text,
    "mtime_ns": file_stat.st_mtime_ns,
    "size": file_stat.st_size,
    "updated_at": timestamp,
}

if diff_mode and existing:
    old_snippet = existing.get("snippet", "")
    old_start = existing.get("start_line", 1)
    old_end = existing.get("end_line", old_start - 1)
    old_hash = existing.get("snippet_hash", "unknown")
    if old_snippet == snippet_text:
        print(f"(no diff) {rel_path} lines {start_line}-{end_line} unchanged compared to cached snapshot.")
    else:
        old_lines = old_snippet.splitlines()
        diff = list(difflib.unified_diff(
            old_lines,
            snippet_text.splitlines(),
            fromfile=f"cached/{rel_path}:{old_start}-{old_end}",
            tofile=f"updated/{rel_path}:{start_line}-{end_line}",
            lineterm="",
        ))
        if not diff:
            print(f"(no diff) {rel_path} lines {start_line}-{end_line} unchanged compared to cached snapshot.")
        else:
            print(format_header(start_line, end_line, total_lines, snippet_hash, note="diff vs cached"))
            for line in diff:
                print(line)
    write_cache(cache_payload)
    sys.exit(0)

print(format_header(start_line, end_line, total_lines, snippet_hash))
if snippet_text:
    print(snippet_text)
else:
    print("(no content in selected range)")

write_cache(cache_payload)
PY
}

gc_env_sync_ports() {
  GC_PORT_RESERVATIONS=""
  GC_DB_HOST_PORT="${GC_DB_HOST_PORT:-${DB_HOST_PORT:-${DB_PORT:-3306}}}"
  DB_NAME="${DB_NAME:-$GC_DB_NAME}"
  DB_USER="${DB_USER:-$GC_DB_USER}"
  DB_PASSWORD="${DB_PASSWORD:-$GC_DB_PASSWORD}"
  DB_ROOT_PASSWORD="${DB_ROOT_PASSWORD:-$GC_DB_ROOT_PASSWORD}"
  DB_HOST_PORT="${DB_HOST_PORT:-$GC_DB_HOST_PORT}"
  GC_API_HOST_PORT="${GC_API_HOST_PORT:-${API_HOST_PORT:-3000}}"
  GC_WEB_HOST_PORT="${GC_WEB_HOST_PORT:-${WEB_HOST_PORT:-5173}}"
  GC_ADMIN_HOST_PORT="${GC_ADMIN_HOST_PORT:-${ADMIN_HOST_PORT:-5174}}"
  GC_PROXY_HOST_PORT="${GC_PROXY_HOST_PORT:-${PROXY_HOST_PORT:-8080}}"
  API_HOST_PORT="${API_HOST_PORT:-$GC_API_HOST_PORT}"
  WEB_HOST_PORT="${WEB_HOST_PORT:-$GC_WEB_HOST_PORT}"
  ADMIN_HOST_PORT="${ADMIN_HOST_PORT:-$GC_ADMIN_HOST_PORT}"
  PROXY_HOST_PORT="${PROXY_HOST_PORT:-$GC_PROXY_HOST_PORT}"
  local api_base_default="http://localhost:${GC_API_HOST_PORT}/api/v1"
  GC_API_BASE_URL="${GC_API_BASE_URL:-$api_base_default}"
  VITE_API_BASE="${VITE_API_BASE:-$GC_API_BASE_URL}"
  local expected_health="${GC_API_BASE_URL%/}/health"
  if [[ -z "${GC_API_HEALTH_URL:-}" ]]; then
    GC_API_HEALTH_URL="$expected_health"
    gc_set_env_var GC_API_HEALTH_URL "$GC_API_HEALTH_URL"
  elif [[ "$GC_API_HEALTH_URL" == "http://localhost:${GC_API_HOST_PORT}/health" && "$expected_health" != "$GC_API_HEALTH_URL" ]]; then
    GC_API_HEALTH_URL="$expected_health"
    gc_set_env_var GC_API_HEALTH_URL "$GC_API_HEALTH_URL"
  fi
  local proxy_origin="http://localhost:${GC_PROXY_HOST_PORT}"
  GC_WEB_URL="${GC_WEB_URL:-${proxy_origin}/}"
  GC_ADMIN_URL="${GC_ADMIN_URL:-${proxy_origin}/admin/}"
  gc_reserve_port db "$GC_DB_HOST_PORT"
  gc_reserve_port api "$GC_API_HOST_PORT"
  gc_reserve_port web "$GC_WEB_HOST_PORT"
  gc_reserve_port admin "$GC_ADMIN_HOST_PORT"
  gc_reserve_port proxy "$GC_PROXY_HOST_PORT"

  if gc_reports_enabled; then
    gc_reports_initialize
    gc_reports_touch_activity "$(date +%s)"
  fi
}

gc_sanitize_env_file() {
  local env_file="$1"
  python3 - <<'PY' "$env_file"
import pathlib, re, sys
path = pathlib.Path(sys.argv[1])
if not path.exists():
    raise SystemExit(0)
pattern = re.compile(r"^(?:export\s+)?([A-Za-z_][A-Za-z0-9_]*)=(.*)$")
ansi_re = re.compile(r"\x1b\[[0-9;]*m")
whitespace_re = re.compile(r"\s")
lines = path.read_text().splitlines()
cleaned = []
for line in lines:
    stripped = line.strip()
    if not stripped:
        continue
    if stripped.startswith('#'):
        cleaned.append(line)
        continue
    match = pattern.match(line)
    if not match:
        # drop invalid line
        continue
    key, value = match.groups()
    value = ansi_re.sub('', value).strip()
    if '➜' in value or 'remapping' in value or value.startswith('Port '):
        continue
    if key.endswith('_HOST_PORT') or key in {'DB_HOST_PORT', 'DB_PORT', 'MYSQL_HOST_PORT'}:
        if not value.isdigit():
            continue
    if whitespace_re.search(value) and not (value.startswith('"') and value.endswith('"')) and not (value.startswith("'") and value.endswith("'")):
        # unquoted whitespace breaks sourcing; drop
        continue
    cleaned.append(f"{key}={value}")
path.write_text('\n'.join(cleaned) + ('\n' if cleaned else ''))
PY
}

gc_load_env() {
  local env_file="$(gc_env_file)"
  if [[ -f "$env_file" ]]; then
    gc_sanitize_env_file "$env_file"
    set -a
    # shellcheck disable=SC1090
    source "$env_file"
    set +a
  fi
  GC_DB_NAME="${GC_DB_NAME:-${DB_NAME:-app}}"
  GC_DB_USER="${GC_DB_USER:-${DB_USER:-app}}"
  GC_DB_PASSWORD="${GC_DB_PASSWORD:-${DB_PASSWORD:-app_pass}}"
  GC_DB_ROOT_PASSWORD="${GC_DB_ROOT_PASSWORD:-${DB_ROOT_PASSWORD:-root}}"
  gc_env_sync_ports
}

GC_API_KEYS_METADATA=(
  "openai|OpenAI Codex|OPENAI_API_KEY|AI automation commands (plan, generate, work-on-tasks)|GC_OPENAI_API_KEY,GC_OPENAI_KEY"
  "jira|Jira Automation|JIRA_API_TOKEN|Backlog integrations that sync with Jira (create-jira-tasks)|GC_JIRA_API_TOKEN"
  "github|GitHub Auto Reports|GC_GITHUB_TOKEN|Crash/stall reports published as GitHub issues|"
)

gc_find_api_key_entry() {
  local query input lower entry key_id label primary desc alias_csv
  query="${1:-}"
  [[ -n "$query" ]] || return 1
  lower="$(to_lower "$query")"
  for entry in "${GC_API_KEYS_METADATA[@]}"; do
    IFS='|' read -r key_id label primary desc alias_csv <<<"$entry"
    if [[ "$lower" == "$(to_lower "$key_id")" || "$lower" == "$(to_lower "$primary")" ]]; then
      printf '%s\n' "$entry"
      return 0
    fi
    if [[ -n "$alias_csv" ]]; then
      IFS=',' read -ra input <<<"$alias_csv"
      for alias in "${input[@]}"; do
        alias="${alias//[[:space:]]/}"
        [[ -n "$alias" ]] || continue
        if [[ "$lower" == "$(to_lower "$alias")" ]]; then
          printf '%s\n' "$entry"
          return 0
        fi
      done
    fi
  done
  return 1
}

gc_apply_api_key_aliases() {
  local entry key_id label primary desc alias_csv value alias
  local alias_arr
  for entry in "${GC_API_KEYS_METADATA[@]}"; do
    IFS='|' read -r key_id label primary desc alias_csv <<<"$entry"
    value="${!primary:-}"
    if [[ -z "$value" && -n "$alias_csv" ]]; then
      IFS=',' read -ra alias_arr <<<"$alias_csv"
      for alias in "${alias_arr[@]}"; do
        alias="${alias//[[:space:]]/}"
        [[ -n "$alias" ]] || continue
        if [[ -n "${!alias:-}" ]]; then
          value="${!alias}"
          break
        fi
      done
    fi
    [[ -n "$value" ]] || continue
    export "$primary"="$value"
    if [[ -n "$alias_csv" ]]; then
      IFS=',' read -ra alias_arr <<<"$alias_csv"
      for alias in "${alias_arr[@]}"; do
        alias="${alias//[[:space:]]/}"
        [[ -n "$alias" && "$alias" != "$primary" ]] || continue
        if [[ -z "${!alias:-}" ]]; then
          export "$alias"="$value"
        fi
      done
    fi
  done
}

gc_api_keys_loaded=0

gc_load_api_keys() {
  if (( gc_api_keys_loaded )); then
    gc_apply_api_key_aliases
    return
  fi
  gc_ensure_config_dir
  local keys_file
  keys_file="$(gc_keys_file)"
  if [[ -f "$keys_file" ]]; then
    gc_sanitize_env_file "$keys_file"
    set -a
    # shellcheck disable=SC1090
    source "$keys_file"
    set +a
  fi
  gc_apply_api_key_aliases
  gc_api_keys_loaded=1
}

gc_read_env_file_var() {
  local file="$1" key="$2"
  [[ -f "$file" ]] || return 0
  python3 - <<'PY' "$file" "$key"
import pathlib, sys
path = pathlib.Path(sys.argv[1])
key = sys.argv[2]
if not path.exists():
    raise SystemExit(0)
for raw in path.read_text().splitlines():
    stripped = raw.strip()
    if not stripped or stripped.startswith('#'):
        continue
    if raw.startswith(f"{key}="):
        value = raw[len(key) + 1:].strip()
    elif raw.startswith(f"export {key}="):
        value = raw[len(key) + 8:].strip()
    else:
        continue
    if len(value) >= 2 and value[0] == value[-1] and value[0] in {"'", '"'}:
        value = value[1:-1]
    print(value)
    break
PY
}

gc_api_keys_list() {
  gc_load_api_keys
  gc_ensure_config_dir
  local keys_file status_header="Status"
  keys_file="$(gc_keys_file)"
  printf "API keys status (storage file: %s)\n\n" "$keys_file"
  printf "%-22s %-20s %-24s %s\n" "Service" "Environment" "$status_header" "Used for"
  printf "%-22s %-20s %-24s %s\n" "-------" "-----------" "------" "-------"
  local entry key_id label primary desc alias_csv value stored_value status alias
  local alias_arr
  for entry in "${GC_API_KEYS_METADATA[@]}"; do
    IFS='|' read -r key_id label primary desc alias_csv <<<"$entry"
    value="${!primary:-}"
    if [[ -z "$value" && -n "$alias_csv" ]]; then
      IFS=',' read -ra alias_arr <<<"$alias_csv"
      for alias in "${alias_arr[@]}"; do
        alias="${alias//[[:space:]]/}"
        [[ -n "$alias" ]] || continue
        if [[ -n "${!alias:-}" ]]; then
          value="${!alias}"
          break
        fi
      done
    fi
    stored_value="$(gc_read_env_file_var "$keys_file" "$primary")"
    if [[ -n "$value" ]]; then
      if [[ -n "$stored_value" ]]; then
        status="configured (stored)"
      else
        status="configured (env)"
      fi
    else
      status="missing"
    fi
    printf "%-22s %-20s %-24s %s\n" "$label" "$primary" "$status" "$desc"
  done
  if [[ -n "${GC_GITHUB_TOKEN:-}" && -z "${GC_GITHUB_REPO:-}" ]]; then
    printf "\nHint: set GC_GITHUB_REPO (owner/name) so GitHub Auto Reports knows where to file issues.\n"
  fi
  printf "\nSet a value with: gpt-creator keys set <service>\n"
}

gc_api_keys_set() {
  local query="${1:-}"
  [[ -n "$query" ]] || die "keys set requires a service name or environment variable"
  local entry
  if ! entry="$(gc_find_api_key_entry "$query")"; then
    die "Unknown API key: ${query}"
  fi
  IFS='|' read -r key_id label primary desc alias_csv <<<"$entry"
  gc_load_api_keys
  gc_ensure_config_dir
  local keys_file value alias
  local alias_arr
  keys_file="$(gc_keys_file)"
  printf "Updating %s (%s)\n" "$label" "$primary"
  printf "Press ENTER without a value to remove the stored credential.\n"
  if [[ -n "$alias_csv" ]]; then
    printf "Aliases: %s\n" "${alias_csv//,/ }"
  fi
  if [[ -t 0 ]]; then
    read -rsp "Enter value: " value
    printf '\n'
  else
    info "Reading ${primary} from stdin (input will be visible)."
    if ! read -r value; then
      die "Failed to read value from stdin"
    fi
  fi
  value="${value//$'\r'/}"
  value="$(printf '%s' "$value" | sed -e 's/^[[:space:]]*//' -e 's/[[:space:]]*$//')"
  if [[ -z "$value" ]]; then
    gc_remove_env_var "$keys_file" "$primary"
    unset "$primary"
    if [[ -n "$alias_csv" ]]; then
      IFS=',' read -ra alias_arr <<<"$alias_csv"
      for alias in "${alias_arr[@]}"; do
        alias="${alias//[[:space:]]/}"
        [[ -n "$alias" ]] || continue
        unset "$alias"
      done
    fi
    ok "Removed stored value for ${label}"
    return 0
  fi
  gc_write_env_var "$keys_file" "$primary" "$value"
  chmod 600 "$keys_file" 2>/dev/null || true
  export "$primary"="$value"
  if [[ -n "$alias_csv" ]]; then
    IFS=',' read -ra alias_arr <<<"$alias_csv"
    for alias in "${alias_arr[@]}"; do
      alias="${alias//[[:space:]]/}"
      [[ -n "$alias" && "$alias" != "$primary" ]] || continue
      export "$alias"="$value"
    done
  fi
  gc_apply_api_key_aliases
  ok "Stored credentials for ${label}"
  printf "Saved to %s\n" "$keys_file"
}

gc_create_env_if_missing() {
  local env_file="$(gc_env_file)"
  if [[ -f "$env_file" ]]; then
    return
  fi
  local slug
  slug="$(basename "${PROJECT_ROOT:-$PWD}")"
  slug=$(printf '%s' "$slug" | tr -c '[:alnum:]' '_')
  slug=$(printf '%s' "$slug" | tr '[:upper:]' '[:lower:]')
  slug=$(printf '%.12s' "$slug")
  [[ -n "$slug" ]] || slug="app"
  local db_name="${slug}_db"
  local db_user="gc_${slug}_user"
  local db_password="$(gc_random_string)"
  local db_root_password="$(gc_random_string)"
  cat > "$env_file" <<EOF
# gpt-creator environment
DB_NAME=${db_name}
DB_USER=${db_user}
DB_PASSWORD=${db_password}
DB_ROOT_USER=root
DB_ROOT_PASSWORD=${db_root_password}
DB_HOST=127.0.0.1
DB_PORT=3306
DB_HOST_PORT=3306
API_HOST_PORT=3000
WEB_HOST_PORT=5173
ADMIN_HOST_PORT=5174
PROXY_HOST_PORT=8080
DATABASE_URL=mysql://${db_user}:${db_password}@127.0.0.1:3306/${db_name}
VITE_API_BASE=http://localhost:3000/api/v1
# Optional: GitHub issue reporting
GC_GITHUB_REPO=bekirdag/gpt-creator
GC_GITHUB_TOKEN=
# GC_REPORTER=
# GC_REPORT_ASSIGNEE=
EOF
  chmod 600 "$env_file" || true
}

VERSION="0.2.0"
APP_NAME="gpt-creator"

# Defaults (override via env)
CODEX_BIN="${CODEX_BIN:-codex}"
CODEX_MODEL="${CODEX_MODEL:-gpt-5-codex}"
CODEX_FALLBACK_MODEL="${CODEX_FALLBACK_MODEL:-gpt-5-codex}"
CODEX_REASONING_EFFORT="${CODEX_REASONING_EFFORT:-high}"
EDITOR_CMD="${EDITOR_CMD:-code}"
DOCKER_BIN="${DOCKER_BIN:-docker}"
MYSQL_BIN="${MYSQL_BIN:-mysql}"

# Colors (TTY-only)
if [[ -t 1 ]]; then
  c_reset=$'\033[0m'; c_dim=$'\033[2m'; c_bold=$'\033[1m'
  c_red=$'\033[31m'; c_yellow=$'\033[33m'; c_cyan=$'\033[36m'; c_green=$'\033[32m'
else
  c_reset=; c_dim=; c_bold=; c_red=; c_yellow=; c_cyan=; c_green=
fi

ts() { date +"%Y-%m-%dT%H:%M:%S"; }
die() { echo "${c_red}✖${c_reset} $*" >&2; exit 1; }
info(){ echo "${c_cyan}➜${c_reset} $*"; }
ok()  { echo "${c_green}✔${c_reset} $*"; }
warn(){ echo "${c_yellow}!${c_reset} $*"; }

gc_format_duration_compact() {
  local total="${1:-0}"
  if [[ ! "$total" =~ ^[0-9]+$ ]]; then
    total=0
  fi
  local hours=$(( total / 3600 ))
  local minutes=$(( (total % 3600) / 60 ))
  local seconds=$(( total % 60 ))
  local parts=()
  if (( hours > 0 )); then
    parts+=("${hours}H")
  fi
  if (( minutes > 0 || hours > 0 )); then
    parts+=("${minutes}M")
  fi
  parts+=("${seconds}S")
  printf '%s' "${parts[*]}"
}

gc_format_tokens_compact() {
  local raw="${1:-0}"
  if [[ ! "$raw" =~ ^[0-9]+$ ]]; then
    raw=0
  fi
  if (( raw >= 1000000000 )); then
    local rounded=$(( (raw + 500000000) / 1000000000 ))
    printf '%dB' "$rounded"
  elif (( raw >= 1000000 )); then
    local rounded=$(( (raw + 500000) / 1000000 ))
    printf '%dM' "$rounded"
  elif (( raw >= 1000 )); then
    local rounded=$(( (raw + 500) / 1000 ))
    printf '%dK' "$rounded"
  else
    printf '%d' "$raw"
  fi
}

gc_render_task_banner() {
  local header_position="top"
  case "$1" in
    --header-bottom)
      header_position="bottom"
      shift
      ;;
    --header-top)
      shift
      ;;
  esac

  local header="${1:?header text required}"
  shift
  local -a lines=("$@")

  local min_width=23
  local banner_margin=4
  local header_padding_extra=8
  local inner_width="$min_width"

  local line
  for line in "${lines[@]}"; do
    local length=${#line}
    local candidate=$(( length + banner_margin ))
    if (( candidate > inner_width )); then
      inner_width=$candidate
    fi
  done

  local header_width=$(( inner_width + header_padding_extra ))
  if (( header_width < ${#header} + 2 )); then
    header_width=$(( ${#header} + 2 ))
  fi

  local shade_line
  printf -v shade_line '%*s' "$header_width" ''
  shade_line="${shade_line// /░}"

  local header_pad_left=$(( (header_width - ${#header}) / 2 ))
  local header_pad_right=$(( header_width - header_pad_left - ${#header} ))
  (( header_pad_left < 0 )) && header_pad_left=0
  (( header_pad_right < 0 )) && header_pad_right=0
  local header_line_left header_line_right
  printf -v header_line_left '%*s' "$header_pad_left" ''
  printf -v header_line_right '%*s' "$header_pad_right" ''
  local header_line="${header_line_left// /░}${header}${header_line_right// /░}"

  local border_inner
  printf -v border_inner '%*s' "$inner_width" ''
  local border_line="|${border_inner// /-}|"

  if [[ "$header_position" == "top" ]]; then
    info "$shade_line"
    info "$header_line"
    info "$shade_line"
  fi

  info "$border_line"
  for line in "${lines[@]}"; do
    local line_length=${#line}
    local pad_total=$(( inner_width - line_length ))
    (( pad_total < 0 )) && pad_total=0
    local pad_left=$(( pad_total / 2 ))
    local pad_right=$(( pad_total - pad_left ))
    local padded_line
    printf -v padded_line '|%*s%s%*s|' "$pad_left" '' "$line" "$pad_right" ''
    info "$padded_line"
    info "$border_line"
  done

  if [[ "$header_position" == "bottom" ]]; then
    printf '\n'
    info "$shade_line"
    info "$header_line"
    info "$shade_line"
  fi
}

gc_user_config_root() {
  python3 - <<'PY'
import os
import pathlib
import sys

def user_config_dir():
    # Windows
    if sys.platform.startswith("win"):
        for key in ("APPDATA", "LOCALAPPDATA"):
            base = os.environ.get(key)
            if base:
                return base
    # macOS
    try:
        home = pathlib.Path.home()
    except Exception:
        home = None
    if sys.platform == "darwin" and home is not None:
        return str(home / "Library" / "Application Support")
    # XDG (Linux/Unix)
    base = os.environ.get("XDG_CONFIG_HOME")
    if base:
        return base
    if home is not None:
        return str(home / ".config")
    return "."

print(user_config_dir())
PY
}

gc_config_dir() {
  local root
  root="$(gc_user_config_root)"
  [[ -n "$root" ]] || root="."
  printf '%s\n' "${root%/}/gpt-creator"
}

gc_keys_file() {
  printf '%s/api-keys.env\n' "$(gc_config_dir)"
}

gc_ensure_config_dir() {
  local dir
  dir="$(gc_config_dir)"
  mkdir -p "$dir"
}

abs_path() {
  python3 - "$1" <<'PY' 2>/dev/null || perl -MCwd=abs_path -e 'print abs_path(shift)."\n"' "$1" || echo "$1"
import os,sys; print(os.path.abspath(sys.argv[1]))
PY
}

to_lower() {
  printf '%s' "$1" | tr '[:upper:]' '[:lower:]'
}

slugify_name() {
  local s="${1:-}"
  s="$(to_lower "$s")"
  s="$(printf '%s' "$s" | tr -cs 'a-z0-9' '-')"
  s="$(printf '%s' "$s" | sed -E 's/-+/-/g; s/^-+//; s/-+$//')"
  printf '%s\n' "${s:-gptcreator}"
}

GC_PORT_RESERVATIONS=""

gc_port_for_service() {
  local service="$1"
  local entry
  for entry in $GC_PORT_RESERVATIONS; do
    local svc="${entry%%:*}"
    if [[ "$svc" == "$service" ]]; then
      printf '%s\n' "${entry#*:}"
      return 0
    fi
  done
  return 1
}

gc_unreserve_port() {
  local service="$1"
  [[ -n "$service" ]] || return 0
  local entry new_list=""
  for entry in $GC_PORT_RESERVATIONS; do
    local svc="${entry%%:*}"
    if [[ "$svc" == "$service" ]]; then
      continue
    fi
    if [[ -z "$new_list" ]]; then
      new_list="$entry"
    else
      new_list+=" $entry"
    fi
  done
  GC_PORT_RESERVATIONS="$new_list"
}

gc_reserve_port() {
  local service="$1" port="$2"
  [[ -n "$service" && -n "$port" ]] || return 0
  gc_unreserve_port "$service"
  if [[ -z "${GC_PORT_RESERVATIONS:-}" ]]; then
    GC_PORT_RESERVATIONS="${service}:${port}"
  else
    GC_PORT_RESERVATIONS+=" ${service}:${port}"
  fi
}

gc_port_is_reserved() {
  local port="$1"
  local entry
  for entry in $GC_PORT_RESERVATIONS; do
    if [[ "${entry#*:}" == "$port" ]]; then
      return 0
    fi
  done
  return 1
}

gc_port_reserved_by_other() {
  local port="$1" service="$2"
  local entry
  for entry in $GC_PORT_RESERVATIONS; do
    local svc="${entry%%:*}"
    local val="${entry#*:}"
    if [[ "$val" == "$port" && "$svc" != "$service" ]]; then
      return 0
    fi
  done
  return 1
}

GC_PROGRESS_DIR_MIGRATIONS=(
  "docs/delivery::staging/docs"
  "Design::staging/docs"
  "legal_editor_tmp::artifacts"
)

GC_PROGRESS_FILE_MIGRATIONS=(
  "tmp_*::artifacts/tmp"
  "final_*::artifacts/final"
  "dump_*::artifacts/dump"
  "diff*::artifacts/diff"
  "change_*::artifacts/change"
  "changes_*::artifacts/change"
  "codex_*::artifacts/codex"
  "delivery_plan*::artifacts/delivery"
  "qaDoc.json::artifacts/delivery"
  "backlog.md::artifacts/planning"
  "plan.md::artifacts/planning"
  "tasks.json::staging/plan/legacy"
  "encoded*::artifacts/encoded"
  "session_lifecycle*::artifacts/session"
  "breadcrumbs*.json::artifacts/ui"
  "cli_content.json::artifacts/context"
  "focusTrap.json::artifacts/ui"
  "navStore.json::artifacts/ui"
  "service_content.json::artifacts/context"
  "login_diff.txt::artifacts/diff"
  "login_json.txt::artifacts/context"
  "output_payload.json::artifacts/output"
  "changes_output.json::artifacts/change"
  "changes_strings.txt::artifacts/change"
  "*.patch::artifacts/patches"
  "*_patch.jsonstr::artifacts/patches"
  "*.rej::artifacts/patches"
  "*.rej.orig::artifacts/patches"
  "*.orig::artifacts/patches"
)

GC_PROGRESS_MIGRATION_LAST_COUNT=0

gc_move_progress_artifact() {
  local source_path="$1"
  local dest_dir="$2"
  [[ -e "$source_path" ]] || return 1
  mkdir -p "$dest_dir"
  local base name ext target candidate idx
  base="$(basename "$source_path")"
  name="$base"
  ext=""
  if [[ -f "$source_path" ]]; then
    if [[ "$base" == .* ]]; then
      if [[ "$base" == *.* ]]; then
        ext=".${base##*.}"
        name="${base%$ext}"
        [[ -z "$name" ]] && name="$base"
      fi
    else
      if [[ "$base" == *.* ]]; then
        ext=".${base##*.}"
        name="${base%$ext}"
      fi
    fi
  fi
  target="${dest_dir}/${base}"
  if [[ -e "$target" ]]; then
    idx=2
    while :; do
      if [[ -n "$ext" && "$name" != "$base" ]]; then
        candidate="${dest_dir}/${name}-${idx}${ext}"
      else
        candidate="${dest_dir}/${base}-${idx}"
      fi
      if [[ ! -e "$candidate" ]]; then
        target="$candidate"
        break
      fi
      idx=$((idx + 1))
    done
  fi
  if mv -- "$source_path" "$target"; then
    printf '%s\t%s\n' "$source_path" "$target"
    return 0
  fi
  return 1
}

gc_migrate_progress_artifacts() {
  local project_root="$1"
  local work_dir_name="${GC_WORK_DIR_NAME:-.gpt-creator}"
  local gc_root="${project_root}/${work_dir_name}"
  local skip="${GC_SKIP_PROGRESS_MIGRATION:-0}"
  [[ -d "$project_root" ]] || return 0
  [[ "$skip" == "1" ]] && return 0
  if [[ -n "${GC_ROOT:-}" && "$project_root" == "$GC_ROOT" ]]; then
    return 0
  fi
  GC_PROGRESS_MIGRATION_LAST_COUNT=0
  local -a moved=()
  local entry src_rel dest_rel src_path dest_dir record

  for entry in "${GC_PROGRESS_DIR_MIGRATIONS[@]}"; do
    src_rel="${entry%%::*}"
    dest_rel="${entry#*::}"
    src_path="${project_root}/${src_rel}"
    [[ -d "$src_path" ]] || continue
    dest_dir="${gc_root}/${dest_rel}"
    if record="$(gc_move_progress_artifact "$src_path" "$dest_dir")"; then
      moved+=("$record")
    fi
  done

  shopt -s nullglob dotglob
  for entry in "${GC_PROGRESS_FILE_MIGRATIONS[@]}"; do
    src_rel="${entry%%::*}"
    dest_rel="${entry#*::}"
    local matches=("$project_root"/$src_rel)
    local path
    for path in "${matches[@]}"; do
      [[ -e "$path" ]] || continue
      [[ "$path" == "$gc_root"* ]] && continue
      dest_dir="${gc_root}/${dest_rel}"
      if record="$(gc_move_progress_artifact "$path" "$dest_dir")"; then
        moved+=("$record")
      fi
    done
  done
  shopt -u nullglob dotglob

  if ((${#moved[@]} > 0)); then
    local log_dir="${gc_root}/logs"
    mkdir -p "$log_dir"
    local log_file="${log_dir}/progress-migration.log"
    {
      printf -- '--- %s ---\n' "$(date '+%Y-%m-%d %H:%M:%S')"
      local row src_abs dst_abs src_relpath dst_relpath
      for row in "${moved[@]}"; do
        src_abs="${row%%$'\t'*}"
        dst_abs="${row#*$'\t'}"
        src_relpath="${src_abs#$project_root/}"
        dst_relpath="${dst_abs#$project_root/}"
        printf -- '%s -> %s\n' "$src_relpath" "$dst_relpath"
      done
    } >>"$log_file"
    info "Migrated ${#moved[@]} work artifacts into .gpt-creator (see ${log_file#$project_root/})."
  fi
  GC_PROGRESS_MIGRATION_LAST_COUNT=${#moved[@]}
}

# Manual command helper to sweep legacy artifacts
cmd_sweep_artifacts() {
  local -a projects=()
  local arg project_path prev_skip
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project|-p)
        [[ -n "${2:-}" ]] || die "--project requires a directory path"
        project_path="$(abs_path "$2")"
        projects+=("$project_path")
        shift 2
        ;;
      -h|--help)
        cat <<'EOF'
Usage: gpt-creator sweep-artifacts [--project PATH] [PATH...]

Sweep legacy Codex work artifacts (tmp/final/diff/etc.) into the standard
.gpt-creator folder structure. Defaults to the current directory when no path
is supplied. You can pass multiple project directories to tidy them in batch.
EOF
        return 0
        ;;
      --)
        shift
        while [[ $# -gt 0 ]]; do
          projects+=("$(abs_path "$1")")
          shift
        done
        ;;
      -*)
        die "Unknown flag for sweep-artifacts: $1"
        ;;
      *)
        projects+=("$(abs_path "$1")")
        shift
        ;;
    esac
  done

  if ((${#projects[@]} == 0)); then
    projects+=("$(abs_path "${PROJECT_ROOT:-$PWD}")")
  fi

  local root moved_any=0 count prev_skip_set prev_skip_val
  for root in "${projects[@]}"; do
    if [[ ! -d "$root" ]]; then
      warn "Skipping missing directory: ${root}"
      continue
    fi
    info "Tidying progress artifacts under ${root}"
    prev_skip_set=0
    prev_skip_val=""
    if [[ ${GC_SKIP_PROGRESS_MIGRATION+x} ]]; then
      prev_skip_set=1
      prev_skip_val="$GC_SKIP_PROGRESS_MIGRATION"
    else
      prev_skip_set=0
      prev_skip_val=""
    fi
    GC_SKIP_PROGRESS_MIGRATION=0
    gc_migrate_progress_artifacts "$root"
    count=${GC_PROGRESS_MIGRATION_LAST_COUNT:-0}
    if (( prev_skip_set )); then
      GC_SKIP_PROGRESS_MIGRATION="$prev_skip_val"
    else
      unset GC_SKIP_PROGRESS_MIGRATION
    fi
    if (( count > 0 )); then
      ok "Relocated ${count} artifact(s) into ${root}/.gpt-creator."
      moved_any=1
    else
      info "No legacy artifacts found outside .gpt-creator."
    fi
  done

  return 0
}

cmd_tidy_progress() {
  warn "'tidy-progress' has been renamed to 'sweep-artifacts'. Running the renamed command."
  cmd_sweep_artifacts "$@"
}

# Context directories inside project
ensure_ctx() {
  local root="${1:-}" TMP_DIR=""
  if [[ -z "${root}" ]]; then root="${PROJECT_ROOT:-$PWD}"; fi
  PROJECT_ROOT="$(abs_path "$root")"
  GC_DIR="${PROJECT_ROOT}/.gpt-creator"
  STAGING_DIR="${GC_DIR}/staging"
  INPUT_DIR="${STAGING_DIR}/inputs"
  PLAN_DIR="${STAGING_DIR}/plan"
  LOG_DIR="${GC_DIR}/logs"
  ART_DIR="${GC_DIR}/artifacts"
  TMP_DIR="${GC_DIR}/tmp"
  mkdir -p "$GC_DIR" "$STAGING_DIR" "$INPUT_DIR" "$PLAN_DIR" "$LOG_DIR" "$ART_DIR" "$TMP_DIR"
  gc_migrate_progress_artifacts "$PROJECT_ROOT"
  gc_configure_tmpdir "$PROJECT_ROOT"
  gc_create_env_if_missing
  gc_load_env
  local base_name
  base_name="$(basename "$PROJECT_ROOT")"
  PROJECT_SLUG="$(slugify_name "${GC_DOCKER_PROJECT_NAME:-$base_name}")"
  GC_DOCKER_PROJECT_NAME="${GC_DOCKER_PROJECT_NAME:-$PROJECT_SLUG}"
  COMPOSE_PROJECT_NAME="${COMPOSE_PROJECT_NAME:-$GC_DOCKER_PROJECT_NAME}"
  GC_FAIL_LOG_DIR="$LOG_DIR"
  export GC_DOCKER_PROJECT_NAME COMPOSE_PROJECT_NAME PROJECT_SLUG

  if gc_reports_enabled; then
    gc_reports_initialize
    gc_reports_touch_activity "$(date +%s)"
  fi
}

gc_project_templates_root() {
  local root="${CLI_ROOT}/project_templates"
  mkdir -p "$root"
  printf '%s\n' "$root"
}

gc_find_primary_rfp() {
  local search_root="${1:-.}"
  find "$search_root" -maxdepth 3 -type f \
    \( -iname 'rfp.md' -o -iname '*rfp*.md' -o -iname '*request*for*proposal*.md' \) \
    | sort | head -n 1
}

gc_bootstrap_state_dir() {
  printf '%s\n' "${PLAN_DIR}/bootstrap"
}

gc_bootstrap_state_file() {
  printf '%s\n' "$(gc_bootstrap_state_dir)/state.json"
}

gc_capture_error_context() {
  local status="${1:-0}"
  local command="${2:-}"
  (( status == 0 )) && return
  GC_LAST_ERROR_STATUS="$status"
  GC_LAST_ERROR_CMD="$command"
}

gc_logs_dir() {
  local dir="${GC_FAIL_LOG_DIR:-}"
  if [[ -z "$dir" ]]; then
    if [[ -n "${PROJECT_ROOT:-}" ]]; then
      dir="${PROJECT_ROOT}/.gpt-creator/logs"
    else
      dir="${PWD}/.gpt-creator/logs"
    fi
  fi
  if ! mkdir -p "$dir" 2>/dev/null; then
    return 1
  fi
  printf '%s\n' "$dir"
}

gc_reports_enabled() {
  (( GC_REPORTS_ON != 0 ))
}

gc_reports_current_user() {
  if [[ -n "${GC_REPORTER:-}" ]]; then
    printf '%s\n' "$GC_REPORTER"
    return 0
  fi
  local name
  name="$(git config user.name 2>/dev/null || true)"
  if [[ -z "$name" ]]; then
    name="${USER:-}"
  fi
  if [[ -z "$name" ]]; then
    name="$(whoami 2>/dev/null || true)"
  fi
  printf '%s\n' "${name:-maintainer}"
}

gc_reports_escape() {
  local s="${1:-}"
  s="${s//\\/\\\\}"
  s="${s//$'\n'/\\n}"
  s="${s//\"/\\\"}"
  printf '%s' "$s"
}

gc_reports_issue_file() {
  local kind="${1:-generic}"
  local dir="${GC_REPORTS_STORE_DIR:-}"
  if [[ -z "$dir" ]]; then
    local base
    base="$(gc_logs_dir)" || return 1
    dir="${base}/issue-reports"
  fi
  if ! mkdir -p "$dir" 2>/dev/null; then
    return 1
  fi
  local stamp random file
  stamp="$(date -u +"%Y%m%dT%H%M%SZ")"
  random="$(printf '%04x%04x' "$RANDOM" "$RANDOM")"
  file="${dir}/${stamp}-${kind}-${random}.yml"
  if [[ -e "$file" ]]; then
    file="${dir}/${stamp}-${kind}-${random}-${RANDOM}.yml"
  fi
  if ! : >"$file" 2>/dev/null; then
    return 1
  fi
  printf '%s\n' "$file"
}

gc_reports_write_issue() {
  local kind="${1:-generic}"
  local summary="${2:-}"
  local definition="${3:-}"
  local priority="${4:-P2-medium}"
  local file
  file="$(gc_reports_issue_file "$kind")" || return 1
  local summary_escaped
  summary_escaped="$(gc_reports_escape "$summary")"
  local reporter
  reporter="$(gc_reports_current_user)"
  local timestamp
  timestamp="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
  {
    printf 'summary: "%s"\n' "$summary_escaped"
    printf 'priority: %s\n' "$priority"
    printf 'issue_definition: |\n'
    if [[ -n "$definition" ]]; then
      while IFS= read -r line || [[ -n "$line" ]]; do
        printf '  %s\n' "$line"
      done <<<"$definition"
    else
      printf '  (no additional details provided)\n'
    fi
    printf 'metadata:\n'
    printf '  type: %s\n' "$kind"
    printf '  timestamp: "%s"\n' "$timestamp"
    printf '  reporter: "%s"\n' "$(gc_reports_escape "$reporter")"
    printf '  command: "%s"\n' "$(gc_reports_escape "${GC_INVOCATION:-$0}")"
    if [[ -n "${GC_LAST_ERROR_STATUS:-}" ]]; then
      printf '  exit_code: %s\n' "${GC_LAST_ERROR_STATUS}"
    fi
    if [[ -n "${GC_LAST_ERROR_CMD:-}" ]]; then
      printf '  last_command: "%s"\n' "$(gc_reports_escape "${GC_LAST_ERROR_CMD}")"
    fi
    printf '  working_dir: "%s"\n' "$(gc_reports_escape "$PWD")"
    printf '  status: open\n'
    printf '  likes: 0\n'
    printf '  comments: 0\n'
  } >"$file"
  printf '%s\n' "$file"
}

gc_reports_sync_github() {
  local report_file="${1:-}"
  local kind="${2:-generic}"
  local summary="${3:-}"
  local definition="${4:-}"
  local priority="${5:-P2-medium}"
  local cli_version="${VERSION:-}"
  local binary_path="${GC_SELF_PATH:-}"

  if [[ -z "$binary_path" || ! -r "$binary_path" ]]; then
    if [[ -n "${CLI_ROOT:-}" && -r "${CLI_ROOT}/bin/${APP_NAME}" ]]; then
      binary_path="${CLI_ROOT}/bin/${APP_NAME}"
    else
      binary_path="$(command -v "${APP_NAME}" 2>/dev/null || true)"
    fi
  fi

  local repo="${GC_GITHUB_REPO:-}"
  local token="${GC_GITHUB_TOKEN:-}"
  if [[ -z "$repo" || -z "$token" ]]; then
    return 0
  fi

  local response
  if ! response="$(
    GC_REPORT_SUMMARY="$summary" \
    GC_REPORT_DEFINITION="$definition" \
    GC_REPORT_PRIORITY="$priority" \
    GC_REPORT_KIND="$kind" \
    GC_REPORT_FILE="$report_file" \
    GC_REPORT_COMMAND="${GC_INVOCATION:-}" \
    GC_REPORT_EXIT="${GC_LAST_ERROR_STATUS:-}" \
    GC_REPORT_LAST_CMD="${GC_LAST_ERROR_CMD:-}" \
    GC_REPORT_WORKDIR="$PWD" \
    GC_REPORT_PROJECT="${PROJECT_ROOT:-$PWD}" \
    GC_REPORTER="${GC_REPORTER:-}" \
    GC_REPORT_VERSION="$cli_version" \
    GC_REPORT_BINARY="${binary_path:-}" \
    python3 - <<'PY' "$repo" "$token"
import json
import os
import sys
import urllib.error
import urllib.request
import hashlib
import pathlib

repo, token = sys.argv[1:3]

summary = (os.environ.get("GC_REPORT_SUMMARY") or "").strip()
definition = (os.environ.get("GC_REPORT_DEFINITION") or "").strip()
priority = (os.environ.get("GC_REPORT_PRIORITY") or "P2-medium").strip() or "P2-medium"
kind = (os.environ.get("GC_REPORT_KIND") or "generic").strip() or "generic"
report_file = os.environ.get("GC_REPORT_FILE", "")
invocation = os.environ.get("GC_REPORT_COMMAND", "")
exit_code = os.environ.get("GC_REPORT_EXIT", "")
last_cmd = os.environ.get("GC_REPORT_LAST_CMD", "")
project_root = os.environ.get("GC_REPORT_PROJECT") or os.getcwd()
workdir = os.environ.get("GC_REPORT_WORKDIR") or os.getcwd()
reporter = os.environ.get("GC_REPORTER", "")
version = (os.environ.get("GC_REPORT_VERSION") or "").strip()
binary_path = (os.environ.get("GC_REPORT_BINARY") or "").strip()

title = summary or f"{kind.capitalize()} report"
title = title.strip()[:120] or "Automated crash report"

body_lines = []
if definition:
    body_lines.append(definition)
else:
    body_lines.append("(no additional details provided)")
body_lines.extend(["", "---", ""])

binary_hash = ""
if binary_path:
    try:
        path = pathlib.Path(binary_path)
        if path.is_file():
            hasher = hashlib.sha256()
            with path.open("rb") as handle:
                for chunk in iter(lambda: handle.read(1024 * 1024), b""):
                    hasher.update(chunk)
            binary_hash = hasher.hexdigest()
    except (OSError, ValueError):
        binary_hash = ""

signature = ""
if version or binary_hash:
    payload_source = f"{version}:{binary_hash}".encode("utf-8")
    signature = hashlib.sha256(payload_source).hexdigest()

metadata = [
    ("Priority", priority),
    ("Report Type", kind),
    ("Reporter", reporter),
    ("Invocation", invocation),
    ("Exit Code", exit_code),
    ("Last Command", last_cmd),
    ("Project Root", project_root),
    ("Working Directory", workdir),
    ("Report File", report_file),
    ("CLI Version", version),
    ("CLI Binary SHA256", binary_hash),
    ("CLI Signature", signature),
]

for label, value in metadata:
    if value:
        body_lines.append(f"- **{label}**: {value}")

watermark = ""
if signature:
    watermark = f"{version or 'unknown'}:{signature}"
elif version:
    watermark = f"{version}:unsigned"
if watermark:
    body_lines.extend(["", f"<!-- gpt-creator:{watermark} -->"])

labels = [
    "auto-report",
    f"kind:{kind}",
    f"priority:{priority}",
]
if version:
    labels.append(f"cli-version:{version.replace(' ', '_')}")

payload = {
    "title": title,
    "body": "\n".join(body_lines),
    "labels": labels,
}

request = urllib.request.Request(
    f"https://api.github.com/repos/{repo}/issues",
    data=json.dumps(payload).encode("utf-8"),
    headers={
        "Authorization": f"Bearer {token}",
        "Accept": "application/vnd.github+json",
        "Content-Type": "application/json",
        "X-GitHub-Api-Version": "2022-11-28",
    },
    method="POST",
)

try:
    with urllib.request.urlopen(request) as resp:
        data = json.loads(resp.read().decode("utf-8"))
except urllib.error.HTTPError as err:
    message = err.read().decode("utf-8", "ignore")
    print(f"HTTP {err.code}: {message}", file=sys.stderr)
    sys.exit(1)
except Exception as exc:
    print(f"GitHub issue request failed: {exc}", file=sys.stderr)
    sys.exit(1)

html_url = data.get("html_url", "")
number = data.get("number")
print(html_url)
print(number if number is not None else "")
PY
  )"; then
    warn "Failed to create GitHub issue for $(basename "$report_file")."
    return 0
  fi

  local issue_url=""
  local issue_number=""
  IFS=$'\n' read -r issue_url issue_number <<<"$response"
  if [[ -n "$issue_url" ]]; then
    gc_reports_set_metadata_field "$report_file" issue_url "\"$issue_url\""
    if [[ -n "$issue_number" ]]; then
      gc_reports_set_metadata_field "$report_file" issue_number "$issue_number"
    fi
    info "GitHub issue created -> ${issue_url}"
  fi
}

gc_reports_record_issue() {
  local kind="${1:-generic}"
  local summary="${2:-}"
  local definition="${3:-}"
  local priority="${4:-P2-medium}"
  local report_file
  report_file="$(gc_reports_write_issue "$kind" "$summary" "$definition" "$priority")" || return 1
  gc_reports_sync_github "$report_file" "$kind" "$summary" "$definition" "$priority"
  printf '%s\n' "$report_file"
}

gc_reports_touch_activity() {
  local timestamp="${1:-$(date +%s)}"
  local command="${2:-}"
  local file="${GC_REPORTS_ACTIVITY_FILE:-}"
  [[ -n "$file" ]] || return 0
  if [[ -n "$command" ]]; then
    command="${command//$'\n'/ }"
    printf '%s\t%s\n' "$timestamp" "$command" >"$file" 2>/dev/null || true
  else
    printf '%s\n' "$timestamp" >"$file" 2>/dev/null || true
  fi
}

gc_reports_activity_trap() {
  gc_reports_touch_activity "$(date +%s)" "$1"
  return 0
}

gc_reports_handle_crash() {
  local status="${1:-1}"
  gc_reports_enabled || return 0
  local summary
  printf -v summary "Crash (exit %s) while running '%s'" "$status" "${GC_INVOCATION:-$0}"
  local log_dir
  if ! log_dir="$(gc_logs_dir)"; then
    log_dir="<unknown>"
  fi
  local -a lines
  lines=("The CLI exited unexpectedly with status ${status}.")
  if [[ -n "${GC_LAST_ERROR_CMD:-}" ]]; then
    lines+=("Last command observed before exit: ${GC_LAST_ERROR_CMD}")
  fi
  if [[ -n "${GC_LAST_CRASH_LOG:-}" ]]; then
    lines+=("Crash log stored at: ${GC_LAST_CRASH_LOG}")
  fi
  lines+=("Inspect logs under ${log_dir} for further diagnostics.")
  local definition=""
  if ((${#lines[@]})); then
    printf -v definition '%s\n' "${lines[@]}"
    definition="${definition%$'\n'}"
  fi
  local path
  if path="$(gc_reports_record_issue "crash" "$summary" "$definition" "P0-critical")"; then
    warn "Issue report recorded for crash → ${path}"
  fi
}

gc_reports_handle_idle() {
  local idle_seconds="${1:-0}"
  local heartbeat="${2:-}"
  local last_command="${3:-}"
  gc_reports_enabled || return 0
  local summary
  printf -v summary "Idle/stall detected after %ss while running '%s'" "$idle_seconds" "${GC_INVOCATION:-$0}"
  local log_dir
  if ! log_dir="$(gc_logs_dir)"; then
    log_dir="<unknown>"
  fi
  if [[ -n "$last_command" ]]; then
    last_command="${last_command//$'\n'/ }"
  fi
  local -a lines
  lines=("No CLI activity recorded for ${idle_seconds} seconds.")
  if [[ -n "$heartbeat" ]]; then
    lines+=("Heartbeat file: ${heartbeat}")
  fi
  if [[ -n "$last_command" ]]; then
    lines+=("Last command observed: ${last_command}")
  fi
  lines+=("Inspect processes and logs under ${log_dir} to verify whether the command stalled.")
  local definition=""
  if ((${#lines[@]})); then
    printf -v definition '%s\n' "${lines[@]}"
    definition="${definition%$'\n'}"
  fi
  local path
  if path="$(gc_reports_record_issue "idle" "$summary" "$definition" "P1-high")"; then
    warn "Issue report recorded for idle stall → ${path}"
  fi
}

gc_reports_watchdog_loop() {
  local timeout="${1:-0}"
  local interval="${2:-0}"
  local activity_file="${3:-}"
  local main_pid="${4:-0}"
  local sentinel="${5:-}"

  (( timeout > 0 )) || return 0
  (( interval > 0 )) || interval="$timeout"
  [[ -n "$activity_file" && -n "$main_pid" ]] || return 0

  while kill -0 "$main_pid" 2>/dev/null; do
    sleep "$interval" || break
    [[ -f "$activity_file" ]] || continue
    local raw
    raw="$(cat "$activity_file" 2>/dev/null)" || continue
    local payload="$raw"
    local last="${payload%%$'\t'*}"
    local activity_command=""
    if [[ "$payload" == *$'\t'* ]]; then
      activity_command="${payload#*$'\t'}"
    fi
    [[ "$last" =~ ^[0-9]+$ ]] || continue
    local now
    now="$(date +%s)"
    local delta=$(( now - last ))
    if (( delta >= timeout )); then
      if [[ -n "$sentinel" ]]; then
        printf '%s\n' "$now" >"$sentinel" 2>/dev/null || true
      fi
      gc_reports_handle_idle "$delta" "$activity_file" "$activity_command"
      break
    fi
  done
}

gc_reports_start_watchdog() {
  local timeout="${1:-0}"
  local interval="${2:-0}"
  local activity="${3:-}"
  local main_pid="${4:-0}"
  local sentinel="${5:-}"

  if (( timeout <= 0 )); then
    return 0
  fi
  if (( interval <= 0 || interval > timeout )); then
    interval="$timeout"
  fi
  if [[ -z "$activity" || -z "$main_pid" ]]; then
    return 0
  fi
  if [[ -n "${GC_REPORTS_WATCHDOG_PID:-}" ]] && kill -0 "$GC_REPORTS_WATCHDOG_PID" 2>/dev/null; then
    return 0
  fi

  gc_reports_watchdog_loop "$timeout" "$interval" "$activity" "$main_pid" "$sentinel" &
  GC_REPORTS_WATCHDOG_PID=$!
}

gc_reports_initialize() {
  gc_reports_enabled || return 0
  if (( GC_REPORTS_INITIALIZED )); then
    return 0
  fi
  local log_dir
  if ! log_dir="$(gc_logs_dir)"; then
    warn "Unable to determine log directory for issue reporting"
    return 0
  fi
  GC_REPORTS_STORE_DIR="${log_dir}/issue-reports"
  if ! mkdir -p "$GC_REPORTS_STORE_DIR" 2>/dev/null; then
    warn "Unable to prepare reports directory at ${GC_REPORTS_STORE_DIR}"
    return 0
  fi
  GC_REPORTS_ACTIVITY_FILE="${GC_REPORTS_STORE_DIR}/heartbeat-${GC_MAIN_PID}.txt"
  GC_REPORTS_IDLE_SENTINEL="${GC_REPORTS_STORE_DIR}/idle-${GC_MAIN_PID}.flag"
  if ! : >"$GC_REPORTS_ACTIVITY_FILE" 2>/dev/null; then
    warn "Unable to initialize heartbeat tracking for issue reporting"
    return 0
  fi
  GC_REPORTS_INITIALIZED=1
  gc_reports_touch_activity "$(date +%s)"
  gc_reports_start_watchdog "$GC_REPORTS_IDLE_TIMEOUT" "$GC_REPORTS_CHECK_INTERVAL" "$GC_REPORTS_ACTIVITY_FILE" "$GC_MAIN_PID" "$GC_REPORTS_IDLE_SENTINEL"
  return 0
}

gc_reports_cleanup() {
  if [[ -n "${GC_REPORTS_WATCHDOG_PID:-}" ]]; then
    kill "$GC_REPORTS_WATCHDOG_PID" 2>/dev/null || true
    wait "$GC_REPORTS_WATCHDOG_PID" 2>/dev/null || true
    GC_REPORTS_WATCHDOG_PID=""
  fi
}

gc_reports_dir() {
  if [[ -n "${GC_REPORTS_STORE_DIR:-}" ]]; then
    printf '%s\n' "$GC_REPORTS_STORE_DIR"
    return 0
  fi
  local base
  base="$(gc_logs_dir)" || return 1
  local dir="${base}/issue-reports"
  if ! mkdir -p "$dir" 2>/dev/null; then
    return 1
  fi
  printf '%s\n' "$dir"
}

gc_reports_set_metadata_field() {
  local report_file="${1:?report file required}"
  local key="${2:?metadata key required}"
  local value="${3:-}"
  python3 - <<'PY' "$report_file" "$key" "$value"
import sys

path, key, value = sys.argv[1:4]

try:
    lines = []
    with open(path, 'r', encoding='utf-8', errors='ignore') as fh:
        lines = fh.read().splitlines()
except FileNotFoundError:
    raise SystemExit(f"report file not found: {path}")

metadata_found = False
metadata_active = False
inserted = False
result = []

for line in lines:
    if line.strip() == "metadata:":
        metadata_found = True
        metadata_active = True
        result.append(line)
        continue
    if metadata_active:
        if line.startswith("  "):
            stripped = line.strip()
            if ":" in stripped:
                current_key = stripped.split(":", 1)[0].strip()
                if current_key == key:
                    result.append(f"  {key}: {value}".rstrip())
                    inserted = True
                    continue
        else:
            if not inserted:
                result.append(f"  {key}: {value}".rstrip())
                inserted = True
            metadata_active = False
    result.append(line)

if metadata_active and not inserted:
    result.append(f"  {key}: {value}".rstrip())
    inserted = True

if not metadata_found:
    result.append("metadata:")
    result.append(f"  {key}: {value}".rstrip())

with open(path, 'w', encoding='utf-8', errors='ignore') as fh:
    fh.write('\n'.join(result).rstrip() + '\n')
PY
}

gc_reports_resolve_slug() {
  local slug="${1:-}"
  [[ -n "$slug" ]] || return 1
  local dir
  dir="$(gc_reports_dir)" || return 1
  local candidate
  for ext in yml yaml; do
    candidate="${dir}/${slug}.${ext}"
    if [[ -f "$candidate" ]]; then
      printf '%s\n' "$candidate"
      return 0
    fi
  done
  candidate="${dir}/${slug}"
  if [[ -f "$candidate" ]]; then
    printf '%s\n' "$candidate"
    return 0
  fi
  local -a matches=()
  while IFS= read -r file; do
    local base
    base="$(basename "$file")"
    base="${base%.*}"
    if [[ "$base" == "$slug"* ]]; then
      matches+=("$file")
    fi
  done < <(find "$dir" -maxdepth 1 -type f \( -name '*.yml' -o -name '*.yaml' \) -print 2>/dev/null)
  local count="${#matches[@]}"
  if (( count == 1 )); then
    printf '%s\n' "${matches[0]}"
    return 0
  fi
  if (( count > 1 )); then
    warn "Multiple reports match slug '${slug}'."
    local entry
    for entry in "${matches[@]}"; do
      warn "  $(basename "$entry")"
    done
    return 2
  fi
  return 1
}

gc_reports_run_work() {
  local slug="${1:?slug required}"
  local branch_hint="${2:-}"
  local push_after="${3:-1}"
  local prompt_only="${4:-0}"
  local assignee_override="${5:-}"

  local report_path
  if ! report_path="$(gc_reports_resolve_slug "$slug")"; then
    warn "No issue report found for slug: ${slug}"
    return 1
  fi

  local branch="${branch_hint:-report/${slug}}"
  local push_flag="$push_after"
  if [[ "$push_flag" != "0" ]]; then
    push_flag=1
  fi
  local prompt_only_flag="$prompt_only"
  if [[ "$prompt_only_flag" != "0" ]]; then
    prompt_only_flag=1
  fi

  local report_dir="${GC_DIR}/reports/${slug}"
  mkdir -p "$report_dir"
  local prompt_path="${report_dir}/work.md"
  local project_root="${PROJECT_ROOT:-$PWD}"

  if ! python3 - <<'PY' "$report_path" "$prompt_path" "$project_root" "$slug" "$branch" "$push_flag"
import sys

report_path, prompt_path, project_root, slug, branch, push_flag = sys.argv[1:6]
push = push_flag == "1"

with open(report_path, 'r', encoding='utf-8', errors='ignore') as fh:
    raw_lines = fh.read().splitlines()

summary = ""
priority = ""
definition_lines = []
metadata = {}
collect_definition = False
metadata_active = False

for line in raw_lines:
    if line.startswith("summary:") and not summary:
        value = line.split(":", 1)[1].strip()
        if value.startswith('"') and value.endswith('"') and len(value) >= 2:
            value = value[1:-1]
        summary = value
    elif line.startswith("priority:") and not priority:
        priority = line.split(":", 1)[1].strip()
    elif line.startswith("issue_definition:"):
        collect_definition = True
        continue
    elif collect_definition:
        if line.startswith("  "):
            definition_lines.append(line[2:])
        else:
            collect_definition = False
    if line.strip() == "metadata:":
        metadata_active = True
        continue
    if metadata_active:
        if line.startswith("  "):
            stripped = line.strip()
            if ":" in stripped:
                k, v = stripped.split(":", 1)
                metadata[k.strip()] = v.strip().strip('"')
        else:
            metadata_active = False

definition_text = "\n".join(definition_lines).strip()
issue_type = metadata.get("type", "unknown")
status = metadata.get("status", "open")
timestamp = metadata.get("timestamp", "")
likes = metadata.get("likes", "0")
comments = metadata.get("comments", "0")
try:
    popularity = int(likes) + int(comments)
except ValueError:
    popularity = 0

summary_for_commit = summary.replace('"', '').strip()
if len(summary_for_commit) > 64:
    summary_for_commit = summary_for_commit[:61] + "..."

instructions = []
instructions.append(f"# Resolve Issue {slug}\n")
instructions.append("## Summary")
instructions.append(f"- Summary: {summary or '(not provided)'}")
instructions.append(f"- Priority: {priority or 'unknown'}")
instructions.append(f"- Type: {issue_type}")
instructions.append(f"- Current Status: {status}")
instructions.append(f"- Popularity Score: {popularity} (likes={likes}, comments={comments})")
if timestamp:
    instructions.append(f"- Reported At: {timestamp}")
instructions.append(f"- Report File: {report_path}")
instructions.append(f"- Working Branch: {branch}")
instructions.append("")
instructions.append("## Issue Definition")
instructions.append("```")
instructions.append(definition_text or "(no issue definition provided)")
instructions.append("```")
instructions.append("")
instructions.append("## Workflow Requirements")
instructions.append(f"1. Checkout the branch `{branch}` (create it if missing).")
instructions.append("2. Investigate and resolve the described issue with deterministic steps.")
instructions.append("3. Run any relevant checks or tests (e.g. `gpt-creator verify acceptance`) to confirm the fix.")
instructions.append(f"4. Stage and commit the changes with a concise message (suggested: `fix: {slug} {summary_for_commit}`).")
if push:
    instructions.append(f"5. Push the branch to origin via `git push origin {branch}`.")
instructions.append("6. Provide a short summary of the fix in the commit message body if additional context is required.")
instructions.append("")
instructions.append("## Notes for Codex")
instructions.append("- Operate deterministically and avoid modifying unrelated files.")
instructions.append("- Do not edit the issue report YAML; the CLI updates metadata automatically.")
instructions.append("- Focus on resolving the root cause and keep diffs as small as possible.")
instructions.append("- If the issue cannot be resolved, leave the repository unchanged and exit with a failure code describing the blocker.")
instructions.append("")
instructions.append("## Repository Context")
instructions.append(f"- Project Root: {project_root}")
instructions.append(f"- Branch: {branch}")
instructions.append("")

with open(prompt_path, 'w', encoding='utf-8', errors='ignore') as fh:
    fh.write("\n".join(instructions).rstrip() + "\n")
PY
  then
    warn "Failed to prepare Codex prompt for report ${slug}"
    return 1
  fi

  info "Prepared Codex prompt → ${prompt_path}"

  local now_utc
  now_utc="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
  local assignee="${assignee_override:-${GC_REPORT_ASSIGNEE:-$(gc_reports_current_user)}}"
  if (( prompt_only_flag )); then
    [[ -n "$assignee" ]] && gc_reports_set_metadata_field "$report_path" assigned "\"$(gc_reports_escape "$assignee")\""
    gc_reports_set_metadata_field "$report_path" branch "\"$(gc_reports_escape "$branch")\""
    gc_reports_set_metadata_field "$report_path" status open
    info "Prompt generated (skipping Codex execution due to --prompt-only)."
    info "Run: ${CODEX_BIN:-codex} exec --model ${CODEX_MODEL} --cd \"${PROJECT_ROOT:-$PWD}\" < ${prompt_path}"
    return 0
  fi

  if [[ -n "$assignee" ]]; then
    gc_reports_set_metadata_field "$report_path" assigned "\"$(gc_reports_escape "$assignee")\""
  fi
  gc_reports_set_metadata_field "$report_path" status in-progress
  gc_reports_set_metadata_field "$report_path" branch "\"$(gc_reports_escape "$branch")\""
  gc_reports_set_metadata_field "$report_path" last_started "\"$now_utc\""

  if codex_call "report-${slug}" --prompt "$prompt_path"; then
    local completed_utc
    completed_utc="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
    gc_reports_set_metadata_field "$report_path" status resolved
    gc_reports_set_metadata_field "$report_path" last_completed "\"$completed_utc\""
    ok "Codex resolved report ${slug}"
    return 0
  else
    local failed_utc
    failed_utc="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
    gc_reports_set_metadata_field "$report_path" status open
    gc_reports_set_metadata_field "$report_path" last_failed "\"$failed_utc\""
    warn "Codex failed to resolve report ${slug}"
    return 1
  fi
}

gc_reports_set_idle_timeout() {
  local value="${1:-}"
  if [[ -z "$value" ]]; then
    die "--reports-idle-timeout requires a value in seconds"
  fi
  if ! [[ "$value" =~ ^[0-9]+$ ]]; then
    die "--reports-idle-timeout expects an integer number of seconds (received: $value)"
  fi
  GC_REPORTS_IDLE_TIMEOUT="$value"
}

gc_reports_extract_global_flags() {
  GC_FILTERED_ARGS=()
  local -a args=("$@")
  local idx=0
  while (( idx < ${#args[@]} )); do
    local arg="${args[idx]}"
    case "$arg" in
      --)
        GC_FILTERED_ARGS+=("${args[@]:idx}")
        return 0
        ;;
      --reports-on)
        GC_REPORTS_ON=1
        ;;
      --reports-off)
        GC_REPORTS_ON=0
        ;;
      --reports-idle-timeout=*)
        gc_reports_set_idle_timeout "${arg#*=}"
        ;;
      --reports-idle-timeout)
        (( idx + 1 < ${#args[@]} )) || die "--reports-idle-timeout requires a value in seconds"
        idx=$((idx + 1))
        gc_reports_set_idle_timeout "${args[idx]}"
        ;;
      *)
        GC_FILTERED_ARGS+=("$arg")
        ;;
    esac
    idx=$((idx + 1))
  done
}

gc_write_crash_log() {
  local status="${1:-1}"
  if (( GC_CRASH_LOGGED )); then
    return
  fi

  local log_dir
  log_dir="$(gc_logs_dir)" || return

  local log_file="${log_dir}/crash.log"
  local timestamp
  timestamp="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
  local invocation="${GC_INVOCATION:-$0}"
  local last_command="${GC_LAST_ERROR_CMD:-}"

  {
    printf 'timestamp=%s\n' "$timestamp"
    printf 'command=%s\n' "$invocation"
    printf 'exit_code=%s\n' "$status"
    if [[ -n "$last_command" ]]; then
      printf 'last_command=%s\n' "$last_command"
    fi
    if [[ -n "${PROJECT_ROOT:-}" ]]; then
      printf 'project_root=%s\n' "$PROJECT_ROOT"
    fi
    printf 'working_dir=%s\n' "$PWD"
    printf -- '---\n'
  } >>"$log_file" 2>/dev/null || return

  GC_LAST_CRASH_LOG="$log_file"
  GC_CRASH_LOGGED=1
}

gc_exit_handler() {
  local status="${1:-0}"
  if (( status != 0 )); then
    if [[ -z "${GC_LAST_ERROR_STATUS:-}" || "${GC_LAST_ERROR_STATUS}" -eq 0 ]]; then
      GC_LAST_ERROR_STATUS="$status"
    fi
    gc_write_crash_log "$status"
    if gc_reports_enabled; then
      gc_reports_handle_crash "$status"
    fi
  fi
  gc_reports_cleanup
}

gc_record_codex_usage() {
  local log_file="${1:-}"
  local task="${2:-}"
  local model="${3:-}"
  local prompt_file="${4:-}"
  local exit_code="${5:-0}"

  GC_LAST_CODEX_PROMPT_TOKENS=0
  GC_LAST_CODEX_COMPLETION_TOKENS=0
  GC_LAST_CODEX_TOTAL_TOKENS=0

  [[ -n "$log_file" && -f "$log_file" ]] || return 0

  local usage_dir="${LOG_DIR:-${PROJECT_ROOT:-$PWD}/.gpt-creator/logs}"
  mkdir -p "$usage_dir"
  local usage_file="${usage_dir}/codex-usage.ndjson"
  local timestamp
  timestamp="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"

  local py_output=""
  local tmp_output
  tmp_output="$(mktemp "${TMPDIR:-/tmp}/gc-usage-XXXXXX")" || {
    warn "Failed to record Codex usage for task=${task} model=${model}."
    return 1
  }

  if python3 - <<'PY' "$log_file" "$usage_file" "$timestamp" "$task" "$model" "$prompt_file" "$exit_code" "${GC_COMMAND_FAILURE_CACHE:-}" "${GC_COMMAND_STREAM_CACHE:-}" "${GC_COMMAND_FILE_CACHE:-}" "${GC_COMMAND_SCAN_CACHE:-}" >"$tmp_output"
import base64
import hashlib
import json
import os
import pathlib
import re
import shlex
import sys
from typing import Optional

log_path = pathlib.Path(sys.argv[1])
usage_path = pathlib.Path(sys.argv[2])
timestamp = sys.argv[3]
task = sys.argv[4] or None
model = sys.argv[5] or None
prompt_file = sys.argv[6] or None
exit_code = int(sys.argv[7])
cmd_cache_arg = sys.argv[8] if len(sys.argv) > 8 else ""
cmd_cache_path = pathlib.Path(cmd_cache_arg) if cmd_cache_arg else None
stream_cache_arg = sys.argv[9] if len(sys.argv) > 9 else ""
stream_cache_path = pathlib.Path(stream_cache_arg) if stream_cache_arg else None
file_cache_arg = sys.argv[10] if len(sys.argv) > 10 else ""
file_cache_path = pathlib.Path(file_cache_arg) if file_cache_arg else None
scan_cache_arg = sys.argv[11] if len(sys.argv) > 11 else ""
scan_cache_path = pathlib.Path(scan_cache_arg) if scan_cache_arg else None

if log_path.exists():
    raw_text = log_path.read_text(encoding="utf-8", errors="ignore")
else:
    raw_text = ""

fields = {}
def parse_number(text: str) -> int:
    cleaned = (text or "").strip()
    if not cleaned:
        raise ValueError("empty")
    cleaned = cleaned.strip("[]{}()")
    cleaned = cleaned.lstrip("≈~<>≤≥=")
    cleaned = cleaned.replace(",", "").replace("_", "").replace(" ", "")
    if not cleaned:
        raise ValueError("empty")
    suffix = ""
    if cleaned and cleaned[-1].lower() in ("k", "m", "b", "g", "t"):
        suffix = cleaned[-1].lower()
        cleaned = cleaned[:-1]
    if not cleaned:
        raise ValueError("empty")
    number_match = re.match(r"^[-+]?(?:\d+|\d*\.\d+)$", cleaned)
    if not number_match:
        raise ValueError("unparsable")
    value = float(cleaned)
    multipliers = {
        "": 1,
        "k": 1_000,
        "m": 1_000_000,
        "b": 1_000_000_000,
        "g": 1_000_000_000,
        "t": 1_000_000_000_000,
    }
    factor = multipliers.get(suffix, 1)
    return int(round(value * factor))

def capture(field: str, value: str) -> None:
    if not value:
        return
    try:
        fields[field] = parse_number(value)
    except Exception:
        pass

number_pattern = r'((?:\d[\d,._]*|\d*\.\d+)(?:[kKmMbBgGtT]?))'
line_patterns = [
    ("total_tokens", re.compile(r'tokens[\s_\-]*used[^0-9]{0,16}' + number_pattern, re.IGNORECASE)),
    ("total_tokens", re.compile(r'tokens[\s_\-]*consumed[^0-9]{0,16}' + number_pattern, re.IGNORECASE)),
    ("total_tokens", re.compile(r'total[\s_\-]*tokens?(?:\s*(?:used|consumed))?["\']?[^0-9]{0,16}' + number_pattern, re.IGNORECASE)),
    ("prompt_tokens", re.compile(r'prompt[\s_\-]*tokens?(?:\s*(?:used|consumed))?["\']?[^0-9]{0,16}' + number_pattern, re.IGNORECASE)),
    ("completion_tokens", re.compile(r'completion[\s_\-]*tokens?(?:\s*(?:used|consumed))?["\']?[^0-9]{0,16}' + number_pattern, re.IGNORECASE)),
    ("cached_tokens", re.compile(r'cached[\s_\-]*tokens?["\']?[^0-9]{0,16}' + number_pattern, re.IGNORECASE)),
    ("prompt_tokens", re.compile(r'input[\s_\-]*tokens?["\']?[^0-9]{0,16}' + number_pattern, re.IGNORECASE)),
    ("completion_tokens", re.compile(r'output[\s_\-]*tokens?["\']?[^0-9]{0,16}' + number_pattern, re.IGNORECASE)),
    ("prompt_tokens", re.compile(r'\bprompt\s*=\s*' + number_pattern, re.IGNORECASE)),
    ("completion_tokens", re.compile(r'\bcompletion\s*=\s*' + number_pattern, re.IGNORECASE)),
    ("total_tokens", re.compile(r'\btotal\s*=\s*' + number_pattern, re.IGNORECASE)),
    ("cached_tokens", re.compile(r'\bcached\s*=\s*' + number_pattern, re.IGNORECASE)),
]

for line in raw_text.splitlines():
    if not line:
        continue
    for field, pattern in line_patterns:
        for match in pattern.finditer(line):
            capture(field, match.group(1))

if "total_tokens" not in fields:
    prompt_val = fields.get("prompt_tokens")
    completion_val = fields.get("completion_tokens")
    if prompt_val is not None or completion_val is not None:
        total = (prompt_val or 0) + (completion_val or 0)
        fields["total_tokens"] = total

record = {
    "timestamp": timestamp,
    "task": task,
    "model": model,
    "prompt_file": prompt_file,
    "exit_code": exit_code,
    "usage_captured": bool(fields),
}

for key in ("prompt_tokens", "completion_tokens", "total_tokens", "cached_tokens", "billable_units", "request_units"):
    if key in fields:
        record[key] = fields[key]

limit_needles = [
    "usage limit",
    "usage-limit",
    "usage cap",
    "usage-cap",
    "quota exceeded",
    "quota has been reached",
    "exceeded your current quota",
    "exceeded your quota",
    "quota reached",
    "credit balance is too low",
    "billing hard limit",
    "hard usage limit",
    "usage credits exhausted",
]

limit_message = None
if raw_text:
    for line in raw_text.splitlines():
        lower = line.lower()
        if not lower.strip():
            continue
        if any(needle in lower for needle in limit_needles):
            limit_message = line.strip()
            break

if limit_message:
    record["limit_detected"] = True
    record["limit_message"] = limit_message

command_failure_lines = []
command_stream_lines = []
command_file_lines = []
command_scan_lines = []
command_guard_lines = []
guard_entries = []
failure_remediation_notes = {}

cached_failure_cache = {}
cached_failure_counts = {}
cached_failure_details = {}
if cmd_cache_path:
    try:
        cache_raw = cmd_cache_path.read_text(encoding="utf-8")
        cached_failure_cache = json.loads(cache_raw) if cache_raw.strip() else {}
        if not isinstance(cached_failure_cache, dict):
            cached_failure_cache = {}
    except Exception:
        cached_failure_cache = {}

if cached_failure_cache:
    for value in cached_failure_cache.values():
        if not isinstance(value, dict):
            continue
        command_text = value.get("command")
        if not isinstance(command_text, str):
            continue
        try:
            count = int(value.get("count") or 0)
        except Exception:
            count = 0
        if count < 0:
            count = 0
        cached_failure_counts[command_text] = count
        cached_failure_details.setdefault(command_text, value)
if raw_text:
    lines_list = raw_text.splitlines()
    total_lines = len(lines_list)
    exec_pattern = re.compile(
        r"exec\s+[^\s]+\s+-lc\s+(?:'(?P<sqcmd>[^']*)'|\"(?P<dqcmd>[^\"]*)\"|(?P<plain>\S+))(?:\s+in\s+(?P<cwd>\S+))?"
    )
    result_pattern = re.compile(
        r"^\[(?P<ts>[^]]+)\]\s+[^\s]+\s+-lc\s+(?:(?P<sq>'(?P<sqcmd>[^']*)')|(?P<dq>\"(?P<dqcmd>[^\"]*)\")|(?P<plain>\S+))\s+(?P<outcome>succeeded|exited)\s*(?P<rest>.*)$"
    )
    timestamp_pattern = re.compile(r"^\[\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\]")
    line_timestamp_pattern = re.compile(r"^\[(?P<ts>[^]]+)\]")
    project_root_hint = os.getenv("PROJECT_ROOT", "")
    try:
        project_root_path = pathlib.Path(project_root_hint).resolve() if project_root_hint else None
    except (OSError, RuntimeError, ValueError):
        project_root_path = None
def normalise_candidate_path(raw: str, cwd: str) -> Optional[pathlib.Path]:
    candidate = (raw or "").strip()
    if not candidate:
        return None
        candidate = candidate.split("|", 1)[0]
        candidate = candidate.split(">", 1)[0]
        candidate = candidate.split("<", 1)[0]
        candidate = candidate.split(";", 1)[0]
        candidate = candidate.strip()
        if candidate.startswith(("'", '"')) and candidate.endswith(candidate[0]) and len(candidate) >= 2:
            candidate = candidate[1:-1].strip()
        if not candidate:
            return None
        candidate = os.path.expanduser(candidate)
        if os.path.isabs(candidate):
            path_obj = pathlib.Path(candidate)
        else:
            base = pathlib.Path(cwd) if cwd else project_root_path
            if base:
                path_obj = base / candidate
            else:
                path_obj = pathlib.Path(candidate)
        try:
            resolved = path_obj.resolve(strict=False)
        except (OSError, RuntimeError):
            resolved = path_obj
        if not resolved.exists() or not resolved.is_file():
            return None
        return resolved

    def build_file_info(path_obj: pathlib.Path, start: Optional[int], end: Optional[int], mode: str) -> Optional[dict]:
        try:
            stat = path_obj.stat()
        except OSError:
            return None
        mtime_ns = getattr(stat, "st_mtime_ns", None)
        if mtime_ns is None:
            mtime_ns = int(stat.st_mtime * 1_000_000_000)
        size = int(stat.st_size)
        rel_path = str(path_obj)
        if project_root_path:
            try:
                rel_path = str(path_obj.relative_to(project_root_path))
            except ValueError:
                rel_path = str(path_obj)
        max_chars = 2000
        max_lines = 80
        excerpt_lines: list[str] = []
        if start is not None and end is not None:
            lower = max(1, min(start, end))
            upper = max(lower, max(start, end))
            try:
                with path_obj.open("r", encoding="utf-8", errors="replace") as handle:
                    for idx, line in enumerate(handle, start=1):
                        if idx < lower:
                            continue
                        if idx > upper:
                            break
                        excerpt_lines.append(line.rstrip("\n"))
                        if len(excerpt_lines) >= max_lines:
                            break
            except OSError:
                excerpt_lines = []
            summary = f"{rel_path} lines {lower}-{upper}"
            range_value = [lower, upper]
        else:
            try:
                with path_obj.open("r", encoding="utf-8", errors="replace") as handle:
                    for _, line in zip(range(max_lines), handle):
                        excerpt_lines.append(line.rstrip("\n"))
            except OSError:
                excerpt_lines = []
            summary = f"{rel_path} (sample)"
            range_value = None
        excerpt_text = "\n".join(excerpt_lines)
        if len(excerpt_text) > max_chars:
            excerpt_text = excerpt_text[: max_chars - 3].rstrip() + "..."
        return {
            "path": str(path_obj),
            "rel_path": rel_path,
            "range": range_value,
            "mode": mode,
            "excerpt": excerpt_text,
            "mtime_ns": int(mtime_ns),
            "size": size,
            "summary": summary,
        }

    sed_chunk_pattern = re.compile(
        r"^sed\s+-n\s+['\"]?(?P<start>\d+)\s*,\s*(?P<end>\d+)[pP]['\"]?\s+(?P<path>[^|;]+)"
    )
    cat_pattern = re.compile(r"^cat\s+(?P<path>[^>|;&]+)$")

    def parse_file_read(command: str, cwd: str) -> Optional[dict]:
        command = (command or "").strip()
        if not command:
            return None
        sed_match = sed_chunk_pattern.match(command)
        if sed_match:
            path_obj = normalise_candidate_path(sed_match.group("path"), cwd)
            if not path_obj:
                return None
            try:
                start_val = int(sed_match.group("start"))
                end_val = int(sed_match.group("end"))
            except (TypeError, ValueError):
                return None
            return build_file_info(path_obj, start_val, end_val, "sed")
        cat_match = cat_pattern.match(command)
        if cat_match:
            path_obj = normalise_candidate_path(cat_match.group("path"), cwd)
            if not path_obj:
                return None
            return build_file_info(path_obj, None, None, "cat")
        return None

    failures = {}
    command_sequence = []
    i = 0
    while i < total_lines:
        line = lines_list[i]
        exec_match = exec_pattern.search(line)
        if exec_match:
            raw_cmd = (
                exec_match.group("sqcmd")
                or exec_match.group("dqcmd")
                or exec_match.group("plain")
                or ""
            )
            command_text = raw_cmd.strip()
            command_cwd = exec_match.group("cwd") or ""
            ts_match = line_timestamp_pattern.match(line)
            command_timestamp = ts_match.group("ts") if ts_match else ""
            current_entry = None
            if command_text:
                current_entry = {
                    "command": command_text,
                    "timestamp": command_timestamp,
                    "cwd": command_cwd,
                }
                command_sequence.append(current_entry)
            j = i + 1
            while j < total_lines:
                result_line = lines_list[j]
                result_match = result_pattern.match(result_line)
                if result_match:
                    outcome = result_match.group("outcome")
                    exit_value = 0
                    if outcome == "exited":
                        rest_text = result_match.group("rest") or ""
                        code_match = re.search(r"(-?\d+)", rest_text)
                        if code_match:
                            try:
                                exit_value = int(code_match.group(1))
                            except Exception:
                                exit_value = 1
                        else:
                            exit_value = 1
                    output_lines = []
                    k = j + 1
                    while k < total_lines and not timestamp_pattern.match(lines_list[k]):
                        output_lines.append(lines_list[k])
                        k += 1
                    summary_text = "\n".join(output_lines).strip()
                    stored_lines = output_lines[:80]
                    truncated_flag = len(output_lines) > len(stored_lines)
                    preview_text = "\n".join(stored_lines).strip()
                    if len(preview_text) > 2000:
                        preview_text = preview_text[:1997] + "..."
                        truncated_flag = True
                    if current_entry is not None:
                        current_entry["output_lines"] = stored_lines
                        current_entry["output_line_count"] = len(output_lines)
                        current_entry["output_truncated"] = truncated_flag
                        current_entry["output"] = preview_text
                    if exit_value != 0:
                        trimmed_summary = summary_text
                        if trimmed_summary.count("\n") > 30:
                            summary_rows = trimmed_summary.splitlines()
                            trimmed_summary = "\n".join(summary_rows[:30])
                            trimmed_summary += "\n... (output truncated) ..."
                        if len(trimmed_summary) > 1800:
                            trimmed_summary = trimmed_summary[:1797] + "..."
                        digest_source = f"{project_root_hint}\n{command_text}\n{exit_value}\n{trimmed_summary}"
                        digest = hashlib.sha256(digest_source.encode("utf-8", "ignore")).hexdigest()[:12]
                        entry = failures.get(digest)
                        if entry:
                            entry["count"] += 1
                        else:
                            entry = {
                                "command": command_text,
                                "exit": exit_value,
                                "summary": trimmed_summary,
                                "count": 1,
                                "digest": digest,
                            }
                        failures[digest] = entry
                    i = k - 1
                    break
                elif timestamp_pattern.match(result_line):
                    i = j - 1
                    break
                else:
                    j += 1
        i += 1

    sed_chunk_pattern = re.compile(
        r"^sed\s+-n\s+['\"]?(?P<start>\d+)\s*,\s*(?P<end>\d+)[pP]['\"]?\s+(?P<path>[^|;]+)"
    )

    def parse_sed_chunk(command: str):
        match = sed_chunk_pattern.match(command)
        if not match:
            return None
        try:
            start = int(match.group("start"))
            end = int(match.group("end"))
        except Exception:
            return None
        path = match.group("path").strip()
        path = path.split("|", 1)[0].strip()
        path = path.split(";", 1)[0].strip()
        if path.startswith(("'", '"')) and path.endswith(path[0]) and len(path) >= 2:
            path = path[1:-1]
        if not path:
            return None
        return {"file": path, "start": start, "end": end}

    build_artifact_dirs = {"dist", "dist-tests", "build", "coverage", "out", "tmp", ".next", "node_modules", "public-build"}

    def is_build_artifact_path(rel_path: str) -> bool:
        if not rel_path:
            return False
        normalized = rel_path.replace("\\", "/").strip("/")
        if not normalized:
            return False
        parts = [part for part in normalized.split("/") if part]
        for idx, part in enumerate(parts):
            if part in build_artifact_dirs:
                if part == "dist" and idx > 0 and parts[idx - 1] == "src":
                    continue
                return True
        return False

    def finalize_sequence(seq, results):
        if not seq:
            return
        coverage_lines = seq["coverage_end"] - seq["coverage_start"] + 1
        if seq["count"] >= 2 and coverage_lines >= 180:
            seq["coverage_lines"] = coverage_lines
            results.append(seq)

    stream_sequences = []
    current_seq = None
    gap_threshold = 40
    for entry in command_sequence:
        command_text = entry.get("command") or ""
        workdir_path = resolve_workdir(entry.get("cwd") or "", project_root_path)
        pnpm_issues = []
        if command_text:
            try:
                parts = shlex.split(command_text)
            except ValueError:
                parts = command_text.split()
            if parts and parts[0] == "pnpm":
                task_token = ""
                idx = 1
                skip_next = False
                while idx < len(parts):
                    token = parts[idx]
                    if skip_next:
                        skip_next = False
                        idx += 1
                        continue
                    if token in {"-C", "--dir", "--filter", "-F"}:
                        skip_next = True
                        idx += 1
                        continue
                    if token.startswith("-"):
                        idx += 1
                        continue
                    task_token = token
                    idx += 1
                    break
                if task_token == "run" and idx < len(parts):
                    task_token = parts[idx]
                pnpm_task = task_token
                if pnpm_task in {"test", "build"}:
                    modules_paths = [
                        workdir_path / "node_modules",
                        workdir_path / "node_modules" / ".pnpm",
                    ]
                    if project_root_path:
                        modules_paths.extend([
                            project_root_path / "node_modules",
                            project_root_path / "node_modules" / ".pnpm",
                        ])
                    modules_present = any(path.exists() for path in modules_paths)
                    if not modules_present:
                        pnpm_issues.append("node_modules missing; run `pnpm install` before invoking pnpm test/build.")
                    # Check for cached failures of the same command.
                    failure_entry = cached_failure_details.get(command_text)
                    if failure_entry:
                        try:
                            prev_count = int(failure_entry.get("count") or 0)
                        except Exception:
                            prev_count = 0
                        if prev_count > 0:
                            summary = (failure_entry.get("last_summary") or failure_entry.get("summary") or "").strip()
                            pnpm_issues.append(
                                f"Command previously failed {prev_count} time(s){': ' + summary if summary else ''}"
                            )
                    # Check for failed pnpm install
                    for value in cached_failure_cache.values():
                        if not isinstance(value, dict):
                            continue
                        cmd_text = value.get("command")
                        if isinstance(cmd_text, str) and cmd_text.strip().startswith("pnpm install"):
                            try:
                                install_fail_count = int(value.get("count") or 0)
                            except Exception:
                                install_fail_count = 0
                            if install_fail_count > 0:
                                install_summary = (value.get("last_summary") or value.get("summary") or "").strip()
                                pnpm_issues.append(
                                    f"`pnpm install` previously failed {install_fail_count} time(s){': ' + install_summary if install_summary else ''}"
                                )
                            break
                if pnpm_issues:
                    digest_source = f"{project_root_hint}\n{command_text}\n{entry.get('cwd') or ''}"
                    guard_digest = hashlib.sha256(digest_source.encode('utf-8', 'ignore')).hexdigest()[:12]
                    encoded_command = base64.b64encode(command_text.encode('utf-8')).decode('ascii') if command_text else ""
                    guard_message = ' '.join(pnpm_issues)
                    encoded_message = base64.b64encode(guard_message.encode('utf-8')).decode('ascii') if guard_message else ""
                    command_guard_lines.append(
                        f"CMDGUARD	{guard_digest}	0	{len(pnpm_issues)}	{encoded_command}	{encoded_message}"
                    )
                    guard_entries.append({
                        "command": command_text,
                        "issues": list(pnpm_issues),
                    })
        parsed = parse_sed_chunk(command_text)
        if not parsed:
            if current_seq:
                finalize_sequence(current_seq, stream_sequences)
                current_seq = None
            continue
        start = min(parsed["start"], parsed["end"])
        end = max(parsed["start"], parsed["end"])
        length = max(0, end - start + 1)
        if length <= 0:
            if current_seq:
                finalize_sequence(current_seq, stream_sequences)
                current_seq = None
            continue
        if current_seq and current_seq["file"] == parsed["file"]:
            prev = current_seq["entries"][-1]
            gap = start - prev["end"]
            if gap <= gap_threshold or start <= prev["end"]:
                current_seq["entries"].append(
                    {"start": start, "end": end, "command": entry["command"], "length": length}
                )
                current_seq["coverage_start"] = min(current_seq["coverage_start"], start)
                current_seq["coverage_end"] = max(current_seq["coverage_end"], end)
                current_seq["total_lines"] += length
                current_seq["count"] += 1
                continue
            finalize_sequence(current_seq, stream_sequences)
        current_seq = {
            "file": parsed["file"],
            "entries": [{"start": start, "end": end, "command": entry["command"], "length": length}],
            "coverage_start": start,
            "coverage_end": end,
            "total_lines": length,
            "count": 1,
        }
    if current_seq:
        finalize_sequence(current_seq, stream_sequences)

    file_cache_data = {}
    if file_cache_path:
        try:
            contents = file_cache_path.read_text(encoding="utf-8")
            file_cache_data = json.loads(contents) if contents.strip() else {}
            if not isinstance(file_cache_data, dict):
                file_cache_data = {}
        except Exception:
            file_cache_data = {}

    for entry in command_sequence:
        parsed_read = parse_file_read(entry.get("command"), entry.get("cwd") or "")
        if not parsed_read:
            continue
        digest_source_parts = [
            parsed_read.get("path", ""),
            parsed_read.get("mode", ""),
            str(parsed_read.get("range") or "full"),
            str(parsed_read.get("mtime_ns") or 0),
        ]
        digest_source = "::".join(digest_source_parts)
        digest = hashlib.sha256(digest_source.encode("utf-8", "ignore")).hexdigest()[:12]
        existing_entry = None
        if file_cache_data:
            existing_entry = file_cache_data.get(digest)
        prev_count = 0
        repeat_flag = False
        if isinstance(existing_entry, dict):
            try:
                prev_count = int(existing_entry.get("count") or 0)
            except Exception:
                prev_count = 0
            repeat_flag = prev_count > 0
        new_count = prev_count + 1
        record_entry = existing_entry or {}
        if not record_entry.get("first_seen"):
            record_entry["first_seen"] = timestamp
        record_entry["last_seen"] = timestamp
        record_entry["path"] = parsed_read.get("path")
        record_entry["rel_path"] = parsed_read.get("rel_path")
        record_entry["range"] = parsed_read.get("range")
        record_entry["mode"] = parsed_read.get("mode")
        record_entry["count"] = new_count
        record_entry["mtime_ns"] = parsed_read.get("mtime_ns")
        record_entry["size"] = parsed_read.get("size")
        record_entry["summary"] = parsed_read.get("summary")
        record_entry["excerpt"] = parsed_read.get("excerpt")
        record_entry["last_task"] = task
        rel_hint = record_entry.get("rel_path") or parsed_read.get("rel_path") or ""
        abs_hint = record_entry.get("path") or parsed_read.get("path") or ""
        build_artifact = False
        if rel_hint and is_build_artifact_path(rel_hint):
            build_artifact = True
        elif abs_hint:
            try:
                abs_path_obj = pathlib.Path(abs_hint)
                if project_root_path:
                    try:
                        rel_from_root = abs_path_obj.resolve().relative_to(project_root_path)
                        if is_build_artifact_path(str(rel_from_root)):
                            build_artifact = True
                    except Exception:
                        if is_build_artifact_path(str(abs_path_obj)):
                            build_artifact = True
                else:
                    if is_build_artifact_path(str(abs_path_obj)):
                        build_artifact = True
            except Exception:
                build_artifact = False
        if build_artifact:
            record_entry["category"] = "build-artifact"
            parsed_read["category"] = "build-artifact"
            parsed_read["excerpt"] = ""
            record_entry["excerpt"] = ""
        file_cache_data[digest] = record_entry
        summary_text = parsed_read.get("summary") or ""
        excerpt_text = parsed_read.get("excerpt") or ""
        encoded_summary = base64.b64encode(summary_text.encode("utf-8")).decode("ascii")
        encoded_excerpt = base64.b64encode(excerpt_text.encode("utf-8")).decode("ascii") if excerpt_text else ""
        repeat_flag_int = 1 if repeat_flag else 0
        command_file_lines.append(
            f"CMDFILE\t{digest}\t{repeat_flag_int}\t{new_count}\t{encoded_summary}\t{encoded_excerpt}"
        )

    scan_cache_data = {}
    if scan_cache_path:
        try:
            contents = scan_cache_path.read_text(encoding="utf-8")
            scan_cache_data = json.loads(contents) if contents.strip() else {}
            if not isinstance(scan_cache_data, dict):
                scan_cache_data = {}
        except Exception:
            scan_cache_data = {}

    for entry in command_sequence:
        classification = classify_directory_crawl(entry.get("command") or "")
        if not classification:
            continue
        digest_source = f"{project_root_hint}\n{entry.get('command') or ''}\n{entry.get('cwd') or ''}"
        digest = hashlib.sha256(digest_source.encode("utf-8", "ignore")).hexdigest()[:12]
        prev_count = 0
        repeat_flag = False
        existing_entry = None
        if scan_cache_data:
            existing_entry = scan_cache_data.get(digest)
            if isinstance(existing_entry, dict):
                try:
                    prev_count = int(existing_entry.get("count") or 0)
                except Exception:
                    prev_count = 0
                repeat_flag = prev_count > 0
            else:
                existing_entry = None
        new_count = prev_count + 1
        record_entry = existing_entry or {}
        if not record_entry.get("first_seen"):
            record_entry["first_seen"] = timestamp
        record_entry["last_seen"] = timestamp
        command_text = entry.get("command") or ""
        record_entry["command"] = command_text
        cwd_raw = entry.get("cwd") or ""
        record_entry["cwd"] = cwd_raw
        cwd_display = cwd_raw
        if project_root_path:
            try:
                resolved_cwd = resolve_workdir(cwd_raw, project_root_path)
            except Exception:
                resolved_cwd = None
            if resolved_cwd:
                try:
                    rel_cwd = resolved_cwd.relative_to(project_root_path)
                    cwd_display = "." if str(rel_cwd) in {"", "."} else str(rel_cwd)
                except Exception:
                    try:
                        cwd_display = str(resolved_cwd)
                    except Exception:
                        cwd_display = cwd_raw
        record_entry["cwd_display"] = cwd_display
        record_entry["count"] = new_count
        record_entry["message"] = classification
        output_lines = entry.get("output_lines") or []
        try:
            line_count = int(entry.get("output_line_count") or len(output_lines))
        except Exception:
            line_count = len(output_lines)
        truncated_flag = bool(entry.get("output_truncated"))
        if output_lines:
            max_preview_lines = 12
            preview_lines = []
            for raw_line in output_lines[:max_preview_lines]:
                preview_lines.append((raw_line or "").strip())
            record_entry["lines"] = preview_lines
            record_entry["line_count"] = line_count
            record_entry["truncated"] = int(truncated_flag or len(output_lines) > max_preview_lines)
            preview_text = (entry.get("output") or "\n".join(preview_lines)).strip()
            if len(preview_text) > 480:
                preview_text = preview_text[:477] + "..."
                record_entry["truncated"] = 1
            record_entry["preview"] = preview_text
        else:
            if "lines" not in record_entry:
                record_entry["lines"] = []
            if "line_count" not in record_entry or line_count:
                record_entry["line_count"] = line_count
            if truncated_flag:
                record_entry["truncated"] = 1
            if "preview" not in record_entry:
                record_entry["preview"] = ""
        scan_cache_data[digest] = record_entry
        encoded_command = base64.b64encode(command_text.encode("utf-8")).decode("ascii") if command_text else ""
        encoded_message = base64.b64encode(classification.encode("utf-8")).decode("ascii")
        repeat_flag_int = 1 if repeat_flag else 0
        command_scan_lines.append(
            f"CMDSCAN\t{digest}\t{repeat_flag_int}\t{new_count}\t{encoded_command}\t{encoded_message}"
        )

    if scan_cache_path:
        if len(scan_cache_data) > 80:
            sorted_items = sorted(
                scan_cache_data.items(),
                key=lambda item: item[1].get("last_seen", ""),
                reverse=True,
            )
            scan_cache_data = dict(sorted_items[:80])
        try:
            scan_cache_path.parent.mkdir(parents=True, exist_ok=True)
            scan_cache_path.write_text(json.dumps(scan_cache_data, indent=2), encoding="utf-8")
        except Exception:
            pass

    def remediation_message(command_text, failure_count=1, exit_code=None):
        cmd = (command_text or "").strip()
        lower = cmd.lower()
        base = "Investigate the failure output above and fix the root cause."
        if "pnpm" in lower and "build" in lower:
            base = "Review the pnpm build errors and update the source code or configuration."
        elif "pnpm" in lower and "test" in lower:
            base = "Fix the failing tests or prerequisites before running this test command again."
        elif "pnpm" in lower and "install" in lower:
            base = "Resolve the installation issue (dependency or network) before retrying `pnpm install`."
        elif "pnpm" in lower:
            base = "Resolve the pnpm command failure before rerunning."
        elif "npm" in lower and "run" in lower:
            base = "Fix the npm script failure before rerunning the command." 
        elif lower.startswith("go test") or " go test" in lower:
            base = "Correct the Go test failure before re-running `go test`."
        elif "pytest" in lower:
            base = "Fix the pytest errors before rerunning the tests."
        elif "make " in lower:
            base = "Address the make target failure before rerunning."
        if failure_count and failure_count > 1:
            suffix = f" It has already failed {failure_count} time(s); do not rerun until the fix is applied and documented."
        else:
            suffix = " Do not rerun until the fix is applied and documented."
        return base + suffix

    if failures:
        cache_data = dict(cached_failure_cache)
        for digest, entry in failures.items():
            summary_line = entry.get("summary") or ""
            summary_line = re.sub(r'\s+\n', '\n', summary_line)
            summary_line = re.sub(r'\n\s+', '\n', summary_line).strip()
            entry["summary"] = summary_line
            if cmd_cache_path:
                existing = cache_data.get(digest)
                if isinstance(existing, dict):
                    prev_count = int(existing.get("count", 0))
                    existing["count"] = prev_count + entry["count"]
                    existing["last_seen"] = timestamp
                    existing["last_summary"] = summary_line
                    existing["command"] = entry["command"]
                    existing["exit"] = entry["exit"]
                    if "first_seen" not in existing:
                        existing["first_seen"] = timestamp
                    entry["repeat"] = prev_count > 0
                    entry["total_failures"] = existing["count"]
                else:
                    cache_data[digest] = {
                        "command": entry["command"],
                        "exit": entry["exit"],
                        "summary": summary_line,
                        "count": entry["count"],
                        "first_seen": timestamp,
                        "last_seen": timestamp,
                        "last_summary": summary_line,
                    }
                    entry["repeat"] = False
                    entry["total_failures"] = entry["count"]
            else:
                entry["repeat"] = False
                entry["total_failures"] = entry["count"]

        if cmd_cache_path:
            if len(cache_data) > 50:
                sorted_items = sorted(
                    cache_data.items(),
                    key=lambda item: item[1].get("last_seen", ""),
                    reverse=True,
                )
                cache_data = dict(sorted_items[:50])
            try:
                cmd_cache_path.parent.mkdir(parents=True, exist_ok=True)
                cmd_cache_path.write_text(json.dumps(cache_data, indent=2), encoding="utf-8")
            except Exception:
                pass

        guard_failure_commands = set()
        for entry in failures.values():
            summary_single = entry.get("summary") or ""
            summary_single = re.sub(r"\s+", " ", summary_single).strip()
            if len(summary_single) > 240:
                summary_single = summary_single[:237] + "..."
            encoded_command = base64.b64encode(entry["command"].encode("utf-8")).decode("ascii")
            encoded_summary = (
                base64.b64encode(summary_single.encode("utf-8")).decode("ascii") if summary_single else ""
            )
            repeat_flag = 1 if entry.get("repeat") else 0
            total_failures = entry.get("total_failures", entry["count"])
            remediation_note = remediation_message(entry.get("command"), total_failures, entry.get("exit"))
            command_text_clean = (entry.get("command") or "").strip()
            if command_text_clean:
                failure_remediation_notes[command_text_clean] = remediation_note
                if command_text_clean not in guard_failure_commands:
                    issues = [remediation_note]
                    if summary_single:
                        issues.append(f"Last failure: {summary_single}")
                    guard_digest_source = f"{project_root_hint}\n{command_text_clean}\nremediation"
                    guard_digest = hashlib.sha256(guard_digest_source.encode("utf-8", "ignore")).hexdigest()[:12]
                    encoded_guard_command = base64.b64encode(command_text_clean.encode("utf-8")).decode("ascii")
                    encoded_guard_message = base64.b64encode("; ".join(issues).encode("utf-8")).decode("ascii")
                    repeat_flag_guard = 1 if total_failures and total_failures > 1 else 0
                    command_guard_lines.append(
                        f"CMDGUARD\t{guard_digest}\t{repeat_flag_guard}\t{len(issues)}\t{encoded_guard_command}\t{encoded_guard_message}"
                    )
                    guard_entries.append({
                        "command": command_text_clean,
                        "issues": issues,
                    })
                    guard_failure_commands.add(command_text_clean)
            command_failure_lines.append(
                f"CMDFAIL\t{repeat_flag}\t{total_failures}\t{entry['exit']}\t{entry['digest']}\t{encoded_command}\t{encoded_summary}"
            )

    if stream_sequences:
        stream_cache_data = {}
        if stream_cache_path:
            try:
                contents = stream_cache_path.read_text(encoding="utf-8")
                stream_cache_data = json.loads(contents) if contents.strip() else {}
                if not isinstance(stream_cache_data, dict):
                    stream_cache_data = {}
            except Exception:
                stream_cache_data = {}
        for seq in stream_sequences:
            coverage_start = seq["coverage_start"]
            coverage_end = seq["coverage_end"]
            coverage_lines = seq["coverage_lines"]
            command_examples = [item["command"] for item in seq["entries"][:3]]
            summary_text = (
                f"{seq['file']}: sequential sed chunks {coverage_start}-{coverage_end} covering ~{coverage_lines} lines via {seq['count']} commands."
            )
            if command_examples:
                summary_text += f" Example: {command_examples[0]}"
                if len(command_examples) > 1:
                    summary_text += f" → {command_examples[1]}"
            advice_text = (
                f"Switch to targeted search (e.g. rg -n '<term>' {seq['file']} -C20) or "
                f"use gpt-creator show-file {seq['file']} --range {coverage_start}:{min(coverage_end, coverage_start + 200)} "
                "to inspect slices without streaming entire files."
            )
            digest_source = f"{project_root_hint}\n{seq['file']}\n{coverage_start}-{coverage_end}"
            digest = hashlib.sha256(digest_source.encode("utf-8", "ignore")).hexdigest()[:12]
            prev_count = 0
            repeat_flag = False
            existing_entry = None
            if stream_cache_path:
                existing_entry = stream_cache_data.get(digest)
                if isinstance(existing_entry, dict):
                    try:
                        prev_count = int(existing_entry.get("count", 0))
                    except Exception:
                        prev_count = 0
                    repeat_flag = prev_count > 0
                else:
                    existing_entry = None
            new_count = prev_count + 1
            if stream_cache_path:
                record_entry = existing_entry or {}
                if not record_entry.get("first_seen"):
                    record_entry["first_seen"] = timestamp
                record_entry["last_seen"] = timestamp
                record_entry["file"] = seq["file"]
                record_entry["coverage"] = [coverage_start, coverage_end]
                record_entry["count"] = new_count
                record_entry["summary"] = summary_text
                record_entry["advice"] = advice_text
                record_entry["commands"] = command_examples
                stream_cache_data[digest] = record_entry
            encoded_summary = base64.b64encode(summary_text.encode("utf-8")).decode("ascii")
            encoded_advice = base64.b64encode(advice_text.encode("utf-8")).decode("ascii") if advice_text else ""
            repeat_flag_int = 1 if repeat_flag else 0
            command_stream_lines.append(
                f"CMDSTREAM\t{digest}\t{repeat_flag_int}\t{new_count}\t{encoded_summary}\t{encoded_advice}"
            )
        if stream_cache_path:
            if len(stream_cache_data) > 50:
                sorted_items = sorted(
                    stream_cache_data.items(),
                    key=lambda item: item[1].get("last_seen", ""),
                    reverse=True,
                )
                stream_cache_data = dict(sorted_items[:50])
            try:
                stream_cache_path.parent.mkdir(parents=True, exist_ok=True)
                stream_cache_path.write_text(json.dumps(stream_cache_data, indent=2), encoding="utf-8")
            except Exception:
                pass

    if file_cache_path:
        if len(file_cache_data) > 120:
            sorted_items = sorted(
                file_cache_data.items(),
                key=lambda item: item[1].get("last_seen", ""),
                reverse=True,
            )
            file_cache_data = dict(sorted_items[:120])
        try:
            file_cache_path.parent.mkdir(parents=True, exist_ok=True)
            file_cache_path.write_text(json.dumps(file_cache_data, indent=2), encoding="utf-8")
        except Exception:
            pass

usage_path.parent.mkdir(parents=True, exist_ok=True)
with usage_path.open("a", encoding="utf-8") as fh:
    fh.write(json.dumps(record, sort_keys=True, separators=(",", ":")))
    fh.write("\n")

for entry_line in command_failure_lines:
    print(entry_line)

for entry_line in command_stream_lines:
    print(entry_line)

for entry_line in command_scan_lines:
    print(entry_line)

for entry_line in command_guard_lines:
    print(entry_line)

for entry_line in command_file_lines:
    print(entry_line)

if fields:
    prompt_value = int(fields.get("prompt_tokens") or 0)
    completion_value = int(fields.get("completion_tokens") or 0)
    total_value = int(fields.get("total_tokens") or (prompt_value + completion_value))
    print(f"USAGE\t{prompt_value}\t{completion_value}\t{total_value}")

if limit_message:
    print(f"LIMIT_DETECTED\t{limit_message}")
PY
  then
    py_output="$(<"$tmp_output")"
    rm -f "$tmp_output" || true
    if [[ -n "$py_output" ]]; then
      while IFS= read -r line; do
        [[ -z "$line" ]] && continue
        case "$line" in
          LIMIT_DETECTED$'\t'*)
            GC_CODEX_USAGE_LIMIT_REACHED=1
            GC_CODEX_USAGE_LIMIT_MESSAGE="${line#LIMIT_DETECTED$'\t'}"
            ;;
          LIMIT_DETECTED)
            GC_CODEX_USAGE_LIMIT_REACHED=1
            GC_CODEX_USAGE_LIMIT_MESSAGE=""
            ;;
          USAGE$'\t'*)
            IFS=$'\t' read -r _ prompt_val completion_val total_val <<<"$line"
            if [[ "$prompt_val" =~ ^[0-9]+$ ]]; then
              GC_LAST_CODEX_PROMPT_TOKENS=$((prompt_val))
            else
              GC_LAST_CODEX_PROMPT_TOKENS=0
            fi
            if [[ "$completion_val" =~ ^[0-9]+$ ]]; then
              GC_LAST_CODEX_COMPLETION_TOKENS=$((completion_val))
            else
              GC_LAST_CODEX_COMPLETION_TOKENS=0
            fi
            if [[ "$total_val" =~ ^[0-9]+$ ]]; then
              GC_LAST_CODEX_TOTAL_TOKENS=$((total_val))
            else
              GC_LAST_CODEX_TOTAL_TOKENS=$((GC_LAST_CODEX_PROMPT_TOKENS + GC_LAST_CODEX_COMPLETION_TOKENS))
            fi
            ;;
          CMDFAIL$'\t'*)
            local rest="${line#CMDFAIL$'\t'}"
            local repeat_flag total_fail exit_status digest encoded_command encoded_summary
            IFS=$'\t' read -r repeat_flag total_fail exit_status digest encoded_command encoded_summary <<<"$rest"
            local command_text summary_text
            command_text="$(gc_decode_base64 "${encoded_command:-}")"
            summary_text="$(gc_decode_base64 "${encoded_summary:-}")"
            command_text="${command_text//$'\r'/ }"
            command_text="${command_text//$'\n'/ }"
            summary_text="${summary_text//$'\r'/ }"
            summary_text="${summary_text//$'\n'/ }"
            command_text="${command_text#"${command_text%%[![:space:]]*}"}"
            command_text="${command_text%"${command_text##*[![:space:]]}"}"
            summary_text="${summary_text#"${summary_text%%[![:space:]]*}"}"
            summary_text="${summary_text%"${summary_text##*[![:space:]]}"}"
            if [[ ${#summary_text} -gt 160 ]]; then
              summary_text="${summary_text:0:157}..."
            fi
            if [[ "$repeat_flag" == "1" && -n "$digest" ]]; then
              if [[ "$GC_COMMAND_FAILURE_WARN_DIGESTS" != *"|$digest|"* ]]; then
                if [[ -n "$command_text" ]]; then
                  warn "Repeated command failure detected (x${total_fail}): ${command_text}"
                else
                  warn "Repeated command failure detected (x${total_fail})."
                fi
                if [[ -n "$summary_text" ]]; then
                  warn "  Last failure: ${summary_text}"
                fi
                GC_COMMAND_FAILURE_WARN_DIGESTS+="|$digest|"
              fi
            fi
            ;;
          CMDSTREAM$'\t'*)
            local rest="${line#CMDSTREAM$'\t'}"
            local digest repeat_flag total_seen encoded_summary encoded_advice
            IFS=$'\t' read -r digest repeat_flag total_seen encoded_summary encoded_advice <<<"$rest"
            local summary_text advice_text
            summary_text="$(gc_decode_base64 "${encoded_summary:-}")"
            advice_text="$(gc_decode_base64 "${encoded_advice:-}")"
            summary_text="${summary_text//$'\r'/ }"
            summary_text="${summary_text//$'\n'/ }"
            advice_text="${advice_text//$'\r'/ }"
            advice_text="${advice_text//$'\n'/ }"
            summary_text="${summary_text#"${summary_text%%[![:space:]]*}"}"
            summary_text="${summary_text%"${summary_text##*[![:space:]]}"}"
            advice_text="${advice_text#"${advice_text%%[![:space:]]*}"}"
            advice_text="${advice_text%"${advice_text##*[![:space:]]}"}"
            if [[ "$GC_COMMAND_STREAM_WARN_DIGESTS" != *"|$digest|"* ]]; then
              local repeat_note=""
              if [[ "$repeat_flag" == "1" ]]; then
                repeat_note=" (repeat)"
              fi
              warn "Sequential sed/cat streaming detected${repeat_note}: ${summary_text}"
              if [[ -n "$advice_text" ]]; then
                warn "  Recommendation: ${advice_text}"
              fi
              if [[ "$total_seen" =~ ^[0-9]+$ && "$total_seen" -gt 1 ]]; then
                warn "  Observed ${total_seen} times; pivot to targeted searches to cut token usage."
              fi
              GC_COMMAND_STREAM_WARN_DIGESTS+="|$digest|"
            fi
            ;;
          CMDSCAN$'\t'*)
            local rest="${line#CMDSCAN$'\t'}"
            local digest repeat_flag total_seen encoded_command encoded_message
            IFS=$'\t' read -r digest repeat_flag total_seen encoded_command encoded_message <<<"$rest"
            local command_text message_text
            command_text="$(gc_decode_base64 "${encoded_command:-}")"
            message_text="$(gc_decode_base64 "${encoded_message:-}")"
            command_text="${command_text//$'\r'/ }"
            command_text="${command_text//$'\n'/ }"
            message_text="${message_text//$'\r'/ }"
            message_text="${message_text//$'\n'/ }"
            command_text="${command_text#"${command_text%%[![:space:]]*}"}"
            command_text="${command_text%"${command_text##*[![:space:]]}"}"
            message_text="${message_text#"${message_text%%[![:space:]]*}"}"
            message_text="${message_text%"${message_text##*[![:space:]]}"}"
            if [[ "$GC_COMMAND_SCAN_WARN_DIGESTS" != *"|$digest|"* ]]; then
              local repeat_note=""
              if [[ "$repeat_flag" == "1" ]]; then
                repeat_note=" (repeat)"
              fi
              warn "Directory crawl detected${repeat_note}: ${command_text:-<command unavailable>}"
              if [[ -n "$message_text" ]]; then
                warn "  Reason: ${message_text}"
              fi
              if [[ "$total_seen" =~ ^[0-9]+$ && "$total_seen" -gt 1 ]]; then
                warn "  Observed ${total_seen} times; declare new focus before exploring other areas."
              fi
              GC_COMMAND_SCAN_WARN_DIGESTS+="|$digest|"
            fi
            ;;
          CMDGUARD$'\t'*)
            local rest="${line#CMDGUARD$'\t'}"
            local digest repeat_flag issue_count encoded_command encoded_message
            IFS=$'\t' read -r digest repeat_flag issue_count encoded_command encoded_message <<<"$rest"
            local command_text message_text
            command_text="$(gc_decode_base64 "${encoded_command:-}")"
            message_text="$(gc_decode_base64 "${encoded_message:-}")"
            command_text="${command_text//$'\r'/ }"
            command_text="${command_text//$'\n'/ }"
            message_text="${message_text//$'\r'/ }"
            message_text="${message_text//$'\n'/ }"
            command_text="${command_text#"${command_text%%[![:space:]]*}"}"
            command_text="${command_text%"${command_text##*[![:space:]]}"}"
            message_text="${message_text#"${message_text%%[![:space:]]*}"}"
            message_text="${message_text%"${message_text##*[![:space:]]}"}"
            if [[ "$GC_COMMAND_GUARD_WARN_DIGESTS" != *"|$digest|"* ]]; then
              warn "Guardrails blocked expensive command: ${command_text:-pnpm}"
              if [[ -n "$message_text" ]]; then
                warn "  Fix before retry: ${message_text}"
              fi
              if [[ "$issue_count" =~ ^[0-9]+$ && "$issue_count" -gt 1 ]]; then
                warn "  Multiple pre-check issues detected; update plan/focus once resolved."
              fi
              GC_COMMAND_GUARD_WARN_DIGESTS+="|$digest|"
            fi
            ;;
          CMDFILE$'\t'*)
            local rest="${line#CMDFILE$'\t'}"
            local digest repeat_flag total_seen encoded_summary encoded_excerpt
            IFS=$'\t' read -r digest repeat_flag total_seen encoded_summary encoded_excerpt <<<"$rest"
            local summary_text excerpt_text
            summary_text="$(gc_decode_base64 "${encoded_summary:-}")"
            excerpt_text="$(gc_decode_base64 "${encoded_excerpt:-}")"
            summary_text="${summary_text//$'\r'/ }"
            summary_text="${summary_text//$'\n'/ }"
            excerpt_text="${excerpt_text//$'\r'/ }"
            excerpt_text="${excerpt_text//$'\n'/ }"
            summary_text="${summary_text#"${summary_text%%[![:space:]]*}"}"
            summary_text="${summary_text%"${summary_text##*[![:space:]]}"}"
            excerpt_text="${excerpt_text#"${excerpt_text%%[![:space:]]*}"}"
            excerpt_text="${excerpt_text%"${excerpt_text##*[![:space:]]}"}"
            if [[ "$repeat_flag" == "1" ]]; then
              if [[ "$GC_COMMAND_FILE_WARN_DIGESTS" != *"|$digest|"* ]]; then
                warn "Repeated file read detected: ${summary_text}"
                if [[ -n "$excerpt_text" ]]; then
                  local preview="$excerpt_text"
                  if [[ ${#preview} -gt 140 ]]; then
                    preview="${preview:0:137}..."
                  fi
                  warn "  Refer to cached excerpt instead of re-running cat/sed: \"${preview}\""
                else
                  warn "  Refer to the cached excerpt listed in the prompt instead of re-reading the file."
                fi
                if [[ "$total_seen" =~ ^[0-9]+$ && "$total_seen" -gt 1 ]]; then
                  warn "  Observed ${total_seen} reads for this slice; reuse the cached snippet unless the file changed."
                fi
                GC_COMMAND_FILE_WARN_DIGESTS+="|$digest|"
              fi
            else
              if [[ -n "$summary_text" ]]; then
                info "Cached file excerpt saved: ${summary_text}"
              fi
            fi
            ;;
        esac
      done <<<"$py_output"
    fi
    local limit_value="${GC_CODEX_MAX_TOKENS_PER_TASK:-0}"
    if [[ "$limit_value" =~ ^[0-9]+$ ]]; then
      limit_value=$((limit_value))
      if (( limit_value > 0 && GC_LAST_CODEX_TOTAL_TOKENS > limit_value )); then
        GC_CODEX_USAGE_LIMIT_REACHED=1
        if [[ -z "${GC_CODEX_USAGE_LIMIT_MESSAGE:-}" ]]; then
          GC_CODEX_USAGE_LIMIT_MESSAGE="Codex tokens ${GC_LAST_CODEX_TOTAL_TOKENS} exceeded per-task limit ${limit_value}."
        fi
      fi
    fi
    rm -f "$log_file" || true
    return 0
  else
    rm -f "$tmp_output" || true
    warn "Failed to record Codex usage for task=${task} model=${model}."
    return 1
  fi
}

gc_bootstrap_reset_state() {
  local file
  file="$(gc_bootstrap_state_file)"
  rm -f "$file"
}

gc_bootstrap_mark_step() {
  local step="${1:?step required}"
  local status="${2:?status required}"
  local file
  file="$(gc_bootstrap_state_file)"
  mkdir -p "$(dirname "$file")"
  python3 - <<'PY' "$file" "$step" "$status"
import json
import sys
import time
from pathlib import Path

path = Path(sys.argv[1])
step = sys.argv[2]
status = sys.argv[3]

if status == 'reset':
    if path.exists():
        try:
            data = json.loads(path.read_text(encoding='utf-8'))
        except Exception:
            data = {}
        steps = data.get('steps', {})
        steps.pop(step, None)
        data['steps'] = steps
        path.write_text(json.dumps(data, indent=2) + '\n', encoding='utf-8')
    sys.exit(0)

try:
    data = json.loads(path.read_text(encoding='utf-8'))
except Exception:
    data = {}

steps = data.setdefault('steps', {})
steps[step] = {
    'status': status,
    'updated_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
}
if status == 'done':
    data['last_completed'] = step
elif status == 'failed':
    data['failed_step'] = step
else:
    data.pop('failed_step', None)

path.write_text(json.dumps(data, indent=2) + '\n', encoding='utf-8')
PY
}

gc_bootstrap_mark_complete() {
  local file
  file="$(gc_bootstrap_state_file)"
  mkdir -p "$(dirname "$file")"
  python3 - <<'PY' "$file"
import json
import sys
import time
from pathlib import Path

path = Path(sys.argv[1])
try:
    data = json.loads(path.read_text(encoding='utf-8'))
except Exception:
    data = {}

data['completed_at'] = time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())
path.write_text(json.dumps(data, indent=2) + '\n', encoding='utf-8')
PY
}

gc_bootstrap_step_is_done() {
  local step="${1:?step required}"
  local file
  file="$(gc_bootstrap_state_file)"
  [[ -f "$file" ]] || return 1
  python3 - "$file" "$step" <<'PY'
import json
import sys
from pathlib import Path

path = Path(sys.argv[1])
step = sys.argv[2]
try:
    data = json.loads(path.read_text(encoding='utf-8'))
except Exception:
    data = {}
steps = data.get('steps') or {}
status = steps.get(step, {}).get('status')
sys.exit(0 if status == 'done' else 1)
PY
}

gc_bootstrap_run_step() {
  local step="${1:?step required}"
  shift
  if gc_bootstrap_step_is_done "$step"; then
    info "Step '${step}' already completed; skipping."
    return 0
  fi
  if "$@"; then
    gc_bootstrap_mark_step "$step" "done"
    return 0
  else
    gc_bootstrap_mark_step "$step" "failed"
    return 1
  fi
}

gc_bootstrap_have_rfp() {
  local stage="${STAGING_DIR:-}" input="${INPUT_DIR:-}"
  [[ -n "$stage" && -f "$stage/docs/rfp.md" ]] && return 0
  [[ -n "$stage" && -f "$stage/rfp.md" ]] && return 0
  [[ -n "$input" && -f "$input/rfp.md" ]] && return 0
  return 1
}

gc_auto_project_template() {
  local project_root="${1:?project root required}"
  local templates_root="${2:?templates root required}"
  shift 2
  local -a template_dirs=("$@")
  local count=${#template_dirs[@]}
  (( count )) || return 1
  if (( count == 1 )); then
    printf '%s\n' "${template_dirs[0]}"
    return 0
  fi

  local rfp_path
  rfp_path="$(gc_find_primary_rfp "$project_root")"
  [[ -n "$rfp_path" ]] || return 1

  python3 - "$rfp_path" "${template_dirs[@]}" <<'PY'
import json
import re
import sys
from pathlib import Path

rfp_path = Path(sys.argv[1])
rfp_text = rfp_path.read_text(encoding='utf-8', errors='ignore').lower()
templates = sys.argv[2:]

stopwords = {"the","and","with","from","base","template","project","app","service","system"}

def template_tokens(path: Path):
    tokens = set()
    name = path.name.lower()
    tokens.update(token for token in re.split(r'[^a-z0-9]+', name) if len(token) >= 3)
    tags_file = path / 'tags.txt'
    if tags_file.exists():
        for line in tags_file.read_text(encoding='utf-8', errors='ignore').splitlines():
            for token in re.split(r'[^a-z0-9]+', line.lower()):
                if len(token) >= 3:
                    tokens.add(token)
    template_json = path / 'template.json'
    if template_json.exists():
        try:
            data = json.loads(template_json.read_text(encoding='utf-8', errors='ignore'))
        except Exception:
            data = {}
        for field in ("tags", "keywords", "stack"):
            value = data.get(field)
            if isinstance(value, str):
                for token in re.split(r'[^a-z0-9]+', value.lower()):
                    if len(token) >= 3:
                        tokens.add(token)
            elif isinstance(value, list):
                for entry in value:
                    text = str(entry).lower()
                    for token in re.split(r'[^a-z0-9]+', text):
                        if len(token) >= 3:
                            tokens.add(token)
    return {token for token in tokens if token not in stopwords}

scores = []
for template in templates:
    path = Path(template)
    tokens = template_tokens(path)
    score = 0
    for token in tokens:
        if token and token in rfp_text:
            score += rfp_text.count(token)
    scores.append((score, template))

scores.sort(reverse=True)
if scores and scores[0][0] > 0:
    print(scores[0][1])
PY
}

gc_copy_project_template() {
  local template_dir="${1:?template directory required}"
  local project_root="${2:?project root required}"
  python3 - "$template_dir" "$project_root" <<'PY'
import shutil
import sys
from pathlib import Path

src = Path(sys.argv[1])
dest = Path(sys.argv[2])

for path in src.rglob('*'):
    if path.name in {'.git', '.DS_Store'}:
        continue
    rel = path.relative_to(src)
    target = dest / rel
    if path.is_dir():
        target.mkdir(parents=True, exist_ok=True)
        continue
    target.parent.mkdir(parents=True, exist_ok=True)
    if target.exists():
        print(f"SKIP {rel}")
        continue
    shutil.copy2(path, target)
    print(f"COPY {rel}")
PY
}

gc_apply_project_template() {
  local project_root="${1:?project root required}"
  local template_request="${2:-auto}"
  local templates_root
  templates_root="$(gc_project_templates_root)"

  mapfile -t available_templates < <(find "$templates_root" -mindepth 1 -maxdepth 1 -type d | sort)
  (( ${#available_templates[@]} )) || {
    info "No project templates available under ${templates_root}; continuing without scaffolding."
    return 0
  }

  local chosen=""
  local request_lower="$(to_lower "$template_request")"
  if [[ "$request_lower" == "skip" ]]; then
    info "Skipping project template scaffolding (per flag)."
    return 0
  fi

  if [[ "$request_lower" != "auto" ]]; then
    for tpl in "${available_templates[@]}"; do
      local tpl_name="$(basename "$tpl")"
      if [[ "$(to_lower "$tpl_name")" == "$request_lower" ]]; then
        chosen="$tpl"
        template_request="$tpl_name"
        break
      fi
    done
    if [[ -z "$chosen" ]]; then
      warn "Template '${template_request}' not found under ${templates_root}; available: $(printf '%s ' "${available_templates[@]##*/}")"
      return 1
    fi
  else
    chosen="$(gc_auto_project_template "$project_root" "$templates_root" "${available_templates[@]}")"
    if [[ -z "$chosen" ]]; then
      info "No matching project template determined automatically; continuing without scaffolding."
      return 0
    fi
  fi

  local template_name="$(basename "$chosen")"
  info "Applying project template → ${template_name}"
  if ! gc_copy_project_template "$chosen" "$project_root"; then
    warn "Failed to copy template '${template_name}'"
    return 1
  fi
}

gc_parse_jira_tasks() {
  local jira_file="${1:?jira markdown path required}"
  local out_json="${2:?output json path required}"
  python3 - <<'PY' "$jira_file" "$out_json"
import json
import re
import sys
import time
from pathlib import Path

jira_path, out_path = sys.argv[1:3]
lines = Path(jira_path).read_text(encoding='utf-8').splitlines()

epic_id = ""
epic_title = ""
story_id = ""
story_title = ""
tasks = []
current = None
section = None

dashes = r'[\-\u2012\u2013\u2014\u2015]'


def normalise_list(value: str):
    cleaned = value.replace('+', ',').replace('/', ',').replace('&', ',')
    parts = [part.strip() for part in re.split(r',|;|\\band\\b', cleaned, flags=re.I) if part.strip()]
    seen = set()
    ordered = []
    for item in parts:
        key = item.lower()
        if key not in seen:
            seen.add(key)
            ordered.append(item)
    return ordered


def flush_current():
    global current, tasks, section
    if current is None:
        return
    description_lines = [line.rstrip() for line in current['description_lines'] if line.strip()]
    description = "\n".join(description_lines).strip()
    current['description'] = description
    del current['description_lines']
    tasks.append(current)
    current = None
    section = None


for raw in lines:
    stripped = raw.strip()
    if not stripped:
        if current is not None and section == 'description':
            current['description_lines'].append('')
        continue

    if stripped.lower() in {'**epic**', '**story**', '### **story**'}:
        continue

    epic_heading = re.match(r'^##\s+Epic\s+([A-Za-z0-9_.-]+)\s+' + dashes + r'\s+(.*)$', stripped)
    epic_bold = re.match(r'^\*\*([Ee][A-Za-z0-9_.:-]+)\s*' + dashes + r'\s*(.+?)\*\*$', stripped)
    if epic_heading or epic_bold:
        flush_current()
        if epic_heading:
            epic_id, epic_title = epic_heading.group(1).strip(), epic_heading.group(2).strip()
        else:
            epic_id, epic_title = epic_bold.group(1).strip(), epic_bold.group(2).strip()
        story_id = ""
        story_title = ""
        continue

    story_heading = re.match(r'^###\s+Story\s+([A-Za-z0-9_.-]+)\s+' + dashes + r'\s+(.*)$', stripped)
    story_bold = re.match(r'^\*\*([Ss][A-Za-z0-9_.:-]+)\s*' + dashes + r'\s*(.+?)\*\*$', stripped)
    if story_heading or story_bold:
        flush_current()
        if story_heading:
            story_id, story_title = story_heading.group(1).strip(), story_heading.group(2).strip()
        else:
            story_id, story_title = story_bold.group(1).strip(), story_bold.group(2).strip()
        continue

    task_match = re.match(r'^\*\*([Tt][A-Za-z0-9_.:-]+)\s*' + dashes + r'\s*(.+?)\*\*$', stripped)
    if task_match:
        flush_current()
        task_id, task_title = task_match.groups()
        current = {
            'epic_id': epic_id,
            'epic_title': epic_title,
            'story_id': story_id,
            'story_title': story_title,
            'id': task_id.strip(),
            'title': task_title.strip(),
            'assignees': [],
            'tags': [],
            'estimate': '',
            'description_lines': [],
            'acceptance_criteria': [],
            'dependencies': [],
        }
        section = None
        continue

    if current is None:
        continue

    if '**Description:**' in stripped:
        section = 'description'
        after = stripped.split('**Description:**', 1)[1].strip()
        if after:
            current['description_lines'].append(after)
        continue

    if '**Acceptance Criteria:**' in stripped:
        section = 'ac'
        after = stripped.split('**Acceptance Criteria:**', 1)[1].strip()
        if after:
            current['acceptance_criteria'].append(after)
        continue

    if '**Dependencies:**' in stripped:
        section = 'dependencies'
        after = stripped.split('**Dependencies:**', 1)[1].strip()
        if after:
            current['dependencies'] = normalise_list(after)
        continue

    if section == 'ac':
        if stripped.startswith('*') or stripped.startswith('-'):
            current['acceptance_criteria'].append(stripped.lstrip('*- ').rstrip())
            continue
        else:
            section = None

    if section == 'dependencies':
        if stripped.startswith('*') or stripped.startswith('-'):
            current['dependencies'].extend(normalise_list(stripped.lstrip('*- ')))
            continue
        else:
            section = None

    segments = [seg.strip() for seg in re.split(r'[·•]', stripped) if seg.strip()]
    meta_consumed = False
    for seg in segments:
        plain = seg.replace('**', '')
        lower = plain.lower()
        if lower.startswith('assignee:'):
            value = plain.split(':', 1)[1].strip()
            if value:
                current['assignees'] = normalise_list(value)
                meta_consumed = True
        elif lower.startswith('tags:'):
            value = plain.split(':', 1)[1].strip()
            if value:
                current['tags'] = normalise_list(value)
                meta_consumed = True
        elif lower.startswith('estimate:'):
            value = plain.split(':', 1)[1].strip()
            if value and not current['estimate']:
                current['estimate'] = value
                meta_consumed = True

    if section == 'description' or (not meta_consumed and section is None):
        current['description_lines'].append(raw.rstrip())

flush_current()

payload = {
    'generated_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
    'tasks': tasks
}
Path(out_path).write_text(json.dumps(payload, indent=2) + '\n', encoding='utf-8')
PY
}

gc_build_tasks_db() {
  local tasks_json="${1:?tasks json path required}"
  local db_path="${2:?sqlite db path required}"
  local force_flag="${3:-0}"
  python3 - <<'PY' "$tasks_json" "$db_path" "$force_flag"
import json
import re
import sqlite3
import sys
import time
from collections import OrderedDict
from pathlib import Path

tasks_json_path = Path(sys.argv[1])
db_path = Path(sys.argv[2])
force = sys.argv[3] == '1'

payload = json.loads(tasks_json_path.read_text(encoding='utf-8'))
all_tasks = payload.get('tasks') or []
generated_at = time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())

def slugify(text: str) -> str:
    slug = re.sub(r'[^a-z0-9]+', '-', (text or '').lower()).strip('-')
    return slug or 'item'

def story_key_for(task: dict) -> str:
    return '|'.join([
        (task.get('story_id') or '').strip(),
        (task.get('story_title') or '').strip(),
        (task.get('epic_id') or '').strip(),
        (task.get('epic_title') or '').strip(),
    ])

grouped = OrderedDict()
for task in all_tasks:
    key = story_key_for(task)
    grouped.setdefault(key, {
        'story_id': (task.get('story_id') or '').strip(),
        'story_title': (task.get('story_title') or '').strip(),
        'epic_id': (task.get('epic_id') or '').strip(),
        'epic_title': (task.get('epic_title') or '').strip(),
        'tasks': []
    })
    grouped[key]['tasks'].append(task)

db_path.parent.mkdir(parents=True, exist_ok=True)
conn = sqlite3.connect(str(db_path))
conn.row_factory = sqlite3.Row
cur = conn.cursor()
cur.execute('PRAGMA foreign_keys = ON')
cur.execute('PRAGMA journal_mode = WAL')

def list_to_text(values):
    return ', '.join(str(item).strip() for item in values if str(item).strip()) if values else None

def as_text(value):
    if isinstance(value, list):
        return list_to_text(value)
    if isinstance(value, dict):
        return json.dumps(value, ensure_ascii=False)
    if value is None:
        return None
    text = str(value).strip()
    return text if text else None

def ensure_table():
    cur.execute('''
        CREATE TABLE IF NOT EXISTS metadata (
          key TEXT PRIMARY KEY,
          value TEXT NOT NULL
        )
    ''')
    cur.execute('''
        CREATE TABLE IF NOT EXISTS epics (
          epic_key TEXT PRIMARY KEY,
          epic_id TEXT,
          title TEXT,
          slug TEXT,
          created_at TEXT NOT NULL,
          updated_at TEXT NOT NULL
        )
    ''')
    cur.execute('''
        CREATE TABLE IF NOT EXISTS stories (
          story_slug TEXT PRIMARY KEY,
          story_key TEXT UNIQUE,
          story_id TEXT,
          story_title TEXT,
          epic_key TEXT,
          epic_title TEXT,
          sequence INTEGER,
          status TEXT,
          completed_tasks INTEGER,
          total_tasks INTEGER,
          last_run TEXT,
          updated_at TEXT NOT NULL,
          created_at TEXT NOT NULL,
          FOREIGN KEY(epic_key) REFERENCES epics(epic_key)
        )
    ''')
    cur.execute('''
        CREATE TABLE IF NOT EXISTS tasks (
          id INTEGER PRIMARY KEY AUTOINCREMENT,
          story_slug TEXT NOT NULL,
          position INTEGER NOT NULL,
          task_id TEXT,
          title TEXT,
          description TEXT,
          estimate TEXT,
          assignees_json TEXT,
          tags_json TEXT,
          acceptance_json TEXT,
          dependencies_json TEXT,
          tags_text TEXT,
          story_points TEXT,
          dependencies_text TEXT,
          assignee_text TEXT,
          document_reference TEXT,
          idempotency TEXT,
          rate_limits TEXT,
          rbac TEXT,
          messaging_workflows TEXT,
          performance_targets TEXT,
          observability TEXT,
          acceptance_text TEXT,
          endpoints TEXT,
          sample_create_request TEXT,
          sample_create_response TEXT,
          user_story_ref_id TEXT,
          epic_ref_id TEXT,
          status TEXT NOT NULL DEFAULT 'pending',
          started_at TEXT,
          completed_at TEXT,
          last_run TEXT,
          story_id TEXT,
          story_title TEXT,
          epic_key TEXT,
          epic_title TEXT,
          updated_at TEXT NOT NULL,
          created_at TEXT NOT NULL,
          UNIQUE(story_slug, position),
          FOREIGN KEY(story_slug) REFERENCES stories(story_slug)
        )
    ''')
    cur.execute('''
        CREATE TABLE IF NOT EXISTS task_progress (
          id INTEGER PRIMARY KEY AUTOINCREMENT,
          task_id INTEGER,
          story_slug TEXT NOT NULL,
          task_position INTEGER NOT NULL,
          run_stamp TEXT,
          status TEXT,
          log_path TEXT,
          prompt_path TEXT,
          output_path TEXT,
          attempts INTEGER,
          tokens_total INTEGER,
          duration_seconds INTEGER,
          apply_status TEXT,
          changes_applied INTEGER,
          notes_json TEXT,
          written_json TEXT,
          patched_json TEXT,
          commands_json TEXT,
          occurred_at TEXT,
          created_at TEXT NOT NULL,
          updated_at TEXT NOT NULL,
          FOREIGN KEY(task_id) REFERENCES tasks(id) ON DELETE CASCADE
        )
    ''')

ensure_table()

def column_exists(table: str, column: str) -> bool:
    cur.execute(f"PRAGMA table_info({table})")
    return any(row['name'] == column for row in cur.fetchall())

def ensure_column(table: str, column: str, definition: str):
    if not column_exists(table, column):
        cur.execute(f"ALTER TABLE {table} ADD COLUMN {column} {definition}")

ensure_column('stories', 'completed_tasks', 'INTEGER')
ensure_column('stories', 'total_tasks', 'INTEGER')
ensure_column('stories', 'status', "TEXT DEFAULT 'pending'")
ensure_column('stories', 'last_run', 'TEXT')
ensure_column('stories', 'epic_title', 'TEXT')

ensure_column('tasks', 'story_id', 'TEXT')
ensure_column('tasks', 'story_title', 'TEXT')
ensure_column('tasks', 'epic_key', 'TEXT')
ensure_column('tasks', 'epic_title', 'TEXT')
ensure_column('tasks', 'status', "TEXT DEFAULT 'pending'")
ensure_column('tasks', 'started_at', 'TEXT')
ensure_column('tasks', 'completed_at', 'TEXT')
ensure_column('tasks', 'last_run', 'TEXT')
ensure_column('tasks', 'tags_text', 'TEXT')
ensure_column('tasks', 'story_points', 'TEXT')
ensure_column('tasks', 'dependencies_text', 'TEXT')
ensure_column('tasks', 'assignee_text', 'TEXT')
ensure_column('tasks', 'document_reference', 'TEXT')
ensure_column('tasks', 'idempotency', 'TEXT')
ensure_column('tasks', 'rate_limits', 'TEXT')
ensure_column('tasks', 'rbac', 'TEXT')
ensure_column('tasks', 'messaging_workflows', 'TEXT')
ensure_column('tasks', 'performance_targets', 'TEXT')
ensure_column('tasks', 'observability', 'TEXT')
ensure_column('tasks', 'acceptance_text', 'TEXT')
ensure_column('tasks', 'endpoints', 'TEXT')
ensure_column('tasks', 'sample_create_request', 'TEXT')
ensure_column('tasks', 'sample_create_response', 'TEXT')
ensure_column('tasks', 'user_story_ref_id', 'TEXT')
ensure_column('tasks', 'epic_ref_id', 'TEXT')
ensure_column('tasks', 'last_log_path', 'TEXT')
ensure_column('tasks', 'last_prompt_path', 'TEXT')
ensure_column('tasks', 'last_output_path', 'TEXT')
ensure_column('tasks', 'last_attempts', 'INTEGER')
ensure_column('tasks', 'last_tokens_total', 'INTEGER')
ensure_column('tasks', 'last_duration_seconds', 'INTEGER')
ensure_column('tasks', 'last_apply_status', 'TEXT')
ensure_column('tasks', 'last_changes_applied', 'INTEGER')
ensure_column('tasks', 'last_notes_json', 'TEXT')
ensure_column('tasks', 'last_written_json', 'TEXT')
ensure_column('tasks', 'last_patched_json', 'TEXT')
ensure_column('tasks', 'last_commands_json', 'TEXT')
ensure_column('tasks', 'last_progress_at', 'TEXT')
ensure_column('tasks', 'last_progress_run', 'TEXT')

prior_story_slugs = {}
prior_story_state = {}
prior_task_state = {}

if not force:
    try:
        for row in cur.execute('SELECT story_slug, story_key, status, completed_tasks, total_tasks, last_run, updated_at, created_at FROM stories'):
            story_slug = row['story_slug']
            story_key = row['story_key']
            if story_key:
                prior_story_slugs[story_key] = story_slug
            prior_story_state[story_slug] = {
                'status': row['status'] or 'pending',
                'completed_tasks': int(row['completed_tasks'] or 0),
                'total_tasks': int(row['total_tasks'] or 0),
                'last_run': row['last_run'],
                'updated_at': row['updated_at'],
                'created_at': row['created_at'],
            }
    except sqlite3.OperationalError:
        pass

    try:
        for row in cur.execute('SELECT story_slug, position, task_id, status, started_at, completed_at, last_run FROM tasks'):
            base = {
                'status': row['status'] or 'pending',
                'started_at': row['started_at'],
                'completed_at': row['completed_at'],
                'last_run': row['last_run'],
            }
            prior_task_state[('pos', row['story_slug'], row['position'])] = base
            task_id = (row['task_id'] or '').strip().lower()
            if task_id:
                prior_task_state[('id', row['story_slug'], task_id)] = base
    except sqlite3.OperationalError:
        pass

cur.execute('DELETE FROM tasks')
cur.execute('DELETE FROM stories')
cur.execute('DELETE FROM epics')

used_story_slugs = set(prior_story_slugs.values())
used_story_slugs.discard('')

def assign_story_slug(preferred: str, story_key: str) -> str:
    if not preferred:
        preferred = 'story'
    slug = slugify(preferred)
    if story_key in prior_story_slugs:
        return prior_story_slugs[story_key]
    base = slug or 'story'
    slug = base
    i = 2
    while slug in used_story_slugs:
        slug = f"{base}-{i}"
        i += 1
    used_story_slugs.add(slug)
    return slug

epics_inserted = {}
story_count = 0
task_count = 0
restored_stories = 0
restored_tasks = 0

for sequence, (story_key, info) in enumerate(grouped.items(), start=1):
    tasks = info['tasks']
    if not tasks:
        continue
    story_id = info['story_id']
    story_title = info['story_title']
    epic_id = info['epic_id']
    epic_title = info['epic_title']

    preferred_slug_source = story_id or story_title or epic_id or f'story-{sequence}'
    story_slug = assign_story_slug(preferred_slug_source, story_key)
    restored = story_slug in prior_story_state
    if restored:
        restored_stories += 1

    epic_key = (epic_id or epic_title or '').strip()
    if not epic_key:
        epic_key = None
    else:
        epic_slug = slugify(epic_key)
        if epic_key not in epics_inserted:
            cur.execute('''
                INSERT OR REPLACE INTO epics(epic_key, epic_id, title, slug, created_at, updated_at)
                VALUES(?, ?, ?, ?, ?, ?)
            ''', (
                epic_key,
                epic_id or None,
                epic_title or None,
                epic_slug,
                generated_at,
                generated_at,
            ))
            epics_inserted[epic_key] = True

    completed_tasks = 0
    story_status = 'pending'
    story_total = len(tasks)

    for position, task in enumerate(tasks):
        task_id = (task.get('id') or '').strip()
        description = (task.get('description') or '').strip()
        estimate = (task.get('estimate') or '').strip()
        assignees = task.get('assignees') or []
        tags = task.get('tags') or []
        acceptance = task.get('acceptance_criteria') or []
        dependencies = task.get('dependencies') or []

        key_id = ('id', story_slug, task_id.lower()) if task_id else None
        key_pos = ('pos', story_slug, position)
        restore = None
        if key_id and key_id in prior_task_state and not force:
            restore = prior_task_state[key_id]
        elif key_pos in prior_task_state and not force:
            restore = prior_task_state[key_pos]

        status = (restore or {}).get('status') or 'pending'
        started_at = (restore or {}).get('started_at')
        completed_at = (restore or {}).get('completed_at')
        last_run = (restore or {}).get('last_run')

        if status == 'complete':
            completed_tasks += 1

        if restore:
            restored_tasks += 1

        tags_text = list_to_text(tags)
        dependencies_text = list_to_text(dependencies)
        assignee_text = list_to_text(assignees)
        story_points = as_text(task.get('story_points')) or (estimate if estimate else None)
        document_reference = as_text(task.get('document_reference') or task.get('document_ref'))
        idempotency_text = as_text(task.get('idempotency'))
        rate_limits = as_text(task.get('rate_limits'))
        rbac_text = as_text(task.get('rbac') or task.get('rbac_requirements'))
        messaging_workflows = as_text(task.get('messaging_workflows') or task.get('messaging_and_workflows'))
        performance_targets = as_text(task.get('performance_targets'))
        observability = as_text(task.get('observability'))
        acceptance_text = '\n'.join(item.strip() for item in acceptance if item.strip()) if acceptance else None
        if not acceptance_text:
            acceptance_text = as_text(task.get('acceptance_text'))
        endpoints = as_text(task.get('endpoints'))
        sample_create_request = as_text(task.get('sample_create_request') or task.get('sample_request'))
        sample_create_response = as_text(task.get('sample_create_response') or task.get('sample_response'))
        user_story_ref_id = as_text(task.get('user_story_ref_id') or task.get('user_story_reference_id') or story_id)
        epic_ref_id = as_text(task.get('epic_ref_id') or task.get('epic_reference_id') or epic_id)

        cur.execute('''
            INSERT INTO tasks (
              story_slug, position, task_id, title, description, estimate,
              assignees_json, tags_json, acceptance_json, dependencies_json,
              tags_text, story_points, dependencies_text, assignee_text,
              document_reference, idempotency, rate_limits, rbac,
              messaging_workflows, performance_targets, observability,
              acceptance_text, endpoints, sample_create_request, sample_create_response,
              user_story_ref_id, epic_ref_id,
              status, started_at, completed_at, last_run,
              story_id, story_title, epic_key, epic_title,
              updated_at, created_at
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            story_slug,
            position,
            task_id or None,
            (task.get('title') or '').strip() or None,
            description or None,
            estimate or None,
            json.dumps(assignees, ensure_ascii=False),
            json.dumps(tags, ensure_ascii=False),
            json.dumps(acceptance, ensure_ascii=False),
            json.dumps(dependencies, ensure_ascii=False),
            tags_text,
            story_points,
            dependencies_text,
            assignee_text,
            document_reference,
            idempotency_text,
            rate_limits,
            rbac_text,
            messaging_workflows,
            performance_targets,
            observability,
            acceptance_text,
            endpoints,
            sample_create_request,
            sample_create_response,
            user_story_ref_id,
            epic_ref_id,
            status,
            started_at,
            completed_at,
            last_run,
            story_id or None,
            story_title or None,
            epic_key,
            epic_title or None,
            generated_at,
            generated_at,
        ))

    if completed_tasks >= story_total and story_total > 0:
        story_status = 'complete'
    elif completed_tasks > 0:
        story_status = 'in-progress'
    else:
        story_status = 'pending'

    if restored and not force:
        state = prior_story_state.get(story_slug, {})
        if state:
            story_status = state.get('status') or story_status
            restored_completed = int(state.get('completed_tasks') or completed_tasks)
            completed_tasks = max(completed_tasks, restored_completed)
            story_total = state.get('total_tasks') or story_total

    cur.execute('''
        INSERT OR REPLACE INTO stories (
          story_slug, story_key, story_id, story_title,
          epic_key, epic_title, sequence, status,
          completed_tasks, total_tasks, last_run,
          updated_at, created_at
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    ''', (
        story_slug,
        story_key,
        story_id or None,
        story_title or None,
        epic_key,
        epic_title or None,
        sequence,
        story_status,
        completed_tasks,
        story_total,
        (prior_story_state.get(story_slug) or {}).get('last_run') if restored and not force else None,
        generated_at,
        (prior_story_state.get(story_slug) or {}).get('created_at', generated_at) if restored and not force else generated_at,
    ))

    story_count += 1
    task_count += story_total

cur.execute('INSERT OR REPLACE INTO metadata(key, value) VALUES(?, ?)', ('generated_at', generated_at))
cur.execute('INSERT OR REPLACE INTO metadata(key, value) VALUES(?, ?)', ('source', str(tasks_json_path)))

conn.commit()
conn.close()

print(f'STORIES {story_count}')
print(f'TASKS {task_count}')
print(f'RESTORED_STORIES {restored_stories}')
print(f'RESTORED_TASKS {restored_tasks}')
PY
}

gc_build_context_file() {
  local dest_file="${1:?context destination required}"
  local staging_dir="${2:-$GC_STAGING_DIR}"
  local max_lines="${GC_CONTEXT_FILE_LINES:-200}"
  local allow_ui_dump="${GC_CONTEXT_INCLUDE_UI:-0}"
  local doc_snippet_env="${GC_PROMPT_DOC_SNIPPETS:-}"
  local doc_snippet_mode=0
  if [[ -n "$doc_snippet_env" ]]; then
    case "${doc_snippet_env,,}" in
      0|false|no) doc_snippet_mode=0 ;;
      *) doc_snippet_mode=1 ;;
    esac
  fi
  local -A gc_context_pointer_seen=()
  local -a skip_patterns=()
  if declare -p GC_CONTEXT_SKIP_PATTERNS >/dev/null 2>&1; then
    skip_patterns=("${GC_CONTEXT_SKIP_PATTERNS[@]}")
  fi
  mkdir -p "$(dirname "$dest_file")"
  {
    echo "# Project Context (auto-generated)"
    echo
    shopt -s nullglob
    shopt -s globstar 2>/dev/null || true
    local f
    local -a patterns=(
      "$staging_dir"/pdr.* 
      "$staging_dir"/sds.* 
      "$staging_dir"/openapi.* 
      "$staging_dir"/*.md 
      "$staging_dir"/*.mdx 
      "$staging_dir"/*.adoc 
      "$staging_dir"/*.mmd 
      "$staging_dir"/*.sql 
      "$staging_dir"/*.yml 
      "$staging_dir"/*.yaml
    )
    if [[ "$allow_ui_dump" == "1" ]]; then
      patterns+=("$staging_dir"/*ui*pages*.* "$staging_dir"/*rfp*.*)
    fi
    for f in "${patterns[@]}"; do
      [[ -f "$f" ]] || continue
      local base_name
      base_name="$(basename "$f")"

      local pointer_digest=""
      if (( doc_snippet_mode )); then
        pointer_digest="$(GC_CONTEXT_POINTER_FILE="$f" python3 <<'PY'
import hashlib
import os
import pathlib

path = pathlib.Path(os.environ.get("GC_CONTEXT_POINTER_FILE", ""))
chunk = b""
try:
    with path.open("rb") as handle:
        chunk = handle.read(8192)
except Exception:
    chunk = b""

if not chunk:
    print("")
else:
    print(hashlib.sha256(chunk).hexdigest()[:12])
PY
)"
        if [[ -n "$pointer_digest" ]]; then
          local existing=""
          if [[ -n "${gc_context_pointer_seen[$pointer_digest]+_}" ]]; then
            existing="${gc_context_pointer_seen[$pointer_digest]}"
          fi
          if [[ -n "$existing" ]]; then
            echo ""
            echo "----- FILE: ${base_name} -----"
            echo "(duplicate staged doc skipped; same initial content as ${existing})"
            continue
          fi
          gc_context_pointer_seen[$pointer_digest]="$base_name"
        fi
      fi

      local skip_file=0
      if ((${#skip_patterns[@]} > 0)); then
        local pattern
        for pattern in "${skip_patterns[@]}"; do
          [[ -n "$pattern" ]] || continue
          if [[ "$base_name" == $pattern || "$f" == $pattern ]]; then
            skip_file=1
            break
          fi
          # Treat plain patterns without glob characters as substring matches.
          if [[ "$pattern" != *'*'* && "$pattern" != *'?'* ]]; then
            if [[ "$f" == *"$pattern"* ]]; then
              skip_file=1
              break
            fi
          fi
        done
      fi
      (( skip_file )) && continue

      echo ""
      echo "----- FILE: ${base_name} -----"
      local mime=""
      if command -v file >/dev/null 2>&1; then
        mime="$(file -b --mime-type "$f" 2>/dev/null || true)"
      fi
      if [[ -n "$mime" ]]; then
        case "$mime" in
          text/*|application/json|application/vnd.openxmlformats-officedocument*|application/xml|application/xhtml+xml)
            ;;
          *)
            echo "(skipped non-text file; path: $f)"
            continue
            ;;
        esac
      fi
      if (( doc_snippet_mode )); then
        GC_CONTEXT_POINTER_MODE=1 GC_CONTEXT_POINTER_DIGEST="$pointer_digest" GC_DUMP_FILE="$f" GC_MAX_LINES="$max_lines" python3 <<'PY'
import json
import os
import pathlib
import re

path = pathlib.Path(os.environ.get("GC_DUMP_FILE", ""))
if not path:
    raise SystemExit(0)

try:
    limit = int(os.environ.get("GC_MAX_LINES", "200"))
except Exception:
    limit = 200

pointer_mode = os.environ.get("GC_CONTEXT_POINTER_MODE", "").strip().lower() not in {"", "0", "false"}

def format_bytes(num: int) -> str:
    if num < 0:
        return f"{num} B"
    units = ["B", "KB", "MB", "GB", "TB"]
    value = float(num)
    for unit in units:
        if value < 1024 or unit == units[-1]:
            if unit == "B":
                return f"{int(value)} {unit}"
            return f"{value:.1f} {unit}"
        value /= 1024
    return f"{value:.1f} TB"

if pointer_mode:
    digest = os.environ.get("GC_CONTEXT_POINTER_DIGEST", "").strip()
    try:
        stat = path.stat()
        size_bytes = stat.st_size
    except Exception:
        size_bytes = -1

    preview_lines = []
    pointer_limit = max(0, min(12, limit))
    try:
        with path.open("r", encoding="utf-8", errors="replace") as handle:
            for _ in range(pointer_limit):
                line = handle.readline()
                if not line:
                    break
                preview_lines.append(line.rstrip("\n"))
    except Exception as exc:
        print(f"(failed to read staged doc: {exc})")
        raise SystemExit(0)

    descriptor = path.name
    if size_bytes >= 0:
        descriptor += f" — {format_bytes(size_bytes)}"
    if digest:
        descriptor += f" (sha256≈{digest})"

    print(f"(context trimmed for doc-snippet mode) {descriptor}")
    if preview_lines:
        print("Preview:")
        for raw_line in preview_lines:
            clean = raw_line.strip()
            if len(clean) > 160:
                clean = clean[:160].rstrip() + " …"
            if not clean:
                clean = "(blank line)"
            print(f"- {clean}")
    else:
        print("(no preview available)")
    raise SystemExit(0)

try:
    raw = path.read_text(encoding="utf-8", errors="replace")
except Exception as exc:
    print(f"(failed to read text: {exc})")
    raise SystemExit(0)

try:
    parsed = json.loads(raw)
    preview = json.dumps(parsed, indent=2)[:4000]
    print("JSON summary:")
    print(preview)
    raise SystemExit(0)
except Exception:
    pass

ext = path.suffix.lower()
css_exts = {".css", ".scss", ".sass", ".less", ".pcss", ".styl"}
markup_exts = {".html", ".htm", ".vue", ".jsx", ".tsx"}

def emit(lines):
    if limit > 0 and len(lines) > limit:
        omitted = len(lines) - limit
        lines = lines[:limit]
        lines.append(f"... ({omitted} line(s) truncated) ...")
    print("\n".join(lines))
    raise SystemExit(0)

if ext in css_exts:
    tokens = re.findall(r"--([a-z0-9_-]+)", raw, re.I)
    unique = sorted({token for token in tokens if token})
    lines = ["CSS variables (first 40):"]
    for token in unique[:40]:
        lines.append(f"- --{token}")
    if len(unique) > 40:
        lines.append(f"... ({len(unique) - 40} additional variables omitted)")
    emit(lines)

if ext in markup_exts:
    clean = re.sub(r"<script[\s\S]*?</script>", "", raw, flags=re.I)
    clean = re.sub(r"<style[\s\S]*?</style>", "", clean, flags=re.I)
    text = re.sub(r"<[^>]+>", " ", clean)
    text = re.sub(r"\s+", " ", text).strip()
    if not text:
        emit(["(markup collapsed to empty after stripping tags)"])
    chunks = [text[i:i + 200] for i in range(0, len(text), 200)]
    lines = ["Markup summary:"] + chunks[:5]
    if len(chunks) > 5:
        lines.append(f"... ({len(chunks) - 5} additional chunks omitted)")
    emit(lines)

lines = raw.splitlines()
result = []
max_width = 160
table_run = 0
table_notice = False
sql_insert_run = 0
sql_notice = False

def truncate(line: str) -> str:
    if len(line) <= max_width:
        return line
    return line[:max_width].rstrip() + " …"

for original in lines:
    line = original.rstrip()
    stripped = line.lstrip()

    if not stripped:
        if not result or result[-1] != "":
            result.append("")
        continue

    pipe_count = line.count("|")
    is_table_like = pipe_count >= 6 or (pipe_count >= 3 and "," in line and len(line) > 80)
    if is_table_like:
        table_run += 1
    else:
        table_run = 0
        table_notice = False
    if is_table_like and table_run > 30:
        if not table_notice:
            result.append("... (additional table rows truncated) ...")
            table_notice = True
        continue

    upper = stripped.upper()
    if upper.startswith("INSERT INTO") or upper.startswith("UPDATE "):
        sql_insert_run += 1
    else:
        sql_insert_run = 0
        sql_notice = False
    if sql_insert_run > 40:
        if not sql_notice:
            result.append("... (repetitive SQL statements truncated) ...")
            sql_notice = True
        continue

    line = truncate(line.replace("\t", "  "))
    result.append(line)

if not result:
    result = ["(no textual content captured)"]

if limit > 0 and len(result) > limit:
    omitted = len(result) - limit
    result = result[:limit]
    result.append(f"... ({omitted} line(s) truncated) ...")

print("\n".join(result))
PY
      else
        GC_DUMP_FILE="$f" GC_MAX_LINES="$max_lines" python3 <<'PY'
import json
import os
import pathlib
import re

path = pathlib.Path(os.environ.get("GC_DUMP_FILE", ""))
if not path:
    raise SystemExit(0)
try:
    limit = int(os.environ.get("GC_MAX_LINES", "200"))
except Exception:
    limit = 200

try:
    raw = path.read_text(encoding="utf-8", errors="replace")
except Exception as exc:
    print(f"(failed to read text: {exc})")
    raise SystemExit(0)

ext = path.suffix.lower()
css_exts = {".css", ".scss", ".sass", ".less", ".pcss", ".styl"}
markup_exts = {".html", ".htm", ".vue", ".jsx", ".tsx"}

def emit(lines):
    if limit > 0 and len(lines) > limit:
        omitted = len(lines) - limit
        lines = lines[:limit]
        lines.append(f"... ({omitted} line(s) truncated) ...")
    print("\n".join(lines))
    raise SystemExit(0)

if ext in css_exts:
    tokens = re.findall(r"--([a-z0-9_-]+)", raw, re.I)
    unique = sorted({token for token in tokens if token})
    lines = ["CSS variables (first 40):"]
    for token in unique[:40]:
        lines.append(f"- --{token}")
    if len(unique) > 40:
        lines.append(f"... ({len(unique) - 40} additional variables omitted)")
    emit(lines)

if ext in markup_exts:
    clean = re.sub(r"<script[\s\S]*?</script>", "", raw, flags=re.I)
    clean = re.sub(r"<style[\s\S]*?</style>", "", clean, flags=re.I)
    text = re.sub(r"<[^>]+>", " ", clean)
    text = re.sub(r"\s+", " ", text).strip()
    if not text:
        emit(["(markup collapsed to empty after stripping tags)"])
    chunks = [text[i:i + 200] for i in range(0, len(text), 200)]
    lines = ["Markup summary:"] + chunks[:5]
    if len(chunks) > 5:
        lines.append(f"... ({len(chunks) - 5} additional chunks omitted)")
    emit(lines)

try:
    parsed = json.loads(raw)
    preview = json.dumps(parsed, indent=2)[:4000]
    emit(["JSON summary:", preview])
except Exception:
    pass

lines = raw.splitlines()
result = []
max_width = 160
table_run = 0
table_notice = False
sql_insert_run = 0
sql_notice = False

def truncate(line: str) -> str:
    if len(line) <= max_width:
        return line
    return line[:max_width].rstrip() + " …"

for original in lines:
    line = original.rstrip()
    stripped = line.lstrip()

    if not stripped:
        if not result or result[-1] != "":
            result.append("")
        continue

    pipe_count = line.count("|")
    is_table_like = pipe_count >= 6 or (pipe_count >= 3 and "," in line and len(line) > 80)
    if is_table_like:
        table_run += 1
    else:
        table_run = 0
        table_notice = False
    if is_table_like and table_run > 30:
        if not table_notice:
            result.append("... (additional table rows truncated) ...")
            table_notice = True
        continue

    upper = stripped.upper()
    if upper.startswith("INSERT INTO") or upper.startswith("UPDATE "):
        sql_insert_run += 1
    else:
        sql_insert_run = 0
        sql_notice = False
    if sql_insert_run > 40:
        if not sql_notice:
            result.append("... (repetitive SQL statements truncated) ...")
            sql_notice = True
        continue

    line = truncate(line.replace("\t", "  "))
    result.append(line)

emit(result or ["(no textual content captured)"])
PY
      fi
    done
    shopt -u globstar 2>/dev/null || true
    shopt -u nullglob || true
  } >"$dest_file"
}

gc_discovery_is_stale() {
  local discovery_file="${1:-}"
  shift || true
  local -a required_keys=("$@")
  if [[ -z "$discovery_file" || ! -f "$discovery_file" || ! -s "$discovery_file" ]]; then
    return 0
  fi
  python3 - "$discovery_file" "${required_keys[@]}" <<'PY'
import pathlib, sys

file_path = pathlib.Path(sys.argv[1])
required = sys.argv[2:]

try:
    raw = file_path.read_text(encoding="utf-8")
except Exception:
    sys.exit(0)

entries = {}
for line in raw.splitlines():
    stripped = line.strip()
    if not stripped.startswith("- "):
        continue
    payload = stripped[2:]
    if ":" not in payload:
        continue
    key, value = payload.split(":", 1)
    entries[key.strip()] = value.strip()

if not required:
    required = ["pdr", "sds", "rfp", "jira", "ui_pages", "openapi", "sql"]

def is_empty(value: str) -> bool:
    if value is None:
        return True
    text = value.strip()
    return text in {"", "null", "~"}

for key in required:
    if is_empty(entries.get(key)):
        sys.exit(0)

sys.exit(1)
PY
}

gc_refresh_discovery_if_needed() {
  local discovery_file="${STAGING_DIR}/discovery.yaml"
  local -a required=("pdr" "sds" "rfp" "jira" "ui_pages" "openapi")
  if gc_discovery_is_stale "$discovery_file" "${required[@]}"; then
    info "Discovery manifest missing context; running scan + normalize."
    if ! cmd_scan --project "$PROJECT_ROOT"; then
      warn "Automatic scan failed; rerun 'gpt-creator scan' manually."
      return
    fi
    if ! cmd_normalize --project "$PROJECT_ROOT"; then
      warn "Automatic normalize failed; rerun 'gpt-creator normalize' manually."
      return
    fi
    if ! gc_discovery_is_stale "$discovery_file" "${required[@]}"; then
      ok "Discovery manifest refreshed."
    else
      warn "Discovery manifest still incomplete after refresh; check project docs."
    fi
  fi
}

gc_clear_active_task() {
  unset GC_ACTIVE_TASK_DB
  unset GC_ACTIVE_TASK_SLUG
  unset GC_ACTIVE_TASK_INDEX
  unset GC_ACTIVE_RUN_STAMP
  unset GC_ACTIVE_TASK_NUMBER
  unset GC_ACTIVE_TASK_ID
  unset GC_ACTIVE_TASK_REPORT
  unset GC_ACTIVE_TASK_ARCHIVE
  unset GC_ACTIVE_TASK_PROMPT
  unset GC_ACTIVE_TASK_OUTPUT
}

gc_finalize_active_task() {
  local signal="${1:-INT}"
  local reason="${2:-Interrupted}"
  local report_path="${GC_ACTIVE_TASK_REPORT:-}"
  [[ -n "$report_path" ]] || return 0

  local archive_path="${GC_ACTIVE_TASK_ARCHIVE:-}"
  local prompt_path="${GC_ACTIVE_TASK_PROMPT:-}"
  local output_path="${GC_ACTIVE_TASK_OUTPUT:-}"
  local task_number="${GC_ACTIVE_TASK_NUMBER:-unknown}"
  local task_id="${GC_ACTIVE_TASK_ID:-}"
  local slug="${GC_ACTIVE_TASK_SLUG:-unknown}"
  local tasks_db="${GC_ACTIVE_TASK_DB:-}"
  local task_index="${GC_ACTIVE_TASK_INDEX:-}"
  local run_stamp="${GC_ACTIVE_RUN_STAMP:-manual}"
  local timestamp
  timestamp="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"

  mkdir -p "$(dirname "$report_path")"
  local prompt_entry="$prompt_path"
  local output_entry="$output_path"
  local report_entry_display="$report_path"
  if [[ -n "$PROJECT_ROOT" ]]; then
    local project_prefix="${PROJECT_ROOT%/}/"
    if [[ "$prompt_entry" == "$project_prefix"* ]]; then
      prompt_entry="${prompt_entry#$project_prefix}"
    fi
    if [[ "$output_entry" == "$project_prefix"* ]]; then
      output_entry="${output_entry#$project_prefix}"
    fi
    if [[ "$report_entry_display" == "$project_prefix"* ]]; then
      report_entry_display="${report_entry_display#$project_prefix}"
    fi
  fi

  {
    printf 'task_number: %s\n' "$task_number"
    printf 'task_id: %s\n' "${task_id:-}"
    printf 'task_title: %s\n' "(interrupted)"
    printf 'story_slug: %s\n' "$slug"
    printf 'status: %s\n' "interrupted"
    printf 'timestamp: %s\n' "$timestamp"
    printf 'attempts: %s\n' "0"
    printf 'apply_status: %s\n' "interrupted"
    printf 'changes_applied: %s\n' "false"
    printf 'prompt_path: %s\n' "${prompt_entry:-"(none)"}"
    printf 'output_path: %s\n' "${output_entry:-"(none)"}"
    printf 'notes:\n'
    printf '  - %s by signal %s; rerun work-on-tasks to resume.\n' "$reason" "$signal"
  } >"$report_path"

  if [[ -n "$archive_path" ]]; then
    mkdir -p "$(dirname "$archive_path")"
    cp -f "$report_path" "$archive_path" 2>/dev/null || true
  fi

  if [[ -n "$tasks_db" && -n "$slug" && -n "$task_index" ]]; then
    local report_entry_db="$report_entry_display"
    local prompt_db="$prompt_entry"
    local output_db="$output_entry"
    local notes_payload="Interrupted by signal ${signal}; rerun work-on-tasks to resume."
    gc_record_task_progress "$tasks_db" "$slug" "$task_index" "$run_stamp" "on-hold" "$report_entry_db" "$prompt_db" "$output_db" "0" "0" "0" "interrupted" "false" "$notes_payload" "" "" "" "$timestamp"
    gc_update_task_state "$tasks_db" "$slug" "$task_index" "on-hold" "$run_stamp"
  fi

  gc_clear_active_task
}

gc_interrupt_handler() {
  local signal="${1:-INT}"
  trap - INT TERM TSTP QUIT
  warn "Received ${signal}; finalising active task and exiting."
  gc_finalize_active_task "$signal" "Interrupted"
  local code=128
  case "$signal" in
    INT) code=130 ;;
    TERM) code=143 ;;
    TSTP) code=148 ;;
    QUIT) code=131 ;;
  esac
  exit "$code"
}

gc_build_context_digest() {
  local context_file="${1:?context file required}"
  local digest_file="${2:?digest destination required}"
  local limit_lines="${3:-400}"
  [[ -f "$context_file" ]] || {
    printf '%s\n' "(warn) context digest skipped; missing source ${context_file}" >&2
    return 1
  }
  python3 - <<'PY' "$context_file" "$digest_file" "$limit_lines"
import hashlib
import math
import pathlib
import sys

context_path = pathlib.Path(sys.argv[1])
dest_path = pathlib.Path(sys.argv[2])
try:
    limit = int(sys.argv[3])
except Exception:
    limit = 400

if limit <= 0:
    dest_path.write_text("", encoding="utf-8")
    raise SystemExit(0)

try:
    raw = context_path.read_text(encoding="utf-8", errors="replace")
except Exception as exc:
    dest_path.write_text(f"(failed to read shared context: {exc})\n", encoding="utf-8")
    raise SystemExit(0)

lines = raw.splitlines()
sections = []
current_name = None
current_lines: list[str] = []

for line in lines:
    if line.startswith("----- FILE: ") and line.endswith(" -----"):
        if current_name is not None:
            sections.append((current_name, current_lines))
        current_name = line[len("----- FILE: "):-len(" -----")].strip() or "unnamed"
        current_lines = []
        continue
    if current_name is not None:
        current_lines.append(line.rstrip())

if current_name is not None:
    sections.append((current_name, current_lines))

output: list[str] = []
output.append("Hint: use --context-mode raw to include the literal tail.")
remaining = max(limit - len(output), 10)

if not sections:
    output.append("(no staged context files indexed)")
    dest_path.write_text("\n".join(output) + "\n", encoding="utf-8")
    raise SystemExit(0)

per_header_cost = 2  # heading + blank separator
remaining_sections = len(sections)
seen_digests: set[str] = set()
duplicate_examples: list[str] = []
duplicate_count = 0

for index, (name, section_lines) in enumerate(sections, 1):
    if remaining <= 0:
        output.append("… (context digest truncated; raise --context-lines or switch to raw)")
        break

    digest_src = "\n".join(section_lines).encode("utf-8", "replace")
    digest = hashlib.sha256(digest_src).hexdigest()[:12]
    if digest in seen_digests:
        duplicate_count += 1
        if len(duplicate_examples) < 4:
            duplicate_examples.append(name)
        continue
    seen_digests.add(digest)

    heading = f"### {name} (sha256 {digest})"
    output.append(heading)
    remaining -= 1
    if remaining <= 0:
        output.append("… (context digest truncated; raise --context-lines or switch to raw)")
        break

    remaining_sections = len(sections) - index
    # Reserve at least two lines per remaining section for future headings.
    reserved_for_rest = max(0, remaining_sections * per_header_cost)
    available_for_this = max(3, min(12, remaining - reserved_for_rest))

    sample_lines = []
    for raw_line in section_lines:
        stripped = raw_line.strip()
        if not stripped:
            continue
        if stripped.startswith("... (additional "):
            continue
        if stripped.startswith("----- FILE:"):
            continue
        stripped = stripped.replace("\t", "  ")
        if len(stripped) > 160:
            stripped = stripped[:160].rstrip() + " …"
        sample_lines.append(stripped)
        if len(sample_lines) >= available_for_this:
            break

    if not sample_lines:
        sample_lines = ["(no excerpt captured; file may be binary or truncated)"]

    for sample in sample_lines:
        if remaining <= 0:
            output.append("… (context digest truncated; raise --context-lines or switch to raw)")
            break
        output.append(sample)
        remaining -= 1

    if remaining <= 0:
        break

    if index != len(sections):
        output.append("")
        remaining -= 1

if duplicate_count:
    suffix = "" if duplicate_count == 1 else "s"
    message = f"(Skipped {duplicate_count} additional context file{suffix} with duplicate content"
    if duplicate_examples:
        shown = ", ".join(duplicate_examples)
        if duplicate_count > len(duplicate_examples):
            shown += ", …"
        message += f": {shown}"
    message += ")"
    output.append("")
    output.append(message)

dest_path.parent.mkdir(parents=True, exist_ok=True)
dest_path.write_text("\n".join(output).rstrip() + "\n", encoding="utf-8")
PY
}

gc_refresh_context_tail() {
  local context_file="${1:?context file required}"
  local tail_file="${2:?tail file required}"
  local mode="${3:-digest}"
  local limit="${4:-0}"

  [[ -n "$tail_file" ]] || {
    printf '%s\n' "$mode"
    return 0
  }

  if (( limit <= 0 )); then
    : >"$tail_file"
    printf '%s\n' "$mode"
    return 0
  fi

  local effective_mode="$mode"
  case "$mode" in
    digest)
      if gc_build_context_digest "$context_file" "$tail_file" "$limit"; then
        printf '%s\n' "$effective_mode"
        return 0
      fi
      warn "Failed to build context digest (limit=${limit}); falling back to raw tail."
      effective_mode="raw"
      ;;
  esac

  case "$effective_mode" in
    raw)
      if ! tail -n "$limit" "$context_file" >"$tail_file" 2>/dev/null; then
        cp "$context_file" "$tail_file"
      fi
      ;;
    *)
      if ! tail -n "$limit" "$context_file" >"$tail_file" 2>/dev/null; then
        cp "$context_file" "$tail_file"
      fi
      ;;
  esac

  printf '%s\n' "$effective_mode"
}

gc_update_work_state() {
  local db_path="${1:?tasks database path required}"
  local story_slug="${2:?story slug required}"
  local status="${3:?status required}"
  local completed="${4:-0}"
  local total="${5:-0}"
  local run_stamp="${6:-manual}"
  python3 - <<'PY' "$db_path" "$story_slug" "$status" "$completed" "$total" "$run_stamp"
import sqlite3
import sys
import time

db_path, story_slug, status, completed, total, run_stamp = sys.argv[1:7]
story_slug = (story_slug or "").strip()
status_requested = (status or "pending").strip().lower()
completed = int(completed or 0)
total = int(total or 0)
run_stamp = run_stamp or 'manual'
timestamp = time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())

conn = sqlite3.connect(db_path)
conn.row_factory = sqlite3.Row
cur = conn.cursor()

row = cur.execute(
    """
    SELECT
        COUNT(*) AS total_count,
        SUM(CASE WHEN LOWER(COALESCE(status, '')) = 'complete' THEN 1 ELSE 0 END) AS complete_count,
        SUM(CASE WHEN LOWER(COALESCE(status, '')) = 'in-progress' THEN 1 ELSE 0 END) AS in_progress_count
      FROM tasks
     WHERE LOWER(COALESCE(story_slug, '')) = ?
    """,
    (story_slug.lower(),)
).fetchone()

total_count = int(row["total_count"] or 0)
complete_count = int(row["complete_count"] or 0)
in_progress_count = int(row["in_progress_count"] or 0)
pending_count = max(total_count - complete_count - in_progress_count, 0)

if total_count > 0:
    status_final = status_requested
    if complete_count >= total_count:
        status_final = "complete"
    elif complete_count > 0 or in_progress_count > 0:
        if status_final == "complete":
            status_final = "in-progress"
        elif status_final not in {"blocked", "on-hold", "in-progress"}:
            status_final = "in-progress"
    else:
        if status_final not in {"blocked", "on-hold"}:
            status_final = "pending"
    completed_value = complete_count
    total_value = total_count
else:
    status_final = status_requested or "pending"
    completed_value = completed
    total_value = total

cur.execute(
    """
    UPDATE stories
       SET status = ?,
           completed_tasks = ?,
           total_tasks = ?,
           last_run = ?,
           updated_at = ?
     WHERE story_slug = ?
    """,
    (status_final, completed_value, total_value, run_stamp, timestamp, story_slug)
)
if cur.rowcount == 0:
    cur.execute(
        """
        INSERT INTO stories (story_slug, story_key, status, completed_tasks, total_tasks, last_run, created_at, updated_at)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """,
        (story_slug, story_slug, status_final, completed_value, total_value, run_stamp, timestamp, timestamp)
    )

conn.commit()
conn.close()
PY
}

gc_update_task_state() {
  local db_path="${1:?tasks database path required}"
  local story_slug="${2:?story slug required}"
  local position="${3:?task position required}"
  local status="${4:?status required}"
  local run_stamp="${5:-manual}"
  python3 - <<'PY' "$db_path" "$story_slug" "$position" "$status" "$run_stamp"
import sqlite3
import sys
import time

db_path, story_slug, position, status, run_stamp = sys.argv[1:6]
position = int(position)
run_stamp = run_stamp or 'manual'
timestamp = time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())

conn = sqlite3.connect(db_path)
conn.row_factory = sqlite3.Row
cur = conn.cursor()
row = cur.execute(
    'SELECT id, status, started_at, completed_at FROM tasks WHERE story_slug = ? AND position = ?',
    (story_slug, position)
).fetchone()

if row is not None:
    fields = [
        ('status', status),
        ('last_run', run_stamp),
        ('updated_at', timestamp),
    ]

    started_at = row['started_at']
    completed_at = row['completed_at']

    if status == 'in-progress' and not started_at:
        fields.append(('started_at', timestamp))
    elif status == 'complete':
        if not started_at:
            fields.append(('started_at', timestamp))
        fields.append(('completed_at', timestamp))
    elif status == 'pending':
        fields.append(('started_at', None))
        fields.append(('completed_at', None))
    elif status == 'blocked':
        if not started_at:
            fields.append(('started_at', timestamp))

    set_clause = ', '.join(f"{col} = ?" for col, _ in fields)
    params = [value for _, value in fields] + [story_slug, position]
    cur.execute(f'UPDATE tasks SET {set_clause} WHERE story_slug = ? AND position = ?', params)

conn.commit()
conn.close()
PY
}

gc_record_task_progress() {
  local db_path="${1:?tasks database path required}"
  local story_slug="${2:?story slug required}"
  local position="${3:?task position required}"
  local run_stamp="${4:-manual}"
  local status="${5:-}"
  local log_path="${6:-}"
  local prompt_path="${7:-}"
  local output_path="${8:-}"
  local attempts="${9:-0}"
  local tokens_total="${10:-0}"
  local duration_seconds="${11:-0}"
  local apply_status="${12:-}"
  local changes_applied="${13:-0}"
  local notes_text="${14:-}"
  local written_text="${15:-}"
  local patched_text="${16:-}"
  local commands_text="${17:-}"
  local occurred_at="${18:-}"
  python3 - <<'PY' "$db_path" "$story_slug" "$position" "$run_stamp" "$status" "$log_path" "$prompt_path" "$output_path" "$attempts" "$tokens_total" "$duration_seconds" "$apply_status" "$changes_applied" "$notes_text" "$written_text" "$patched_text" "$commands_text" "$occurred_at"
import json
import sqlite3
import sys
import time

(
    db_path,
    story_slug,
    position,
    run_stamp,
    status,
    log_path,
    prompt_path,
    output_path,
    attempts,
    tokens_total,
    duration_seconds,
    apply_status,
    changes_applied,
    notes_text,
    written_text,
    patched_text,
    commands_text,
    occurred_at,
) = sys.argv[1:19]

def parse_int(value):
    text = (value or "").strip()
    if not text:
        return None
    try:
        return int(float(text))
    except (TypeError, ValueError):
        return None

def parse_bool(value):
    text = (value or "").strip().lower()
    return 1 if text in {"1", "true", "yes", "y", "on"} else 0

position = int(position)
attempts_int = parse_int(attempts) or 0
tokens_int = parse_int(tokens_total)
duration_int = parse_int(duration_seconds)
changes_int = parse_bool(changes_applied)
run_stamp = (run_stamp or "manual").strip() or "manual"
status = (status or "").strip() or None
log_path = (log_path or "").strip() or None
prompt_path = (prompt_path or "").strip() or None
output_path = (output_path or "").strip() or None
apply_status = (apply_status or "").strip() or None
notes_text = notes_text or ""
written_text = written_text or ""
patched_text = patched_text or ""
commands_text = commands_text or ""

def as_json(text):
    items = [line.strip() for line in text.splitlines() if line.strip()]
    return json.dumps(items, ensure_ascii=False) if items else None

notes_json = as_json(notes_text)
written_json = as_json(written_text)
patched_json = as_json(patched_text)
commands_json = as_json(commands_text)

timestamp = (occurred_at or "").strip()
if not timestamp:
    timestamp = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())

conn = sqlite3.connect(db_path)
conn.row_factory = sqlite3.Row
cur = conn.cursor()

def ensure_column(table: str, column: str, definition: str) -> None:
    cur.execute(f"PRAGMA table_info({table})")
    if not any(row["name"] == column for row in cur.fetchall()):
        cur.execute(f"ALTER TABLE {table} ADD COLUMN {column} {definition}")

cur.execute(
    '''
    CREATE TABLE IF NOT EXISTS task_progress (
      id INTEGER PRIMARY KEY AUTOINCREMENT,
      task_id INTEGER,
      story_slug TEXT NOT NULL,
      task_position INTEGER NOT NULL,
      run_stamp TEXT,
      status TEXT,
      log_path TEXT,
      prompt_path TEXT,
      output_path TEXT,
      attempts INTEGER,
      tokens_total INTEGER,
      duration_seconds INTEGER,
      apply_status TEXT,
      changes_applied INTEGER,
      notes_json TEXT,
      written_json TEXT,
      patched_json TEXT,
      commands_json TEXT,
      occurred_at TEXT,
      created_at TEXT NOT NULL,
      updated_at TEXT NOT NULL,
      FOREIGN KEY(task_id) REFERENCES tasks(id) ON DELETE CASCADE
    )
    '''
)

for column, definition in (
    ("last_log_path", "TEXT"),
    ("last_prompt_path", "TEXT"),
    ("last_output_path", "TEXT"),
    ("last_attempts", "INTEGER"),
    ("last_tokens_total", "INTEGER"),
    ("last_duration_seconds", "INTEGER"),
    ("last_apply_status", "TEXT"),
    ("last_changes_applied", "INTEGER"),
    ("last_notes_json", "TEXT"),
    ("last_written_json", "TEXT"),
    ("last_patched_json", "TEXT"),
    ("last_commands_json", "TEXT"),
    ("last_progress_at", "TEXT"),
    ("last_progress_run", "TEXT"),
):
    ensure_column("tasks", column, definition)

task_row = cur.execute(
    "SELECT id FROM tasks WHERE story_slug = ? AND position = ?",
    (story_slug, position),
).fetchone()
task_id = task_row["id"] if task_row else None

progress_row = (
    task_id,
    story_slug,
    position,
    run_stamp,
    status,
    log_path,
    prompt_path,
    output_path,
    attempts_int,
    tokens_int,
    duration_int,
    apply_status,
    changes_int,
    notes_json,
    written_json,
    patched_json,
    commands_json,
    timestamp,
    timestamp,
    timestamp,
)

cur.execute(
    '''
    INSERT INTO task_progress (
      task_id, story_slug, task_position, run_stamp, status, log_path,
      prompt_path, output_path, attempts, tokens_total, duration_seconds,
      apply_status, changes_applied, notes_json, written_json, patched_json,
      commands_json, occurred_at, created_at, updated_at
    )
    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    ''',
    progress_row,
)

task_updates = []
params = []

def set_field(column: str, value):
    task_updates.append(f"{column} = ?")
    params.append(value)

set_field("last_log_path", log_path)
set_field("last_prompt_path", prompt_path)
set_field("last_output_path", output_path)
set_field("last_attempts", attempts_int)
set_field("last_tokens_total", tokens_int)
set_field("last_duration_seconds", duration_int)
set_field("last_apply_status", apply_status)
set_field("last_changes_applied", changes_int)
set_field("last_notes_json", notes_json)
set_field("last_written_json", written_json)
set_field("last_patched_json", patched_json)
set_field("last_commands_json", commands_json)
set_field("last_progress_at", timestamp)
set_field("last_progress_run", run_stamp)
set_field("updated_at", timestamp)

if task_row:
    cur.execute(
        f'''
        UPDATE tasks
           SET {", ".join(task_updates)}
         WHERE id = ?
        ''',
        params + [task_id],
    )

conn.commit()
conn.close()
PY
}

gc_update_throughput_metrics() {
  local db_path="${1:?tasks database path required}"
  local action="${2:?throughput action required}"
  local story_slug="${3:-}"
  local position="${4:-}"
  python3 - <<'PY' "$db_path" "$action" "$story_slug" "$position"
import re
import sqlite3
import sys
import time

db_path, action, story_slug, position = sys.argv[1:5]
action = (action or "").strip().lower()
story_slug = (story_slug or "").strip()
position = (position or "").strip()

conn = sqlite3.connect(db_path)
conn.row_factory = sqlite3.Row
cur = conn.cursor()
cur.execute('''
    CREATE TABLE IF NOT EXISTS metadata (
      key TEXT PRIMARY KEY,
      value TEXT NOT NULL
    )
''')

def get_meta(key, default=None):
    row = cur.execute('SELECT value FROM metadata WHERE key = ?', (key,)).fetchone()
    if not row or row['value'] is None:
        return default
    return row['value']

def set_meta(key, value):
    cur.execute('INSERT OR REPLACE INTO metadata(key, value) VALUES(?, ?)', (key, value))

def meta_float(key, default=0.0):
    value = get_meta(key, None)
    if value is None:
        return default
    try:
        return float(value)
    except (TypeError, ValueError):
        return default

def meta_int(key, default=0):
    value = get_meta(key, None)
    if value is None:
        return default
    try:
        return int(float(value))
    except (TypeError, ValueError):
        return default

def parse_points(raw):
    if raw is None:
        return 0.0
    if isinstance(raw, (int, float)):
        return float(raw)
    text = str(raw).strip()
    if not text:
        return 0.0
    normalized = text.lower().replace(',', '.')
    match = re.search(r'-?\d+(?:\.\d+)?', normalized)
    if not match:
        return 0.0
    try:
        return float(match.group(0))
    except ValueError:
        return 0.0

now_ts = time.time()
window_started_at = meta_float('throughput.window_started_at', now_ts)
if window_started_at <= 0 or window_started_at > now_ts + 315360000:
    window_started_at = now_ts
window_points = meta_float('throughput.window_points', 0.0)
total_points = meta_float('throughput.total_points', 0.0)
total_seconds = meta_float('throughput.total_seconds', 0.0)
samples = meta_int('throughput.samples', 0)
rate = meta_float('throughput.rate_sp_per_hour', 0.0)

points_increment = 0.0
if action == 'task-complete' and story_slug and position:
    try:
        idx = int(position)
    except ValueError:
        idx = None
    if idx is not None:
        row = cur.execute(
            'SELECT story_points, estimate, status FROM tasks WHERE story_slug = ? AND position = ?',
            (story_slug, idx)
        ).fetchone()
        if row is not None:
            status = (row['status'] or '').strip().lower()
            if status == 'complete':
                points_increment = parse_points(row['story_points'])
                if points_increment <= 0:
                    points_increment = parse_points(row['estimate'])
                if points_increment < 0:
                    points_increment = 0.0

if points_increment > 0:
    window_points += points_increment

elapsed = max(0.0, now_ts - window_started_at)
threshold = 3600.0
should_flush = False
if action == 'flush':
    if elapsed > 0 or window_points > 0:
        should_flush = True
elif elapsed >= threshold:
    should_flush = True

updated = False
window_points_before = window_points
elapsed_before = elapsed

if should_flush and (elapsed > 0 or window_points > 0):
    total_points += window_points
    total_seconds += elapsed
    samples += 1
    if total_points <= 0 or total_seconds <= 0:
        rate = 0.0
    else:
        rate = (total_points / total_seconds) * 3600.0
    set_meta('throughput.rate_sp_per_hour', f"{rate:.6f}")
    set_meta('throughput.last_window_points', f"{window_points:.6f}")
    set_meta('throughput.last_window_seconds', f"{elapsed:.3f}")
    window_points = 0.0
    window_started_at = now_ts
    updated = True

if action == 'init':
    window_started_at = now_ts
    window_points = 0.0

timestamp_iso = time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime(now_ts))
set_meta('throughput.window_started_at', f"{window_started_at:.3f}")
set_meta('throughput.window_points', f"{window_points:.6f}")
set_meta('throughput.total_points', f"{total_points:.6f}")
set_meta('throughput.total_seconds', f"{total_seconds:.3f}")
set_meta('throughput.samples', str(max(samples, 0)))
set_meta('throughput.updated_at', timestamp_iso)

conn.commit()
conn.close()

if updated:
    hours = elapsed_before / 3600.0 if elapsed_before > 0 else 0.0
    print(f"Throughput updated -> {rate:.2f} SP/hour (window: {hours:.2f}h, points: {window_points_before:.2f}, samples: {samples})")
PY
}

gc_rewind_backlog_from_task() {
  local db_path="${1:?tasks database path required}"
  local task_ref_raw="${2:?task reference required}"
  local story_hint_raw="${3:-}"
  python3 - "$db_path" "$task_ref_raw" "$story_hint_raw" <<'PY'
import re
import sqlite3
import sys
import time

args = sys.argv[1:]
if len(args) < 2:
    print("Task reference required.", file=sys.stderr)
    sys.exit(1)

db_path, task_ref = args[0], args[1]
story_hint = args[2] if len(args) > 2 else ""
task_ref = (task_ref or "").strip()
if not task_ref:
    print("Task reference required.", file=sys.stderr)
    sys.exit(1)

conn = sqlite3.connect(db_path)
conn.row_factory = sqlite3.Row
cur = conn.cursor()

timestamp = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())

stories = cur.execute(
    "SELECT story_slug, story_key, story_id, story_title, sequence FROM stories ORDER BY sequence ASC, story_slug ASC"
).fetchall()

if not stories:
    print("No stories available in backlog.", file=sys.stderr)
    sys.exit(1)

def clean(value: str) -> str:
    return (value or "").strip()

def norm(value: str) -> str:
    return clean(value).lower()

def slug_norm(value: str) -> str:
    text = norm(value)
    if not text:
        return ""
    return re.sub(r"[^a-z0-9]+", "-", text).strip("-")

story_by_keys = {}
story_order = []
for story in stories:
    slug = clean(story["story_slug"])
    if not slug:
        continue
    story_order.append(story)
    keys = {
        norm(story["story_key"]),
        norm(story["story_id"]),
        norm(slug),
        slug_norm(story["story_key"]),
        slug_norm(story["story_id"]),
        slug_norm(slug),
    }
    for key in keys:
        if key and key not in story_by_keys:
            story_by_keys[key] = story

def resolve_story(token: str):
    token_norm = norm(token)
    if token_norm in story_by_keys:
        return story_by_keys[token_norm]
    token_slug = slug_norm(token)
    if token_slug and token_slug in story_by_keys:
        return story_by_keys[token_slug]
    return None

def locate_by_story_and_position(story_token: str, position_text: str):
    story = resolve_story(story_token)
    if story is None:
        print(f"Story not found for reference '{story_token}'.", file=sys.stderr)
        sys.exit(1)
    try:
        position_input = int(position_text)
    except ValueError:
        print(f"Invalid task position '{position_text}'. Use a positive integer (1-based).", file=sys.stderr)
        sys.exit(1)
    if position_input <= 0:
        print(f"Task position must be at least 1 (got {position_input}).", file=sys.stderr)
        sys.exit(1)
    index = position_input - 1
    row = cur.execute(
        """
        SELECT t.story_slug, t.position, t.task_id, t.title, s.story_title
          FROM tasks t
          LEFT JOIN stories s ON s.story_slug = t.story_slug
         WHERE LOWER(COALESCE(t.story_slug, '')) = ?
           AND t.position = ?
        """,
        (norm(story["story_slug"]), index),
    ).fetchone()
    if row is None:
        print(
            f"No task found for story '{story['story_slug']}' at position {position_input}.",
            file=sys.stderr,
        )
        sys.exit(1)
    return row

def locate_by_task_id(task_id: str):
    token = norm(task_id)
    if not token:
        return None
    row = cur.execute(
        """
        SELECT t.story_slug, t.position, t.task_id, t.title, s.story_title, s.sequence
          FROM tasks t
          LEFT JOIN stories s ON s.story_slug = t.story_slug
         WHERE LOWER(COALESCE(t.task_id, '')) = ?
         ORDER BY s.sequence ASC, t.position ASC
         LIMIT 1
        """,
        (token,),
    ).fetchone()
    return row

target_row = None

if ":" in task_ref:
    story_part, position_part = task_ref.split(":", 1)
    story_part = story_part.strip()
    position_part = position_part.strip()
    if not story_part or not position_part:
        print(
            "Invalid task reference. Expected format STORY:POSITION (e.g. auth-login:3).",
            file=sys.stderr,
        )
        sys.exit(1)
    target_row = locate_by_story_and_position(story_part, position_part)
else:
    target_row = locate_by_task_id(task_ref)
    if target_row is None:
        print(
            "Task reference not found. Use a task id (e.g. TASK-123) or story-slug:position.",
            file=sys.stderr,
        )
        sys.exit(1)

target_story_slug = clean(target_row["story_slug"])
target_position = int(target_row["position"] or 0)
target_task_id = clean(target_row["task_id"])
target_title = clean(target_row["title"]) or "(untitled task)"
target_story_title = clean(target_row["story_title"])

story_hint_clean = clean(story_hint)
if story_hint_clean:
    hint_story = resolve_story(story_hint_clean)
    if hint_story is None:
        print(
            f"Story reference '{story_hint}' not found; expected story '{target_story_slug}'.",
            file=sys.stderr,
        )
        sys.exit(1)
    hint_slug = clean(hint_story["story_slug"])
    if norm(hint_slug) != norm(target_story_slug):
        print(
            f"Story reference '{story_hint}' resolves to '{hint_slug}', which does not match task story '{target_story_slug}'.",
            file=sys.stderr,
        )
        sys.exit(1)

ordered_slugs = [clean(story["story_slug"]) for story in story_order if clean(story["story_slug"])]
try:
    story_index = ordered_slugs.index(target_story_slug)
except ValueError:
    print(f"Story slug '{target_story_slug}' missing from ordered backlog.", file=sys.stderr)
    sys.exit(1)

affected_slugs = ordered_slugs[story_index:]
if not affected_slugs:
    print(f"No stories found from '{target_story_slug}'.", file=sys.stderr)
    sys.exit(1)

cur.execute(
    """
    UPDATE tasks
       SET status = 'pending',
           started_at = NULL,
           completed_at = NULL,
           last_run = NULL,
           updated_at = ?
     WHERE story_slug = ?
       AND position >= ?
    """,
    (timestamp, target_story_slug, target_position),
)

if len(affected_slugs) > 1:
    placeholders = ",".join("?" * (len(affected_slugs) - 1))
    cur.execute(
        f"""
        UPDATE tasks
           SET status = 'pending',
               started_at = NULL,
               completed_at = NULL,
               last_run = NULL,
               updated_at = ?
         WHERE story_slug IN ({placeholders})
        """,
        (timestamp, *affected_slugs[1:]),
    )

placeholders = ",".join("?" * len(affected_slugs))
cur.execute(
    f"""
    UPDATE stories
       SET status = 'pending',
           last_run = NULL,
           updated_at = ?
     WHERE story_slug IN ({placeholders})
    """,
    (timestamp, *affected_slugs),
)

conn.commit()
conn.close()

print("\t".join([
    target_story_slug,
    target_story_title or target_story_slug,
    str(target_position + 1),
    target_task_id,
    target_title.replace("\t", " ").replace("\n", " "),
]))
PY
}

gc_align_task_story_slugs() {
  local db_path="${1:?tasks database path required}"
  python3 - <<'PY' "$db_path"
import sqlite3
import sys
import time
import re
import difflib

db_path = sys.argv[1]
conn = sqlite3.connect(db_path)
conn.row_factory = sqlite3.Row
cur = conn.cursor()

def normalize(value: str) -> str:
    return (value or "").strip()

def slug_norm(value: str) -> str:
    value = normalize(value).lower()
    if not value:
        return ""
    return re.sub(r"[^a-z0-9]+", "-", value).strip("-")

story_norm_map = {}
story_norm_keys = []
for story in cur.execute("SELECT story_slug, story_key, story_id, story_title FROM stories"):
    slug = normalize(story["story_slug"])
    if not slug:
        continue
    canonical = slug
    for candidate in {
        story["story_slug"],
        story["story_key"],
        story["story_id"],
        story["story_title"],
    }:
        norm = slug_norm(candidate)
        if norm and norm not in story_norm_map:
            story_norm_map[norm] = canonical
    lower_slug = canonical.lower()
    if lower_slug not in story_norm_map:
        story_norm_map[lower_slug] = canonical

story_norm_keys = list(story_norm_map.keys())

updates = []
timestamp = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())

for task in cur.execute("SELECT id, story_slug, story_id, story_title FROM tasks"):
    current_slug = normalize(task["story_slug"])
    story_id = normalize(task["story_id"])
    story_title = normalize(task["story_title"])

    target_slug = None
    candidates = [
        story_id,
        current_slug,
        story_title,
    ]

    for candidate in candidates:
        norm = slug_norm(candidate)
        if norm and norm in story_norm_map:
            target_slug = story_norm_map[norm]
            break

    if not target_slug:
        combined = slug_norm(f"{story_id}-{story_title}") or slug_norm(current_slug)
        if combined:
            match = difflib.get_close_matches(combined, story_norm_keys, n=1, cutoff=0.84)
            if match:
                target_slug = story_norm_map[match[0]]

    if target_slug and target_slug != current_slug:
        updates.append((target_slug, timestamp, task["id"]))

if updates:
    cur.executemany("UPDATE tasks SET story_slug = ?, updated_at = ? WHERE id = ?", updates)
    conn.commit()

conn.close()
PY
}

gc_reset_task_progress() {
  local db_path="${1:?tasks database path required}"
  python3 - <<'PY' "$db_path"
import sqlite3
import sys
import time

db_path = sys.argv[1]
timestamp = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())

conn = sqlite3.connect(db_path)
cur = conn.cursor()

cur.execute(
    """
    UPDATE stories
       SET status = 'pending',
           completed_tasks = 0,
           last_run = NULL,
           updated_at = ?
    """,
    (timestamp,),
)
cur.execute(
    """
    UPDATE tasks
       SET status = 'pending',
           started_at = NULL,
           completed_at = NULL,
           last_run = NULL,
           last_log_path = NULL,
           last_prompt_path = NULL,
           last_output_path = NULL,
           last_attempts = NULL,
           last_tokens_total = NULL,
           last_duration_seconds = NULL,
           last_apply_status = NULL,
           last_changes_applied = NULL,
           last_notes_json = NULL,
           last_written_json = NULL,
           last_patched_json = NULL,
           last_commands_json = NULL,
           last_progress_at = NULL,
           last_progress_run = NULL,
           updated_at = ?
    """,
    (timestamp,),
)

try:
    cur.execute("DELETE FROM task_progress")
except sqlite3.OperationalError:
    pass

conn.commit()
conn.close()
PY
}

gc_sync_story_totals() {
  local db_path="${1:?tasks database path required}"
  python3 - <<'PY' "$db_path"
import sqlite3
import sys
import time

db_path = sys.argv[1]
timestamp = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())

conn = sqlite3.connect(db_path)
conn.row_factory = sqlite3.Row
cur = conn.cursor()

stories = cur.execute("SELECT story_slug, status FROM stories").fetchall()

def normalize(value: str) -> str:
    return (value or "").strip()

def status_for_counts(total: int, completed: int) -> str:
    if total <= 0:
        return "pending"
    if completed >= total:
        return "complete"
    if completed > 0:
        return "in-progress"
    return "pending"

updates = []
for story in stories:
    slug = normalize(story["story_slug"])
    if not slug:
        continue
    slug_lower = slug.lower()
    row = cur.execute(
        """
        SELECT
            COUNT(*) AS total_count,
            SUM(CASE WHEN LOWER(COALESCE(status, '')) = 'complete' THEN 1 ELSE 0 END) AS complete_count
          FROM tasks
         WHERE LOWER(COALESCE(story_slug, '')) = ?
        """,
        (slug_lower,),
    ).fetchone()
    total = int(row["total_count"] or 0)
    completed = int(row["complete_count"] or 0)
    story_status = status_for_counts(total, completed)
    updates.append((total, completed, story_status, timestamp, slug))

if updates:
    cur.executemany(
        """
        UPDATE stories
           SET total_tasks = ?,
               completed_tasks = ?,
               status = ?,
               updated_at = ?
         WHERE story_slug = ?
        """,
        updates,
    )
    conn.commit()

conn.close()
PY
}

gc_fetch_story_task_counts() {
  local db_path="${1:?tasks database path required}"
  local story_slug="${2:?story slug required}"
  python3 - <<'PY' "$db_path" "$story_slug"
import sqlite3
import sys

db_path, story_slug = sys.argv[1:3]
slug = (story_slug or "").strip()
if not slug:
    print("0\t0")
    sys.exit(0)

conn = sqlite3.connect(db_path)
cur = conn.cursor()

slug_lower = slug.lower()
row = cur.execute(
    """
    SELECT
        COUNT(*) AS total_count,
        SUM(CASE WHEN LOWER(COALESCE(status, '')) = 'complete' THEN 1 ELSE 0 END) AS complete_count
      FROM tasks
     WHERE LOWER(COALESCE(story_slug, '')) = ?
    """,
    (slug_lower,),
).fetchone()
conn.close()

total = int(row[0] or 0)
completed = int(row[1] or 0)
print(f"{total}\t{completed}")
PY
}

gc_trim_memory() {
  local phase="${1:-memory-cycle}"
  info "[memory] Reclaiming resources (${phase})"

  if command -v python3 >/dev/null 2>&1; then
    python3 - <<'PY' >/dev/null 2>&1 || true
import gc
gc.collect()
PY
  fi

  if command -v purge >/dev/null 2>&1; then
    purge >/dev/null 2>&1 || true
  elif command -v sync >/dev/null 2>&1; then
    sync || true
  fi

  if command -v docker >/dev/null 2>&1; then
    docker container prune -f >/dev/null 2>&1 || true
    docker image prune -f >/dev/null 2>&1 || true
  fi
}

gc_count_pending_tasks() {
  local db_path="${1:?tasks database path required}"
  python3 - <<'PY' "$db_path"
import sqlite3
import sys

db_path = sys.argv[1]
conn = sqlite3.connect(db_path)
cur = conn.cursor()
row = cur.execute("SELECT COUNT(*) FROM tasks WHERE LOWER(COALESCE(status, 'pending')) != 'complete'").fetchone()
pending = row[0] if row else 0
conn.close()
print(pending)
PY
}

gc_tasks_db_has_rows() {
  local db_path="${1:?tasks database path required}"
  [[ -f "$db_path" ]] || return 1
  python3 - <<'PY' "$db_path"
import sqlite3
import sys

db_path = sys.argv[1]
try:
    conn = sqlite3.connect(db_path)
except sqlite3.DatabaseError:
    sys.exit(1)

stories = 0
tasks = 0
try:
    cur = conn.cursor()
    cur.execute("SELECT COUNT(*) FROM stories")
    row = cur.fetchone()
    stories = int(row[0]) if row and row[0] is not None else 0
    cur.execute("SELECT COUNT(*) FROM tasks")
    row = cur.fetchone()
    tasks = int(row[0]) if row and row[0] is not None else 0
except sqlite3.DatabaseError:
    conn.close()
    sys.exit(1)
finally:
    try:
        conn.close()
    except Exception:
        pass

sys.exit(0 if stories > 0 and tasks > 0 else 1)
PY
}

gc_has_legacy_tasks_json() {
  local json_dir="${PLAN_DIR}/create-jira-tasks/json"
  [[ -f "${json_dir}/epics.json" ]] || return 1
  [[ -d "${json_dir}/stories" ]] || return 1
  [[ -d "${json_dir}/tasks" ]] || return 1
  return 0
}

gc_rebuild_tasks_db_from_json() {
  local force_flag="${1:-0}"
  local json_dir="${PLAN_DIR}/create-jira-tasks/json"
  local epics_json="${json_dir}/epics.json"
  local stories_dir="${json_dir}/stories"
  local tasks_dir="${json_dir}/tasks"
  local refined_dir="${json_dir}/refined"
  [[ -f "$epics_json" ]] || return 1
  [[ -d "$stories_dir" ]] || return 1
  [[ -d "$tasks_dir" ]] || return 1

  local payload="${json_dir}/tasks_payload.json"
  python3 "${CLI_ROOT}/src/lib/create-jira-tasks/to_payload.py" \
    "$epics_json" "$stories_dir" "$tasks_dir" "$refined_dir" "$payload" || return 1

  local tasks_workspace="${PLAN_DIR}/tasks"
  mkdir -p "$tasks_workspace"
  local db_path="${tasks_workspace}/tasks.db"
  python3 "${CLI_ROOT}/src/lib/create-jira-tasks/to_sqlite.py" \
    "$payload" "$db_path" "$force_flag" || return 1

  return 0
}

# docker compose helper (prefers "docker compose" then docker-compose)
docker_compose() {
  local project_name="${GC_DOCKER_PROJECT_NAME:-${COMPOSE_PROJECT_NAME:-}}"
  if [[ -z "$project_name" ]]; then
    local base="$(basename "${PROJECT_ROOT:-$PWD}")"
    project_name="$(slugify_name "$base")"
  fi
  local compose_verbose_flag=""
  if [[ -n "${GC_DOCKER_VERBOSE:-}" ]]; then
    compose_verbose_flag="--verbose"
  fi
  if command -v docker >/dev/null 2>&1 && docker compose version >/dev/null 2>&1; then
    if [[ -n "$compose_verbose_flag" ]]; then
      COMPOSE_PROJECT_NAME="$project_name" docker compose "$compose_verbose_flag" "$@"
    else
      COMPOSE_PROJECT_NAME="$project_name" docker compose "$@"
    fi
  elif command -v docker-compose >/dev/null 2>&1; then
    if [[ -n "$compose_verbose_flag" ]]; then
      docker-compose -p "$project_name" "$compose_verbose_flag" "$@"
    else
      docker-compose -p "$project_name" "$@"
    fi
  else
    die "Docker CLI not available. Install Docker Desktop or docker-compose before running this command."
  fi
}

gc_compose_port() {
  local compose_file="$1" service="$2" container_port="${3:-}"
  [[ -f "$compose_file" ]] || return 1
  local output=""
  if command -v docker >/dev/null 2>&1 && docker compose version >/dev/null 2>&1; then
    output="$(COMPOSE_PROJECT_NAME="$GC_DOCKER_PROJECT_NAME" docker compose -f "$compose_file" port "$service" "$container_port" 2>/dev/null | head -n1)"
  fi
  if [[ -z "$output" ]] && command -v docker-compose >/dev/null 2>&1; then
    output="$(docker-compose -p "$GC_DOCKER_PROJECT_NAME" -f "$compose_file" port "$service" "$container_port" 2>/dev/null | head -n1)"
  fi
  [[ -n "$output" ]] || return 1
  output="${output##*:}"
  [[ "$output" =~ ^[0-9]+$ ]] || return 1
  printf '%s\n' "$output"
}

gc_container_name() {
  local service="$1"
  printf '%s-%s\n' "${GC_DOCKER_PROJECT_NAME}" "$service"
}

container_exists() {
  local name="$1"
  docker ps -a --format '{{.Names}}' | grep -Fxq "$name"
}

container_state() {
  local name="$1"
  docker inspect -f '{{.State.Status}}' "$name" 2>/dev/null || echo "absent"
}

gc_start_created_containers() {
  local compose_file="$1"
  shift || true
  local -a services=("$@")
  local service container state started any_started=0

  for service in "${services[@]}"; do
    container="$(gc_container_name "$service")"
    if ! container_exists "$container"; then
      continue
    fi
    state="$(container_state "$container")"
    if [[ "$state" == "created" ]]; then
      info "Container ${container} stuck in 'created'; attempting manual start"
      if docker start "$container" >/dev/null 2>&1; then
        started=1
        any_started=1
      else
        warn "Failed to start ${container}; attempting compose start fallback"
        docker_compose -f "$compose_file" start "$service" >/dev/null 2>&1 || true
      fi
    fi
  done

  if (( any_started == 1 )); then
    local wait_seconds=0
    local timeout="${GC_DOCKER_HEALTH_TIMEOUT:-10}"
    local poll_interval="${GC_DOCKER_HEALTH_INTERVAL:-1}"
    (( poll_interval <= 0 )) && poll_interval=1
    while (( wait_seconds < timeout )); do
      local all_ready=1
      for service in "${services[@]}"; do
        container="$(gc_container_name "$service")"
        if ! container_exists "$container"; then
          continue
        fi
        state="$(container_state "$container")"
        case "$state" in
          running|healthy) continue ;;
          exited|dead)
            warn "Container ${container} exited unexpectedly (state=${state}). Check logs."
            all_ready=0
            ;;
          *)
            all_ready=0
            ;;
        esac
      done
      if (( all_ready == 1 )); then
        break
      fi
      sleep "$poll_interval"
      (( wait_seconds += poll_interval )) || true
    done
  fi
}

port_in_use() {
  local port="$1"
  if command -v lsof >/dev/null 2>&1; then
    lsof -nP -iTCP:"$port" -sTCP:LISTEN >/dev/null 2>&1 && return 0
  elif command -v netstat >/dev/null 2>&1; then
    netstat -an 2>/dev/null | grep -E "\.${port} .*LISTEN" >/dev/null && return 0
  fi
  return 1
}

find_free_port() {
  local start="${1:-3306}"
  local port="$start"; local limit=$((start+100))
  while (( port <= limit )); do
    if ! port_in_use "$port" && ! gc_port_is_reserved "$port"; then
      echo "$port"
      return 0
    fi
    ((port++)) || true
  done
  echo "$start"  # fallback
}

gc_pick_port() {
  local label="$1"
  local default="$2"
  shift 2
  local service_key="$(slugify_name "$label")"
  [[ -n "$service_key" ]] || service_key="$label"
  local existing_port=""
  existing_port="$(gc_port_for_service "$service_key" 2>/dev/null || true)"
  gc_unreserve_port "$service_key"
  local port=""
  local env_name value
  for env_name in "$@"; do
    value="${!env_name:-}"
    if [[ -n "$value" ]] && [[ "$value" =~ ^[0-9]+$ ]]; then
      port="$value"
      break
    fi
  done
  [[ -n "$port" ]] || port="$default"
  if [[ ! "$port" =~ ^[0-9]+$ ]] || (( port < 1 || port > 65535 )); then
    port="$default"
  fi
  local original="$port"
  local attempts=0
  local limit=200
  while (( attempts < limit )); do
    if (( port < 1 || port > 65535 )); then
      port="$default"
    fi
    if ! port_in_use "$port" && ! gc_port_reserved_by_other "$port" "$service_key"; then
      break
    fi
    ((port++))
    ((attempts++))
  done
  while gc_port_reserved_by_other "$port" "$service_key"; do
    ((port++))
  done
  if (( attempts >= limit )); then
    warn "Unable to find free port for ${label}; using ${port}" >&2
  elif [[ -n "$original" && "$port" != "$original" ]]; then
    info "Port ${original} in use; remapping ${label} to ${port}" >&2
  elif [[ -z "$existing_port" && "$port" != "$default" ]]; then
    info "Port ${default} in use; remapping ${label} to ${port}" >&2
  fi
  gc_reserve_port "$service_key" "$port"
  echo "$port"
}

wait_for_endpoint() {
  local url="$1" label="$2"
  local max_time="${3:-${GC_DOCKER_HEALTH_TIMEOUT:-10}}"
  local delay="${4:-${GC_DOCKER_HEALTH_INTERVAL:-1}}"
  (( delay <= 0 )) && delay=1
  (( max_time <= 0 )) && max_time=1
  local attempts=$(( (max_time + delay - 1) / delay ))
  (( attempts < 1 )) && attempts=1
  local i=1
  while (( i <= attempts )); do
    if curl -fsS --max-time 2 "$url" >/dev/null 2>&1; then
      ok "${label} ready → ${url}"
      return 0
    fi
    sleep "$delay"
    ((i++)) || true
  done
  warn "${label} not ready after ${max_time}s → ${url}"
  return 1
}

gc_sha256_file() {
  local path="$1"
  [[ -f "$path" ]] || return 1
  python3 - <<'PY' "$path"
import hashlib, pathlib, sys
path = pathlib.Path(sys.argv[1])
data = path.read_bytes()
print(hashlib.sha256(data).hexdigest())
PY
}

gc_host_prepare_pnpm() {
  if command -v pnpm >/dev/null 2>&1; then
    return 0
  fi
  if command -v corepack >/dev/null 2>&1; then
    corepack enable pnpm >/dev/null 2>&1 || true
    local version="${GC_PNPM_VERSION:-10.17.1}"
    corepack prepare "pnpm@${version}" --activate >/dev/null 2>&1 || \
      corepack use "pnpm@${version}" >/dev/null 2>&1 || true
  fi
  command -v pnpm >/dev/null 2>&1
}

gc_refresh_stack_prepare_node_modules() {
  [[ "${GC_SKIP_HOST_PNPM_INSTALL:-0}" == "1" ]] && return 0
  local root="${PROJECT_ROOT:-$PWD}"
  local lock_file="${root}/pnpm-lock.yaml"
  local has_manifest=0
  if [[ -f "${root}/pnpm-workspace.yaml" || -f "${root}/package.json" ]]; then
    has_manifest=1
  fi
  (( has_manifest )) || return 0

  local modules_dir="${root}/node_modules/.pnpm"
  local stamp_file="${root}/node_modules/.pnpm-lock.hash"
  local need_install=0
  local lock_hash=""

  if [[ -f "$lock_file" ]]; then
    lock_hash="$(gc_sha256_file "$lock_file" 2>/dev/null || true)"
  fi

  if [[ ! -d "$modules_dir" ]]; then
    need_install=1
  else
    if [[ -z "$lock_hash" ]]; then
      need_install=1
    else
      local stamp_hash=""
      [[ -f "$stamp_file" ]] && stamp_hash="$(cat "$stamp_file" 2>/dev/null || true)"
      if [[ "$lock_hash" != "$stamp_hash" ]]; then
        need_install=1
      fi
    fi
  fi

  if (( need_install )); then
    if [[ ! -f "$lock_file" ]]; then
      info "pnpm lockfile missing; generating via install"
    fi
    info "Installing workspace dependencies via pnpm (host)"
    if ! gc_host_prepare_pnpm; then
      warn "pnpm is not available on the host; skipping host install (containers may retry)."
      return 0
    fi
    local install_rc=0
    if (cd "$root" && CI=1 PNPM_IGNORE_NODE_VERSION=1 pnpm install --frozen-lockfile --unsafe-perm --prefer-offline --engine-strict=false --reporter=append-only); then
      install_rc=0
    else
      warn "pnpm install --frozen-lockfile failed; retrying without frozen lockfile"
      if (cd "$root" && CI=1 PNPM_IGNORE_NODE_VERSION=1 pnpm install --unsafe-perm --prefer-offline --engine-strict=false --no-frozen-lockfile --reporter=append-only); then
        install_rc=0
      else
        install_rc=1
      fi
    fi
    if (( install_rc == 0 )); then
      lock_hash="$(gc_sha256_file "$lock_file" 2>/dev/null || true)"
      if [[ -n "$lock_hash" ]]; then
        mkdir -p "${root}/node_modules"
        printf '%s' "$lock_hash" > "$stamp_file"
      else
        rm -f "$stamp_file"
      fi
      ok "Host dependencies installed"
    else
      warn "Host pnpm install failed; containers will attempt dependency installation."
    fi
  fi
}

render_template_file() {
  local src="$1" dest="$2"
  local db_name="${GC_DB_NAME:-${DB_NAME:-app}}"
  local db_user="${GC_DB_USER:-${DB_USER:-app}}"
  local db_pass="${GC_DB_PASSWORD:-${DB_PASSWORD:-app_pass}}"
  local db_host_port="${GC_DB_HOST_PORT:-${DB_HOST_PORT:-3306}}"
  local db_root_pass="${GC_DB_ROOT_PASSWORD:-${DB_ROOT_PASSWORD:-root}}"
  local project_slug="${GC_DOCKER_PROJECT_NAME:-${PROJECT_SLUG:-$(slugify_name "$(basename "${PROJECT_ROOT:-$PWD}")")}}"
  local api_host_port="${GC_API_HOST_PORT:-${API_HOST_PORT:-3000}}"
  local web_host_port="${GC_WEB_HOST_PORT:-${WEB_HOST_PORT:-5173}}"
  local admin_host_port="${GC_ADMIN_HOST_PORT:-${ADMIN_HOST_PORT:-5174}}"
  local proxy_host_port="${GC_PROXY_HOST_PORT:-${PROXY_HOST_PORT:-8080}}"
  python3 - <<'PY' "$src" "$dest" "$db_name" "$db_user" "$db_pass" "$db_host_port" "$db_root_pass" "$project_slug" "$api_host_port" "$web_host_port" "$admin_host_port" "$proxy_host_port"
import pathlib, sys
args = sys.argv[1:]
src, dest, db_name, db_user, db_pass, db_host_port = args[:6]
db_root_pass = args[6] if len(args) > 6 else ''
project_slug = args[7] if len(args) > 7 else 'gptcreator'
api_host_port = args[8] if len(args) > 8 else '3000'
web_host_port = args[9] if len(args) > 9 else '5173'
admin_host_port = args[10] if len(args) > 10 else '5174'
proxy_host_port = args[11] if len(args) > 11 else '8080'
text = pathlib.Path(src).read_text()
text = text.replace('{{DB_NAME}}', db_name)
text = text.replace('{{DB_USER}}', db_user)
text = text.replace('{{DB_PASSWORD}}', db_pass)
text = text.replace('{{DB_HOST_PORT}}', db_host_port)
text = text.replace('{{DB_ROOT_PASSWORD}}', db_root_pass)
text = text.replace('{{PROJECT_SLUG}}', project_slug)
text = text.replace('{{API_HOST_PORT}}', api_host_port)
text = text.replace('{{WEB_HOST_PORT}}', web_host_port)
text = text.replace('{{ADMIN_HOST_PORT}}', admin_host_port)
text = text.replace('{{PROXY_HOST_PORT}}', proxy_host_port)
pathlib.Path(dest).write_text(text)
PY
}

gc_render_sql() {
  local src="$1" dest="$2" database="$3" app_user="$4" app_pass="$5"
  python3 - <<'PY' "$src" "$dest" "$database" "$app_user" "$app_pass"
import pathlib, re, sys
src, dest, db_name, app_user, app_pass = sys.argv[1:6]
text = pathlib.Path(src).read_text()

text = text.replace('{{DB_NAME}}', db_name)
text = text.replace('{{DB_USER}}', app_user)
text = text.replace('{{DB_PASSWORD}}', app_pass)

def rewrite_add_column_if_not_exists(sql):
    alter_pattern = re.compile(r'ALTER\s+TABLE\s+(`?)([A-Za-z_][A-Za-z0-9_]*)\1\s+(.*?);', re.IGNORECASE | re.DOTALL)
    parts = []
    last_idx = 0

    def find_clause_end(body, start_idx):
        depth = 0
        i = start_idx
        while i < len(body):
            ch = body[i]
            if ch == '(':
                depth += 1
            elif ch == ')':
                if depth > 0:
                    depth -= 1
            elif ch == ',' and depth == 0:
                return i
            elif ch == ';' and depth == 0:
                return i
            i += 1
        return len(body)

    def split_clauses(body):
        clauses = []
        current = []
        depth = 0
        for ch in body:
            if ch == '(':
                depth += 1
            elif ch == ')':
                if depth > 0:
                    depth -= 1
            if ch == ',' and depth == 0:
                clauses.append(''.join(current))
                current = []
                continue
            current.append(ch)
        tail = ''.join(current)
        if tail.strip():
            clauses.append(tail)
        return clauses

    add_col_pattern = re.compile(r'ADD\s+COLUMN\s+IF\s+NOT\s+EXISTS\s+(`?)([A-Za-z_][A-Za-z0-9_]*)\1\s+', re.IGNORECASE)
    add_key_pattern = re.compile(r'ADD\s+(UNIQUE\s+)?KEY\s+(`?)([A-Za-z_][A-Za-z0-9_]*)\2', re.IGNORECASE)
    add_constraint_pattern = re.compile(r'ADD\s+CONSTRAINT\s+(`?)([A-Za-z_][A-Za-z0-9_]*)\1\s+FOREIGN\s+KEY', re.IGNORECASE)

    for match in alter_pattern.finditer(sql):
        table_name = match.group(2)
        body = match.group(3)
        clauses = split_clauses(body)

        column_additions = []
        index_additions = []
        constraint_additions = []
        leftover_clauses = []

        for clause in clauses:
            clause_stripped = clause.strip()
            if not clause_stripped:
                continue
            col_match = add_col_pattern.match(clause_stripped)
            if col_match:
                definition = clause_stripped[col_match.end():].strip().rstrip(',')
                col_name = col_match.group(2)
                quote = col_match.group(1) or ''
                column_additions.append((col_name, quote, definition))
                continue
            key_match = add_key_pattern.match(clause_stripped)
            if key_match:
                index_name = key_match.group(3)
                index_additions.append((clause_stripped, index_name))
                continue
            constraint_match = add_constraint_pattern.match(clause_stripped)
            if constraint_match:
                constraint_name = constraint_match.group(2)
                constraint_additions.append((clause_stripped, constraint_name))
                continue
            leftover_clauses.append(clause.rstrip())

        if not (column_additions or index_additions or constraint_additions):
            continue

        parts.append(sql[last_idx:match.start()])

        dynamic_sql = []
        for col_name, quote, definition in column_additions:
            column_token = f"{quote}{col_name}{quote}"
            ddl = f"ALTER TABLE `{table_name}` ADD COLUMN {column_token} {definition}".strip()
            ddl_escaped = ddl.replace("'", "''")
            dynamic_sql.append(
                "SET @ddl := (\n"
                "  SELECT IF(\n"
                "    EXISTS(SELECT 1 FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = DATABASE() AND TABLE_NAME = '"
                + table_name + "' AND COLUMN_NAME = '" + col_name + "'),\n"
                "    'DO 0',\n"
                "    '" + ddl_escaped + "'\n"
                "  )\n"
                ");\nPREPARE stmt FROM @ddl;\nEXECUTE stmt;\nDEALLOCATE PREPARE stmt;\n"
            )

        for clause_text, index_name in index_additions:
            ddl = f"ALTER TABLE `{table_name}` {clause_text}".strip()
            ddl_escaped = ddl.replace("'", "''")
            dynamic_sql.append(
                "SET @ddl := (\n"
                "  SELECT IF(\n"
                "    EXISTS(SELECT 1 FROM INFORMATION_SCHEMA.STATISTICS WHERE TABLE_SCHEMA = DATABASE() AND TABLE_NAME = '"
                + table_name + "' AND INDEX_NAME = '" + index_name + "'),\n"
                "    'DO 0',\n"
                "    '" + ddl_escaped + "'\n"
                "  )\n"
                ");\nPREPARE stmt FROM @ddl;\nEXECUTE stmt;\nDEALLOCATE PREPARE stmt;\n"
            )

        for clause_text, constraint_name in constraint_additions:
            ddl = f"ALTER TABLE `{table_name}` {clause_text}".strip()
            ddl_escaped = ddl.replace("'", "''")
            dynamic_sql.append(
                "SET @ddl := (\n"
                "  SELECT IF(\n"
                "    EXISTS(SELECT 1 FROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS WHERE TABLE_SCHEMA = DATABASE() AND TABLE_NAME = '"
                + table_name + "' AND CONSTRAINT_NAME = '" + constraint_name + "'),\n"
                "    'DO 0',\n"
                "    '" + ddl_escaped + "'\n"
                "  )\n"
                ");\nPREPARE stmt FROM @ddl;\nEXECUTE stmt;\nDEALLOCATE PREPARE stmt;\n"
            )

        if leftover_clauses:
            remaining_body = ',\n'.join(leftover_clauses)
            dynamic_sql.append(f"ALTER TABLE `{table_name}`\n{remaining_body}\n;")

        parts.append('\n'.join(dynamic_sql))
        last_idx = match.end()

    parts.append(sql[last_idx:])
    return ''.join(parts)

text = rewrite_add_column_if_not_exists(text)

old_slug_update = """UPDATE instructors
SET slug = LOWER(
  REPLACE(
    REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(
      TRIM(COALESCE(NULLIF(display_name,''), CONCAT(TRIM(first_name),' ',TRIM(last_name)))),
      'ç','c'),'ğ','g'),'ı','i'),'ö','o'),'ş','s'),'ü','u'
    )
  , ' ', '-')
)
WHERE slug IS NULL;"""

new_slug_update = """UPDATE instructors
SET slug = LOWER(
  REPLACE(
    REPLACE(
      REPLACE(
        REPLACE(
          REPLACE(
            REPLACE(
              REPLACE(
                TRIM(COALESCE(NULLIF(display_name,''), CONCAT(TRIM(first_name),' ',TRIM(last_name)))),
                'ç','c'),
              'ğ','g'),
            'ı','i'),
          'ö','o'),
        'ş','s'),
      'ü','u'),
    ' ', '-')
)
WHERE slug IS NULL;"""

text = text.replace(old_slug_update, new_slug_update)

old_unique_block = """ALTER TABLE instructors
  MODIFY slug VARCHAR(120) NOT NULL,
  ADD UNIQUE KEY uq_instructors_slug (slug);"""

new_unique_block = """ALTER TABLE instructors
  MODIFY slug VARCHAR(120) NOT NULL;

SET @ddl := (
  SELECT IF(
    EXISTS(SELECT 1 FROM INFORMATION_SCHEMA.STATISTICS WHERE TABLE_SCHEMA = DATABASE() AND TABLE_NAME = 'instructors' AND INDEX_NAME = 'uq_instructors_slug'),
    'DO 0',
    'ALTER TABLE `instructors` ADD UNIQUE KEY `uq_instructors_slug` (`slug`)' 
  )
);
PREPARE stmt FROM @ddl;
EXECUTE stmt;
DEALLOCATE PREPARE stmt;"""

text = text.replace(old_unique_block, new_unique_block)

text = re.sub(r'^\s*USE\s+`?[^`]+`?;', lambda _m: f"USE `{db_name}`;", text, flags=re.IGNORECASE | re.MULTILINE)
text = re.sub(r'CREATE\s+DATABASE\s+IF\s+NOT\s+EXISTS\s+`[^`]+`', lambda _m: f"CREATE DATABASE IF NOT EXISTS `{db_name}`", text, flags=re.IGNORECASE)
text = re.sub(r'ON\s+`[^`]+`\.\*\s+TO', lambda _m: f"ON `{db_name}`.* TO", text, flags=re.IGNORECASE)
text = re.sub(r"(CREATE\s+USER[^']*')([^']+)(')", lambda m: f"{m.group(1)}{app_user}{m.group(3)}", text, flags=re.IGNORECASE)
text = re.sub(r"(IDENTIFIED\s+BY\s+')([^']+)(')", lambda m: f"{m.group(1)}{app_pass}{m.group(3)}", text, flags=re.IGNORECASE)
text = re.sub(r"(TO\s+')([^']+)('@)", lambda m: f"{m.group(1)}{app_user}{m.group(3)}", text, flags=re.IGNORECASE)

def wrap_add_column(match):
    prefix, name = match.group(1), match.group(2)
    if name.startswith('`') and name.endswith('`'):
        return match.group(0)
    return f"{prefix}`{name}`"

text = re.sub(r"(?i)(ADD\s+COLUMN\s+)([A-Za-z_][A-Za-z0-9_]*)", wrap_add_column, text)

for ident in ("row_number", "field_name", "error_code"):
    pattern = re.compile(rf"(?<![`'])\b{re.escape(ident)}\b(?![`'])", re.IGNORECASE)
    text = pattern.sub(lambda m: f"`{m.group(0)}`", text)

def drop_check_constraint(src_text, name):
    pattern = re.compile(rf"CONSTRAINT\s+{re.escape(name)}\s+CHECK\s*\(", re.IGNORECASE)
    while True:
        match = pattern.search(src_text)
        if not match:
            return src_text
        start = match.start()
        comma_idx = src_text.rfind(',', 0, start)
        if comma_idx == -1:
            comma_idx = start
        depth = 1
        i = match.end()
        while i < len(src_text):
            ch = src_text[i]
            if ch == '(':
                depth += 1
            elif ch == ')':
                depth -= 1
                if depth == 0:
                    i += 1
                    break
            i += 1
        else:
            return src_text
        src_text = src_text[:comma_idx] + src_text[i:]

for constraint in ("ck_seo_target", "ck_legal_rev_publish"):
    text = drop_check_constraint(text, constraint)

pathlib.Path(dest).write_text(text)
PY
}

gc_temp_file() {
  local dir="$1" prefix="$2" suffix="$3"
  python3 - <<'PY' "$dir" "$prefix" "$suffix"
import os, sys, tempfile, pathlib
dir_path, prefix, suffix = sys.argv[1:4]
path = pathlib.Path(dir_path)
path.mkdir(parents=True, exist_ok=True)
fd, temp_path = tempfile.mkstemp(prefix=prefix, suffix=suffix, dir=str(path))
os.close(fd)
print(temp_path)
PY
}

gc_execute_sql() {
  local compose_file="$1" sql_file="$2" database="$3"
  local root_user="$4" root_pass="$5" app_user="$6" app_pass="$7" fallback_init="$8" label="$9"
  local import_ok=0
  [[ -n "$label" ]] || label="operation"
  local container_host="127.0.0.1"

  if [[ -f "$compose_file" ]]; then
    if docker_compose -f "$compose_file" ps >/dev/null 2>&1; then
      info "Using docker-compose db service for ${label}"
      docker_compose -f "$compose_file" up -d db >/dev/null 2>&1 || true
      if [[ -n "$root_pass" ]]; then
        if docker_compose -f "$compose_file" exec -T db mysql -h"${container_host}" -u"${root_user}" -p"${root_pass}" "${database}" < "${sql_file}"; then
          import_ok=1
        else
          warn "Root ${label} failed; retrying as ${app_user}"
        fi
      else
        if docker_compose -f "$compose_file" exec -T db mysql -h"${container_host}" -u"${root_user}" "${database}" < "${sql_file}"; then
          import_ok=1
        fi
      fi
      if [[ "$import_ok" -ne 1 ]]; then
        if docker_compose -f "$compose_file" exec -T db mysql -h"${container_host}" -u"${app_user}" ${app_pass:+-p"${app_pass}"} "${database}" < "${sql_file}"; then
          import_ok=1
        fi
      fi
      if [[ "$import_ok" -ne 1 && -n "$fallback_init" && -f "$fallback_init" ]]; then
        local fallback_output fallback_user fallback_pass
        fallback_output="$(python3 - "$fallback_init" <<'PY'
import re, sys
text = open(sys.argv[1]).read()
user = re.search(r"CREATE USER IF NOT EXISTS '([^']+)'", text)
password = re.search(r"IDENTIFIED BY '([^']+)'", text)
if user and password:
    print(user.group(1))
    print(password.group(1))
PY
)"
        if [[ -n "$fallback_output" ]]; then
          IFS=$'\n' read -r fallback_user fallback_pass _ <<<"$fallback_output"
          unset IFS
          if [[ -n "$fallback_user" && -n "$fallback_pass" ]]; then
            if docker_compose -f "$compose_file" exec -T db mysql -h"${container_host}" -u"${fallback_user}" ${fallback_pass:+-p"${fallback_pass}"} "${database}" < "${sql_file}"; then
              import_ok=1
            fi
          fi
        fi
      fi
      if [[ "$import_ok" -eq 1 ]]; then
        return 0
      fi
    fi
  fi

  local host="${DB_HOST:-127.0.0.1}"
  local port="${DB_HOST_PORT:-${GC_DB_HOST_PORT:-${DB_PORT:-3306}}}"
  if ${MYSQL_BIN} -h "$host" -P "$port" -u "$root_user" ${root_pass:+-p"${root_pass}"} "$database" < "$sql_file"; then
    return 0
  fi

  if ${MYSQL_BIN} -h "$host" -P "$port" -u "$app_user" ${app_pass:+-p"${app_pass}"} "$database" < "$sql_file"; then
    return 0
  fi

  return 1
}

gc_refresh_stack_collect_sql() {
  local root="$1"
  python3 - <<'PY' "$root"
import os
import re
import shlex
import sys
from pathlib import Path

root = os.path.abspath(sys.argv[1])

candidate_dirs = []
seen_dirs = set()
for rel in [
    os.path.join('.gpt-creator', 'staging', 'sql'),
    os.path.join('.gpt-creator', 'staging'),
    os.path.join('staging', 'sql'),
    os.path.join('staging'),
    os.path.join('db'),
    os.path.join('database'),
    os.path.join('sql'),
    os.path.join('data', 'sql'),
    os.path.join('data'),
    '.',
]:
    path = os.path.abspath(os.path.join(root, rel))
    if os.path.isdir(path) and path not in seen_dirs:
        candidate_dirs.append(path)
        seen_dirs.add(path)

ignore_dirs = {
    '.git', '.hg', '.svn', '.tox', '.pytest_cache', '.idea', '.vscode',
    '__pycache__', 'node_modules', 'vendor', 'dist', 'build', 'tmp', 'temp'
}

entries = []
seen_files = set()

for base in candidate_dirs:
    for dirpath, dirnames, filenames in os.walk(base):
        dirnames[:] = [d for d in dirnames if d not in ignore_dirs]
        for fname in filenames:
            if not fname.lower().endswith('.sql'):
                continue
            full = os.path.abspath(os.path.join(dirpath, fname))
            if full in seen_files:
                continue
            seen_files.add(full)
            rel_path = os.path.relpath(full, root)
            base_name = os.path.basename(full)
            rel_norm = rel_path.replace('\\', '/')
            if rel_norm.startswith('.gpt-creator/staging/') and (base_name.startswith('import-') or base_name.startswith('seed-')):
                continue
            lower = fname.lower()
            dir_lower = dirpath.lower()
            label = 'schema'
            if 'init' in lower or 'init' in dir_lower:
                label = 'init'
            elif any(token in lower or token in dir_lower for token in ('seed', 'fixture', 'sample', 'data-seed', 'seed-data')):
                label = 'seed'
            elif any(token in lower for token in ('dump', 'schema', 'structure', 'backup', 'snapshot')):
                label = 'schema'
            try:
                mtime = os.path.getmtime(full)
            except OSError:
                mtime = 0
            entries.append((label, mtime, full))

order_map = {'init': 0, 'schema': 1, 'seed': 2}
entries.sort(key=lambda item: (order_map.get(item[0], 3), item[1], item[2]))

init_list = [path for label, _, path in entries if label == 'init']
schema_list = [path for label, _, path in entries if label == 'schema']
seed_list = [path for label, _, path in entries if label == 'seed']
all_list = [path for _, _, path in entries]

db_create_re = re.compile(r"CREATE\s+DATABASE\s+(?:IF\s+NOT\s+EXISTS\s+)?(?P<name>`[^`]+`|\"[^\"]+\"|'[^']+'|[A-Za-z0-9_]+)", re.IGNORECASE)
use_re = re.compile(r"\bUSE\s+(?P<name>`[^`]+`|\"[^\"]+\"|'[^']+'|[A-Za-z0-9_]+)", re.IGNORECASE)
create_user_re = re.compile(r"CREATE\s+USER\s+(?:IF\s+NOT\s+EXISTS\s+)?'(?P<user>[^']+)'(?:\s*@\s*'(?P<host>[^']*)')?\s+IDENTIFIED(?:\s+WITH\s+[A-Za-z0-9_]+)?\s+BY\s+'(?P<pw>[^']+)'", re.IGNORECASE)
alter_user_re = re.compile(r"ALTER\s+USER\s+'(?P<user>[^']+)'(?:\s*@\s*'(?P<host>[^']*)')?\s+IDENTIFIED(?:\s+WITH\s+[A-Za-z0-9_]+)?\s+BY\s+'(?P<pw>[^']+)'", re.IGNORECASE)
grant_re = re.compile(r"GRANT\s+.+?\s+ON\s+(?P<db>`[^`]+`|\"[^\"]+\"|'[^']+'|[A-Za-z0-9_]+(?:\\.[^\s;]+)?)\s+TO\s+'(?P<user>[^']+)'", re.IGNORECASE)

def normalise_identifier(token: str) -> str:
    token = token.strip().rstrip(';').strip()
    if token.endswith('.*'):
        token = token[:-2]
    if '.' in token:
        token = token.split('.', 1)[0]
    if token and token[0] in "`\"'" and token[-1] == token[0]:
        token = token[1:-1]
    return token.strip()

db_name = ''
app_user = ''
app_password = ''
user_host = ''

for label, _, path in entries:
    if db_name and app_user and app_password:
        break
    try:
        text = Path(path).read_text(encoding='utf-8', errors='ignore')
    except Exception:
        continue
    if not db_name:
        match = db_create_re.search(text)
        if match:
            db_name = normalise_identifier(match.group('name'))
    if not db_name:
        match = use_re.search(text)
        if match:
            db_name = normalise_identifier(match.group('name'))
    if not app_user or not app_password:
        match = create_user_re.search(text) or alter_user_re.search(text)
        if match:
            app_user = match.group('user')
            app_password = match.group('pw')
            host = match.group('host') if match.group('host') is not None else ''
            user_host = host or '%'
    if app_user and not db_name:
        for m in grant_re.finditer(text):
            if m.group('user') == app_user:
                candidate = normalise_identifier(m.group('db'))
                if candidate and candidate != '*':
                    db_name = candidate
                    break

def emit_array(name, values):
    if values:
        joined = ' '.join(shlex.quote(v) for v in values)
        print(f"{name}=({joined})")
    else:
        print(f"{name}=()")

def emit_value(name, value):
    quoted = shlex.quote(value) if value else "''"
    print(f"{name}={quoted}")

emit_array('refresh_sql_init_files', init_list)
emit_array('refresh_sql_schema_files', schema_list)
emit_array('refresh_sql_seed_files', seed_list)
emit_array('refresh_sql_all_files', all_list)
emit_value('refresh_sql_default_db_name', db_name)
emit_value('refresh_sql_default_db_user', app_user)
emit_value('refresh_sql_default_db_password', app_password)
emit_value('refresh_sql_default_user_host', user_host or '%')
PY
}

gc_refresh_stack_exec_mysql() {
  local container_id="$1" sql_file="$2" user="$3" password="$4" database="$5"
  local port="${6:-3306}"

  [[ -n "$container_id" ]] || return 1
  [[ -f "$sql_file" ]] || return 1
  local -a cmd=(docker exec -i "$container_id" mysql --protocol=TCP -h 127.0.0.1 -P "$port" "-u${user}")
  if [[ -n "$password" ]]; then
    cmd+=("-p${password}")
  fi
  if [[ -n "$database" ]]; then
    cmd+=("$database")
  fi
  if ! "${cmd[@]}" <"$sql_file"; then
    return 1
  fi
  return 0
}

gc_refresh_stack_exec_inline_sql() {
  local container_id="$1" user="$2" password="$3" database="$4"
  local port="${5:-3306}"
  local sql_content
  sql_content="$(cat)"
  local -a cmd=(docker exec -i "$container_id" mysql --protocol=TCP -h 127.0.0.1 -P "$port" "-u${user}")
  if [[ -n "$password" ]]; then
    cmd+=("-p${password}")
  fi
  if [[ -n "$database" ]]; then
    cmd+=("$database")
  fi
  if ! printf "%s" "$sql_content" | "${cmd[@]}"; then
    return 1
  fi
  return 0
}

gc_refresh_stack_inspect_containers() {
  local compose_file="${1:?compose file required}"
  local -a container_ids=()
  mapfile -t container_ids < <(docker_compose -f "$compose_file" ps --all -q 2>/dev/null | awk 'NF')
  if (( ${#container_ids[@]} == 0 )); then
    mapfile -t container_ids < <(docker_compose -f "$compose_file" ps -q 2>/dev/null | awk 'NF')
  fi
  if (( ${#container_ids[@]} == 0 )); then
    printf '%s\n' "No containers found for project ${GC_DOCKER_PROJECT_NAME}."
    return 1
  fi

  local inspect_json=""
  if ! inspect_json="$(docker inspect "${container_ids[@]}" 2>/dev/null)"; then
    printf '%s\n' "Failed to inspect Docker containers for project ${GC_DOCKER_PROJECT_NAME}."
    return 1
  fi

  python3 - <<'PY' <<<"${inspect_json}"
import json
import sys

try:
    data = json.load(sys.stdin)
except Exception as exc:
    print(f"Unable to parse docker inspect output: {exc}")
    sys.exit(1)

if not isinstance(data, list):
    data = [data]

if not data:
    print("No container state data returned by docker inspect.")
    sys.exit(1)

pending = []
failures = []
healthy = []

for entry in data:
    name = (entry.get("Name") or "").lstrip("/")
    labels = entry.get("Config", {}).get("Labels", {}) or {}
    service = labels.get("com.docker.compose.service") or name
    state = entry.get("State") or {}
    status = (state.get("Status") or "").lower()
    health = (state.get("Health", {}).get("Status") or "").lower()
    exit_code = state.get("ExitCode")

    detail = f"{service}: status={status or 'unknown'}"
    if health:
        detail += f", health={health}"
    if exit_code not in (None, 0):
        detail += f", exit_code={exit_code}"

    if status == "running":
        if health in ("", "healthy"):
            healthy.append(detail)
        elif health == "starting":
            pending.append(detail)
        else:
            failures.append(detail)
    elif status in ("created", "starting"):
        pending.append(detail)
    else:
        failures.append(detail)

if failures:
    print("Container failures detected:")
    for line in failures:
        print(f"  - {line}")
    sys.exit(1)

if pending:
    print("Containers still starting:")
    for line in pending:
        print(f"  - {line}")
    sys.exit(2)

print("All containers running and healthy:")
for line in healthy:
    print(f"  - {line}")
sys.exit(0)
PY
}

gc_refresh_stack_wait_for_containers() {
  local compose_file="${1:?compose file required}"
  local timeout="${2:-${GC_DOCKER_HEALTH_TIMEOUT:-10}}"
  local interval="${3:-${GC_DOCKER_HEALTH_INTERVAL:-1}}"
  (( interval <= 0 )) && interval=1
  local elapsed=0
  local output rc

  while (( elapsed <= timeout )); do
    output="$(gc_refresh_stack_inspect_containers "$compose_file")"
    rc=$?
    if (( rc == 0 )); then
      while IFS= read -r line; do
        [[ -z "$line" ]] && continue
        info "$line"
      done <<<"$output"
      return 0
    elif (( rc == 2 )); then
      while IFS= read -r line; do
        [[ -z "$line" ]] && continue
        info "$line"
      done <<<"$output"
      sleep "$interval"
      (( elapsed += interval ))
      continue
    else
      while IFS= read -r line; do
        [[ -z "$line" ]] && continue
        warn "$line"
      done <<<"$output"
      return 1
    fi
  done

  warn "Timed out after ${timeout}s waiting for containers to report healthy state."
  output="$(gc_refresh_stack_inspect_containers "$compose_file")"
  rc=$?
  local log_fn=warn
  if (( rc == 0 )); then
    log_fn=info
  fi
  while IFS= read -r line; do
    [[ -z "$line" ]] && continue
    "$log_fn" "$line"
  done <<<"$output"
  (( rc == 0 )) || return 1
  return 0
}

# ---------- Scan helpers ----------
has_pattern() { LC_ALL=C grep -E -i -m 1 -q -- "$1" "$2" 2>/dev/null; }
classify_file() {
  local path="$1"
  local name="${path##*/}"
  local lower="$(to_lower "$name")"
  local path_norm="$(to_lower "$path")"
  local ext="${lower##*.}"
  local type="" conf=0

  case "$ext" in
    md)
      if [[ "$lower" == *pdr* ]]; then type="pdr"; conf=0.95
      elif [[ "$lower" == *sds* ]]; then type="sds"; conf=0.92
      elif [[ "$lower" == *rfp* ]]; then type="rfp"; conf=0.9
      elif [[ "$lower" == *jira* ]]; then type="jira"; conf=0.88
      elif [[ "$lower" == *ui*pages* || "$lower" == *website*ui*pages* ]]; then type="ui_pages"; conf=0.85
      elif has_pattern '\bJIRA\b|Issue Key' "$path"; then type="jira"; conf=0.6
      fi
      ;;
    yml|yaml|json)
      if has_pattern '^[[:space:]]*openapi[[:space:]]*:[[:space:]]*3' "$path" || has_pattern '"openapi"[[:space:]]*:' "$path" || has_pattern '"swagger"[[:space:]]*:' "$path"; then
        type="openapi"; conf=0.94
      fi
      ;;
    sql)
      type="sql"; conf=0.65
      if has_pattern 'CREATE[[:space:]]+TABLE' "$path"; then conf=0.8; fi
      ;;
    mmd)
      type="mermaid"; conf=0.7
      ;;
    html)
      local is_html=0
      if [[ "$path_norm" == *"page_samples"* || "$path_norm" == *"page-samples"* ]]; then
        is_html=1
      elif echo "$lower" | grep -Eq '(abo|auth|prg|evt|ctn)[0-9]+\.html'; then
        is_html=1
      fi
      if [[ $is_html -eq 1 ]]; then
        type="page_sample_html"; conf=0.7
      fi
      ;;
    css)
      local is_css=0
      if [[ "$path_norm" == *"page_samples"* || "$path_norm" == *"samples"* ]]; then
        is_css=1
      elif [[ "$lower" == *style.css ]]; then
        is_css=1
      fi
      if [[ $is_css -eq 1 ]]; then
        type="page_sample_css"; conf=0.6
      fi
      ;;
  esac

  if [[ -n "$type" ]]; then
    printf '%s|%.2f\n' "$type" "$conf"
  fi
}

cmd_scan() {
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"

  info "Scanning ${PROJECT_ROOT} for project artifacts…"
  local manifest="${GC_DIR}/scan.tsv"
  local tmp="${manifest}.tmp"
  printf "type\tconfidence\tpath\n" > "$tmp"

  while IFS= read -r -d '' f; do
    local hit
    hit="$(classify_file "$f")" || true
    if [[ -n "$hit" ]]; then
      local type conf
      IFS='|' read -r type conf <<<"$hit"
      printf "%s\t%.2f\t%s\n" "$type" "$conf" "$f" >> "$tmp"
    fi
  done < <(find "$PROJECT_ROOT" \
      \( -name '.git' -o -name 'node_modules' -o -name 'dist' -o -name 'build' -o -name '.venv' -o -name '.gpt-creator' \) -prune -o -type f -print0)

  mv "$tmp" "$manifest"

  local scan_json="${STAGING_DIR}/scan.json"
  python3 - <<'PY' "$manifest" "$PROJECT_ROOT" "$scan_json"
import csv, json, sys, time, pathlib
manifest, root, out = sys.argv[1:4]
rows = []
with open(manifest, newline='') as fh:
    reader = csv.DictReader(fh, delimiter='\t')
    for row in reader:
        if not row['type']:
            continue
        rows.append({
            "type": row['type'],
            "confidence": float(row['confidence'] or 0),
            "path": str(pathlib.Path(row['path']).resolve())
        })
scan = {
    "project_root": str(pathlib.Path(root).resolve()),
    "generated_at": time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
    "artifacts": rows
}
pathlib.Path(out).parent.mkdir(parents=True, exist_ok=True)
with open(out, 'w') as fh:
    json.dump(scan, fh, indent=2)
print(out)
PY
  ok "Scan manifest → ${scan_json}"
}

cmd_normalize() {
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"

  local scan_json="${STAGING_DIR}/scan.json"
  if [[ ! -f "$scan_json" ]]; then
    warn "No scan.json found, running scan first."
    cmd_scan --project "$PROJECT_ROOT"
  fi

  scan_json="${STAGING_DIR}/scan.json"
  python3 - <<'PY' "$scan_json" "$INPUT_DIR" "$PLAN_DIR"
import json, sys, shutil, pathlib, time
scan_path, input_dir, plan_dir = sys.argv[1:4]
input_dir = pathlib.Path(input_dir)
plan_dir = pathlib.Path(plan_dir)
input_dir.mkdir(parents=True, exist_ok=True)
plan_dir.mkdir(parents=True, exist_ok=True)

data = json.load(open(scan_path))
project_root = pathlib.Path(data.get('project_root', '.')).resolve()
artifacts = data.get('artifacts', [])

unique_types = {"pdr", "sds", "rfp", "jira", "ui_pages", "openapi"}
unique = {}
for entry in artifacts:
    t = entry.get('type')
    if t in unique_types:
        if t not in unique or entry['confidence'] > unique[t]['confidence']:
            unique[t] = entry

multi_map = {
    "sql": pathlib.Path('sql'),
    "mermaid": pathlib.Path('mermaid'),
    "page_sample_html": pathlib.Path('page_samples'),
    "page_sample_css": pathlib.Path('page_samples')
}
collected = {key: [] for key in multi_map}
for entry in artifacts:
    t = entry.get('type')
    if t in multi_map:
        collected[t].append(entry)

provenance = []

def copy_file(src_path, rel_dest, entry):
    src = pathlib.Path(src_path)
    dest = input_dir / rel_dest
    dest.parent.mkdir(parents=True, exist_ok=True)
    shutil.copy2(src, dest)
    provenance.append({
        "type": entry.get('type'),
        "source": str(src),
        "destination": str(dest.relative_to(input_dir)),
        "confidence": entry.get('confidence', 0)
    })

name_map = {
    "pdr": pathlib.Path('pdr.md'),
    "sds": pathlib.Path('sds.md'),
    "rfp": pathlib.Path('rfp.md'),
    "jira": pathlib.Path('jira.md'),
    "ui_pages": pathlib.Path('ui-pages.md')
}

for t, entry in unique.items():
    src = entry['path']
    if t == 'openapi':
        suffix = pathlib.Path(src).suffix.lower()
        if suffix in {'.yaml', '.yml'}:
            rel = pathlib.Path('openapi.yaml')
        elif suffix == '.json':
            rel = pathlib.Path('openapi.json')
        else:
            rel = pathlib.Path('openapi.src')
    else:
        rel = name_map[t]
    copy_file(src, rel, entry)

for t, entries in collected.items():
    dest_root = multi_map[t]
    for entry in entries:
        src = pathlib.Path(entry['path'])
        try:
            rel = src.resolve().relative_to(project_root)
        except ValueError:
            rel = pathlib.Path(src.name)
        rel = dest_root / rel
        copy_file(src, rel, entry)

# discovery.yaml (summary)
import io
from textwrap import indent
summary = io.StringIO()
summary.write('generated_at: %s\n' % time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()))
summary.write('project_root: %s\n' % project_root)
summary.write('artifacts:\n')
for entry in artifacts:
    summary.write('  - type: %s\n' % entry.get('type'))
    summary.write('    confidence: %.2f\n' % entry.get('confidence', 0))
    summary.write('    path: %s\n' % entry.get('path'))
(input_dir / '..' / 'discovery.yaml').resolve().write_text(summary.getvalue())

prov = {
    'generated_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
    'entries': provenance
}
(plan_dir / 'provenance.json').write_text(json.dumps(prov, indent=2))
PY

  ok "Normalized inputs → ${INPUT_DIR}"
}

cmd_plan() {
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"

  local openapi=""
  for cand in "$INPUT_DIR/openapi.yaml" "$INPUT_DIR/openapi.yml" "$INPUT_DIR/openapi.json" "$INPUT_DIR/openapi.src"; do
    [[ -f "$cand" ]] && { openapi="$cand"; break; }
  done
  local sql_dir="$INPUT_DIR/sql"

  python3 - <<'PY' "$openapi" "$sql_dir" "$PLAN_DIR"
import json, os, re, sys, time, pathlib
from collections import OrderedDict
openapi_path, sql_dir, plan_dir = sys.argv[1:4]
plan_dir = pathlib.Path(plan_dir)
plan_dir.mkdir(parents=True, exist_ok=True)

routes = []
schemas = []
openapi_loaded = False
if openapi_path:
    try:
        text = pathlib.Path(openapi_path).read_text()
        if openapi_path.endswith('.json'):
            data = json.loads(text)
            openapi_loaded = True
        else:
            try:
                import yaml  # type: ignore
                data = yaml.safe_load(text)  # type: ignore
                openapi_loaded = True
            except Exception:
                data = None
        if openapi_loaded and isinstance(data, dict):
            for path, methods in (data.get('paths') or {}).items():
                if isinstance(methods, dict):
                    for method, body in methods.items():
                        if not isinstance(body, dict):
                            continue
                        routes.append({
                            'method': method.upper(),
                            'path': path,
                            'summary': body.get('summary') or ''
                        })
            schemas = list((data.get('components') or {}).get('schemas') or {})
        else:
            raise ValueError('fallback parser')
    except Exception:
        routes = []
        schemas = []
        text = pathlib.Path(openapi_path).read_text() if openapi_path else ''
        current_path = None
        for line in text.splitlines():
            if re.match(r'^\s*/[^\s]+:\s*$', line):
                current_path = line.strip().rstrip(':')
                continue
            if current_path:
                m = re.match(r'^\s{2,}(get|post|put|patch|delete|options|head):\s*$', line, re.I)
                if m:
                    routes.append({'method': m.group(1).upper(), 'path': current_path, 'summary': ''})
                    continue
                if re.match(r'^\S', line):
                    current_path = None

        in_components = False
        in_schemas = False
        for line in text.splitlines():
            stripped = line.strip()
            if not stripped:
                continue
            if re.match(r'^components:\s*$', stripped):
                in_components = True
                in_schemas = False
                continue
            if in_components and re.match(r'^schemas:\s*$', stripped):
                in_schemas = True
                continue
            indent = len(line) - len(line.lstrip(' '))
            if in_schemas:
                if indent <= 2 and not stripped.startswith('#') and not stripped.startswith('schemas:'):
                    in_schemas = False
                    continue
                if indent == 4 and re.match(r'^[A-Za-z0-9_.-]+:\s*$', stripped):
                    name = stripped.split(':', 1)[0]
                    schemas.append(name)

sql_tables = []
sql_dir_path = pathlib.Path(sql_dir)
if sql_dir and sql_dir_path.is_dir():
    for sql_file in sql_dir_path.rglob('*.sql'):
        try:
            text = sql_file.read_text()
        except Exception:
            continue
        for m in re.finditer(r'CREATE\s+TABLE\s+`?([A-Za-z0-9_]+)`?', text, flags=re.IGNORECASE):
            sql_tables.append(m.group(1))

schema_set = {s.lower() for s in schemas}
table_set = {t.lower() for t in sql_tables}
only_in_openapi = sorted(schema_set - table_set)
only_in_sql = sorted(table_set - schema_set)

routes_path = plan_dir / 'routes.md'
entities_path = plan_dir / 'entities.md'
tasks_path = plan_dir / 'tasks.json'
plan_todo = plan_dir / 'PLAN_TODO.md'

def write_routes():
    lines = ['# Routes', '']
    if routes:
        for item in sorted(routes, key=lambda r: (r['path'], r['method'])):
            summary = f" — {item['summary']}" if item.get('summary') else ''
            lines.append(f"- `{item['method']} {item['path']}`{summary}")
    else:
        lines.append('No routes detected — ensure openapi.yaml is present in staging/inputs.')
    routes_path.write_text('\n'.join(lines) + '\n')


def write_entities():
    lines = ['# Entities', '']
    lines.append('## OpenAPI Schemas')
    if schemas:
        for name in sorted(schemas):
            lines.append(f'- {name}')
    else:
        lines.append('- (none found)')
    lines.append('')
    lines.append('## SQL Tables')
    if sql_tables:
        for name in sorted(sql_tables):
            lines.append(f'- {name}')
    else:
        lines.append('- (none found)')
    lines.append('')
    lines.append('## Detected deltas')
    if only_in_openapi:
        lines.append('- Only in OpenAPI: ' + ', '.join(only_in_openapi))
    if only_in_sql:
        lines.append('- Only in SQL: ' + ', '.join(only_in_sql))
    if not only_in_openapi and not only_in_sql:
        lines.append('- None (schemas and tables aligned on name)')
    entities_path.write_text('\n'.join(lines) + '\n')


def write_tasks():
    tasks = []
    if only_in_openapi:
        tasks.append({
            'id': 'align-openapi-sql',
            'title': 'Align OpenAPI schemas with SQL tables',
            'details': f"Create tables or update schemas for: {', '.join(only_in_openapi)}"
        })
    if only_in_sql:
        tasks.append({
            'id': 'document-sql-gap',
            'title': 'Document SQL tables missing from OpenAPI',
            'details': f"Expose or document SQL tables not covered by API: {', '.join(only_in_sql)}"
        })
    tasks_path.write_text(json.dumps({'generated_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()), 'tasks': tasks}, indent=2))


def write_plan_todo():
    lines = [
        '# Build Plan',
        '',
        '- Validate discovery outputs under `staging/inputs`.',
        '- Review `routes.md` & `entities.md` for coverage and deltas.',
        '- Implement generation steps for API, DB, Web, Admin, Docker.',
        '- Run `gpt-creator generate all --project <path>` if not already executed.',
        '- Bring the stack up with `gpt-creator run up` and smoke test.',
        '- Execute `gpt-creator verify all` to satisfy acceptance & NFR gates.',
        '- Iterate on Jira tasks using `gpt-creator iterate` until checks pass.'
    ]
    plan_todo.write_text('\n'.join(lines) + '\n')

write_routes()
write_entities()
write_tasks()
write_plan_todo()
PY

  ok "Plan artifacts created under ${PLAN_DIR}"
}

copy_template_tree() {
  local src="$1" dest="$2"
  [[ -d "$src" ]] || die "Template directory not found: $src"
  find "$src" -type d ! -name '.DS_Store' | while IFS= read -r dir; do
    local rel="${dir#$src}"
    mkdir -p "$dest/$rel"
  done
  find "$src" -type f | while IFS= read -r file; do
    local base="$(basename "$file")"
    [[ "$base" == '.DS_Store' ]] && continue
    local rel="${file#$src/}"
    local target="$dest/$rel"
    if [[ "$target" == *.tmpl ]]; then
      target="${target%.tmpl}"
      mkdir -p "$(dirname "$target")"
      render_template_file "$file" "$target"
    else
      mkdir -p "$(dirname "$target")"
      cp "$file" "$target"
    fi
  done
}

cmd_generate() {
  local facet="${1:-}"; shift || true
  [[ -n "$facet" ]] || die "generate requires a facet: api|web|admin|db|docker|all"
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"
  local templates="$CLI_ROOT/templates"

  case "$facet" in
    api)
      local out="$PROJECT_ROOT/apps/api"
      mkdir -p "$out"
      copy_template_tree "$templates/api/nestjs" "$out"
      ok "API scaffolded → ${out}"
      ;;
    web)
      local out="$PROJECT_ROOT/apps/web"
      mkdir -p "$out"
      copy_template_tree "$templates/web/vue3" "$out"
      ok "Web scaffolded → ${out}"
      ;;
    admin)
      local out="$PROJECT_ROOT/apps/admin"
      mkdir -p "$out"
      copy_template_tree "$templates/admin/vue3" "$out"
      ok "Admin scaffolded → ${out}"
      ;;
    db)
      local out="$PROJECT_ROOT/db"
      mkdir -p "$out"
      copy_template_tree "$templates/db/mysql" "$out"
      ok "DB artifacts scaffolded → ${out}"
      ;;
    docker)
      local out="$PROJECT_ROOT/docker"
      mkdir -p "$out"
      local preferred="${GC_DB_HOST_PORT:-${DB_HOST_PORT:-${MYSQL_HOST_PORT:-3306}}}"
      gc_unreserve_port db
      if port_in_use "$preferred"; then
        local next; next="$(find_free_port "$preferred")"
        if [[ "$next" != "$preferred" ]]; then
          info "Port $preferred in use; remapping MySQL to $next"
          preferred="$next"
        fi
      fi
      GC_DB_HOST_PORT="$preferred"
      DB_HOST_PORT="$GC_DB_HOST_PORT"
      MYSQL_HOST_PORT="$GC_DB_HOST_PORT"
      gc_reserve_port db "$GC_DB_HOST_PORT"
      gc_set_env_var DB_HOST_PORT "$GC_DB_HOST_PORT"
      gc_set_env_var MYSQL_HOST_PORT "$GC_DB_HOST_PORT"
      gc_set_env_var GC_DB_HOST_PORT "$GC_DB_HOST_PORT"
      local api_host_port="$(gc_pick_port "API" 3000 GC_API_HOST_PORT API_HOST_PORT)"
      GC_API_HOST_PORT="$api_host_port"
      API_HOST_PORT="$GC_API_HOST_PORT"
      gc_set_env_var API_HOST_PORT "$API_HOST_PORT"
      gc_set_env_var GC_API_HOST_PORT "$GC_API_HOST_PORT"
      gc_reserve_port api "$GC_API_HOST_PORT"
      local web_host_port="$(gc_pick_port "Web" 5173 GC_WEB_HOST_PORT WEB_HOST_PORT)"
      GC_WEB_HOST_PORT="$web_host_port"
      WEB_HOST_PORT="$GC_WEB_HOST_PORT"
      gc_set_env_var WEB_HOST_PORT "$WEB_HOST_PORT"
      gc_set_env_var GC_WEB_HOST_PORT "$GC_WEB_HOST_PORT"
      gc_reserve_port web "$GC_WEB_HOST_PORT"
      local admin_host_port="$(gc_pick_port "Admin" 5174 GC_ADMIN_HOST_PORT ADMIN_HOST_PORT)"
      GC_ADMIN_HOST_PORT="$admin_host_port"
      ADMIN_HOST_PORT="$GC_ADMIN_HOST_PORT"
      gc_set_env_var ADMIN_HOST_PORT "$ADMIN_HOST_PORT"
      gc_set_env_var GC_ADMIN_HOST_PORT "$GC_ADMIN_HOST_PORT"
      gc_reserve_port admin "$GC_ADMIN_HOST_PORT"
      local proxy_host_port="$(gc_pick_port "Proxy" 8080 GC_PROXY_HOST_PORT PROXY_HOST_PORT)"
      GC_PROXY_HOST_PORT="$proxy_host_port"
      PROXY_HOST_PORT="$GC_PROXY_HOST_PORT"
      gc_set_env_var PROXY_HOST_PORT "$PROXY_HOST_PORT"
      gc_set_env_var GC_PROXY_HOST_PORT "$GC_PROXY_HOST_PORT"
      gc_reserve_port proxy "$GC_PROXY_HOST_PORT"
      local local_url="mysql://${GC_DB_USER}:${GC_DB_PASSWORD}@127.0.0.1:${GC_DB_HOST_PORT}/${GC_DB_NAME}"
      gc_set_env_var DATABASE_URL "$local_url"
      local api_base_url="http://localhost:${GC_API_HOST_PORT}/api/v1"
      gc_set_env_var GC_API_BASE_URL "$api_base_url"
      gc_set_env_var VITE_API_BASE "$api_base_url"
      local api_health_url="${api_base_url%/}/health"
      gc_set_env_var GC_API_HEALTH_URL "$api_health_url"
      local proxy_base="http://localhost:${GC_PROXY_HOST_PORT}"
      gc_set_env_var GC_WEB_URL "${proxy_base}/"
      gc_set_env_var GC_ADMIN_URL "${proxy_base}/admin/"
      gc_load_env
      copy_template_tree "$templates/docker" "$out"
      if [[ -f "$out/pnpm-entry.sh" ]]; then
        chmod +x "$out/pnpm-entry.sh" || true
      fi
      ok "Docker assets scaffolded → ${out}"
      ;;
    all)
      for f in api db web admin docker; do
        cmd_generate "$f" --project "$PROJECT_ROOT"
      done
      return 0
      ;;
    *) die "Unknown facet: ${facet}";;
  esac
}

cmd_db() {
  local action="${1:-}"; shift || true
  [[ -n "$action" ]] || die "db requires: provision|import|seed"
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"
  local compose_file="$PROJECT_ROOT/docker/docker-compose.yml"

  case "$action" in
    provision)
      [[ -f "$compose_file" ]] || die "Compose file not found at ${compose_file}; run 'gpt-creator generate docker'."
      info "Starting database service via docker compose"
      docker_compose -f "$compose_file" up -d db
      ok "MySQL container provisioned"
      ;;
    import)
      local sql_file
      sql_file="$(find "$INPUT_DIR/sql" -maxdepth 2 -type f -name '*.sql' | head -n1 || true)"
      [[ -n "$sql_file" ]] || die "No staged SQL found under ${INPUT_DIR}/sql"
      info "Importing SQL from ${sql_file}"
      local database="${DB_NAME:-$GC_DB_NAME}"
      local root_user="${DB_ROOT_USER:-root}"
      local root_pass="${DB_ROOT_PASSWORD:-${GC_DB_ROOT_PASSWORD:-}}"
      local app_user="${DB_USER:-$GC_DB_USER}"
      local app_pass="${DB_PASSWORD:-$GC_DB_PASSWORD}"
      local cleanup_files=()
      trap 'for f in "${cleanup_files[@]}"; do [[ -n "$f" && -f "$f" ]] && rm -f "$f"; done; trap - RETURN' RETURN
      local rendered_sql
      rendered_sql="$(gc_temp_file "$STAGING_DIR" "import-" ".sql")"
      cleanup_files+=("$rendered_sql")
      gc_render_sql "$sql_file" "$rendered_sql" "$database" "$app_user" "$app_pass"
      local init_sql="${INPUT_DIR}/sql/db/init.sql"
      if gc_execute_sql "$compose_file" "$rendered_sql" "$database" "$root_user" "$root_pass" "$app_user" "$app_pass" "$init_sql" "import"; then
        ok "Database import finished"
      else
        die "Database import failed"
      fi
      ;;
    seed)
      local seed_file="${PROJECT_ROOT}/db/seed.sql"
      [[ -f "$seed_file" ]] || die "Seed file not found: ${seed_file}"
      info "Seeding database from ${seed_file}"
      local database="${DB_NAME:-$GC_DB_NAME}"
      local root_user="${DB_ROOT_USER:-root}"
      local root_pass="${DB_ROOT_PASSWORD:-${GC_DB_ROOT_PASSWORD:-}}"
      local app_user="${DB_USER:-$GC_DB_USER}"
      local app_pass="${DB_PASSWORD:-$GC_DB_PASSWORD}"
      local cleanup_files=()
      trap 'for f in "${cleanup_files[@]}"; do [[ -n "$f" && -f "$f" ]] && rm -f "$f"; done; trap - RETURN' RETURN
      local rendered_seed
      rendered_seed="$(gc_temp_file "$STAGING_DIR" "seed-" ".sql")"
      cleanup_files+=("$rendered_seed")
      gc_render_sql "$seed_file" "$rendered_seed" "$database" "$app_user" "$app_pass"
      local fallback_init="${PROJECT_ROOT}/db/init.sql"
      if [[ ! -f "$fallback_init" ]]; then
        fallback_init="${INPUT_DIR}/sql/db/init.sql"
      fi
      if gc_execute_sql "$compose_file" "$rendered_seed" "$database" "$root_user" "$root_pass" "$app_user" "$app_pass" "$fallback_init" "seed"; then
        ok "Database seed applied"
      else
        die "Database seed failed"
      fi
      ;;
    *) die "Unknown db action: ${action}";;
  esac
}

cmd_run() {
  local action="${1:-}"; shift || true
  [[ -n "$action" ]] || die "run requires: up|down|logs|open"
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"
  local compose_file="$PROJECT_ROOT/docker/docker-compose.yml"

  case "$action" in
    up)
      [[ -f "$compose_file" ]] || die "Compose file not found at ${compose_file}; generate docker assets first."
      gc_refresh_stack_prepare_node_modules
      docker_compose -f "$compose_file" up -d
      ok "Stack is starting (check docker compose ps)"
      local api_base="${GC_API_BASE_URL:-http://localhost:3000/api/v1}"
      local web_url="${GC_WEB_URL:-http://localhost:8080/}"
      local admin_url="${GC_ADMIN_URL:-http://localhost:8080/admin/}"
      local health_timeout="${GC_DOCKER_HEALTH_TIMEOUT:-10}"
      local health_interval="${GC_DOCKER_HEALTH_INTERVAL:-1}"
      wait_for_endpoint "${api_base%/}/health" "API /health" "$health_timeout" "$health_interval" || true
      local web_ping="${web_url%/}/__vite_ping"
      if ! wait_for_endpoint "$web_ping" "Web (vite ping)" "$health_timeout" "$health_interval"; then
        wait_for_endpoint "${web_url%/}/" "Web" "$health_timeout" "$health_interval" || true
      fi
      local admin_ping="${admin_url%/}/__vite_ping"
      if ! wait_for_endpoint "$admin_ping" "Admin (vite ping)" "$health_timeout" "$health_interval"; then
        wait_for_endpoint "${admin_url%/}/" "Admin" "$health_timeout" "$health_interval" || true
      fi
      ;;
    down)
      [[ -f "$compose_file" ]] || die "Compose file not found at ${compose_file}"
      docker_compose -f "$compose_file" down
      ok "Stack shut down"
      ;;
    logs)
      [[ -f "$compose_file" ]] || die "Compose file not found at ${compose_file}"
      docker_compose -f "$compose_file" logs -f
      ;;
    open)
      if command -v open >/dev/null 2>&1; then
        open "http://localhost:8080" || open "http://localhost:5173" || true
      else
        ${EDITOR_CMD} "$PROJECT_ROOT" || true
      fi
      ;;
    *) die "Unknown run action: ${action}";;
  esac
}

cmd_refresh_stack() {
  local root="" compose_override="" sql_override="" seed_override=""
  local skip_import=0 skip_seed=0
  local only_services="" skip_services=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --compose) compose_override="$(abs_path "$2")"; shift 2;;
      --sql) sql_override="$(abs_path "$2")"; shift 2;;
      --seed) seed_override="$(abs_path "$2")"; shift 2;;
      --no-import) skip_import=1; shift;;
      --no-seed) skip_seed=1; shift;;
      --only-services) only_services="${2:-}"; shift 2;;
      --skip-services) skip_services="${2:-}"; shift 2;;
      -h|--help)
        cat <<'EOHELP'
Usage: gpt-creator refresh-stack [options]

Tear down, rebuild, and prime the local Docker stack (schema + seeds).

Options:
  --project PATH   Project root (defaults to current directory)
  --compose FILE   Override docker-compose file
  --sql FILE       Explicit SQL dump to import (auto-discovered if omitted)
  --seed FILE      Seed SQL file to apply after import
  --only-services LIST  Comma/space separated subset of services to start (e.g. "web,api")
  --skip-services LIST  Comma/space separated services to skip when starting (e.g. "db,admin")
  --no-import      Skip schema import step
  --no-seed        Skip seeding step
  -h, --help       Show this help
EOHELP
        return 0
        ;;
      *) break;;
    esac
  done

  ensure_ctx "$root"

  local -a refresh_sql_init_files=() refresh_sql_schema_files=() refresh_sql_seed_files=() refresh_sql_all_files=()
  local refresh_sql_default_db_name="" refresh_sql_default_db_user="" refresh_sql_default_db_password="" refresh_sql_default_user_host=""
  eval "$(gc_refresh_stack_collect_sql "$PROJECT_ROOT")"

  if [[ -n "$sql_override" ]]; then
    refresh_sql_schema_files=("$sql_override")
  fi
  if [[ -n "$seed_override" ]]; then
    refresh_sql_seed_files=("$seed_override")
  fi

  local env_updated=0
  if [[ -n "$refresh_sql_default_db_name" && "$refresh_sql_default_db_name" != "$GC_DB_NAME" ]]; then
    gc_set_env_var DB_NAME "$refresh_sql_default_db_name"
    gc_set_env_var GC_DB_NAME "$refresh_sql_default_db_name"
    env_updated=1
  fi
  if [[ -n "$refresh_sql_default_db_user" && "$refresh_sql_default_db_user" != "$GC_DB_USER" ]]; then
    gc_set_env_var DB_USER "$refresh_sql_default_db_user"
    gc_set_env_var GC_DB_USER "$refresh_sql_default_db_user"
    env_updated=1
  fi
  if [[ -n "$refresh_sql_default_db_password" && "$refresh_sql_default_db_password" != "$GC_DB_PASSWORD" ]]; then
    gc_set_env_var DB_PASSWORD "$refresh_sql_default_db_password"
    gc_set_env_var GC_DB_PASSWORD "$refresh_sql_default_db_password"
    env_updated=1
  fi
  if (( env_updated )); then
    gc_load_env
    local host_port="${GC_DB_HOST_PORT:-${DB_HOST_PORT:-3306}}"
    local database_url="mysql://${GC_DB_USER}:${GC_DB_PASSWORD}@127.0.0.1:${host_port}/${GC_DB_NAME}"
    gc_set_env_var DATABASE_URL "$database_url"
  fi

  info "Using database '${GC_DB_NAME}' with user '${GC_DB_USER}'"

  local compose_file="$compose_override"
  if [[ -n "$compose_file" ]]; then
    compose_file="$(abs_path "$compose_file")"
  else
    info "Rendering docker assets from templates"
    if ! cmd_generate docker --project "$PROJECT_ROOT"; then
      die "Failed to generate docker assets"
    fi
    if [[ -f "${PROJECT_ROOT}/docker/compose.yaml" ]]; then
      compose_file="${PROJECT_ROOT}/docker/compose.yaml"
    elif [[ -f "${PROJECT_ROOT}/docker/docker-compose.yml" ]]; then
      compose_file="${PROJECT_ROOT}/docker/docker-compose.yml"
    elif [[ -f "${PROJECT_ROOT}/docker-compose.yml" ]]; then
      compose_file="${PROJECT_ROOT}/docker-compose.yml"
    else
      die "Compose file not found after generation. Expected docker/compose.yaml or docker-compose.yml"
    fi
  fi

  info "Refreshing Docker stack for ${GC_DOCKER_PROJECT_NAME}"

  info "Stopping existing containers (removing volumes)"
  docker_compose -f "$compose_file" down -v --remove-orphans || true

  local slug="$GC_DOCKER_PROJECT_NAME"
  local -a stale_containers=(
    "${slug}-db"
    "${slug}-api"
    "${slug}-web"
    "${slug}-admin"
    "${slug}-proxy"
    "${slug}_db"
    "${slug}_api"
    "${slug}_web"
    "${slug}_admin"
    "${slug}_proxy"
  )
  local container
  for container in "${stale_containers[@]}"; do
    if docker ps -a --format '{{.Names}}' | grep -Fxq "$container"; then
      info "Removing leftover container ${container}"
      docker rm -f "$container" >/dev/null 2>&1 || true
    fi
  done

  if (( ${#refresh_sql_all_files[@]} > 0 )); then
    info "Discovered SQL assets:"
    local listed
    for listed in "${refresh_sql_all_files[@]}"; do
      if [[ "$listed" == "$PROJECT_ROOT/"* ]]; then
        info "  - ${listed#$PROJECT_ROOT/}"
      else
        info "  - ${listed}"
      fi
    done
  else
    info "No SQL assets discovered automatically."
  fi

  local -a all_services=(db api web admin proxy)
  local -a services_to_start=()
  if [[ -n "$only_services" ]]; then
    local normalized_only="${only_services//,/ }"
    read -r -a services_to_start <<< "$normalized_only"
  else
    services_to_start=("${all_services[@]}")
  fi
  if [[ -n "$skip_services" ]]; then
    local -a skip_list=()
    local normalized_skip="${skip_services//,/ }"
    read -r -a skip_list <<< "$normalized_skip"
    if (( ${#skip_list[@]} > 0 )); then
      local -a filtered=()
      local svc skip_flag skip_item
      for svc in "${services_to_start[@]}"; do
        skip_flag=0
        for skip_item in "${skip_list[@]}"; do
          [[ -z "$skip_item" ]] && continue
          if [[ "$svc" == "$skip_item" ]]; then
            skip_flag=1
            break
          fi
        done
        if (( skip_flag == 0 )); then
          filtered+=("$svc")
        fi
      done
      services_to_start=("${filtered[@]}")
    fi
  fi
  if (( ${#services_to_start[@]} > 0 )); then
    # Deduplicate and drop empties
    local -a deduped=()
    local svc seen_services=""
    for svc in "${services_to_start[@]}"; do
      [[ -z "$svc" ]] && continue
      case " $seen_services " in
        *" $svc "*) continue ;;
      esac
      deduped+=("$svc")
      seen_services+=" $svc"
    done
    services_to_start=("${deduped[@]}")
  fi

  if (( ${#services_to_start[@]} == 0 )); then
    warn "No services selected to start; skipping docker compose up."
  else
    info "Building and starting containers (${services_to_start[*]})"
    GC_DOCKER_VERBOSE="${GC_DOCKER_VERBOSE:-1}"
    gc_refresh_stack_prepare_node_modules
    docker_compose -f "$compose_file" up -d --build "${services_to_start[@]}"
    gc_start_created_containers "$compose_file" "${services_to_start[@]}"
  fi

  local db_requested=0
  local svc
  for svc in "${services_to_start[@]}"; do
    if [[ "$svc" == "db" ]]; then
      db_requested=1
      break
    fi
  done

  local db_container=""
  if (( db_requested )); then
    db_container="$(docker_compose -f "$compose_file" ps -q db || true)"
    if [[ -n "$db_container" ]]; then
      info "Waiting for MySQL to be ready…"
      local waited=0
      local mysql_timeout="${GC_DOCKER_HEALTH_TIMEOUT:-10}"
      local sleep_interval="${GC_DOCKER_HEALTH_INTERVAL:-1}"
      (( sleep_interval <= 0 )) && sleep_interval=1
      while (( waited < mysql_timeout )); do
        if docker exec -i "$db_container" sh -lc 'mysqladmin ping -h 127.0.0.1 --silent' >/dev/null 2>&1; then
          info "MySQL is ready."
          break
        fi
        sleep "$sleep_interval"
        ((waited += sleep_interval)) || true
      done
      if (( waited >= mysql_timeout )); then
        warn "MySQL readiness timeout after ${mysql_timeout}s (continuing)."
      fi
    else
      warn "Database container did not start; SQL import will be skipped."
    fi
  else
    info "Database service excluded from start; skipping readiness wait."
  fi

  docker_compose -f "$compose_file" ps

  local db_port="3306"
  local root_user="${DB_ROOT_USER:-root}"
  local root_pass="${DB_ROOT_PASSWORD:-${GC_DB_ROOT_PASSWORD:-}}"
  local app_user="${DB_USER:-$GC_DB_USER}"
  local app_pass="${DB_PASSWORD:-$GC_DB_PASSWORD}"
  local db_name="${DB_NAME:-$GC_DB_NAME}"
  local app_host="${refresh_sql_default_user_host:-%}"

  local import_rc=0 seed_rc=0
  local schema_attempted=0 seed_attempted=0

  if [[ -z "$db_container" ]]; then
    if (( skip_import == 0 )); then
      import_rc=1
    fi
    if (( skip_seed == 0 )); then
      seed_rc=1
    fi
  else
    if (( skip_import == 0 || skip_seed == 0 )); then
      local ensure_sql
      ensure_sql="$(python3 - <<'PY' "$db_name" "$app_user" "$app_pass" "$app_host"
import sys

db, user, password, host = sys.argv[1:5]
if not host:
    host = '%'
if not db:
    db = 'app'
if not user:
    user = 'app'

def quote_identifier(name: str) -> str:
    return '`' + name.replace('`', '``') + '`'

def quote_string(value: str) -> str:
    return "'" + value.replace("'", "''") + "'"

statements = [
    f"CREATE DATABASE IF NOT EXISTS {quote_identifier(db)} CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;",
    f"CREATE USER IF NOT EXISTS {quote_string(user)}@{quote_string(host)} IDENTIFIED BY {quote_string(password)};",
    f"GRANT ALL PRIVILEGES ON {quote_identifier(db)}.* TO {quote_string(user)}@{quote_string(host)};",
    "FLUSH PRIVILEGES;",
]
print("\n".join(statements))
PY
)"
      if ! gc_refresh_stack_exec_inline_sql "$db_container" "$root_user" "$root_pass" "" "$db_port" <<<"$ensure_sql"; then
        warn "Failed to ensure database or user; continuing with imports."
      else
        info "Ensured database ${db_name} and user ${app_user}"
      fi
    fi

    if (( skip_import == 0 )) && (( ${#refresh_sql_init_files[@]} + ${#refresh_sql_schema_files[@]} == 0 )); then
      info "No schema SQL files found; skipping import."
      skip_import=1
    fi
    if (( skip_seed == 0 )) && (( ${#refresh_sql_seed_files[@]} == 0 )); then
      info "No seed SQL files found; skipping seeding."
      skip_seed=1
    fi

    if (( skip_import == 0 )); then
      local file display
      for file in "${refresh_sql_init_files[@]}"; do
        [[ -f "$file" ]] || { warn "Init SQL not found: $file"; import_rc=1; continue; }
        display="$file"
        [[ "$display" == "$PROJECT_ROOT/"* ]] && display="${display#$PROJECT_ROOT/}"
        info "Applying init SQL: ${display}"
        ((schema_attempted++))
        if ! gc_refresh_stack_exec_mysql "$db_container" "$file" "$root_user" "$root_pass" "" "$db_port"; then
          warn "Init SQL failed as ${root_user}; retrying as ${app_user}"
          if ! gc_refresh_stack_exec_mysql "$db_container" "$file" "$app_user" "$app_pass" "" "$db_port"; then
            warn "Init SQL failed: ${display}"
            import_rc=1
            continue
          fi
        fi
      done

      for file in "${refresh_sql_schema_files[@]}"; do
        [[ -f "$file" ]] || { warn "Schema SQL not found: $file"; import_rc=1; continue; }
        display="$file"
        [[ "$display" == "$PROJECT_ROOT/"* ]] && display="${display#$PROJECT_ROOT/}"
        info "Importing schema SQL: ${display}"
        ((schema_attempted++))
        if ! gc_refresh_stack_exec_mysql "$db_container" "$file" "$root_user" "$root_pass" "$db_name" "$db_port"; then
          warn "Schema import failed as ${root_user}; retrying as ${app_user}"
          if ! gc_refresh_stack_exec_mysql "$db_container" "$file" "$app_user" "$app_pass" "$db_name" "$db_port"; then
            warn "Schema SQL failed: ${display}"
            import_rc=1
            continue
          fi
        fi
      done
    else
      info "Skipping schema import (--no-import)"
    fi

    if (( skip_seed == 0 )); then
      local seed_file seed_display
      for seed_file in "${refresh_sql_seed_files[@]}"; do
        [[ -f "$seed_file" ]] || { warn "Seed SQL not found: $seed_file"; seed_rc=1; continue; }
        seed_display="$seed_file"
        [[ "$seed_display" == "$PROJECT_ROOT/"* ]] && seed_display="${seed_display#$PROJECT_ROOT/}"
        info "Applying seed SQL: ${seed_display}"
        ((seed_attempted++))
        if ! gc_refresh_stack_exec_mysql "$db_container" "$seed_file" "$root_user" "$root_pass" "$db_name" "$db_port"; then
          warn "Seed import failed as ${root_user}; retrying as ${app_user}"
          if ! gc_refresh_stack_exec_mysql "$db_container" "$seed_file" "$app_user" "$app_pass" "$db_name" "$db_port"; then
            warn "Seed SQL failed: ${seed_display}"
            seed_rc=1
            continue
          fi
        fi
      done
    else
      info "Skipping seeding (--no-seed)"
    fi
  fi

  info "Verifying Docker service health"
  local stack_health_rc=0
  if gc_refresh_stack_wait_for_containers "$compose_file" "${GC_DOCKER_HEALTH_TIMEOUT:-10}" "${GC_DOCKER_HEALTH_INTERVAL:-1}"; then
    ok "Docker services healthy"
  else
    stack_health_rc=1
    warn "Docker services reported issues; inspect compose logs for details."
  fi

  local status=0
  if (( import_rc != 0 )); then
    status=1
  elif (( skip_import == 0 && schema_attempted > 0 )); then
    ok "Database schema imported"
  fi
  if (( seed_rc != 0 )); then
    status=1
  elif (( skip_seed == 0 && seed_attempted > 0 )); then
    ok "Database seeds applied"
  fi
  if (( stack_health_rc != 0 )); then
    status=1
  fi

  if (( status == 0 )); then
    ok "Stack refreshed successfully"
  else
    warn "Stack refresh completed with issues; inspect logs above."
  fi
  return $status
}


cmd_verify() {
  local kind="${1:-all}"; shift || true
  local root=""
  local api_base="${GC_API_BASE_URL:-http://localhost:3000/api/v1}"
  local api_health="${GC_API_HEALTH_URL:-}"
  local web_url="${GC_WEB_URL:-http://localhost:8080/}"
  local admin_url="${GC_ADMIN_URL:-http://localhost:8080/admin/}"
  local api_base_override=0 api_health_override=0 web_url_override=0 admin_url_override=0

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --api-url) api_base="$2"; api_base_override=1; shift 2;;
      --api-health) api_health="$2"; api_health_override=1; shift 2;;
      --web-url) web_url="$2"; web_url_override=1; shift 2;;
      --admin-url) admin_url="$2"; admin_url_override=1; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"

  kind="$(printf '%s' "$kind" | tr '[:upper:]' '[:lower:]')"

  local compose_file="${PROJECT_ROOT}/docker/docker-compose.yml"
  local ports_updated=0
  if [[ -f "$compose_file" ]]; then
    local detected
    if detected="$(gc_compose_port "$compose_file" api 3000)"; then
      if [[ -n "$detected" && "$detected" != "$GC_API_HOST_PORT" ]]; then
        GC_API_HOST_PORT="$detected"; API_HOST_PORT="$detected"; ports_updated=1
      fi
    fi
    if detected="$(gc_compose_port "$compose_file" web 5173)"; then
      if [[ -n "$detected" && "$detected" != "$GC_WEB_HOST_PORT" ]]; then
        GC_WEB_HOST_PORT="$detected"; WEB_HOST_PORT="$detected"; ports_updated=1
      fi
    fi
    if detected="$(gc_compose_port "$compose_file" admin 5173)"; then
      if [[ -n "$detected" && "$detected" != "$GC_ADMIN_HOST_PORT" ]]; then
        GC_ADMIN_HOST_PORT="$detected"; ADMIN_HOST_PORT="$detected"; ports_updated=1
      fi
    fi
    if detected="$(gc_compose_port "$compose_file" proxy 80)"; then
      if [[ -n "$detected" && "$detected" != "$GC_PROXY_HOST_PORT" ]]; then
        GC_PROXY_HOST_PORT="$detected"; PROXY_HOST_PORT="$detected"; ports_updated=1
      fi
    fi
  fi
  (( ports_updated )) && gc_env_sync_ports

  if (( api_base_override == 0 )); then
    api_base="${GC_API_BASE_URL:-$api_base}"
  fi
  if (( web_url_override == 0 )); then
    web_url="${GC_WEB_URL:-$web_url}"
  fi
  if (( admin_url_override == 0 )); then
    admin_url="${GC_ADMIN_URL:-$admin_url}"
  fi
  if (( api_health_override == 0 )); then
    api_health="${GC_API_HEALTH_URL:-$api_health}"
  fi

  local trimmed_base="${api_base%/}"
  api_health="${api_health:-${trimmed_base}/health}"

  local verify_root="$CLI_ROOT/verify"
  [[ -d "$verify_root" ]] || die "verify scripts directory missing at ${verify_root}"

  case "$kind" in
    program_filters) kind="program-filters" ;;
    program-filters|acceptance|openapi|a11y|lighthouse|consent|nfr|all) ;;
    *) die "Unknown verify target: ${kind}";;
  esac

  local -a check_names
  case "$kind" in
    acceptance) check_names=(acceptance) ;;
    openapi|a11y|lighthouse|consent|program-filters)
      check_names=("$kind")
      ;;
    nfr)
      check_names=(openapi a11y lighthouse consent program-filters)
      ;;
    all)
      check_names=(acceptance openapi a11y lighthouse consent program-filters)
      ;;
  esac

  local summary_dir="${PROJECT_ROOT}/.gpt-creator/staging/verify"
  local logs_dir="${summary_dir}/logs"
  mkdir -p "$summary_dir" "$logs_dir"

  local summary_path="${summary_dir}/summary.json"
  local python_available=0
  local python_bin=""
  if command -v python3 >/dev/null 2>&1; then
    python_available=1
    python_bin="$(command -v python3)"
  fi
  local check_order="acceptance,openapi,lighthouse,a11y,consent,program-filters"

  local pass=0 fail=0 skip=0

  update_verify_summary() {
    [[ "$python_available" -eq 1 ]] || return 0
    local name="$1"
    local status="$2"
    local label="$3"
    local message="$4"
    local log_path="$5"
    local report_path="$6"
    local score="$7"
    local duration="$8"
    local run_kind="$9"
    local timestamp="${10}"
    local event=""
    event="$(
      CHECK_NAME="$name" \
      CHECK_STATUS="$status" \
      CHECK_LABEL="$label" \
      CHECK_MESSAGE="$message" \
      CHECK_LOG="$log_path" \
      CHECK_REPORT="$report_path" \
      CHECK_SCORE="$score" \
      CHECK_DURATION="$duration" \
      CHECK_RUN_KIND="$run_kind" \
      CHECK_TIMESTAMP="$timestamp" \
      CHECK_ORDER="$check_order" \
      PROJECT_ROOT="$PROJECT_ROOT" \
      "$python_bin" - "$summary_path" <<'PY'
import json, os, sys, datetime
path = sys.argv[1]
root = os.environ.get("PROJECT_ROOT", "")
name = os.environ.get("CHECK_NAME", "")
if not name:
    sys.exit(0)
label = os.environ.get("CHECK_LABEL", name.title())
status = os.environ.get("CHECK_STATUS", "unknown")
message = os.environ.get("CHECK_MESSAGE", "")
log_path = os.environ.get("CHECK_LOG", "")
report_path = os.environ.get("CHECK_REPORT", "")
score_raw = os.environ.get("CHECK_SCORE", "")
duration_raw = os.environ.get("CHECK_DURATION", "")
run_kind = os.environ.get("CHECK_RUN_KIND", "")
timestamp = os.environ.get("CHECK_TIMESTAMP", datetime.datetime.utcnow().replace(microsecond=0).isoformat() + "Z")
order_raw = os.environ.get("CHECK_ORDER", "")

def relify(path_value):
    if not path_value:
        return ""
    if not os.path.isabs(path_value) and root:
        abs_path = os.path.normpath(os.path.join(root, path_value))
    else:
        abs_path = os.path.normpath(path_value)
    if root:
        try:
            rel = os.path.relpath(abs_path, root)
        except Exception:
            rel = abs_path
    else:
        rel = abs_path
    return rel.replace(os.sep, "/")

score = None
if score_raw:
    try:
        score = float(score_raw)
    except Exception:
        score = None

duration = None
if duration_raw:
    try:
        duration = float(duration_raw)
    except Exception:
        duration = None

try:
    with open(path, "r", encoding="utf-8") as fh:
        data = json.load(fh)
except Exception:
    data = {}

checks = data.get("checks")
if not isinstance(checks, dict):
    checks = {}

entry = checks.get(name, {})
entry["name"] = name
entry["label"] = label
entry["status"] = status
if message:
    entry["message"] = message
elif "message" in entry:
    del entry["message"]

log_rel = relify(log_path)
if log_rel:
    entry["log"] = log_rel
elif "log" in entry:
    del entry["log"]

report_rel = relify(report_path)
if report_rel:
    entry["report"] = report_rel
elif "report" in entry:
    del entry["report"]

if score is not None:
    entry["score"] = score
elif "score" in entry:
    del entry["score"]

if duration is not None:
    entry["duration_seconds"] = duration
elif "duration_seconds" in entry:
    del entry["duration_seconds"]

entry["updated"] = timestamp
if run_kind:
    entry["run_kind"] = run_kind

checks[name] = entry
data["checks"] = checks
data["last_updated"] = timestamp
if run_kind:
    data["last_run_kind"] = run_kind

stats = {"passed": 0, "failed": 0, "skipped": 0, "total": 0}
for chk in checks.values():
    status_value = str(chk.get("status", "")).lower()
    if status_value in ("pass", "passed", "ok", "success"):
        stats["passed"] += 1
    elif status_value in ("skip", "skipped"):
        stats["skipped"] += 1
    else:
        stats["failed"] += 1
    stats["total"] += 1
data["stats"] = stats

order = [part.strip() for part in order_raw.split(",") if part.strip()]
if order:
    merged = []
    seen = set()
    for item in order:
        if item not in seen:
            merged.append(item); seen.add(item)
    for item in checks.keys():
        if item not in seen:
            merged.append(item); seen.add(item)
    data["order"] = merged
else:
    existing = data.get("order")
    merged = []
    seen = set()
    if isinstance(existing, list):
        for item in existing:
            if isinstance(item, str) and item not in seen:
                merged.append(item); seen.add(item)
    for item in checks.keys():
        if item not in seen:
            merged.append(item); seen.add(item)
    data["order"] = merged

with open(path, "w", encoding="utf-8") as fh:
    json.dump(data, fh, indent=2)

event = {
    "name": name,
    "label": label,
    "status": status,
    "message": message,
    "log": entry.get("log", ""),
    "report": entry.get("report", ""),
    "score": entry.get("score"),
    "updated": timestamp,
    "run_kind": run_kind,
    "stats": stats,
    "duration_seconds": entry.get("duration_seconds"),
}
print(json.dumps(event))
PY
    )" || event=""
    if [[ -n "$event" ]]; then
      printf '::verify::%s\n' "$event"
    fi
  }

  run_check() {
    local name="$1"; shift
    local label="$1"; shift
    local -a cmd=("$@")
    local timestamp="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
    local stamp="$(date -u +"%Y%m%d-%H%M%S")"
    local log_file="${logs_dir}/${stamp}-${name}.log"
    SECONDS=0
    set +e
    "${cmd[@]}" 2>&1 | tee "$log_file"
    local exit_status=${PIPESTATUS[0]}
    set -e
    local duration="$SECONDS"
    local status message
    case "$exit_status" in
      0)
        status="pass"
        message="${label} checks passed."
        ((pass++))
        ;;
      3)
        status="skip"
        message="${label} check skipped (missing dependency)."
        ((skip++))
        warn "${label} check skipped (missing dependency)"
        ;;
      *)
        status="fail"
        message="${label} check failed (exit ${exit_status})."
        ((fail++))
        warn "${label} check failed (exit ${exit_status})"
        ;;
    esac
    cp -f "$log_file" "${logs_dir}/${name}-latest.log" 2>/dev/null || true
    local log_rel="${log_file#$PROJECT_ROOT/}"
    log_rel="${log_rel#./}"
    update_verify_summary "$name" "$status" "$label" "$message" "$log_rel" "" "" "$duration" "$kind" "$timestamp"
    return 0
  }

  find_openapi_candidate() {
    local spec=""
    for cand in "$INPUT_DIR/openapi.yaml" "$INPUT_DIR/openapi.yml" "$INPUT_DIR/openapi.json"; do
      if [[ -f "$cand" ]]; then
        spec="$cand"
        break
      fi
    done
    printf '%s' "$spec"
  }

  for name in "${check_names[@]}"; do
    case "$name" in
      acceptance)
        run_check "acceptance" "Acceptance" \
          env PROJECT_ROOT="$PROJECT_ROOT" GC_COMPOSE_FILE="$compose_file" \
          bash "$verify_root/acceptance.sh" "${api_base}" "${web_url}" "${admin_url}" "${api_health}"
        ;;
      openapi)
        run_check "openapi" "OpenAPI" \
          bash "$verify_root/check-openapi.sh" "$(find_openapi_candidate)"
        ;;
      a11y)
        run_check "a11y" "Accessibility" \
          bash "$verify_root/check-a11y.sh" "${web_url}" "${admin_url}"
        ;;
      lighthouse)
        run_check "lighthouse" "Lighthouse" \
          bash "$verify_root/check-lighthouse.sh" "${web_url}" "${admin_url}"
        ;;
      consent)
        run_check "consent" "Consent" \
          bash "$verify_root/check-consent.sh" "${web_url}"
        ;;
      program-filters)
        run_check "program-filters" "Program Filters" \
          bash "$verify_root/check-program-filters.sh" "${api_base}"
        ;;
    esac
  done

  if (( fail > 0 )); then
    die "Verify failed — pass=${pass} fail=${fail} skip=${skip}"
  fi
  ok "Verify complete — pass=${pass} skip=${skip}"
}

gc_exec_with_timeout() {
  local timeout="${1:-0}"
  local stdin_file="${2:-}"
  local log_file="${3:-}"
  shift 3 || true
  local -a cmd=("$@")

  if (( ${#cmd[@]} == 0 )); then
    return 1
  fi

  python3 - "$timeout" "$stdin_file" "$log_file" "${cmd[@]}" <<'PY'
import hashlib
import os
import select
import signal
import subprocess
import sys
import time
from pathlib import Path

def to_int(value: str) -> int:
    try:
        return max(0, int(value))
    except (TypeError, ValueError):
        return 0

timeout = to_int(sys.argv[1] if len(sys.argv) > 1 else "0")
stdin_path = sys.argv[2] if len(sys.argv) > 2 else ""
log_path = sys.argv[3] if len(sys.argv) > 3 else ""
cmd = sys.argv[4:]
max_duration = to_int(os.environ.get("GC_CODEX_EXEC_MAX_DURATION", "0"))

if not cmd:
    sys.exit(1)

stdin = None
if stdin_path:
    try:
        stdin = open(stdin_path, "rb")
    except FileNotFoundError:
        print(f"{stdin_path}: No such file or directory", file=sys.stderr, flush=True)
        sys.exit(1)

log = None
if log_path:
    log_dir = os.path.dirname(log_path)
    if log_dir:
        os.makedirs(log_dir, exist_ok=True)
    log = open(log_path, "w", encoding="utf-8", buffering=1)

diff_cache = {}
diff_dir = None
diff_counter = 0
capturing_diff = False
diff_header = ""
diff_lines = []
diff_repeat_limit = to_int(os.environ.get("GC_CODEX_DIFF_REPEAT_LIMIT", "6"))
diff_repeat_counts = {}
abort_now = False
abort_reason = ""
abort_repeat_count = 0
turn_limit = to_int(os.environ.get("GC_CODEX_MAX_TURNS", "0"))
turn_count = 0

def ensure_diff_dir():
    global diff_dir
    if diff_dir is None and log_path:
        base = Path(log_path)
        diff_dir_path = base.parent / f"{base.name}.diffs"
        diff_dir_path.mkdir(parents=True, exist_ok=True)
        diff_dir = diff_dir_path
    return diff_dir

def emit(text: str) -> None:
    if not text:
        return
    try:
        sys.stdout.write(text)
        sys.stdout.flush()
    except OSError:
        pass
    if log:
        try:
            log.write(text)
            log.flush()
        except OSError:
            pass

def is_diff_line(line: str) -> bool:
    if line == "":
        return True
    prefixes = (
        "diff --git ",
        "index ",
        "@@",
        "--- ",
        "+++ ",
        "+",
        "-",
        " ",
        "Binary files ",
        "No newline at end of file",
        "rename ",
        "similarity index",
        "dissimilarity index",
        "copy from",
        "copy to",
        "new file mode",
        "deleted file mode",
        "old mode",
        "new mode",
    )
    return any(line.startswith(prefix) for prefix in prefixes)

def flush_diff():
    global capturing_diff, diff_header, diff_lines, diff_counter
    global abort_now, abort_reason, abort_repeat_count, timed_out, timeout_type
    if not diff_header:
        capturing_diff = False
        diff_lines = []
        return
    diff_text = "".join(diff_lines)
    digest = hashlib.sha256(diff_text.encode("utf-8")).hexdigest()[:12]
    count = diff_repeat_counts.get(digest, 0) + 1
    diff_repeat_counts[digest] = count
    diff_dir_path = ensure_diff_dir()
    base_dir = diff_dir_path.parent if diff_dir_path is not None else os.getcwd()
    base_dir_str = str(base_dir)
    if digest in diff_cache:
        stored_path = diff_cache[digest]
        if stored_path:
            rel = os.path.relpath(str(stored_path), base_dir_str)
            emit(f"{diff_header.strip()} (repeat #{count}) see {rel}\n")
        else:
            emit(f"{diff_header.strip()} (repeat #{count} diff cached earlier)\n")
    else:
        if diff_dir_path is not None:
            diff_counter += 1
            stored_path = diff_dir_path / f"turn-{diff_counter:03d}-{digest}.patch"
            stored_path.write_text(diff_text, encoding="utf-8")
            diff_cache[digest] = stored_path
            rel = os.path.relpath(str(stored_path), base_dir_str)
            emit(f"{diff_header.strip()} stored patch {rel}\n")
        else:
            diff_cache[digest] = None
            emit(f"{diff_header.strip()} (diff of {len(diff_text)} bytes captured)\n")
    if diff_repeat_limit and count >= diff_repeat_limit and not abort_now:
        abort_now = True
        timed_out = True
        timeout_type = "diff-repeat"
        abort_repeat_count = count
        abort_reason = f"turn diff repeated {count} times"
    capturing_diff = False
    diff_header = ""
    diff_lines.clear()

buffer = ""

def process_line(line: str):
    global capturing_diff, diff_header, diff_lines
    global abort_now, turn_count, abort_reason, timed_out, timeout_type
    stripped = line.rstrip("\n")
    lowered = stripped.lower()
    if turn_limit:
        turn_pos = lowered.find("turn ")
        if turn_pos != -1:
            prev_char = lowered[turn_pos - 1] if turn_pos > 0 else ""
            if turn_pos == 0 or prev_char in {"", " ", "\t", "]"}:
                turn_count += 1
                if turn_count >= turn_limit and not abort_now:
                    abort_now = True
                    timeout_type = "turn-limit"
                    abort_reason = f"turn limit {turn_limit} reached"
                    timed_out = True
                    return
    if capturing_diff:
        if is_diff_line(stripped):
            diff_lines.append(line)
            return
        flush_diff()
        # fall through to treat current line normally
        if abort_now:
            return
    if stripped.lower().startswith("turn diff"):
        capturing_diff = True
        diff_header = stripped
        diff_lines = []
        return
    emit(line)

def process_text(text: str):
    global buffer
    global abort_now
    buffer += text
    while True:
        if "\n" in buffer:
            line, buffer = buffer.split("\n", 1)
            process_line(line + "\n")
            if abort_now:
                break
        else:
            break

preexec = os.setsid if hasattr(os, "setsid") else None

try:
    proc = subprocess.Popen(
        cmd,
        stdin=stdin,
        stdout=subprocess.PIPE,
        stderr=subprocess.STDOUT,
        bufsize=0,
        preexec_fn=preexec,
    )
except FileNotFoundError:
    emit(f"{cmd[0]} not found\n")
    if log:
        log.close()
    sys.exit(127)

start = time.monotonic()
last_activity = start
timed_out = False
timeout_type = None

if not proc.stdout:
    if stdin:
        stdin.close()
    if log:
        log.close()
    sys.exit(1)

fd = proc.stdout.fileno()

try:
    while True:
        now = time.monotonic()
        elapsed = now - start
        if max_duration and elapsed >= max_duration:
            timed_out = True
            timeout_type = "hard"
            break
        if timeout:
            idle_for = now - last_activity
            if idle_for >= timeout:
                timed_out = True
                timeout_type = "idle"
                break
            remaining = timeout - idle_for
            wait_time = remaining if remaining < 0.2 else 0.2
        else:
            wait_time = 0.2
        ready, _, _ = select.select([fd], [], [], wait_time)
        if ready:
            chunk = os.read(fd, 8192)
            if chunk:
                text = chunk.decode("utf-8", errors="replace")
                process_text(text)
                last_activity = time.monotonic()
                if abort_now:
                    break
            else:
                break
        else:
            if proc.poll() is not None:
                break
        if abort_now:
            break
    if not timed_out and not abort_now:
        while True:
            chunk = os.read(fd, 8192)
            if not chunk:
                break
            text = chunk.decode("utf-8", errors="replace")
            process_text(text)
            if abort_now:
                break
except KeyboardInterrupt:
    proc.terminate()
    raise
finally:
    if stdin:
        stdin.close()

proc.stdout.close()

if buffer:
    process_line(buffer)
    buffer = ""

flush_diff()
if abort_now and not timed_out:
    timed_out = True

if timed_out:
    try:
        if preexec:
            os.killpg(proc.pid, signal.SIGTERM)
        else:
            proc.terminate()
    except ProcessLookupError:
        pass
    try:
        proc.wait(5)
    except subprocess.TimeoutExpired:
        try:
            if preexec:
                os.killpg(proc.pid, signal.SIGKILL)
            else:
                proc.kill()
        except ProcessLookupError:
            pass
        proc.wait()
    if timeout_type == "hard":
        emit(f"\n[gc] Command exceeded max runtime of {max_duration} seconds\n")
    elif timeout_type == "diff-repeat":
        limit_msg = diff_repeat_limit if diff_repeat_limit else abort_repeat_count
        emit(f"\n[gc] Command emitted the same diff {abort_repeat_count or limit_msg} times; aborting to prevent an infinite loop\n")
    elif timeout_type == "turn-limit":
        emit(f"\n[gc] Command exceeded the maximum of {turn_limit} Codex turns; aborting to prevent an infinite loop\n")
    else:
        emit(f"\n[gc] Command produced no output for {timeout} seconds\n")
    if log:
        log.flush()
        log.close()
    sys.exit(124)

exit_code = proc.poll()
if exit_code is None:
    exit_code = proc.wait()

if log:
    log.flush()
    log.close()

sys.exit(exit_code)
PY
  return "$?"
}

codex_call() {
  local task="${1:?task}"; shift || true
  local prompt_dir="${GC_DIR}/prompts"
  mkdir -p "$prompt_dir"
  GC_CODEX_CALL_TOKEN_ACCUM=0

  local prompt_file=""
  local output_file=""

  if [[ $# -gt 0 && -f "$1" ]]; then
    prompt_file="$1"
    shift || true
  fi

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --prompt) prompt_file="$2"; shift 2;;
      --output) output_file="$2"; shift 2;;
      *) break;;
    esac
  done

  if [[ -z "$prompt_file" ]]; then
    prompt_file="${prompt_dir}/${task}.md"
    if [[ ! -f "$prompt_file" ]]; then
      cat >"$prompt_file" <<'PROMPT'
# Instruction
You are Codex (gpt-5-codex) assisting the gpt-creator pipeline. Apply requested changes deterministically.
PROMPT
    fi
  fi

  if [[ "${GC_CODEX_USAGE_LIMIT_REACHED:-0}" == "1" ]]; then
    warn "Codex usage limit previously reached; skipping ${task}."
    return 95
  fi

  if command -v "$CODEX_BIN" >/dev/null 2>&1; then
    info "Codex ${task} → model=${CODEX_MODEL}"
    local fallback_model="${CODEX_FALLBACK_MODEL:-gpt-5-codex}"
    local run_codex_model
    run_codex_model() {
      local model="$1"
      shift || true
      local args=(exec --model "$model")
      if [[ -n "${CODEX_PROFILE:-}" ]]; then
        args+=(--profile "$CODEX_PROFILE")
      fi
      if [[ -n "${PROJECT_ROOT:-}" ]]; then
        args+=(--cd "$PROJECT_ROOT")
      fi
      if [[ -n "${CODEX_REASONING_EFFORT:-}" ]]; then
        args+=(-c "model_reasoning_effort=\"${CODEX_REASONING_EFFORT}\"")
      fi
      args+=(--full-auto --sandbox workspace-write --skip-git-repo-check)
      if [[ -n "$output_file" ]]; then
        mkdir -p "$(dirname "$output_file")"
        args+=(--output-last-message "$output_file")
      fi
      local usage_dir="${LOG_DIR:-${PROJECT_ROOT:-$PWD}/.gpt-creator/logs}"
      mkdir -p "$usage_dir"
      local task_slug
      task_slug="$(printf '%s' "$task" | tr '[:upper:]' '[:lower:]')"
      task_slug="$(printf '%s' "$task_slug" | tr -c 'a-z0-9' '_')"
      [[ -n "$task_slug" ]] || task_slug="codex"
      local model_slug
      model_slug="$(printf '%s' "$model" | tr '[:upper:]' '[:lower:]')"
      model_slug="$(printf '%s' "$model_slug" | tr -c 'a-z0-9' '_')"
      [[ -n "$model_slug" ]] || model_slug="model"
      local codex_log=""
      if ! codex_log="$(mktemp "${usage_dir}/codex-${task_slug}-${model_slug}.XXXXXX.log" 2>/dev/null)"; then
        codex_log="$(mktemp 2>/dev/null)" || codex_log=""
      fi
      local exec_timeout=0
      if [[ "${GC_CODEX_EXEC_TIMEOUT:-}" =~ ^[0-9]+$ ]]; then
        exec_timeout=$((GC_CODEX_EXEC_TIMEOUT))
      fi
      if [[ -n "$codex_log" ]]; then
        local cmd_status=0
        if gc_exec_with_timeout "$exec_timeout" "$prompt_file" "$codex_log" "$CODEX_BIN" "${args[@]}"; then
          cmd_status=0
        else
          cmd_status=$?
        fi
        gc_record_codex_usage "$codex_log" "$task" "$model" "$prompt_file" "$cmd_status"
        local attempt_tokens="${GC_LAST_CODEX_TOTAL_TOKENS:-0}"
        if [[ "$attempt_tokens" =~ ^[0-9]+$ ]]; then
          GC_CODEX_CALL_TOKEN_ACCUM=$((GC_CODEX_CALL_TOKEN_ACCUM + attempt_tokens))
        fi
        if [[ "${GC_CODEX_USAGE_LIMIT_REACHED:-0}" == "1" ]]; then
          return 95
        fi
        return "$cmd_status"
      else
        local cmd_status=0
        if gc_exec_with_timeout "$exec_timeout" "$prompt_file" "" "$CODEX_BIN" "${args[@]}"; then
          cmd_status=0
        else
          cmd_status=$?
        fi
        if [[ "${GC_CODEX_USAGE_LIMIT_REACHED:-0}" == "1" ]]; then
          return 95
        fi
        return "$cmd_status"
      fi
    }
    if run_codex_model "$CODEX_MODEL"; then
      return 0
    fi
    local primary_status="$?"
    if [[ "${GC_CODEX_USAGE_LIMIT_REACHED:-0}" == "1" ]]; then
      return "$primary_status"
    fi
    if [[ "$CODEX_MODEL" != "$fallback_model" ]]; then
      warn "Codex model ${CODEX_MODEL} failed; retrying with ${fallback_model}."
      if run_codex_model "$fallback_model"; then
        return 0
      fi
      primary_status="$?"
      if [[ "${GC_CODEX_USAGE_LIMIT_REACHED:-0}" == "1" ]]; then
        return "$primary_status"
      fi
      warn "Codex invocation returned non-zero."
      return "$primary_status"
    fi
    warn "Codex invocation returned non-zero."
    return "$primary_status"
  else
    warn "Codex binary (${CODEX_BIN}) not found — skipping ${task}."
  fi
}

ensure_go_runtime() {
  local min_major=1
  local min_minor=21
  local target_version="1.22.4"
  local requested_bin="${GO_BIN:-}"
  local resolved_bin=""

  if [[ -n "$requested_bin" ]]; then
    if command -v "$requested_bin" >/dev/null 2>&1; then
      resolved_bin="$(command -v "$requested_bin")"
    else
      warn "GO_BIN is set to '${requested_bin}' but that binary was not found on PATH."
    fi
  fi

  if [[ -z "$resolved_bin" ]] && command -v go >/dev/null 2>&1; then
    resolved_bin="$(command -v go)"
  fi

  if [[ -n "$resolved_bin" ]]; then
    local raw_version
    raw_version="$("$resolved_bin" version 2>/dev/null | awk '{print $3}')"
    local version="${raw_version#go}"
    version="${version%%[^0-9.]*}"
    local IFS=.
    read -r version_major version_minor _ <<<"${version}"
    version_major="${version_major:-0}"
    version_minor="${version_minor:-0}"
    if (( 10#$version_major > min_major )) || { (( 10#$version_major == min_major )) && (( 10#$version_minor >= min_minor )); }; then
      GC_GO_BIN="$resolved_bin"
      export GO_BIN="$GC_GO_BIN"
      return
    fi
    warn "Found Go ${version:-unknown} at ${resolved_bin}, but need ≥ ${min_major}.${min_minor}."
  fi

  local os_name arch_name go_os go_arch
  os_name="$(uname -s)"
  arch_name="$(uname -m)"
  case "$os_name" in
    Linux) go_os="linux" ;;
    Darwin) go_os="darwin" ;;
    *)
      die "Unsupported OS (${os_name}) for automatic Go installation. Install Go ${min_major}.${min_minor}+ and set GO_BIN."
      ;;
  esac

  case "$arch_name" in
    x86_64|amd64) go_arch="amd64" ;;
    arm64|aarch64) go_arch="arm64" ;;
    *)
      die "Unsupported architecture (${arch_name}) for automatic Go installation. Install Go ${min_major}.${min_minor}+ and set GO_BIN."
      ;;
  esac

  local base_dir
  if [[ -n "${PLAN_DIR:-}" ]]; then
    base_dir="$PLAN_DIR"
  else
    base_dir="${XDG_CACHE_HOME:-$HOME/.cache}/gpt-creator"
    mkdir -p "$base_dir"
  fi
  local runtime_root="${base_dir}/.runtime/go"
  local archive_basename="go${target_version}.${go_os}-${go_arch}"
  local target_dir="${runtime_root}/${archive_basename}"
  local go_root="${target_dir}/go"
  local go_binary="${go_root}/bin/go"

  mkdir -p "$runtime_root"

  if [[ ! -x "$go_binary" ]]; then
    info "Installing Go ${target_version} (${go_os}-${go_arch})"
    local url="https://go.dev/dl/${archive_basename}.tar.gz"
    local tmp_archive
    tmp_archive="$(mktemp "${runtime_root}/go-${target_version}.XXXXXX")"
    if ! curl -fsSL "$url" -o "$tmp_archive"; then
      rm -f "$tmp_archive"
      die "Failed to download Go toolchain from ${url}"
    fi
    rm -rf "$target_dir"
    mkdir -p "$target_dir"
    if ! tar -xzf "$tmp_archive" -C "$target_dir"; then
      rm -f "$tmp_archive"
      die "Failed to extract Go toolchain archive (${url})"
    fi
    rm -f "$tmp_archive"
  fi

  if [[ ! -x "$go_binary" ]]; then
    die "Go toolchain installation incomplete; expected executable at ${go_binary}"
  fi

  local gopath="${target_dir}/gopath"
  local cache_dir="${target_dir}/cache"
  mkdir -p "${gopath}/bin" "${gopath}/pkg/mod" "${cache_dir}"
  export PATH="${go_root}/bin:${PATH}"
  export GOROOT="${go_root}"
  export GOPATH="${gopath}"
  export GOMODCACHE="${gopath}/pkg/mod"
  export GOCACHE="${cache_dir}"
  GC_GO_BIN="$go_binary"
  export GO_BIN="$GC_GO_BIN"
  ok "Go runtime pinned to ${target_version}"
}

ensure_node_runtime() {
  local root_dir="${1:-$PROJECT_ROOT}"
  local target_version="20.10.0"
  local runtime_root="${PLAN_DIR}/.runtime"
  local archive_dir="node-v${target_version}"

  # If current node already satisfies v20.x with sufficient minor, skip.
  if command -v node >/dev/null 2>&1; then
    local current
    current="$(node -v 2>/dev/null | sed 's/^v//')"
    if [[ "$current" == 20.* ]]; then
      local minor="${current#20.}"
      minor="${minor%%.*}"
      local patch="${current#20.${minor}.}"
      [[ -z "$patch" ]] && patch=0
      if (( 10#${minor:-0} > 10 )) || { (( 10#${minor:-0} == 10 )) && (( 10#${patch:-0} >= 0 )); }; then
        return
      fi
    fi
  fi

  mkdir -p "$runtime_root"

  local os_name="$(uname -s)"
  local arch_name="$(uname -m)"
  local os_tag="" arch_tag="" ext="tar.xz"

  case "$os_name" in
    Darwin)
      os_tag="darwin"
      ext="tar.gz"
      ;;
    Linux)
      os_tag="linux"
      ;;
    *)
      warn "Unsupported OS (${os_name}) for automatic Node runtime; continuing with system Node."
      return
      ;;
  esac

  case "$arch_name" in
    x86_64|amd64)
      arch_tag="x64"
      ;;
    arm64|aarch64)
      arch_tag="arm64"
      ;;
    *)
      warn "Unsupported CPU architecture (${arch_name}) for automatic Node runtime; continuing with system Node."
      return
      ;;
  esac

  local archive_name="node-v${target_version}-${os_tag}-${arch_tag}"
  local target_dir="${runtime_root}/${archive_name}"

  if [[ ! -d "$target_dir" ]]; then
    local url="https://nodejs.org/dist/v${target_version}/${archive_name}.${ext}"
    info "Downloading Node.js ${target_version} (${os_tag}-${arch_tag})"
    local tmp_archive
    tmp_archive="$(mktemp "${runtime_root}/node-${target_version}.XXXXXX")"
    if ! curl -fsSL "$url" -o "$tmp_archive"; then
      warn "Failed to download Node.js runtime from ${url}; continuing with system Node."
      rm -f "$tmp_archive"
      return
    fi
    mkdir -p "$runtime_root"
    if [[ "$ext" == "tar.gz" ]]; then
      tar -xzf "$tmp_archive" -C "$runtime_root"
    else
      tar -xJf "$tmp_archive" -C "$runtime_root"
    fi
    rm -f "$tmp_archive"
  fi

  if [[ ! -d "$target_dir" ]]; then
    warn "Node runtime directory ${target_dir} missing after download; continuing with system Node."
    return
  fi

  export PATH="${target_dir}/bin:${PATH}"
  export NODE_HOME="$target_dir"
  export npm_config_cache="${runtime_root}/npm-cache"
  export PNPM_HOME="${runtime_root}/pnpm-home"
  mkdir -p "$npm_config_cache" "$PNPM_HOME"
  export PATH="${PNPM_HOME}:${PATH}"
  export GC_NODE_RUNTIME="${target_dir}"
  ok "Node.js runtime pinned to ${target_version}"
}

ensure_node_dependencies() {
  local root_dir="${1:-$PROJECT_ROOT}"
  local sentinel="${PLAN_DIR}/.deps-installed"

  ensure_node_runtime "$root_dir"

  if [[ -f "$sentinel" ]]; then
    return
  fi

  local installer_desc=""
  local -a installer_cmd=()

  if [[ -f "$root_dir/pnpm-lock.yaml" || -f "$root_dir/pnpm-workspace.yaml" ]]; then
    if command -v pnpm >/dev/null 2>&1; then
      installer_desc="pnpm workspace"
      installer_cmd=(pnpm install --frozen-lockfile)
    fi
  fi

  if [[ ${#installer_cmd[@]} -eq 0 && -f "$root_dir/package.json" ]]; then
    if command -v pnpm >/dev/null 2>&1; then
      installer_desc="pnpm"
      installer_cmd=(pnpm install)
    elif command -v npm >/dev/null 2>&1; then
      installer_desc="npm"
      installer_cmd=(npm install)
    fi
  fi

  if [[ ${#installer_cmd[@]} -eq 0 ]]; then
    return
  fi

  local log_file="/tmp/gc_deps_install.log"

  info "Ensuring Node.js dependencies via ${installer_desc}"
  local install_status=1
  if [[ ${installer_cmd[0]} == pnpm ]]; then
    if (cd "$root_dir" && CI=1 PNPM_IGNORE_NODE_VERSION=1 "${installer_cmd[@]}" >"$log_file" 2>&1); then
      install_status=0
    fi
  else
    if (cd "$root_dir" && "${installer_cmd[@]}" >"$log_file" 2>&1); then
      install_status=0
    fi
  fi

  if (( install_status == 0 )); then
    mkdir -p "$(dirname "$sentinel")"
    touch "$sentinel"
    ok "Dependencies installed"
  else
    warn "Dependency installation via ${installer_desc} failed; inspect ${log_file}. Continuing anyway."
  fi
}

gc_apply_codex_changes() {
  local output_file="${1:?output file required}"
  local project_root="${2:?project root required}"
  python3 - <<'PY' "$output_file" "$project_root"
import json
import re
import subprocess
import sys
from pathlib import Path
from typing import Optional

output_path = Path(sys.argv[1])
project_root = Path(sys.argv[2])

if not output_path.exists():
    print("no-output", flush=True)
    sys.exit(0)

raw = output_path.read_text(encoding='utf-8').strip()
if not raw:
    print("empty-output", flush=True)
    sys.exit(0)

# Remove code fences if present
if '```' in raw:
    cleaned = []
    fenced = False
    for line in raw.splitlines():
        marker = line.strip()
        if marker.startswith('```'):
            fenced = not fenced
            continue
        if not fenced:
            cleaned.append(line)
    raw = '\n'.join(cleaned).strip()

start = raw.find('{')
end = raw.rfind('}')
if start == -1 or end == -1 or end <= start:
    raise SystemExit("JSON not found in Codex output")

fragment = raw[start:end+1]
prefix = raw[:start].strip()
suffix = raw[end+1:].strip()
has_extra_text = bool(prefix) or bool(suffix)

if has_extra_text:
    raw_dump = output_path.with_suffix(output_path.suffix + '.raw.txt')
    raw_dump.parent.mkdir(parents=True, exist_ok=True)
    raw_dump.write_text(raw, encoding='utf-8')
    rel_dump = raw_dump
    try:
        rel_dump = raw_dump.relative_to(project_root)
    except ValueError:
        rel_dump = raw_dump
    payload = {
        'plan': [],
        'changes': [],
        'commands': [],
        'notes': [
            f"Codex output contained extra text outside the JSON envelope; review {rel_dump}."
        ],
    }
    print('STATUS parse-error')
    print(f"RAW {rel_dump}")
else:
    fragment = re.sub(r'\\"(?=[}\]\n])', r'\\""', fragment)

    while True:
        try:
            payload = json.loads(fragment)
            break
        except json.JSONDecodeError as exc:
            if 'Invalid \\escape' in exc.msg:
                fragment = fragment[:exc.pos] + '\\' + fragment[exc.pos:]
                continue
            decoder = json.JSONDecoder(strict=False)
            try:
                payload = decoder.decode(fragment)
                break
            except json.JSONDecodeError:
                raw_dump = output_path.with_suffix(output_path.suffix + '.raw.txt')
                raw_dump.parent.mkdir(parents=True, exist_ok=True)
                raw_dump.write_text(raw, encoding='utf-8')
                rel_dump = raw_dump
                try:
                    rel_dump = raw_dump.relative_to(project_root)
                except ValueError:
                    rel_dump = raw_dump
                payload = {
                    'plan': [],
                    'changes': [],
                    'commands': [],
                    'notes': [
                        f"Codex output could not be parsed as JSON; review {rel_dump}."
                    ],
                }
                print('STATUS parse-error')
                print(f"RAW {rel_dump}")
                break

def _normalize_focus(items):
    normalized = []
    for raw_item in items:
        if isinstance(raw_item, str):
            candidate = raw_item.strip()
            if len(candidate) >= 2:
                normalized.append(candidate)
    return normalized

def _extract_focus_from_text(text):
    import json
    import ast
    import re

    if not isinstance(text, str):
        return []

    matches = re.findall(
        r'focus\s*:\s*(\[[^\]]*\]|\([^\)]*\)|\{[^\}]*\}|[^;]+)',
        text,
        flags=re.IGNORECASE,
    )
    extracted = []
    for segment in matches:
        segment = segment.strip()
        if not segment:
            continue
        parsed = None
        opening = segment[:1]
        closing = segment[-1:]
        if (opening, closing) in {('[', ']'), ('{', '}'), ('(', ')')}:
            inner = segment[1:-1].strip()
            if opening in {'[', '{'}:
                try:
                    parsed = json.loads(segment)
                except Exception:
                    try:
                        parsed = ast.literal_eval(segment)
                    except Exception:
                        parsed = None
            else:
                parsed = [item.strip() for item in inner.split(',')]
            if isinstance(parsed, (list, tuple)):
                extracted.extend(_normalize_focus(parsed))
                continue
            segment = inner
        parts = [item.strip() for item in re.split(r'[,\n]', segment) if item.strip()]
        extracted.extend(_normalize_focus(parts))
    return extracted

focus_targets = payload.get('focus')
focus_valid = False
if isinstance(focus_targets, list) and focus_targets:
    normalized_focus = _normalize_focus(focus_targets)
    if len(normalized_focus) == len(focus_targets):
        focus_valid = True
        focus_targets = normalized_focus

if not focus_valid:
    inferred_focus = []
    plan_entries = payload.get('plan')
    inferred_plan = []
    if isinstance(plan_entries, list):
        for entry in plan_entries:
            if isinstance(entry, dict):
                entry_focus = entry.get('focus')
                if isinstance(entry_focus, list):
                    inferred_focus.extend(_normalize_focus(entry_focus))
                text_fields = [
                    entry.get('task'),
                    entry.get('step'),
                    entry.get('description'),
                    entry.get('summary'),
                ]
                for text_value in text_fields:
                    if isinstance(text_value, str) and text_value.strip():
                        inferred_plan.append(text_value.strip())
                        break
                else:
                    inferred_plan.append(json.dumps(entry, ensure_ascii=False))
            elif isinstance(entry, str):
                inferred_focus.extend(_extract_focus_from_text(entry))
                inferred_plan.append(entry)
    if inferred_focus:
        seen = set()
        ordered_focus = []
        for item in inferred_focus:
            if item not in seen:
                seen.add(item)
                ordered_focus.append(item)
        focus_targets = ordered_focus
        payload['focus'] = focus_targets
        focus_valid = True
        if inferred_plan:
            payload['plan'] = inferred_plan
        notes_list = payload.get('notes')
        message = "Focus array inferred from plan; include a top-level `focus` list next time."
        if isinstance(notes_list, list):
            notes_list.append(message)
        else:
            payload['notes'] = [message]

if not focus_valid:
    note_focus = []
    notes_field = payload.get('notes')
    if isinstance(notes_field, list):
        for entry in notes_field:
            if not isinstance(entry, str):
                continue
            note_focus.extend(_extract_focus_from_text(entry))
        if note_focus:
            seen = set()
            ordered = []
            for item in note_focus:
                if item not in seen:
                    seen.add(item)
                    ordered.append(item)
            if ordered:
                focus_targets = ordered
                payload['focus'] = focus_targets
                focus_valid = True
                reminder = "Focus array inferred from notes; include a top-level `focus` list next time."
                if isinstance(notes_field, list):
                    notes_field.append(reminder)
                else:
                    payload['notes'] = [reminder]

if not focus_valid:
    pending_changes = payload.get('changes')
    pending_commands = payload.get('commands')
    has_changes = isinstance(pending_changes, list) and any(pending_changes)
    has_commands = isinstance(pending_commands, list) and any(pending_commands)
    if not has_changes and not has_commands:
        focus_targets = []
        payload['focus'] = focus_targets
        focus_valid = True
        notes_list = payload.get('notes')
        message = (
            "Focus omitted because no changes or commands were provided in this response; "
            "declare explicit targets once edits begin."
        )
        if isinstance(notes_list, list):
            notes_list.append(message)
        else:
            payload['notes'] = [message]

if not focus_valid:
    print('STATUS parse-error')
    print('NOTE Focus targets missing or invalid. Add a `focus` array listing the exact files or symbols you will touch, then rerun work-on-tasks.')
    sys.exit(1)

# Normalize change payloads so legacy formats (missing `type`, raw diff strings)
# still apply cleanly without aborting the task workflow.
raw_changes = payload.get('changes') or []
changes = []
for entry in raw_changes:
    if isinstance(entry, str):
        changes.append({
            'type': 'patch',
            'diff': entry,
        })
        continue
    if not isinstance(entry, dict):
        raise ValueError('Change entries must be objects or unified diff strings')
    normalized = dict(entry)
    ctype = normalized.get('type')
    if not ctype:
        if normalized.get('diff'):
            normalized['type'] = 'patch'
        elif 'content' in normalized:
            normalized['type'] = 'file'
    changes.append(normalized)

written = []
patched = []
manual_notes = []

def rewrite_patch_paths(diff_text: str) -> str:
    mapping = {
        'api/': 'apps/api/',
        'web/': 'apps/web/',
        'admin/': 'apps/admin/',
        'site/': 'apps/web/',
    }

    def rewrite_path(path: str) -> str:
        for old, new in mapping.items():
            if path.startswith(old) and not path.startswith(new):
                return new + path[len(old):]
        return path

    lines = diff_text.splitlines()
    rewritten = []
    for line in lines:
        if line.startswith('diff --git a/'):
            parts = line.split()
            if len(parts) >= 4:
                a_path = parts[2][2:]
                b_path = parts[3][2:]
                new_a = rewrite_path(a_path)
                new_b = rewrite_path(b_path)
                if new_a != a_path or new_b != b_path:
                    line = f"diff --git a/{new_a} b/{new_b}"
        elif line.startswith('--- a/'):
            path = line[6:]
            new_path = rewrite_path(path)
            if new_path != path:
                line = f"--- a/{new_path}"
        elif line.startswith('+++ b/'):
            path = line[6:]
            new_path = rewrite_path(path)
            if new_path != path:
                line = f"+++ b/{new_path}"
        rewritten.append(line)
    return '\n'.join(rewritten)

def ensure_diff_headers(diff_text: str, path: str) -> str:
    if 'diff --git ' in diff_text:
        return diff_text

    lines = diff_text.splitlines()
    header = [
        f'diff --git a/{path} b/{path}',
        f'--- a/{path}',
        f'+++ b/{path}',
    ]
    return '\n'.join(header + lines)

def extract_path_from_diff(diff_text: str) -> Optional[str]:
    for line in diff_text.splitlines():
        if line.startswith('+++ b/'):
            candidate = line[6:].strip()
            if candidate and candidate != '/dev/null':
                return candidate
    return None

def ensure_within_root(path: Path) -> Path:
    try:
        full = (project_root / path).resolve(strict=False)
        project = project_root.resolve(strict=True)
    except FileNotFoundError:
        project = project_root.resolve()
        full = (project_root / path).resolve(strict=False)
    if not str(full).startswith(str(project)):
        raise ValueError(f"Path {path} escapes project root")
    return full

for change in changes:
    ctype = change.get('type')
    path = change.get('path')
    if not path:
        if ctype == 'patch':
            inferred = extract_path_from_diff(change.get('diff') or '')
            if inferred:
                path = inferred
                change['path'] = path
    if not path:
        raise ValueError('Change entry missing path')
    if ctype == 'file':
        content = change.get('content', '')
        dest = ensure_within_root(Path(path))
        dest.parent.mkdir(parents=True, exist_ok=True)
        if dest.exists() and dest.is_dir():
            try:
                rel_path = str(dest.relative_to(project_root))
            except ValueError:
                rel_path = str(dest)
            manual_notes.append(
                f"Skipped writing {rel_path} because that path already exists as a directory."
            )
            continue
        dest.write_text(content, encoding='utf-8')
        written.append(str(dest.relative_to(project_root)))
    elif ctype == 'patch':
        diff = change.get('diff')
        if not diff:
            raise ValueError(f"Patch change for {path} missing diff")
        diff = rewrite_patch_paths(diff)
        diff = ensure_diff_headers(diff, path)
        if not diff.endswith('\n'):
            diff += '\n'
        proc = subprocess.run(
            ['git', 'apply', '--whitespace=nowarn', '-'],
            input=diff.encode('utf-8'),
            cwd=str(project_root),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=False,
        )
        if proc.returncode != 0:
            git_err = proc.stderr.decode('utf-8')

            three_way = subprocess.run(
                ['git', 'apply', '--3way', '--whitespace=nowarn', '-'],
                input=diff.encode('utf-8'),
                cwd=str(project_root),
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                check=False,
            )

            if three_way.returncode == 0:
                patched.append(path + ' (3way)')
                continue

            git_err += three_way.stderr.decode('utf-8')

            fallback = subprocess.run(
                ['patch', '-p1', '--forward', '--silent'],
                input=diff.encode('utf-8'),
                cwd=str(project_root),
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                check=False,
            )
            if fallback.returncode != 0:
                # check if patch already applied
                already = subprocess.run(
                    ['git', 'apply', '--reverse', '--check', '-'],
                    input=diff.encode('utf-8'),
                    cwd=str(project_root),
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    check=False,
                )
                if already.returncode == 0:
                    patched.append(path + ' (already applied)')
                    continue

                new_content = None
                diff_lines = diff.splitlines()
                multi_file = sum(1 for line in diff_lines if line.startswith('diff --git ')) > 1
                if not multi_file and any(line.startswith('--- /dev/null') for line in diff_lines):
                    capture = False
                    content_lines = []
                    for line in diff_lines:
                        if line.startswith('@@'):
                            capture = True
                            continue
                        if not capture:
                            continue
                        if not line or line.startswith('diff --git'):
                            continue
                        if line.startswith('+'):
                            content_lines.append(line[1:])
                        elif line.startswith('-') or line.startswith('---') or line.startswith('+++'):
                            continue
                        elif line.startswith('\\'):
                            continue
                        else:
                            content_lines.append(line)
                    if content_lines:
                        new_content = '\n'.join(content_lines)
                        if not new_content.endswith('\n'):
                            new_content += '\n'
                if new_content is not None:
                    dest = ensure_within_root(Path(path))
                    if dest.exists() and dest.is_dir():
                        new_content = None
                    else:
                        dest.parent.mkdir(parents=True, exist_ok=True)
                        if dest.exists():
                            existing = dest.read_text(encoding='utf-8')
                            if existing == new_content:
                                patched.append(path + ' (already exists)')
                                continue
                        dest.write_text(new_content, encoding='utf-8')
                        patched.append(path + ' (reconstructed)')
                        continue

                if new_content is None:
                    try:
                        proc_noctx = subprocess.run(
                            ['git', 'apply', '--reject', '--whitespace=nowarn', '-'],
                            input=diff.encode('utf-8'),
                            cwd=str(project_root),
                            stdout=subprocess.PIPE,
                            stderr=subprocess.PIPE,
                            check=False,
                        )
                        if proc_noctx.returncode == 0:
                            patched.append(path + ' (partial apply)')
                            continue
                        else:
                            git_err += proc_noctx.stderr.decode('utf-8')
                    except Exception:
                        pass

                manual_patch = output_path.with_suffix(output_path.suffix + f".{len(manual_notes)+1}.patch")
                manual_patch.write_text(diff, encoding='utf-8')
                relative_manual = manual_patch
                try:
                    relative_manual = manual_patch.relative_to(project_root)
                except ValueError:
                    pass
                manual_notes.append(f"Patch for {path} could not be applied automatically. Review and apply {relative_manual} manually.")
                patched.append(path + ' (manual)')
                sys.stderr.write(git_err)
                sys.stderr.write(fallback.stderr.decode('utf-8'))
                continue
            patched.append(path + ' (patch)')
        else:
            patched.append(path)
    else:
        raise ValueError(f"Unknown change type: {ctype}")

summary = {
    'written': written,
    'patched': patched,
    'commands': payload.get('commands') or [],
    'notes': (payload.get('notes') or []) + manual_notes,
}
print('APPLIED')
for path in written:
    print(f"WRITE {path}")
for path in patched:
    print(f"PATCH {path}")
for cmd in summary['commands']:
    print(f"CMD {cmd}")
for note in summary['notes']:
    print(f"NOTE {note}")
PY
}

cmd_create_jira_tasks() {
  local args=()
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project)
        args+=(--project "$(abs_path "$2")")
        shift 2
        ;;
      --model)
        args+=(--model "$2")
        shift 2
        ;;
      --force|--dry-run)
        args+=("$1")
        shift
        ;;
      -h|--help)
        # Let the dedicated CLI script handle help output.
        args+=("$1")
        shift
        ;;
      *)
        args+=("$1")
        shift
        ;;
    esac
  done

  local shell_bin="${BASH:-bash}"
  "$shell_bin" "$CLI_ROOT/src/cli/create-jira-tasks.sh" "${args[@]}"
}

cmd_create_tasks() {
  local root="" jira="" force=0
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --jira) jira="$(abs_path "$2")"; shift 2;;
      --force) force=1; shift;;
      -h|--help)
        cat <<'EOUSAGE'
Usage: gpt-creator create-tasks [--project PATH] [--jira FILE] [--force]

Convert Jira markdown tasks into a project-scoped SQLite database stored under .gpt-creator/staging/plan/tasks.

Options:
  --project PATH  Project root (defaults to current directory)
  --jira FILE     Jira markdown source (defaults to staging/inputs/jira.md)
  --force         Rebuild the tasks database without restoring prior progress metadata
EOUSAGE
        return 0
        ;;
      *) break;;
    esac
  done

  ensure_ctx "$root"
  [[ -n "$jira" ]] || jira="${INPUT_DIR}/jira.md"
  [[ -f "$jira" ]] || die "Jira tasks file not found: ${jira}"

  local tasks_dir="${PLAN_DIR}/tasks"
  local parsed_local="${tasks_dir}/parsed.local.json"
  local tasks_db="${tasks_dir}/tasks.db"
  mkdir -p "$tasks_dir"

  info "Parsing Jira backlog → ${parsed_local}"
  gc_parse_jira_tasks "$jira" "$parsed_local"

  info "Building tasks database → ${tasks_db}"
  local db_stats
  if ! db_stats="$(gc_build_tasks_db "$parsed_local" "$tasks_db" "$force")"; then
    die "Failed to build Jira tasks SQLite database"
  fi

  local story_count=0 task_count=0 restored_stories=0 restored_tasks=0
  while IFS= read -r line; do
    case "$line" in
      STORIES\ *) story_count="${line#STORIES }" ;;
      TASKS\ *) task_count="${line#TASKS }" ;;
      RESTORED_STORIES\ *) restored_stories="${line#RESTORED_STORIES }" ;;
      RESTORED_TASKS\ *) restored_tasks="${line#RESTORED_TASKS }" ;;
    esac
  done <<<"$db_stats"

  info "Stories: ${story_count:-0} (restored: ${restored_stories:-0})"
  info "Tasks: ${task_count:-0} (restored statuses: ${restored_tasks:-0})"
  ok "Tasks database updated → ${tasks_db}"

  local legacy_manifest="${tasks_dir}/manifest.json"
  local legacy_stories="${tasks_dir}/stories"
  if [[ -f "$legacy_manifest" || -d "$legacy_stories" ]]; then
    warn "Legacy task manifest/JSON artifacts detected under ${tasks_dir}; they are no longer used now that tasks live in SQLite. Remove them when convenient to avoid confusion."
  fi
}

cmd_backlog() {
  local root="" type_arg="" item_children="" show_progress=0 task_details=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project|--root)
        root="$(abs_path "$2")"
        shift 2
        ;;
      --type)
        type_arg="$2"
        shift 2
        ;;
      --item-children)
        item_children="$2"
        shift 2
        ;;
      --progress)
        show_progress=1
        shift
        ;;
      --task-details)
        task_details="$2"
        shift 2
        ;;
      -h|--help)
        cat <<'EOUSAGE'
Usage: gpt-creator backlog [--project PATH|--root PATH]
                           [--type epics|stories]
                           [--item-children ID]
                           [--progress]
                           [--task-details ID]

Inspect the backlog stored in .gpt-creator/staging/plan/tasks/tasks.db without an interactive prompt.

  --type epics|stories     List backlog summaries (defaults to 'epics' when no other flag is provided).
  --item-children ID       Show the direct children of the epic/story identified by ID (slug, key, or ID).
  --progress               Print an overall progress bar summarising task completion.
  --task-details ID        Show a detailed view for the matching task ID (case-insensitive).

Pass any combination of the flags above; each requested view is printed sequentially.
EOUSAGE
        return 0
        ;;
      *)
        die "Unknown argument for backlog: $1"
        ;;
    esac
  done

  if [[ -z "$type_arg" && -z "$item_children" && "$show_progress" -eq 0 && -z "$task_details" ]]; then
    type_arg="epics"
  fi

  ensure_ctx "$root"
  local tasks_db="${PLAN_DIR}/tasks/tasks.db"
  if [[ ! -f "$tasks_db" ]]; then
    die "Tasks database not found at ${tasks_db}. Run 'gpt-creator create-tasks' first."
  fi

  python3 - <<'PY' "$tasks_db" "${type_arg:-}" "${item_children:-}" "$show_progress" "${task_details:-}"
import sqlite3
import sys
import textwrap

db_path, type_arg, item_children, progress_flag, task_details = sys.argv[1:6]

conn = sqlite3.connect(db_path)
conn.row_factory = sqlite3.Row

def pluralize(value, singular, plural=None):
    try:
        count = int(value or 0)
    except Exception:
        count = 0
    if count == 1:
        return f"{count} {singular}"
    return f"{count} {plural or singular + 's'}"

def empty_counts():
    return {
        "stories": 0,
        "stories_complete": 0,
        "stories_in_progress": 0,
        "stories_pending": 0,
        "tasks": 0,
        "tasks_complete": 0,
        "tasks_in_progress": 0,
        "tasks_pending": 0,
    }

def fetch_stories():
    query = """
        SELECT story_slug, story_id, story_title, epic_key, epic_title,
               status, completed_tasks, total_tasks, sequence
        FROM stories
        ORDER BY COALESCE(sequence, 0), story_title COLLATE NOCASE
    """
    return [dict(row) for row in conn.execute(query)]

def fetch_epics():
    query = """
        SELECT epic_key, epic_id, title, slug
        FROM epics
        ORDER BY title COLLATE NOCASE
    """
    return [dict(row) for row in conn.execute(query)]

def fetch_task_counts():
    query = """
        SELECT story_slug,
               COUNT(*) AS total,
               SUM(CASE WHEN LOWER(COALESCE(status, '')) = 'complete' THEN 1 ELSE 0 END) AS completed,
               SUM(CASE WHEN LOWER(COALESCE(status, '')) = 'in-progress' THEN 1 ELSE 0 END) AS in_progress
        FROM tasks
        GROUP BY story_slug
    """
    counts = {}
    for row in conn.execute(query):
        counts[row["story_slug"]] = {
            "total": row["total"] or 0,
            "completed": row["completed"] or 0,
            "in_progress": row["in_progress"] or 0,
        }
    return counts

def fetch_tasks_for_story(slug):
    query = """
        SELECT position, task_id, title, status, estimate
        FROM tasks
        WHERE story_slug = ?
        ORDER BY position
    """
    return [dict(row) for row in conn.execute(query, (slug,))]

UNASSIGNED_KEY = "__unassigned__"
UNASSIGNED_LABEL = "Unassigned backlog"

def canonical_epic_descriptor(epic_id=None, epic_slug=None, epic_title=None):
    epic_id = (epic_id or "").strip()
    epic_slug = (epic_slug or "").strip()
    epic_title = (epic_title or "").strip()
    for candidate in (epic_slug, epic_id, epic_title):
        if candidate:
            norm = candidate.lower()
            display_title = epic_title or epic_slug or epic_id
            return norm, epic_id, epic_slug, display_title
    return UNASSIGNED_KEY, "", "", UNASSIGNED_LABEL

def derive_pseudo_epic(identifier):
    if not identifier:
        return None
    clean = identifier.replace("/", "-").replace("_", "-")
    parts = [part for part in clean.split("-") if part]
    if len(parts) >= 2:
        return "-".join(parts[:2]).upper()
    if len(parts) == 1:
        return parts[0].upper()
    return None

def determine_epic_for_story(story):
    norm, epic_id, epic_slug, epic_title = canonical_epic_descriptor(
        story.get("epic_id"),
        story.get("epic_key"),
        story.get("epic_title"),
    )
    if norm != UNASSIGNED_KEY:
        return norm, epic_id, epic_slug or epic_id.lower(), epic_title

    for candidate in (story.get("story_id"), story.get("story_slug")):
        pseudo = derive_pseudo_epic((candidate or "").strip())
        if pseudo:
            return pseudo.lower(), pseudo, pseudo.lower(), pseudo
    return norm, epic_id, epic_slug, epic_title

def format_epic_label(epic_id, epic_slug, epic_title):
    epic_title = (epic_title or "").strip()
    epic_id = (epic_id or "").strip()
    epic_slug = (epic_slug or "").strip()
    if epic_title and epic_title != UNASSIGNED_LABEL:
        if epic_id and epic_id.lower() not in epic_title.lower():
            return f"{epic_title} [{epic_id}]"
        return epic_title
    if epic_id:
        return epic_id
    if epic_slug:
        return epic_slug
    return UNASSIGNED_LABEL

def summarise_epics(stories, task_counts):
    summary = {}
    for story in stories:
        norm_key, epic_id, epic_slug, epic_title = determine_epic_for_story(story)
        entry = summary.setdefault(
            norm_key,
            {
                "counts": empty_counts(),
                "epic_id": epic_id,
                "epic_slug": epic_slug,
                "epic_title": epic_title,
            },
        )
        if epic_id and not entry["epic_id"]:
            entry["epic_id"] = epic_id
        if epic_slug and not entry["epic_slug"]:
            entry["epic_slug"] = epic_slug
        story_epic_title = (story.get("epic_title") or "").strip()
        if story_epic_title and entry["epic_title"] in (UNASSIGNED_LABEL, "", None):
            entry["epic_title"] = story_epic_title

        counts = entry["counts"]
        counts["stories"] += 1
        status = (story.get("status") or "pending").strip().lower()
        if status == "complete":
            counts["stories_complete"] += 1
        elif status in {"in-progress", "in progress"}:
            counts["stories_in_progress"] += 1
        else:
            counts["stories_pending"] += 1

        counts_dict = task_counts.get(story["story_slug"], {})
        total = counts_dict.get("total", story.get("total_tasks") or 0) or 0
        completed = counts_dict.get("completed", story.get("completed_tasks") or 0) or 0
        in_progress = counts_dict.get("in_progress", 0) or 0
        pending = max(total - completed - in_progress, 0)

        counts["tasks"] += total
        counts["tasks_complete"] += completed
        counts["tasks_in_progress"] += in_progress
        counts["tasks_pending"] += pending
    return summary

def build_epic_entries(epics, stories, summary):
    epics_by_norm = {}
    for epic in epics:
        norm, epic_id, epic_slug, epic_title = canonical_epic_descriptor(
            epic.get("epic_id"),
            epic.get("epic_key") or epic.get("slug"),
            epic.get("title"),
        )
        epics_by_norm[norm] = {
            "epic_id": epic_id,
            "slug": (epic.get("slug") or epic.get("epic_key") or epic_slug or "").strip(),
            "title": epic_title or format_epic_label(epic_id, epic_slug, epic.get("title")),
            "raw": dict(epic),
        }

    stories_by_norm = {}
    for story in stories:
        norm, _, _, _ = determine_epic_for_story(story)
        stories_by_norm.setdefault(norm, []).append(story)

    entries = []
    all_keys = set(summary.keys()) | set(epics_by_norm.keys())
    if not all_keys:
        all_keys.add(UNASSIGNED_KEY)

    for norm in all_keys:
        meta = summary.get(norm, {})
        epic_info = epics_by_norm.get(norm, {})
        counts = meta.get("counts") or empty_counts()
        epic_id = meta.get("epic_id") or epic_info.get("epic_id") or ""
        epic_slug = meta.get("epic_slug") or epic_info.get("slug") or ""
        epic_title = meta.get("epic_title") or epic_info.get("title") or UNASSIGNED_LABEL
        label = format_epic_label(epic_id, epic_slug, epic_title)
        entries.append(
            {
                "key": None if norm == UNASSIGNED_KEY else norm,
                "label": label,
                "counts": counts,
                "stories": stories_by_norm.get(norm, []),
                "epic": {
                    "epic_id": epic_id,
                    "slug": epic_slug,
                    "title": epic_title,
                },
            }
        )

    entries.sort(key=lambda item: (item["key"] is None, item["label"].lower()))
    return entries

def print_table(headers, rows):
    if not rows:
        print("No records found.")
        return
    widths = [len(str(h)) for h in headers]
    for row in rows:
        for idx, cell in enumerate(row):
            widths[idx] = max(widths[idx], len(str(cell)))

    def fmt(row):
        return "  ".join(str(cell).ljust(widths[idx]) for idx, cell in enumerate(row))

    print(fmt(list(headers)))
    print("  ".join("-" * w for w in widths))
    for row in rows:
        print(fmt(row))

stories = fetch_stories()
epics = fetch_epics()
task_counts = fetch_task_counts()
summary = summarise_epics(stories, task_counts)
entries = build_epic_entries(epics, stories, summary)

stories_by_slug = {}
stories_by_id = {}
for story in stories:
    slug = (story.get("story_slug") or "").strip().lower()
    if slug:
        stories_by_slug[slug] = story
    sid = (story.get("story_id") or "").strip().lower()
    if sid:
        stories_by_id[sid] = story

epic_lookup = {}
for entry in entries:
    epic = entry.get("epic") or {}
    for candidate in (epic.get("slug"), epic.get("epic_id"), epic.get("title"), entry["label"]):
        if candidate and str(candidate).strip():
            epic_lookup[str(candidate).strip().lower()] = entry
    if entry["key"] is None:
        for alias in ("unassigned", "none", "no-epic", "noepic"):
            epic_lookup[alias] = entry

def print_epics_table():
    if not entries:
        print("No epics found in the backlog.")
        return
    headers = ["Epic ID", "Slug", "Title", "Stories", "Tasks", "Progress"]
    rows = []
    for entry in entries:
        counts = entry.get("counts") or empty_counts()
        epic = entry.get("epic") or {}
        epic_id = (epic.get("epic_id") or "").strip() or "-"
        slug = (epic.get("slug") or "").strip() or "-"
        title = entry["label"]
        stories_desc = pluralize(counts["stories"], "story", "stories")
        story_bits = []
        if counts["stories_complete"]:
            story_bits.append(f"{counts['stories_complete']} complete")
        if counts["stories_in_progress"]:
            story_bits.append(f"{counts['stories_in_progress']} in-progress")
        if counts["stories_pending"]:
            story_bits.append(f"{counts['stories_pending']} pending")
        if story_bits:
            stories_desc += f" ({', '.join(story_bits)})"

        tasks_desc = pluralize(counts["tasks"], "task")
        task_bits = []
        if counts["tasks_complete"]:
            task_bits.append(f"{counts['tasks_complete']} complete")
        if counts["tasks_in_progress"]:
            task_bits.append(f"{counts['tasks_in_progress']} in-progress")
        if counts["tasks_pending"]:
            task_bits.append(f"{counts['tasks_pending']} pending")
        if task_bits:
            tasks_desc += f" ({', '.join(task_bits)})"

        total_tasks = counts["tasks"] or 0
        progress = 0.0
        if total_tasks:
            progress = (counts["tasks_complete"] / total_tasks) * 100
        rows.append([
            epic_id,
            slug,
            title,
            stories_desc,
            tasks_desc,
            f"{progress:5.1f}%",
        ])
    print_table(headers, rows)

def print_story_children(entry, identifier):
    stories_for_epic = entry.get("stories") or []
    if not stories_for_epic:
        print(f"No stories found for epic '{identifier}'.")
        return
    headers = ["Story Slug", "Title", "Status", "Epic", "Tasks", "Progress"]
    rows = []
    for story in sorted(stories_for_epic, key=lambda s: (s.get("sequence") or 0, (s.get("story_title") or "").lower())):
        slug = (story.get("story_slug") or "").strip()
        title = (story.get("story_title") or story.get("story_id") or slug or "Story").strip()
        epic_title = (story.get("epic_title") or entry["label"]).strip()
        counts = task_counts.get(story.get("story_slug"), {})
        total = counts.get("total", story.get("total_tasks") or 0) or 0
        complete = counts.get("completed", story.get("completed_tasks") or 0) or 0
        in_progress = counts.get("in_progress", 0) or 0
        pending = max(total - complete - in_progress, 0)
        status, progress, tasks_desc = compute_story_metrics(total, complete, in_progress, pending, story.get("status"))
        rows.append([
            slug or (story.get("story_id") or "-"),
            title,
            status,
            epic_title,
            tasks_desc,
            f"{progress:5.1f}%",
        ])
    print_table(headers, rows)

def truncate(text, width=60):
    text = (text or "").strip()
    if not text:
        return "-"
    if len(text) <= width:
        return text
    return text[: width - 3] + "..."

def print_task_children(story):
    slug = story.get("story_slug")
    tasks = fetch_tasks_for_story(slug)
    if not tasks:
        print(f"No tasks found for story '{slug or story.get('story_id') or story.get('story_title')}'.")
        return
    headers = ["#", "Task ID", "Title", "Status", "Estimate"]
    rows = []
    for task in tasks:
        position = task.get("position")
        index = str((position if position is not None else 0) + 1)
        task_id = (task.get("task_id") or "").strip() or "-"
        title = truncate(task.get("title"), width=80)
        status = (task.get("status") or "pending").strip().lower().replace("_", "-")
        estimate = (task.get("estimate") or "").strip() or "-"
        rows.append([index, task_id, title, status, estimate])
    print_table(headers, rows)

def compute_story_metrics(total, complete, in_progress, pending, status_field):
    status_field = (status_field or "pending").strip().lower().replace("_", "-")
    if total > 0:
        if complete >= total and in_progress == 0 and pending == 0:
            status = "complete"
            progress = 100.0
        elif complete > 0 or in_progress > 0:
            status = "in-progress"
            progress = (complete / total) * 100
        else:
            status = "pending"
            progress = 0.0
    else:
        status = status_field or "pending"
        progress = 100.0 if status == "complete" else 0.0

    if total > 0:
        tasks_desc = f"{complete}/{total} complete"
        if in_progress:
            tasks_desc += f", {in_progress} in-progress"
        if pending and status != "complete":
            tasks_desc += f", {pending} pending"
    else:
        tasks_desc = "0 tasks"

    return status, progress, tasks_desc

def show_item_children(identifier):
    if not identifier:
        return
    ident = identifier.strip().lower()
    entry = epic_lookup.get(ident)
    if entry:
        label = entry["label"]
        epic = entry.get("epic") or {}
        epic_ident = (epic.get("epic_id") or epic.get("slug") or entry["label"] or "unassigned").strip()
        print(f"Stories for epic: {label} ({epic_ident})")
        print_story_children(entry, identifier)
        return

    if ident in stories_by_slug:
        story = stories_by_slug[ident]
    elif ident in stories_by_id:
        story = stories_by_id[ident]
    else:
        story = None

    if story:
        title = story.get("story_title") or story.get("story_id") or story.get("story_slug")
        print(f"Tasks for story: {title} ({story.get('story_slug')})")
        print_task_children(story)
        return

    print(f"No epic or story found for identifier '{identifier}'.", file=sys.stderr)
    sys.exit(1)

def print_stories_overview():
    if not stories:
        print("No stories found in the backlog.")
        return
    headers = ["Story Slug", "Story ID", "Title", "Epic", "Status", "Tasks", "Progress"]
    rows = []
    for story in sorted(
        stories,
        key=lambda s: (
            (s.get("epic_title") or "").lower(),
            s.get("sequence") or 0,
            (s.get("story_title") or "").lower(),
        ),
    ):
        slug = (story.get("story_slug") or "").strip()
        story_id = (story.get("story_id") or "").strip()
        title = (story.get("story_title") or story_id or slug or "Story").strip()
        epic_title = (story.get("epic_title") or "Unassigned").strip()
        counts = task_counts.get(story.get("story_slug"), {})
        total = counts.get("total", story.get("total_tasks") or 0) or 0
        complete = counts.get("completed", story.get("completed_tasks") or 0) or 0
        in_progress = counts.get("in_progress", 0) or 0
        pending = max(total - complete - in_progress, 0)
        status, progress, tasks_desc = compute_story_metrics(total, complete, in_progress, pending, story.get("status"))
        rows.append([
            slug or "-",
            story_id or "-",
            title,
            epic_title,
            status,
            tasks_desc,
            f"{progress:5.1f}%",
        ])
    print_table(headers, rows)

def print_task_details(task_identifier):
    if not task_identifier:
        return
    ident = task_identifier.strip().lower()
    query = """
        SELECT *
        FROM tasks
        WHERE LOWER(COALESCE(task_id, '')) = ?
           OR CAST(id AS TEXT) = ?
    """
    row = conn.execute(query, (ident, ident)).fetchone()
    if row is None:
        print(f"No task found for identifier '{task_identifier}'.", file=sys.stderr)
        sys.exit(1)

    print("Task details")
    print("------------")

    def emit(label, value):
        text = value if isinstance(value, str) else ("" if value is None else str(value))
        if isinstance(text, str):
            text = text.strip()
        print(f"{label}: {text if text else '-'}")

    emit("Task ID", row["task_id"])
    emit("Story Slug", row["story_slug"])
    emit("Story Title", row["story_title"])
    emit("Epic", row["epic_title"] or row["epic_key"])
    emit("Status", row["status"])
    emit("Estimate", row["estimate"])
    emit("Assignees", row["assignee_text"])
    emit("Tags", row["tags_text"])
    emit("Dependencies", row["dependencies_text"])
    emit("Story Points", row["story_points"])
    emit("Document Reference", row["document_reference"])
    emit("Idempotency", row["idempotency"])
    emit("Rate Limits", row["rate_limits"])
    emit("RBAC", row["rbac"])
    emit("Messaging/Workflows", row["messaging_workflows"])
    emit("Performance Targets", row["performance_targets"])
    emit("Observability", row["observability"])
    emit("Endpoints", row["endpoints"])
    emit("Sample Create Request", row["sample_create_request"])
    emit("Sample Create Response", row["sample_create_response"])
    emit("Acceptance Criteria", row["acceptance_text"])
    emit("User Story Ref", row["user_story_ref_id"])
    emit("Epic Ref", row["epic_ref_id"])
    emit("Started At", row["started_at"])
    emit("Completed At", row["completed_at"])
    emit("Last Run", row["last_run"])
    emit("Created At", row["created_at"])
    emit("Updated At", row["updated_at"])

def print_progress():
    row = conn.execute(
        """
        SELECT
          COUNT(*) AS total,
          SUM(CASE WHEN LOWER(COALESCE(status, '')) = 'complete' THEN 1 ELSE 0 END) AS complete,
          SUM(CASE WHEN LOWER(COALESCE(status, '')) = 'in-progress' THEN 1 ELSE 0 END) AS in_progress
        FROM tasks
        """
    ).fetchone()
    total = row["total"] or 0
    complete = row["complete"] or 0
    in_progress = row["in_progress"] or 0
    pending = max(total - complete - in_progress, 0)
    percent = (complete / total * 100) if total else 0.0
    bar_length = 30
    filled_units = int(round((percent / 100) * bar_length))
    filled_units = min(bar_length, max(0, filled_units))
    bar = "#" * filled_units + "-" * (bar_length - filled_units)
    print("Overall backlog progress")
    print(f"Tasks complete: {complete}/{total} ({percent:0.1f}%)")
    print(f"In-progress: {in_progress}, Pending: {pending}")
    print(f"[{bar}]")

try:
    printed = False
    if type_arg:
        t = type_arg.strip().lower()
        if t == "epics":
            print_epics_table()
            printed = True
        elif t == "stories":
            print_stories_overview()
            printed = True
        else:
            print(f"Unsupported backlog type: {type_arg}", file=sys.stderr)
            sys.exit(1)
        if printed and (item_children or progress_flag == "1" or task_details):
            print()
    if item_children:
        show_item_children(item_children)
        printed = True
        if progress_flag == "1" or task_details:
            print()
    if progress_flag == "1":
        print_progress()
        printed = True
        if task_details:
            print()
    if task_details:
        print_task_details(task_details)
finally:
    conn.close()
PY
}

cmd_migrate_tasks_json() {
  local root="" force=0
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --force) force=1; shift;;
      -h|--help)
        cat <<'EOUSAGE'
Usage: gpt-creator migrate-tasks [--project PATH] [--force]

Populate the tasks SQLite database from the JSON outputs produced by
`gpt-creator create-jira-tasks` (under plan/create-jira-tasks/json).

Options:
  --project PATH  Project root (defaults to current directory)
  --force         Rebuild the database without restoring prior task status metadata
EOUSAGE
        return 0
        ;;
      *) break;;
    esac
  done

  ensure_ctx "$root"

  local pipeline_dir="${PLAN_DIR}/create-jira-tasks"
  local json_dir="${pipeline_dir}/json"
  local epics_json="${json_dir}/epics.json"
  local stories_dir="${json_dir}/stories"
  local tasks_dir="${json_dir}/tasks"

  [[ -f "$epics_json" ]] || die "Epics JSON not found: ${epics_json}"
  [[ -d "$stories_dir" ]] || die "Stories JSON directory not found: ${stories_dir}"
  [[ -d "$tasks_dir" ]] || die "Tasks JSON directory not found: ${tasks_dir}"

  local payload="${json_dir}/tasks_payload.json"
  local tasks_workspace="${PLAN_DIR}/tasks"
  mkdir -p "$tasks_workspace"
  local db_path="${tasks_workspace}/tasks.db"

  info "Building tasks payload from JSON → ${payload}"
  info "Updating tasks database → ${db_path}"

  if ! gc_rebuild_tasks_db_from_json "$force"; then
    die "Failed to rebuild tasks database from JSON payload"
  fi

  ok "Tasks database updated → ${db_path}"
}

cmd_refine_tasks() {
  local root="" story_filter="" model_override="" dry_run=0 force=0
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --story) story_filter="$2"; shift 2;;
      --model) model_override="$2"; shift 2;;
      --dry-run) dry_run=1; shift;;
      --force) force=1; shift;;
      -h|--help)
        cat <<'EOUSAGE'
Usage: gpt-creator refine-tasks [--project PATH] [--story SLUG] [--model NAME] [--dry-run] [--force]

Refine tasks stored in the SQLite backlog using Codex, updating each task
record and the refined JSON artifacts as soon as a task is enriched.

Options:
  --project PATH  Project root (defaults to current directory)
  --story SLUG    Limit refinement to a single story slug (optional)
  --model NAME    Override Codex model (defaults to CODEX_MODEL/GC defaults)
  --dry-run       Build prompts without invoking Codex
  --force         Reset refinement progress and reprocess every task
EOUSAGE
        return 0
        ;;
      *) break;;
    esac
  done

  ensure_ctx "$root"

  local tasks_db="${PLAN_DIR}/tasks/tasks.db"
  [[ -f "$tasks_db" ]] || die "Tasks database not found: ${tasks_db}"

  local pipeline_dir="${PLAN_DIR}/create-jira-tasks"
  local json_tasks_dir="${pipeline_dir}/json/tasks"
  [[ -d "$json_tasks_dir" ]] || die "Tasks JSON directory not found: ${json_tasks_dir}"

  local have_refined
  have_refined=$(python3 - <<'PY' "$tasks_db"
import sqlite3, sys

path = sys.argv[1]
conn = sqlite3.connect(path)
cur = conn.cursor()
cur.execute("PRAGMA table_info(tasks)")
cols = {row[1] for row in cur.fetchall()}
added = False
if "refined" not in cols:
    cur.execute("ALTER TABLE tasks ADD COLUMN refined INTEGER DEFAULT 0")
    added = True
if "refined_at" not in cols:
    cur.execute("ALTER TABLE tasks ADD COLUMN refined_at TEXT")
    added = True
if added:
    conn.commit()
cur.execute("PRAGMA table_info(tasks)")
cols = {row[1] for row in cur.fetchall()}
conn.close()
print("1" if "refined" in cols else "0")
PY
  )

  local summary
  summary=$(python3 - <<'PY' "$tasks_db" "$story_filter"
import sqlite3, sys

db_path = sys.argv[1]
filters_raw = sys.argv[2] if len(sys.argv) > 2 else ''
filters = {value.strip().lower() for value in filters_raw.split(',') if value.strip()}

conn = sqlite3.connect(db_path)
cur = conn.cursor()

cur.execute("PRAGMA table_info(tasks)")
cols = {row[1] for row in cur.fetchall()}
have_refined = 'refined' in cols

story_map = {
    (row[0] or '').strip(): (row[1] or '').strip().lower()
    for row in cur.execute("SELECT story_slug, story_id FROM stories")
}

def story_in_scope(slug: str) -> bool:
    lower = slug.lower()
    if not filters:
        return True
    if lower in filters:
        return True
    story_id_lower = story_map.get(slug, '')
    if story_id_lower in filters:
        return True
    return False

if have_refined:
    rows = cur.execute("SELECT story_slug, COALESCE(refined, 0) FROM tasks").fetchall()
else:
    rows = [(slug, 0) for (slug,) in cur.execute("SELECT story_slug FROM tasks").fetchall()]
conn.close()

total_tasks = 0
refined_tasks = 0
stories_total = set()
stories_pending = set()

for slug, refined in rows:
    slug = (slug or '').strip()
    if not slug or not story_in_scope(slug):
        continue
    stories_total.add(slug)
    total_tasks += 1
    try:
        refined_value = int(refined)
    except Exception:
        refined_value = 0
    if refined_value:
        refined_tasks += 1
    else:
        stories_pending.add(slug)

pending_tasks = total_tasks - refined_tasks
print(total_tasks, refined_tasks, pending_tasks, len(stories_total), len(stories_pending))
PY
  ) || die "Failed to summarise tasks backlog"

  local total_tasks refined_tasks pending_tasks total_stories pending_stories
  read -r total_tasks refined_tasks pending_tasks total_stories pending_stories <<<"$summary"

  if (( force )); then
    python3 - <<'PY' "$tasks_db"
import sqlite3, sys
conn = sqlite3.connect(sys.argv[1])
cur = conn.cursor()
cur.execute("UPDATE tasks SET refined = 0, refined_at = NULL")
conn.commit()
conn.close()
PY
    refined_tasks=0
    pending_tasks=$total_tasks
  fi

  info "Backlog summary → tasks: total=${total_tasks}, refined=${refined_tasks}, pending=${pending_tasks}; stories: total=${total_stories}, pending=${pending_stories}${story_filter:+ (filter='${story_filter}')}."

  local codex_cmd="${CODEX_BIN:-${CODEX_CMD:-codex}}"
  if (( dry_run == 0 )) && ! command -v "$codex_cmd" >/dev/null 2>&1; then
    warn "Codex CLI '$codex_cmd' not found; switching to --dry-run."
    dry_run=1
  fi

  local model_name="${model_override:-${CODEX_MODEL:-$GC_DEFAULT_MODEL}}"

  # shellcheck source=src/lib/create-jira-tasks/pipeline.sh
  source "${CLI_ROOT}/src/lib/create-jira-tasks/pipeline.sh"

  local force_flag=0
  local skip_refine=0
  local dry_flag="$dry_run"
  cjt::init "$PROJECT_ROOT" "$model_name" "$force_flag" "$skip_refine" "$dry_flag"
  CJT_DOC_FILES=()
  cjt::build_context_files

  CJT_SYNC_DB=1
  CJT_TASKS_DB_PATH="$tasks_db"
  CJT_IGNORE_REFINE_STATE=1
  CJT_REFINE_FORCE=$force
  CJT_HAVE_REFINED_COLUMN="$have_refined"
  CJT_REFINE_TOTAL_TASKS="$total_tasks"
  CJT_REFINE_REFINED_TASKS="$refined_tasks"
  CJT_REFINE_PENDING_TASKS="$pending_tasks"
  CJT_REFINE_TOTAL_STORIES="$total_stories"
  CJT_REFINE_PENDING_STORIES="$pending_stories"
  if [[ -n "$story_filter" ]]; then
    CJT_ONLY_STORY_SLUG="$story_filter"
  fi

  cjt::refine_tasks
  ok "Task refinement complete"
}

gc_write_task_prompt() {
  local db_path="${1:?tasks db path required}"
  local story_slug="${2:?story slug required}"
  local task_index="${3:?task index required}"
  local prompt_path="${4:?prompt path required}"
  local context_tail="${5:-}"
  local model_name="${6:-$CODEX_MODEL}"
  local project_root="${7:-$PROJECT_ROOT}"
  local staging_dir="${8:-$GC_STAGING_DIR}"
  python3 - "$db_path" "$story_slug" "$task_index" "$prompt_path" "$context_tail" "$model_name" "$project_root" "$staging_dir" <<'PY'
import hashlib
import json
import os
import pathlib
import re
import sqlite3
import sys
from pathlib import Path
from typing import Optional

DB_PATH, STORY_SLUG, TASK_INDEX, PROMPT_PATH, CONTEXT_TAIL_PATH, MODEL_NAME, PROJECT_ROOT, STAGING_DIR = sys.argv[1:9]
TASK_INDEX = int(TASK_INDEX)

conn = sqlite3.connect(DB_PATH)
conn.row_factory = sqlite3.Row
cur = conn.cursor()

story_row = cur.execute(
    'SELECT story_id, story_title, epic_key, epic_title, sequence FROM stories WHERE story_slug = ?',
    (STORY_SLUG,)
).fetchone()
if story_row is None:
    raise SystemExit(f"Story slug not found: {STORY_SLUG}")

task_rows = cur.execute(
    'SELECT task_id, title, description, estimate, assignees_json, tags_json, acceptance_json, dependencies_json, '
    'tags_text, story_points, dependencies_text, assignee_text, document_reference, idempotency, rate_limits, rbac, '
    'messaging_workflows, performance_targets, observability, acceptance_text, endpoints, sample_create_request, '
    'sample_create_response, user_story_ref_id, epic_ref_id, status, last_progress_at, last_progress_run, '
    'last_log_path, last_output_path, last_prompt_path, last_notes_json, last_commands_json, last_apply_status, '
    'last_changes_applied, last_tokens_total, last_duration_seconds '
    'FROM tasks WHERE story_slug = ? ORDER BY position ASC',
    (STORY_SLUG,)
).fetchall()
conn.close()

if TASK_INDEX < 0 or TASK_INDEX >= len(task_rows):
    raise SystemExit(2)

task = task_rows[TASK_INDEX]

def parse_json_list(value):
    if not value:
        return []
    try:
        parsed = json.loads(value)
        if isinstance(parsed, list):
            return [str(item).strip() for item in parsed if str(item).strip()]
    except Exception:
        pass
    return []

def parse_int_field(value):
    if value is None:
        return None
    try:
        return int(value)
    except (TypeError, ValueError):
        try:
            return int(float(str(value).strip()))
        except Exception:
            return None

def format_duration(seconds_value):
    seconds = parse_int_field(seconds_value)
    if seconds is None or seconds <= 0:
        return ""
    minutes, sec = divmod(seconds, 60)
    hours, minutes = divmod(minutes, 60)
    parts = []
    if hours:
        parts.append(f"{hours}h")
    if minutes:
        parts.append(f"{minutes}m")
    if sec or not parts:
        parts.append(f"{sec}s")
    return " ".join(parts)

def clamp_text(text: str, limit: int) -> str:
    if limit <= 0 or not text:
        return text
    if len(text) <= limit:
        return text
    return text[: max(0, limit - 1)].rstrip() + "…"

def build_log_excerpt(path_obj: Path, max_lines: int = 40, max_chars: int = 160) -> list[str]:
    try:
        raw = path_obj.read_text(encoding="utf-8", errors="replace")
    except Exception as exc:
        return [f"(unable to read log: {exc})"]
    lines_local = raw.splitlines()
    if not lines_local:
        return []
    excerpt = []
    if len(lines_local) > max_lines:
        trimmed = len(lines_local) - max_lines
        excerpt.append(f"... trimmed {trimmed} earlier line(s) ...")
        subset = lines_local[-max_lines:]
    else:
        subset = lines_local
    for line in subset:
        excerpt.append(clamp_text(line, max_chars))
    return excerpt

def clean(value: str) -> str:
    return (value or '').strip()


def project_display_name(root: str) -> str:
    if not root:
        return "Project"
    try:
        name = Path(root).name.strip()
    except Exception:
        name = ""
    if not name:
        return "Project"
    tokens = [token for token in re.split(r'[^A-Za-z0-9]+', name) if token]
    if not tokens:
        return "Project"
    words = []
    for token in tokens:
        if len(token) <= 3:
            words.append(token.upper())
        elif token.isupper():
            words.append(token)
        else:
            words.append(token.capitalize())
    return ' '.join(words) or "Project"

assignees = parse_json_list(task['assignees_json'])
tags = parse_json_list(task['tags_json'])
acceptance = parse_json_list(task['acceptance_json'])
dependencies = parse_json_list(task['dependencies_json'])

description = clean(task['description'])
if description:
    description_lines = description.splitlines()
else:
    description_lines = []

tags_text = clean(task['tags_text'])
story_points = clean(task['story_points'])
dependencies_text = clean(task['dependencies_text'])
assignee_text = clean(task['assignee_text'])
document_reference = clean(task['document_reference'])
idempotency_text = clean(task['idempotency'])
rate_limits = clean(task['rate_limits'])
rbac_text = clean(task['rbac'])
messaging_workflows = clean(task['messaging_workflows'])
performance_targets = clean(task['performance_targets'])
observability_text = clean(task['observability'])
acceptance_text_extra = (task['acceptance_text'] or '').strip() if task['acceptance_text'] else ''
endpoints_text = (task['endpoints'] or '').strip() if task['endpoints'] else ''
sample_create_request = (task['sample_create_request'] or '').strip() if task['sample_create_request'] else ''
sample_create_response = (task['sample_create_response'] or '').strip() if task['sample_create_response'] else ''
user_story_ref_id = clean(task['user_story_ref_id'])
epic_ref_id = clean(task['epic_ref_id'])

task_status = clean(task['status'])
last_progress_at = clean(task['last_progress_at'])
last_progress_run = clean(task['last_progress_run'])
last_apply_status = clean(task['last_apply_status'])
last_log_path = clean(task['last_log_path'])
last_output_path = clean(task['last_output_path'])
last_prompt_path = clean(task['last_prompt_path'])
last_changes_applied = parse_int_field(task['last_changes_applied']) or 0
last_tokens_total = parse_int_field(task['last_tokens_total'])
last_duration_seconds = parse_int_field(task['last_duration_seconds'])
last_notes = parse_json_list(task['last_notes_json'])
last_commands = parse_json_list(task['last_commands_json'])

project_display = project_display_name(PROJECT_ROOT)
repo_path = PROJECT_ROOT or '.'
try:
    prompt_dir = Path(PROMPT_PATH).resolve().parent
except Exception:
    prompt_dir = Path(".").resolve()

sample_limit_env = os.getenv("GC_PROMPT_SAMPLE_LINES", "").strip()
try:
    sample_limit = int(sample_limit_env) if sample_limit_env else 80
except ValueError:
    sample_limit = 80
if sample_limit < 0:
    sample_limit = 0

compact_mode = os.getenv("GC_PROMPT_COMPACT", "").strip().lower() not in {"", "0", "false"}

lines = []
lines.append(f"# You are Codex (model: {MODEL_NAME})")
lines.append("")
lines.append(f"You are assisting the {project_display} delivery team. Implement the task precisely using the repository at: {repo_path}")
lines.append("")
lines.append("## Story")

epic_id = clean(story_row['epic_key'])
epic_title = clean(story_row['epic_title'])
story_id = clean(story_row['story_id'])
story_title = clean(story_row['story_title'])
sequence = story_row['sequence']

if compact_mode:
    story_label = story_id or STORY_SLUG
    if story_label and story_title:
        summary = f"- {story_label} — {story_title}"
    elif story_label:
        summary = f"- {story_label}"
    elif story_title:
        summary = f"- {story_title}"
    else:
        summary = "- Story details unavailable"
    extras = []
    if epic_id or epic_title:
        epic_bits = [bit for bit in [epic_id, epic_title] if bit]
        extras.append("epic " + " — ".join(epic_bits))
    if sequence:
        extras.append(f"order {sequence}")
    if extras:
        summary += f" ({'; '.join(extras)})"
    lines.append(summary)
else:
    if epic_id or epic_title:
        parts = [part for part in [epic_id, epic_title] if part]
        lines.append("- Epic: " + " — ".join(parts))
    if story_id or story_title:
        parts = [part for part in [story_id, story_title] if part]
        lines.append("- Story: " + " — ".join(parts))
    if sequence:
        lines.append(f"- Story order: {sequence}")

lines.append("")
lines.append("## Task")
task_id = clean(task['task_id'])
task_title = clean(task['title'])
estimate = clean(task['estimate'])

if compact_mode:
    task_label = task_id or f"Task {TASK_INDEX + 1}"
    summary = f"- {task_label}"
    if task_title:
        summary += f" — {task_title}"
    lines.append(summary)
    meta_bits = []
    if estimate:
        meta_bits.append(f"estimate {estimate}")
    if story_points and story_points != estimate:
        meta_bits.append(f"story points {story_points}")
    if assignees or assignee_text:
        assigned = ", ".join(assignees) if assignees else assignee_text
        meta_bits.append(f"assignees {assigned}")
    if tags:
        tags_summary = ", ".join(tags[:3])
        if len(tags) > 3:
            tags_summary += "…"
        meta_bits.append(f"tags {tags_summary}")
    elif tags_text:
        meta_bits.append(f"tags {tags_text}")
    if document_reference:
        meta_bits.append(f"doc {document_reference}")
    if rate_limits:
        meta_bits.append(f"rate limits {rate_limits}")
    if meta_bits:
        lines.append(f"- Details: {'; '.join(meta_bits)}")
else:
    if task_id:
        lines.append(f"- Task ID: {task_id}")
    if task_title:
        lines.append(f"- Title: {task_title}")
    if estimate:
        lines.append(f"- Estimate: {estimate}")
    if assignees:
        lines.append("- Assignees: " + ", ".join(assignees))
    elif assignee_text:
        lines.append(f"- Assignee: {assignee_text}")
    if tags:
        lines.append("- Tags: " + ", ".join(tags))
    elif tags_text:
        lines.append(f"- Tags: {tags_text}")
    if story_points and story_points != estimate:
        lines.append(f"- Story points: {story_points}")
    elif story_points and not estimate:
        lines.append(f"- Story points: {story_points}")
    if document_reference:
        lines.append(f"- Document reference: {document_reference}")
    if idempotency_text:
        lines.append(f"- Idempotency: {idempotency_text}")
    if rate_limits:
        lines.append(f"- Rate limits: {rate_limits}")
    if rbac_text:
        lines.append(f"- RBAC: {rbac_text}")
    if messaging_workflows:
        lines.append(f"- Messaging & workflows: {messaging_workflows}")
    if performance_targets:
        lines.append(f"- Performance targets: {performance_targets}")
    if observability_text:
        lines.append(f"- Observability: {observability_text}")
    if user_story_ref_id and user_story_ref_id.lower() != story_id.lower():
        lines.append(f"- User story reference ID: {user_story_ref_id}")
    if epic_ref_id and epic_ref_id.lower() != epic_id.lower():
        lines.append(f"- Epic reference ID: {epic_ref_id}")

lines.append("")
lines.append("### Description")
if description_lines:
    lines.extend(description_lines)
else:
    lines.append("(No additional description provided.)")

if acceptance:
    lines.append("")
    lines.append("### Acceptance Criteria")
    for item in acceptance:
        lines.append(f"- {item}")
elif acceptance_text_extra:
    lines.append("")
    lines.append("### Acceptance Criteria")
    lines.extend(acceptance_text_extra.splitlines())

if dependencies:
    lines.append("")
    lines.append("### Dependencies")
    for dep in dependencies:
        lines.append(f"- {dep}")
elif dependencies_text:
    lines.append("")
    lines.append("### Dependencies")
    lines.extend(dependencies_text.splitlines())

if endpoints_text:
    lines.append("")
    lines.append("### Endpoints")
    lines.extend(endpoints_text.splitlines())

doc_snippets_enabled = os.getenv("GC_PROMPT_DOC_SNIPPETS", "").strip().lower() not in {"", "0", "false"}
staging_root = Path(STAGING_DIR).resolve() if STAGING_DIR else None
project_root_path = Path(PROJECT_ROOT).resolve() if PROJECT_ROOT else None

has_previous_attempt = any([
    last_progress_at,
    last_apply_status,
    last_log_path,
    last_output_path,
    last_notes,
    last_commands,
])

if has_previous_attempt:
    def resolve_history_path(raw_path: str) -> Optional[Path]:
        if not raw_path:
            return None
        candidate = Path(raw_path)
        if candidate.is_absolute():
            candidates = [candidate]
        else:
            candidates = []
            for base in [prompt_dir, project_root_path, staging_root]:
                if not base:
                    continue
                base_path = base if isinstance(base, Path) else Path(base)
                candidates.append(base_path / candidate)
            candidates.append(candidate)
        for option in candidates:
            try:
                resolved = option.resolve()
            except Exception:
                resolved = option
            if resolved.exists():
                return resolved
        return candidates[0] if candidates else candidate

    def render_relative(path_obj: Path) -> str:
        for root in filter(None, [project_root_path, staging_root]):
            if isinstance(root, Path):
                try:
                    return str(path_obj.relative_to(root))
                except ValueError:
                    continue
        return str(path_obj)

    lines.append("")
    lines.append("### Previous Attempt Summary")

    status_bits = []
    if task_status:
        status_bits.append(task_status)
    if last_apply_status:
        status_bits.append(f"apply:{last_apply_status}")
    if last_changes_applied:
        status_bits.append(f"changes:{last_changes_applied}")
    if status_bits:
        lines.append(f"- Status: {', '.join(status_bits)}")

    metrics_bits = []
    if last_progress_at:
        metrics_bits.append(f"at {last_progress_at}")
    if last_progress_run:
        metrics_bits.append(f"run {last_progress_run}")
    if last_tokens_total is not None:
        metrics_bits.append(f"tokens {last_tokens_total}")
    duration_text = format_duration(last_duration_seconds)
    if duration_text:
        metrics_bits.append(f"duration {duration_text}")
    if metrics_bits:
        lines.append(f"- Metrics: {', '.join(metrics_bits)}")

    if last_notes:
        lines.append("- Notes:")
        for note in last_notes[:4]:
            lines.append(f"  - {clamp_text(note, 220)}")

    if last_commands:
        lines.append("- Prior command attempts:")
        for cmd in last_commands[:3]:
            lines.append(f"  - {clamp_text(cmd, 160)}")

    log_excerpt_lines: list[str] = []
    log_display = ""
    if last_log_path:
        resolved_path = resolve_history_path(last_log_path)
        if isinstance(resolved_path, Path) and resolved_path.exists():
            log_display = render_relative(resolved_path)
            log_excerpt_lines = build_log_excerpt(resolved_path)
        else:
            log_display = last_log_path
    if log_display:
        lines.append(f"- Log: {log_display}")
        if log_excerpt_lines:
            lines.append("```text")
            lines.extend(log_excerpt_lines)
            lines.append("```")

    if last_output_path:
        output_path_resolved = resolve_history_path(last_output_path)
        if isinstance(output_path_resolved, Path) and output_path_resolved.exists():
            lines.append(f"- Output: {render_relative(output_path_resolved)}")
        else:
            lines.append(f"- Output: {last_output_path}")

    if last_prompt_path:
        prompt_path_resolved = resolve_history_path(last_prompt_path)
        if isinstance(prompt_path_resolved, Path) and prompt_path_resolved.exists():
            lines.append(f"- Prompt: {render_relative(prompt_path_resolved)}")
        else:
            lines.append(f"- Prompt: {last_prompt_path}")

def _split_items(raw: str):
    if not raw:
        return []
    items = re.split(r'[\n;,]+', raw)
    return [item.strip() for item in items if item and item.strip()]

def _collect_candidate_files(ref: str):
    candidates = []
    if not ref:
        return candidates
    ref_stripped = ref.strip()
    ref_lower = ref_stripped.lower()

    def add_path(candidate: Path):
        try:
            resolved = candidate.resolve()
        except Exception:
            resolved = candidate
        if resolved.is_file():
            if resolved not in candidates:
                candidates.append(resolved)

    # Direct path attempts relative to project or staging roots
    for base in filter(None, [project_root_path, staging_root]):
        candidate = base / ref_stripped
        if candidate.is_file():
            add_path(candidate)
    # If ref looks like filename only, search staging dir for matches
    if staging_root and ("." in ref_stripped or "/" not in ref_stripped):
        for match in staging_root.rglob(ref_stripped):
            add_path(match)

    keyword_map = {
        "sds": ["sds.*"],
        "pdr": ["pdr.*"],
        "openapi": ["openapi.*"],
        "swagger": ["openapi.*"],
        "erd": ["*.mmd"],
        "mermaid": ["*.mmd"],
        "schema": ["*.sql", "*.yaml", "*.yml"],
    }
    if staging_root:
        for keyword, patterns in keyword_map.items():
            if keyword in ref_lower:
                for pattern in patterns:
                    for match in staging_root.glob(pattern):
                        add_path(match)

    return candidates

def classify_directory_crawl(command: str) -> Optional[str]:
    if not command:
        return None
    stripped = command.strip()
    if not stripped:
        return None
    tokens = stripped.split()
    if not tokens:
        return None
    cmd = tokens[0]
    args = tokens[1:]
    non_option_args = [tok for tok in args if not tok.startswith('-')]
    if cmd == 'ls':
        if not non_option_args:
            return "ls with no explicit target"
        return None
    if cmd == 'find':
        target = non_option_args[0] if non_option_args else ''
        if not target or target in {'.', './', '..'}:
            return "find without explicit target"
        return None
    if cmd == 'rg':
        if '--files' in tokens:
            return "rg --files directory scan"
        return None
    if cmd == 'fd':
        return "fd directory scan"
    if cmd == 'tree':
        if not non_option_args:
            return "tree with no explicit target"
        return None
    return None

def resolve_workdir(cwd: str, project_root_path: Optional[pathlib.Path]) -> pathlib.Path:
    if cwd:
        candidate = pathlib.Path(cwd)
        if candidate.is_absolute():
            return candidate
        base = project_root_path or pathlib.Path.cwd()
        return base / candidate
    return project_root_path or pathlib.Path.cwd()

def _extract_snippet(path: Path, term: str, limit: int):
    if limit <= 0:
        return ([], False)
    try:
        text = path.read_text(encoding="utf-8", errors="replace")
    except Exception as exc:
        return ([f"(failed to read {path.name}: {exc})"], False)
    lines_local = text.splitlines()
    if not lines_local:
        return ([], False)
    search_terms: list[str] = []
    if term:
        search_terms.append(term.strip())
        search_terms.extend(
            [token for token in re.split(r'[^a-z0-9/_\-.]+', term.lower()) if len(token) >= 3]
        )

    match_index = None
    for needle in search_terms:
        if not needle:
            continue
        needle_lower = needle.lower()
        for idx, line in enumerate(lines_local):
            if needle_lower in line.lower():
                match_index = idx
                break
        if match_index is not None:
            break

    if match_index is None:
        start = 0
    else:
        start = max(0, match_index - max(5, limit // 2))
    end = min(len(lines_local), start + limit)
    snippet = lines_local[start:end]
    truncated = end < len(lines_local)
    if match_index is not None and start > 0:
        snippet.insert(0, "... (preceding lines omitted)")
    if truncated:
        snippet.append("... (additional content truncated)")
    return (snippet, truncated)

def _minify_payload(value: str) -> str:
    if not value:
        return ""
    raw = value.strip()
    if not raw:
        return ""
    try:
        parsed = json.loads(raw)
    except Exception:
        return raw
    try:
        return json.dumps(parsed, separators=(",", ":"), ensure_ascii=False)
    except Exception:
        return raw

def _chunk_text(text: str, width: int = 160) -> list[str]:
    if not text:
        return []
    return [text[i:i + width] for i in range(0, len(text), width)]

def _normalise_space(text: str) -> str:
    return re.sub(r"\s+", " ", text or "").strip()

def _condense_snippet(snippet_lines, term, max_chars=420):
    core = " ".join(line.strip() for line in snippet_lines if line.strip())
    if not core:
        return ""
    core = _normalise_space(core)
    if term:
        lowered = core.lower()
        idx = lowered.find(term.lower())
        if idx > 0:
            start = max(0, idx - 180)
            core = core[start:]
    sentences = re.split(r'(?<=[.!?])\s+', core)
    assembled: list[str] = []
    total = 0
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
        assembled.append(sentence)
        total += len(sentence)
        if total >= max_chars:
            break
    summary = " ".join(assembled) if assembled else core
    summary = summary.strip()
    if len(summary) > max_chars:
        summary = summary[:max_chars].rstrip() + "…"
    return summary

def append_sample_section(title: str, value: str):
    if not value:
        return
    lines.append("")
    heading = f"### {title}"
    payload = _minify_payload(value)
    if sample_limit <= 0:
        digest_src = payload.encode("utf-8", "replace")
        digest = hashlib.sha256(digest_src).hexdigest()[:12]
        preview = payload[:120]
        if payload and len(payload) > 120:
            preview = preview.rstrip() + "…"
        lines.append(f"{heading} (digest — pass --sample-lines N to view payload)")
        if preview:
            preview_clean = preview.replace("\n", " ").strip()
            lines.append(f"- preview: `{preview_clean}`")
        source_lines = len(value.splitlines()) or 1
        lines.append(f"- original lines: {source_lines}; minified chars: {len(payload)}")
        lines.append(f"- sha256: {digest}")
        return

    sample_chunks = _chunk_text(payload)
    truncated = 0
    if sample_limit and len(sample_chunks) > sample_limit:
        truncated = len(sample_chunks) - sample_limit
        sample_chunks = sample_chunks[:sample_limit]
        heading = f"{heading} (first {sample_limit} chunk{'s' if sample_limit != 1 else ''} of minified payload)"

    lines.append(heading)
    if sample_chunks:
        lines.extend(sample_chunks)
    else:
        lines.append("(payload empty after normalisation)")
    if truncated:
        lines.append(f"... ({truncated} additional chunk{'s' if truncated != 1 else ''} truncated)")
        lines.append("... (raise --sample-lines to include more of the payload)")

if sample_create_request:
    append_sample_section("Sample Create Request", sample_create_request)

if sample_create_response:
    append_sample_section("Sample Create Response", sample_create_response)

doc_catalog_entries = []
doc_catalog_path = os.getenv("GC_DOC_CATALOG_PATH", "").strip()
doc_catalog_data = {"version": 1, "documents": {}, "snippets": {}}
doc_catalog_changed = {"value": False}
if doc_catalog_path:
    catalog_path_obj = Path(doc_catalog_path)
    if catalog_path_obj.exists():
        try:
            loaded = json.loads(catalog_path_obj.read_text(encoding="utf-8"))
            if isinstance(loaded, dict):
                docs_section = loaded.get("documents")
                if isinstance(docs_section, dict):
                    doc_catalog_data["documents"] = docs_section
                for key, value in loaded.items():
                    if key != "documents":
                        doc_catalog_data[key] = value
        except Exception:
            pass
documents_store = doc_catalog_data.setdefault("documents", {})
snippet_store = doc_catalog_data.setdefault("snippets", {})

def _relative_path_for_prompt(path_obj: Path) -> str:
    for base in filter(None, [project_root_path, staging_root]):
        if not base:
            continue
        try:
            return str(path_obj.relative_to(base))
        except ValueError:
            continue
    return str(path_obj)

def _build_doc_entry(path_obj: Path):
    try:
        stat = path_obj.stat()
    except OSError:
        return None
    mtime_ns = getattr(stat, "st_mtime_ns", None)
    if mtime_ns is None:
        mtime_ns = int(stat.st_mtime * 1_000_000_000)
    size = int(stat.st_size)
    try:
        resolved_str = str(path_obj.resolve())
    except Exception:
        resolved_str = str(path_obj)
    doc_id = "DOC-" + hashlib.sha256(resolved_str.encode("utf-8", "replace")).hexdigest()[:8].upper()
    existing = documents_store.get(doc_id)
    if isinstance(existing, dict):
        try:
            existing_mtime = int(existing.get("mtime_ns", 0))
        except Exception:
            existing_mtime = -1
        try:
            existing_size = int(existing.get("size", -1))
        except Exception:
            existing_size = -1
        if existing_mtime == int(mtime_ns) and existing_size == size:
            entry = existing.copy()
            entry["doc_id"] = doc_id
            entry["rel_path"] = entry.get("rel_path") or _relative_path_for_prompt(path_obj)
            return entry
    headings = []
    try:
        with path_obj.open("r", encoding="utf-8", errors="replace") as handle:
            for lineno, line in enumerate(handle, start=1):
                stripped = line.strip()
                if not stripped:
                    continue
                title = None
                level = None
                suffix = path_obj.suffix.lower()
                if suffix in {".md", ".markdown"}:
                    match = re.match(r"^(#{1,4})\s+(.*)$", stripped)
                    if match:
                        level = len(match.group(1))
                        title = match.group(2).strip()
                if title is None:
                    match = re.match(r"^((?:\d+\.)+\d*|\d+|[A-Z][.)]|[IVXLCM]+\.)\s+(.*)$", stripped)
                    if match:
                        title = match.group(2).strip()
                        level = level or 2
                if title:
                    headings.append({
                        "title": title,
                        "line": lineno,
                        "level": int(level or 1),
                    })
                if len(headings) >= 80:
                    break
    except Exception:
        headings = []
    entry = {
        "doc_id": doc_id,
        "path": str(path_obj),
        "rel_path": _relative_path_for_prompt(path_obj),
        "mtime_ns": int(mtime_ns),
        "size": size,
        "headings": headings,
    }
    documents_store[doc_id] = {
        key: value for key, value in entry.items() if key != "doc_id"
    }
    doc_catalog_changed["value"] = True
    return entry

def _load_doc_snippet(path_obj: Path, doc_entry: dict) -> str:
    doc_id = doc_entry.get("doc_id")
    if not doc_id:
        return ""
    cached = snippet_store.get(doc_id)
    current_mtime = doc_entry.get("mtime_ns")
    if isinstance(cached, dict) and cached.get("mtime_ns") == current_mtime:
        return cached.get("preview") or ""
    preview_lines: list[str] = []
    try:
        with path_obj.open("r", encoding="utf-8", errors="replace") as handle:
            for idx, line in enumerate(handle):
                if idx >= 80:
                    break
                stripped = line.strip()
                if stripped:
                    preview_lines.append(stripped)
    except Exception:
        preview_lines = []
    snippet_text = _condense_snippet(preview_lines, "", max_chars=360)
    snippet_store[doc_id] = {"preview": snippet_text, "mtime_ns": current_mtime}
    doc_catalog_changed["value"] = True
    return snippet_text

if doc_snippets_enabled and (staging_root or project_root_path):
    seen_paths = set()
    references = _split_items(document_reference)
    endpoints_list = _split_items(endpoints_text)
    candidates = []
    for reference in references:
        for candidate in _collect_candidate_files(reference):
            try:
                candidate_resolved = candidate.resolve()
            except Exception:
                candidate_resolved = candidate
            key = str(candidate_resolved)
            if key in seen_paths:
                continue
            seen_paths.add(key)
            candidates.append(candidate_resolved)
    if staging_root and endpoints_list:
        openapi_candidates = list(staging_root.glob("openapi.*"))
        for candidate in openapi_candidates:
            try:
                candidate_resolved = candidate.resolve()
            except Exception:
                candidate_resolved = candidate
            key = str(candidate_resolved)
            if key in seen_paths:
                continue
            seen_paths.add(key)
            candidates.append(candidate_resolved)
    for path_obj in candidates:
        if not path_obj.exists() or not path_obj.is_file():
            continue
        doc_entry = _build_doc_entry(path_obj)
        if not doc_entry:
            continue
        snippet_text = _load_doc_snippet(path_obj, doc_entry)
        preview_headings = []
        for heading in doc_entry.get("headings", [])[:12]:
            title = heading.get("title") or ""
            line_no = heading.get("line")
            if line_no:
                preview_headings.append(f"{title} (line {line_no})")
            else:
                preview_headings.append(title)
        doc_catalog_entries.append({
            "doc_id": doc_entry["doc_id"],
            "rel_path": doc_entry["rel_path"],
            "headings": preview_headings,
            "snippet": snippet_text,
        })

if doc_catalog_path and doc_catalog_changed["value"]:
    try:
        Path(doc_catalog_path).write_text(json.dumps(doc_catalog_data, indent=2), encoding="utf-8")
    except Exception:
        pass

if doc_catalog_entries:
    lines.append("")
    lines.append("## Documentation Catalog")
    lines.append("Use the catalog below to pick a section, then run `gpt-creator show-file <path> --range START:END` for a narrow excerpt. Avoid cat/sed on these manuals.")
    for entry in doc_catalog_entries[:6]:
        rel_path = entry['rel_path']
        lines.append(f"- {entry['doc_id']} — {rel_path}")
        headings_preview = entry.get("headings") or []
        if headings_preview:
            lines.append("  Sections:")
            for heading in headings_preview[:6]:
                lines.append(f"    • {heading}")
        else:
            lines.append(f"  (No headings detected; use `gpt-creator show-file {rel_path} --range START:END` to inspect a specific slice.)")
        snippet_text = (entry.get("snippet") or "").strip()
        if snippet_text:
            snippet_clean = _normalise_space(snippet_text)[:280].rstrip()
            lines.append(f"  Snippet: {snippet_clean}")
        lines.append("")

guard_entries = []

command_failure_cache = os.getenv("GC_COMMAND_FAILURE_CACHE", "").strip()
failure_entries = []
if command_failure_cache:
    cache_path = Path(command_failure_cache)
    if cache_path.exists():
        try:
            cache_raw = cache_path.read_text(encoding='utf-8')
            cache_data = json.loads(cache_raw) if cache_raw.strip() else {}
        except Exception:
            cache_data = {}
        if isinstance(cache_data, dict):
            for value in cache_data.values():
                try:
                    failure_count = int(value.get("count") or 0)
                except Exception:
                    failure_count = 0
                if failure_count < 2:
                    continue
                command_text = str(value.get("command") or "").strip()
                if not command_text:
                    continue
                summary_text = (value.get("last_summary") or value.get("summary") or "").strip()
                summary_text = re.sub(r'\s+', ' ', summary_text)
                exit_code_val = value.get("exit")
                last_seen_val = value.get("last_seen") or ""
                failure_entries.append((last_seen_val, failure_count, exit_code_val, command_text, summary_text))
            failure_entries.sort(key=lambda item: item[0], reverse=True)

if failure_entries:
    lines.append("")
    lines.append("## Known Command Failures")
    lines.append("The following commands have already failed; do not rerun them until the underlying issue is addressed. Summarise the cached failure instead of executing the command again.")
    max_failures = 4
    for _, failure_count, exit_code_val, command_text, summary_text in failure_entries[:max_failures]:
        exit_label = f"exit {exit_code_val}" if exit_code_val not in (None, "", 0) else "failed"
        plural = "s" if failure_count != 1 else ""
        if summary_text and len(summary_text) > 200:
            summary_text = summary_text[:197] + "..."
        suffix = f" — {summary_text}" if summary_text else ""
        lines.append(f"- `{command_text}` ({exit_label}, {failure_count} attempt{plural}){suffix}")
        remediation_note = failure_remediation_notes.get(command_text.strip())
        if not remediation_note:
            remediation_note = remediation_message(command_text, failure_count, exit_code_val)
        if remediation_note:
            lines.append(f"  -> {remediation_note}")

stream_cache = os.getenv("GC_COMMAND_STREAM_CACHE", "").strip()
stream_entries = []
if stream_cache:
    stream_path = Path(stream_cache)
    if stream_path.exists():
        try:
            stream_raw = stream_path.read_text(encoding="utf-8")
            stream_data = json.loads(stream_raw) if stream_raw.strip() else {}
        except Exception:
            stream_data = {}
        if isinstance(stream_data, dict):
            sorted_entries = sorted(
                stream_data.values(),
                key=lambda item: item.get("last_seen", ""),
                reverse=True,
            )
            for entry in sorted_entries[:4]:
                summary_text = (entry.get("summary") or "").strip()
                advice_text = (entry.get("advice") or "").strip()
                try:
                    occurrences = int(entry.get("count") or 0)
                except Exception:
                    occurrences = 0
                if summary_text:
                    summary_text = re.sub(r"\s+", " ", summary_text)
                    advice_text = re.sub(r"\s+", " ", advice_text)
                    stream_entries.append((summary_text, advice_text, occurrences))

scan_cache = os.getenv("GC_COMMAND_SCAN_CACHE", "").strip()
scan_entries = []
if scan_cache:
    scan_path = Path(scan_cache)
    if scan_path.exists():
        try:
            scan_raw = scan_path.read_text(encoding="utf-8")
            scan_data = json.loads(scan_raw) if scan_raw.strip() else {}
        except Exception:
            scan_data = {}
        if isinstance(scan_data, dict):
            sorted_scans = sorted(
                scan_data.values(),
                key=lambda item: item.get("last_seen", ""),
                reverse=True,
            )
            for entry in sorted_scans[:4]:
                command_text = (entry.get("command") or "").strip()
                preview_lines = entry.get("lines") or []
                if not command_text or not preview_lines:
                    continue
                cwd_display = (entry.get("cwd_display") or entry.get("cwd") or "").strip()
                message_text = (entry.get("message") or "").strip()
                try:
                    occurrences = int(entry.get("count") or 0)
                except Exception:
                    occurrences = 0
                try:
                    line_count_val = int(entry.get("line_count") or len(preview_lines))
                except Exception:
                    line_count_val = len(preview_lines)
                truncated_flag = bool(entry.get("truncated"))
                cleaned_lines = []
                for raw_line in preview_lines[:6]:
                    cleaned = (raw_line or "").strip()
                    if cleaned:
                        cleaned_lines.append(cleaned)
                if not cleaned_lines and line_count_val > 0:
                    cleaned_lines.append("(no cached output)")
                scan_entries.append({
                    "command": command_text,
                    "cwd": cwd_display,
                    "message": message_text,
                    "occurrences": occurrences,
                    "lines": cleaned_lines,
                    "line_count": line_count_val,
                    "truncated": truncated_flag or (line_count_val > len(cleaned_lines)),
                })

if stream_entries:
    lines.append("")
    lines.append("## Command Efficiency Alerts")
    lines.append("Recent runs paged files with sequential sed/cat chunks. Pivot to targeted searches or cached viewers instead of streaming large slices.")
    for summary_text, advice_text, occurrences in stream_entries:
        entry_line = summary_text
        if occurrences > 1:
            entry_line += f" (seen {occurrences}x)"
        if advice_text:
            entry_line += f" — {advice_text}"
        lines.append(f"- {entry_line}")

if scan_entries:
    lines.append("")
    lines.append("## Workspace Directory Snapshots")
    lines.append("Reuse these cached listings instead of rerunning ls/find on the same paths; refresh only if the tree changes.")
    for entry in scan_entries:
        summary_line = f"- `{entry['command']}`"
        details = []
        cwd_display = entry.get("cwd") or ""
        if cwd_display:
            details.append(f"cwd {cwd_display}")
        occurrences = entry.get("occurrences") or 0
        if isinstance(occurrences, int) and occurrences > 1:
            details.append(f"seen {occurrences}x")
        message_text = entry.get("message") or ""
        if message_text:
            details.append(message_text)
        if details:
            summary_line += " — " + "; ".join(details)
        lines.append(summary_line)
        preview_lines = entry.get("lines") or []
        if preview_lines:
            preview_text = ", ".join(preview_lines)
            if len(preview_text) > 200:
                preview_text = preview_text[:197] + "..."
            lines.append(f"    {preview_text}")
        else:
            lines.append("    (no cached output)")
        line_count_val = entry.get("line_count")
        extra_count = 0
        if isinstance(line_count_val, int):
            extra_count = max(0, line_count_val - len(preview_lines))
        if extra_count > 0:
            lines.append(f"    ... (+{extra_count} more)")
        elif entry.get("truncated"):
            lines.append("    ... (truncated)")

file_cache = os.getenv("GC_COMMAND_FILE_CACHE", "").strip()
build_entries = []
file_entries = []
if file_cache:
    file_cache_path = Path(file_cache)
    if file_cache_path.exists():
        try:
            file_raw = file_cache_path.read_text(encoding="utf-8")
            file_data = json.loads(file_raw) if file_raw.strip() else {}
        except Exception:
            file_data = {}
        if isinstance(file_data, dict):
            sorted_files = sorted(
                file_data.values(),
                key=lambda item: item.get("last_seen", ""),
                reverse=True,
            )
            max_file_entries = 6
            max_build_entries = 4
            for entry in sorted_files:
                if len(file_entries) >= max_file_entries and len(build_entries) >= max_build_entries:
                    break
                summary_text = (entry.get("summary") or "").strip()
                excerpt_text = (entry.get("excerpt") or "").strip()
                try:
                    occurrences = int(entry.get("count") or 0)
                except Exception:
                    occurrences = 0
                rel_path = (entry.get("rel_path") or entry.get("path") or "").strip()
                range_value = entry.get("range")
                mode_value = entry.get("mode")
                category = entry.get("category") or ""
                if summary_text:
                    summary_text = re.sub(r"\s+", " ", summary_text)
                    excerpt_text = re.sub(r"\s+", " ", excerpt_text)
                    payload = {
                        "summary": summary_text,
                        "excerpt": excerpt_text,
                        "occurrences": occurrences,
                        "rel_path": rel_path,
                        "range": range_value,
                        "mode": mode_value,
                    }
                    if category == "build-artifact":
                        if len(build_entries) < max_build_entries:
                            build_entries.append(payload)
                        continue
                    file_entries.append(payload)

if build_entries:
    lines.append("")
    lines.append("## Build Artifacts (opt-in)")
    lines.append("Compiled outputs in dist/build/coverage directories are suppressed; inspect sources first and only open these artifacts when absolutely necessary.")
    for entry in build_entries:
        summary_text = entry.get("summary") or ""
        rel_path = entry.get("rel_path") or ""
        occurrences = entry.get("occurrences") or 0
        info_line = summary_text
        if rel_path:
            info_line += f" [{rel_path}]"
        if isinstance(occurrences, int) and occurrences > 1:
            info_line += f" (seen {occurrences}x)"
        lines.append(f"- {info_line}")
        if rel_path:
            lines.append(f"  -> If the compiled output is required, run `gpt-creator show-file \"{rel_path}\" --head 120`; otherwise focus on the source file.")

if file_entries:
    lines.append("")
    lines.append("## Cached File Excerpts")
    lines.append("Reuse the snippets below instead of repeating cat/sed on the same file; refresh only if the file changed. Prefer `gpt-creator show-file <path> --range start:end` or `rg -n '<term>' <path> -C20` to jump to new context.")
    for entry in file_entries:
        summary_text = entry.get("summary") or ""
        excerpt_text = entry.get("excerpt") or ""
        occurrences = entry.get("occurrences") or 0
        entry_line = summary_text
        if isinstance(occurrences, int) and occurrences > 1:
            entry_line += f" (seen {occurrences}x)"
        lines.append(f"- {entry_line}")
        if excerpt_text:
            preview = excerpt_text.strip()
            if len(preview) > 160:
                preview = preview[:157] + "..."
            lines.append(f"  -> {preview}")
        rel_path = entry.get("rel_path") or ""
        range_value = entry.get("range")
        command_hint = ""
        if rel_path:
            if isinstance(range_value, (list, tuple)) and len(range_value) == 2:
                try:
                    start_line = int(range_value[0])
                    end_line = int(range_value[1])
                except Exception:
                    start_line = end_line = None
                if start_line is not None and end_line is not None:
                    command_hint = f"gpt-creator show-file {rel_path} --range {start_line}:{end_line}"
            if not command_hint:
                command_hint = f"gpt-creator show-file {rel_path} --head 120"
        if command_hint:
            lines.append(f"  -> Reopen via `{command_hint}`")
        if rel_path:
            lines.append(f"  -> Use `rg -n \"<term>\" {rel_path} -C20` to search within this file without re-reading it in full.")

if guard_entries:
    lines.append("")
    lines.append("## Command Guard Alerts")
    lines.append("Resolve these issues before rerunning commands that have already failed; focus on remediation instead of immediate retries.")
    for entry in guard_entries[:4]:
        command_label = (entry.get("command") or "pnpm").strip() or "pnpm"
        issues = entry.get("issues") or []
        summary = "; ".join(issues) if issues else "Pre-check violation detected."
        lines.append(f"- {command_label} — {summary}")

lines.append("")

lines.append("## Instructions")
if compact_mode:
    lines.append('- Respond with JSON only: {"plan":[], "changes":[], "commands":[], "notes":[]}.')
    lines.append("- Keep plan bullets brief; record blockers or follow-ups in `notes`.")
    lines.append("- Keep internal narration tight (≤3 short sentences) and focused on the current task.")
    lines.append("- Apply repository changes directly and confirm acceptance criteria (note gaps in `notes`).")
    lines.append("- Prefer pnpm for scripts; mention commands that cannot run because of network limits.")
    lines.append("- Limit the plan to at most three short steps and keep every step focused on this deliverable; send unrelated workstreams to `notes` as follow-ups.")
    lines.append("- Declare your target files/symbols in a `focus` array before running any shell commands; update both the plan and `focus` before touching new areas.")
    lines.append("- Use `gpt-creator show-file <path> [--range/--head/--tail]` to inspect sources; it caches views and supports `--diff`/`--refresh` instead of re-streaming large files.")
    lines.append("- Track file views; if you begin paging with sequential sed/cat ranges, pivot to `rg -n <pattern> <path> -C20` or `gpt-creator show-file <path> --range start:end` instead of streaming the file.")
    lines.append("- When a cached excerpt below covers the context you need, cite it instead of re-running cat/sed; refresh only if the file changed.")
    lines.append("- Before running `pnpm test` or `pnpm build`, confirm dependencies are installed and prior pnpm commands succeeded; fix failures before retrying.")
    lines.append("- Review `Known Command Failures` and `Command Guard Alerts` before retrying a command; capture the remediation steps instead of rerunning immediately.")
    lines.append("")
    lines.append("## Guardrails")
    lines.append("- Stay within this task's scope; avoid spinning up unrelated plans or subprojects.")
    lines.append("- Consult only the referenced docs or clearly relevant files; skip broad repo sweeps.")
    lines.append("- Keep command usage lean and focused on assets needed for the acceptance criteria.")
    lines.append("- Do not run directory-wide listings/searches outside the declared `focus`; revise the plan + focus first.")
    lines.append("- Wrap up once deliverables are met; record blockers or follow-ups succinctly in `notes`.")

    lines.append("")
    lines.append("## Change Format")
    lines.append("- Use unified diffs or full file bodies inside the `changes` array.")
    lines.append("- Omit keys with no content; no markdown fences or extra prose.")
else:
    lines.append("- Draft a short plan before modifying files.")
    lines.append("- Apply changes directly in the repository; commits are not required.")
    lines.append("- Respond with structured JSON described below — no markdown fencing or prose outside the JSON.")
    lines.append("- Verify acceptance criteria before finishing.")
    lines.append("- If blocked, explain why and suggest next steps inside the JSON response.")
    lines.append("- Prefer pnpm for install/build scripts; avoid npm/yarn unless explicitly required.")
    lines.append("- Limit the plan to three or fewer steps dedicated to this deliverable; capture other ideas or future work in `notes`.")
    lines.append("- Keep internal narration tight (≤3 short sentences) and focused on the current task.")
    lines.append("- After writing the plan, add a `focus` array listing the exact files/symbols you will touch. Update the plan and `focus` before exploring new areas.")
    lines.append("- Use `gpt-creator show-file <path>` (with `--diff`/`--refresh` as needed) instead of re-reading the same file segments directly.")
    lines.append("- Track file reads; when you find yourself chaining `sed` or `cat` ranges across the same file, switch to `rg -n <pattern> <path> -C20` or `gpt-creator show-file <path> --range start:end` for focused context.")
    lines.append("- Reuse cached excerpts listed below rather than repeating cat/sed; only re-read files when you know the content changed.")
    lines.append("- Before running `pnpm test` or `pnpm build`, verify dependencies (`pnpm install`) and resolve any previous pnpm failures first.")
    lines.append("- Review `Known Command Failures` and `Command Guard Alerts`; plan remediation before retrying any listed command.")
    lines.append("- Assume limited network access; note any commands that cannot run for that reason instead of failing silently.")

    lines.append("")
    lines.append("## Guardrails")
    lines.append("- Stay strictly within this task's scope; do not re-plan or chase unrelated issues.")
    lines.append("- Read only the documents or files necessary to satisfy the acceptance criteria.")
    lines.append("- Avoid long exploratory command sequences; focus on edits and checks that prove the task.")
    lines.append("- Skip directory sweeps outside your declared `focus` unless you first update the plan and focus targets.")
    lines.append("- Stop when outputs are ready; surface blockers or context gaps inside the JSON `notes`.")

    lines.append("")
    lines.append("## Output JSON schema")
    lines.append("Return a single JSON object with keys exactly as follows (omit null/empty collections when not needed):")
    lines.append("{")
    lines.append("  \"plan\": [\"short step-by-step plan items...\"],")
    lines.append("  \"focus\": [\"src/foo.ts:loadWidget\", \"pkg/utils.ts\"],")
    lines.append("  \"changes\": [")
    lines.append("    { \"type\": \"patch\", \"path\": \"relative/file/path\", \"diff\": \"UNIFIED_DIFF\" },")
    lines.append("    { \"type\": \"file\", \"path\": \"relative/file/path\", \"content\": \"entire file content\" }")
    lines.append("  ],")
    lines.append("  \"commands\": [\"optional shell commands to run (e.g., pnpm install)\"],")
    lines.append("  \"notes\": [\"follow-up items or blockers\"]")
    lines.append("}")
    lines.append("- Use UTF-8, escape newlines as \\n inside JSON strings.")
    lines.append("- Diff entries must be valid unified diffs (git apply compatible) against the current workspace.")
    lines.append("- File entries provide the complete desired file content (for new or fully rewritten files).")
    lines.append("- Do not emit markdown fences, commentary, or additional text outside the JSON object.")
    lines.append("- Any text before or after the JSON object will be treated as an error and retried automatically.")

if CONTEXT_TAIL_PATH:
    context_path = Path(CONTEXT_TAIL_PATH)
    if context_path.exists():
        tail_text = context_path.read_text(encoding='utf-8').splitlines()
        tail_mode = os.getenv("GC_CONTEXT_TAIL_MODE", "digest").strip().lower()
        tail_limit = os.getenv("GC_CONTEXT_TAIL_LIMIT", "").strip()
        if tail_mode == "digest":
            heading = "## Shared Context Digest"
        elif tail_mode == "raw":
            heading = "## Shared Context Tail"
            if tail_limit and tail_limit.isdigit():
                heading += f" (last {int(tail_limit)} line{'s' if int(tail_limit) != 1 else ''})"
        else:
            heading = "## Shared Context"
        lines.append("")
        lines.append(heading)
        lines.append("")
        lines.extend(tail_text)

prompt_path = Path(PROMPT_PATH)
prompt_path.parent.mkdir(parents=True, exist_ok=True)
prompt_path.write_text("\n".join(lines) + "\n", encoding='utf-8')

story_points_meta = story_points or ""
print(f"{task_id}\t{task_title}\t{story_points_meta}")
PY
}

# Return success (0) when note language implies manual follow-up is required.
gc_note_requires_followup() {
  local note_text="${1:-}"
  local note_lower="${note_text,,}"

  [[ -n "$note_lower" ]] || return 1

  if [[ "$note_lower" == *"parse-error"* ]]; then
    return 0
  fi

  if [[ "$note_lower" == *"no manual"* || "$note_lower" == *"no manual steps"* || "$note_lower" == *"no manual verification"* || "$note_lower" == *"no manual review"* || "$note_lower" == *"no review needed"* || "$note_lower" == *"no review required"* || "$note_lower" == *"no action required"* || "$note_lower" == *"no follow-up"* || "$note_lower" == *"no follow up"* ]]; then
    return 1
  fi

  if [[ "$note_lower" == *"optional"* || "$note_lower" == *"recommended"* || "$note_lower" == *"informational"* || "$note_lower" == *"for reference"* ]]; then
    if [[ "$note_lower" != *"requires"* && "$note_lower" != *"required"* && "$note_lower" != *"must"* && "$note_lower" != *"need"* && "$note_lower" != *"needs"* && "$note_lower" != *"needed"* && "$note_lower" != *"blocked"* && "$note_lower" != *"blocker"* && "$note_lower" != *"pending"* ]]; then
      return 1
    fi
  fi

  if [[ "$note_lower" == *"error"* || "$note_lower" == *"failure"* || "$note_lower" == *"failed"* ]]; then
    if [[ "$note_lower" == *"no error"* || "$note_lower" == *"no errors"* || "$note_lower" == *"without error"* || "$note_lower" == *"error free"* || "$note_lower" == *"error-free"* || "$note_lower" == *"errors resolved"* || "$note_lower" == *"errors addressed"* || "$note_lower" == *"failure resolved"* || "$note_lower" == *"failure addressed"* ]]; then
      :
    else
      return 0
    fi
  fi

  local contains_manual=0
  if [[ "$note_lower" == *"manual"* || "$note_lower" == *"manually"* ]]; then
    contains_manual=1
  fi

  if (( contains_manual )); then
    if [[ "$note_lower" == *"manual steps optional"* || "$note_lower" == *"manual testing optional"* || "$note_lower" == *"manual qa optional"* || "$note_lower" == *"manual testing recommended"* || "$note_lower" == *"manual qa recommended"* || "$note_lower" == *"manual review optional"* || "$note_lower" == *"manual verification optional"* ]]; then
      contains_manual=0
    fi
  fi

  if (( contains_manual )); then
    local -a manual_triggers=(
      "requires"
      "required"
      "require"
      "must"
      "need"
      "needs"
      "needed"
      "manual follow-up"
      "manual follow up"
      "manual followup"
      "manual verification"
      "manual review"
      "manual steps"
      "manual patch"
      "manual fix"
      "manual merge"
      "manual deploy"
      "manual migration"
      "manual intervention"
      "manual action"
      "apply manually"
      "manually apply"
      "manually patch"
      "manually merge"
      "manually verify"
      "could not"
      "can't"
      "cannot"
      "unable"
      "failed"
      "failure"
      "todo"
      "tbd"
      "pending"
      "block"
      "blocked"
      "follow-up"
      "follow up"
      "followup"
    )
    for trigger in "${manual_triggers[@]}"; do
      if [[ "$note_lower" == *"$trigger"* ]]; then
        return 0
      fi
    done
  fi

  if [[ "$note_lower" == *"review"* ]]; then
    if [[ "$note_lower" == *"no review"* || "$note_lower" == *"reviewed"* ]]; then
      return 1
    fi
    local -a review_triggers=(
      "needs review"
      "need review"
      "required review"
      "requires review"
      "review required"
      "pending review"
      "awaiting review"
      "please review"
      "for review"
      "manual review"
      "review manually"
      "review and apply"
      "review this change"
    )
    for trigger in "${review_triggers[@]}"; do
      if [[ "$note_lower" == *"$trigger"* ]]; then
        return 0
      fi
    done
  fi

  return 1
}

cmd_work_on_tasks() {
  local root="" resume=1 story_filter="" start_task_ref="" no_verify=0 keep_artifacts=0 memory_cycle=0 force_reset=0
  local batch_size=0 sleep_between=0 context_lines=400 context_file_lines=200 prompt_compact=1 sample_lines=0 doc_snippets=1
  local context_mode="digest"
  local -a context_mode_flags=()
  local -a context_mode_choice_flag=()
  local -a context_file_flag=()
  local -a context_skip_flags=()
  local -a context_skip_patterns=()
  local -a prompt_mode_flag=()
  local -a sample_lines_flag=()
  local -a doc_snippets_flag=()
  local token_limit="${GC_CODEX_MAX_TOKENS_PER_TASK:-0}"
  local token_limit_override=0
  local idle_timeout="${GC_WORK_ON_TASKS_IDLE_TIMEOUT:-0}"
  local throughput_checkpoint_interval=300
  local throughput_next_checkpoint=0

  local codex_timeout_default=600
  local codex_timeout_value="${GC_CODEX_EXEC_TIMEOUT:-}"
  if [[ -z "$codex_timeout_value" ]]; then
    GC_CODEX_EXEC_TIMEOUT="$codex_timeout_default"
  elif [[ "$codex_timeout_value" =~ ^[0-9]+$ ]]; then
    if (( codex_timeout_value <= 0 )); then
      if [[ -n "${GC_CODEX_EXEC_TIMEOUT_INITIAL:-}" ]]; then
        warn "GC_CODEX_EXEC_TIMEOUT (idle timeout) must be greater than zero; defaulting to ${codex_timeout_default}s."
      fi
      GC_CODEX_EXEC_TIMEOUT="$codex_timeout_default"
    fi
  else
    warn "GC_CODEX_EXEC_TIMEOUT ('${codex_timeout_value}') is not numeric; defaulting idle timeout to ${codex_timeout_default}s."
    GC_CODEX_EXEC_TIMEOUT="$codex_timeout_default"
  fi
  local codex_timeout_seconds="${GC_CODEX_EXEC_TIMEOUT}"

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --story|--from-story) story_filter="$2"; shift 2;;
      --from-task|--fresh-from|--task)
        start_task_ref="${2:-}"
        [[ -n "$start_task_ref" ]] || die "--from-task requires a task id or story:position reference"
        shift 2
        ;;
      --fresh) resume=0; shift;;
      --force)
        resume=0
        force_reset=1
        shift
        ;;
      --no-verify) no_verify=1; shift;;
      --keep-artifacts) keep_artifacts=1; shift;;
      --memory-cycle) memory_cycle=1; shift;;
      --batch-size) batch_size="${2:-0}"; shift 2;;
      --sleep-between) sleep_between="${2:-0}"; shift 2;;
      --context-lines)
        context_lines="${2:-}"
        context_mode_flags=(--context-lines "$context_lines")
        shift 2
        ;;
      --context-none)
        context_lines=0
        context_mode_flags=(--context-none)
        shift
        ;;
      --context-mode)
        context_mode="${2:-}"
        context_mode_choice_flag=(--context-mode "$context_mode")
        shift 2
        ;;
      --context-file-lines)
        context_file_lines="${2:-}"
        context_file_flag=(--context-file-lines "$context_file_lines")
        shift 2
        ;;
      --context-skip)
        context_skip_patterns+=("$2")
        context_skip_flags+=(--context-skip "$2")
        shift 2
        ;;
      --prompt-compact)
        prompt_compact=1
        prompt_mode_flag=(--prompt-compact)
        shift
        ;;
      --prompt-expanded)
        prompt_compact=0
        prompt_mode_flag=(--prompt-expanded)
        shift
        ;;
      --context-doc-snippets|--doc-snippets)
        doc_snippets=1
        doc_snippets_flag=(--context-doc-snippets)
        shift
        ;;
      --no-context-doc-snippets|--no-doc-snippets)
        doc_snippets=0
        doc_snippets_flag=()
        shift
        ;;
      --sample-lines)
        sample_lines="${2:-}"
        sample_lines_flag=(--sample-lines "$sample_lines")
        shift 2
        ;;
      --token-limit)
        token_limit="${2:-}"
        token_limit_override=1
        shift 2
        ;;
      --idle-timeout)
        idle_timeout="${2:-}"
        shift 2
        ;;
      --)
        shift
        break
        ;;
      -h|--help)
        cat <<'EOHELP'
Usage: gpt-creator work-on-tasks [options]

Execute tasks from the project SQLite backlog using Codex, with resumable progress.

Options:
  --project PATH       Project root (defaults to current directory)
  --story ID|SLUG      Start from the matching story id or slug (inclusive)
  --from-task REF      Resume from task REF (task id or story-slug:position) and continue forward
  --fresh              Ignore saved progress and start from the first story
  --no-verify          Skip running gpt-creator verify after tasks complete
  --keep-artifacts     Retain Codex prompt/output files for each task (default cleans up)
  --memory-cycle       Process one task at a time, prune caches, and auto-resume to limit memory usage
  --batch-size NUM     Process at most NUM tasks this run, then pause (default: unlimited)
  --sleep-between SEC  Sleep SEC seconds between tasks (default: 0)
  --context-lines NUM  Include the last NUM lines of shared context in each prompt (default: 400)
  --context-none       Skip attaching shared context to task prompts
  --context-mode MODE  Choose 'digest' (default) to summarise context or 'raw' for the literal tail
  --context-file-lines NUM
                        Limit each shared-context file to NUM lines before summarising (default: 200)
  --context-skip GLOB  Ignore matching files when building shared context (repeatable)
  --prompt-compact     Use the compact instruction/schema block (default setting)
  --prompt-expanded    Restore the legacy verbose instruction/schema block
  --context-doc-snippets
                        (default) Pull scoped excerpts for referenced docs/endpoints when available
  --no-context-doc-snippets
                        Disable doc-snippet mode and include staged docs verbatim
  --sample-lines NUM   Include at most NUM chunks of minified sample payloads (default: 0; increase to view raw content)
  --token-limit NUM    Stop after a task exceeds NUM Codex tokens (default: 0 → unlimited)
  --idle-timeout SEC   Abort the run if no forward progress occurs for SEC seconds (default: 0 → disabled)
EOHELP
        return 0
        ;;
      *)
        die "Unknown work-on-tasks option: ${1}"
        ;;
    esac
  done

  if [[ $# -gt 0 ]]; then
    die "Unexpected argument for work-on-tasks: ${1}"
  fi

  [[ "$batch_size" =~ ^[0-9]+$ ]] || die "Invalid --batch-size value: ${batch_size}"
  [[ "$sleep_between" =~ ^[0-9]+$ ]] || die "Invalid --sleep-between value: ${sleep_between}"
  [[ "$context_lines" =~ ^[0-9]+$ ]] || die "Invalid --context-lines value: ${context_lines}"
  [[ "$context_file_lines" =~ ^[0-9]+$ ]] || die "Invalid --context-file-lines value: ${context_file_lines}"
  [[ "$sample_lines" =~ ^-?[0-9]+$ ]] || die "Invalid --sample-lines value: ${sample_lines}"
  if [[ -z "$idle_timeout" ]]; then
    idle_timeout=0
  elif [[ "$idle_timeout" =~ ^[0-9]+$ ]]; then
    :
  else
    die "Invalid --idle-timeout value: ${idle_timeout}"
  fi
  if [[ -z "$token_limit" ]]; then
    token_limit=0
  elif [[ "$token_limit" =~ ^[0-9]+$ ]]; then
    :
  else
    if (( token_limit_override )); then
      die "Invalid --token-limit value: ${token_limit}"
    else
      warn "GC_CODEX_MAX_TOKENS_PER_TASK ('${token_limit}') is not numeric; disabling per-task token guard."
      token_limit=0
    fi
  fi
  batch_size=$((batch_size))
  sleep_between=$((sleep_between))
  context_lines=$((context_lines))
  context_file_lines=$((context_file_lines))
  sample_lines=$((sample_lines))
  token_limit=$((token_limit))
  idle_timeout=$((idle_timeout))
  (( sample_lines >= 0 )) || die "--sample-lines must be zero or positive (got ${sample_lines})"
  context_mode="${context_mode,,}"
  case "$context_mode" in
    digest|raw) ;;
    *) die "Invalid --context-mode value: ${context_mode}" ;;
  esac
  export GC_CODEX_MAX_TOKENS_PER_TASK="$token_limit"
  if (( token_limit > 0 )); then
    info "Per-task Codex token guard active → ${token_limit} tokens."
  fi
  if (( idle_timeout > 0 )); then
    info "Idle watchdog active → ${idle_timeout}s without progress."
  fi

  if [[ -n "$start_task_ref" && $resume -eq 0 ]]; then
    info "--fresh ignored when --from-task is provided; resuming from the specified task instead."
    resume=1
  fi

  if (( memory_cycle )); then
    if (( batch_size == 0 || batch_size > 1 )); then
      info "Memory-cycle enabled; forcing --batch-size 1 for iterative runs."
    fi
    batch_size=1
  fi

  ensure_ctx "$root"
  local tasks_dir="${PLAN_DIR}/tasks"
  local tasks_db="${tasks_dir}/tasks.db"
  mkdir -p "$tasks_dir"

  if ! gc_tasks_db_has_rows "$tasks_db"; then
    die "Task database missing or empty. Run 'gpt-creator create-tasks' (or create-jira-tasks + migrate-tasks) before work-on-tasks."
  fi

  local now_ts
  now_ts="$(date +%s)"
  throughput_next_checkpoint=$((now_ts + throughput_checkpoint_interval))

  local throughput_msg=""
  if throughput_msg="$(gc_update_throughput_metrics "$tasks_db" "flush")"; then
    if [[ -n "$throughput_msg" ]]; then
      info "$throughput_msg"
    fi
  else
    warn "Failed to prime throughput metrics (flush)."
  fi
  if ! gc_update_throughput_metrics "$tasks_db" "init" >/dev/null; then
    warn "Failed to start throughput metrics window."
  fi

  gc_align_task_story_slugs "$tasks_db"
  gc_sync_story_totals "$tasks_db"

  if (( force_reset )); then
    info "Resetting backlog progress to pending (--force)."
    gc_reset_task_progress "$tasks_db"
    gc_sync_story_totals "$tasks_db"
  fi

  local start_task_story_slug="" start_task_story_title="" start_task_position="" start_task_id="" start_task_title=""
  if [[ -n "$start_task_ref" ]]; then
    info "Rewinding backlog starting from task reference '${start_task_ref}'."
    local rewind_info=""
    local original_story_filter="$story_filter"
    if ! rewind_info="$(gc_rewind_backlog_from_task "$tasks_db" "$start_task_ref" "$original_story_filter")"; then
      die "Unable to rewind backlog from task reference '${start_task_ref}'."
    fi
    IFS=$'\t' read -r start_task_story_slug start_task_story_title start_task_position start_task_id start_task_title <<<"$rewind_info"
    if [[ -z "$start_task_story_slug" || -z "$start_task_position" ]]; then
      die "Invalid response while rewinding backlog; aborting."
    fi
    if [[ -n "$original_story_filter" && "${original_story_filter,,}" != "${start_task_story_slug,,}" ]]; then
      info "Normalizing story filter '${original_story_filter}' to story slug '${start_task_story_slug}'."
    fi
    story_filter="$start_task_story_slug"
    gc_sync_story_totals "$tasks_db"
    local story_display="$start_task_story_slug"
    if [[ -n "$start_task_story_title" && "${start_task_story_title,,}" != "${start_task_story_slug,,}" ]]; then
      story_display+=" — ${start_task_story_title}"
    fi
    info "Starting from task ${start_task_position} (${start_task_id:-no-id}) in story ${story_display}."
    if [[ -n "$start_task_title" ]]; then
      info "  ${start_task_title}"
    fi
  fi

  ensure_node_dependencies "$PROJECT_ROOT"
  gc_refresh_discovery_if_needed
  gc_clear_active_task

  if (( prompt_compact )); then
    export GC_PROMPT_COMPACT=1
  else
    unset GC_PROMPT_COMPACT
  fi
  if (( doc_snippets )); then
    export GC_PROMPT_DOC_SNIPPETS=1
  else
    unset GC_PROMPT_DOC_SNIPPETS
  fi

  local state_dir="${PLAN_DIR}/work"
  local runs_dir="${state_dir}/runs"
  local work_logs_root="${LOG_DIR}/work-on-tasks"
  mkdir -p "$runs_dir" "$work_logs_root"

  GC_CONTEXT_FILE_LINES="$context_file_lines"
  if ((${#context_skip_patterns[@]} > 0)); then
    GC_CONTEXT_SKIP_PATTERNS=("${context_skip_patterns[@]}")
  else
    unset GC_CONTEXT_SKIP_PATTERNS
  fi

  export GC_PROMPT_SAMPLE_LINES="$sample_lines"

  local run_stamp="$(date +%Y%m%d_%H%M%S)"
  local run_dir="${runs_dir}/${run_stamp}"
  local run_log_dir="${work_logs_root}/${run_stamp}"
  mkdir -p "$run_dir" "$run_log_dir"
  local doc_catalog_path="${state_dir}/doc-catalog.json"
  if [[ ! -f "$doc_catalog_path" ]]; then
    mkdir -p "$(dirname "$doc_catalog_path")"
    printf '{"version":1,"documents":{}}\n' >"$doc_catalog_path"
  fi
  export GC_DOC_CATALOG_PATH="$doc_catalog_path"
  local last_progress_ts
  local idle_timeout_triggered=0
  local ctx_file="${run_dir}/context.md"
  gc_build_context_file "$ctx_file" "$STAGING_DIR"
  local context_tail=""
  export GC_CONTEXT_TAIL_LIMIT="$context_lines"
  export GC_CONTEXT_TAIL_MODE="$context_mode"
  if (( context_lines > 0 )); then
    case "$context_mode" in
      digest)
        context_tail="${run_dir}/context_digest.md"
        if ! gc_build_context_digest "$ctx_file" "$context_tail" "$context_lines"; then
          warn "Failed to build context digest; falling back to raw tail."
          context_mode="raw"
          export GC_CONTEXT_TAIL_MODE="raw"
        fi
        if [[ "$context_mode" == "raw" ]]; then
          context_tail="${run_dir}/context_tail.md"
          if ! tail -n "$context_lines" "$ctx_file" >"$context_tail" 2>/dev/null; then
            cp "$ctx_file" "$context_tail"
          fi
        fi
        ;;
      raw)
        context_tail="${run_dir}/context_tail.md"
        if ! tail -n "$context_lines" "$ctx_file" >"$context_tail" 2>/dev/null; then
          cp "$ctx_file" "$context_tail"
        fi
        ;;
    esac
  fi

  local context_tail_mode="$context_mode"
  local context_lines_current="$context_lines"
  local context_file_lines_current="$context_file_lines"
  local context_lines_min=0
  local context_file_lines_min=0
  local context_lines_min_default="${GC_CONTEXT_MIN_LINES:-80}"
  local context_file_lines_min_default="${GC_CONTEXT_MIN_FILE_LINES:-60}"
  if ! [[ "$context_lines_min_default" =~ ^[0-9]+$ ]]; then
    context_lines_min_default=80
  fi
  if ! [[ "$context_file_lines_min_default" =~ ^[0-9]+$ ]]; then
    context_file_lines_min_default=60
  fi
  if (( context_lines_current > 0 )); then
    context_lines_min="$context_lines_min_default"
    if (( context_lines_current < context_lines_min )); then
      context_lines_min="$context_lines_current"
    fi
  fi
  if (( context_file_lines_current > 0 )); then
    context_file_lines_min="$context_file_lines_min_default"
    if (( context_file_lines_current < context_file_lines_min )); then
      context_file_lines_min="$context_file_lines_current"
    fi
  fi
  local context_shrink_iterations=0
  local context_last_shrink_tokens=0
  local context_auto_shrink_threshold="${GC_CONTEXT_AUTO_SHRINK_THRESHOLD:-60000}"
  if ! [[ "$context_auto_shrink_threshold" =~ ^[0-9]+$ ]]; then
    context_auto_shrink_threshold=60000
  fi
  if (( token_limit > 0 )); then
    local derived_threshold=$(( (token_limit * 85) / 100 ))
    if (( derived_threshold <= 0 )); then
      derived_threshold="$token_limit"
    fi
    if (( derived_threshold >= token_limit )); then
      local delta=$(( token_limit / 10 ))
      (( delta < 1 )) && delta=1
      derived_threshold=$(( token_limit - delta ))
    fi
    if (( derived_threshold <= 0 )); then
      derived_threshold="$token_limit"
    fi
    context_auto_shrink_threshold="$derived_threshold"
  fi

  info "Work run directory → ${run_dir}"

  local resume_flag=1
  [[ $resume -eq 1 ]] || resume_flag=0

  local work_failed=0
  local any_changes=0
  local manual_followups=0
  local usage_limit_triggered=0
  local batch_limit_reached=0

  gc_touch_progress() {
    last_progress_ts="$(date +%s)"
  }

  gc_check_idle_timeout() {
    if (( idle_timeout > 0 )); then
      local now_ts
      now_ts="$(date +%s)"
      if (( now_ts - last_progress_ts >= idle_timeout )); then
        if (( idle_timeout_triggered == 0 )); then
          idle_timeout_triggered=1
          manual_followups=1
          work_failed=1
          warn "Idle timeout reached (${idle_timeout}s without progress); halting run."
        fi
        return 1
      fi
    fi
    return 0
  }

  gc_touch_progress

  local processed_total=0
  local processed_any_total=0
  local remaining_tasks=0
  local memory_cycle_single=0

  while :; do
    if gc_check_idle_timeout; then :; else break; fi
    local iteration_processed_any=0
    local iteration_processed=0
    local continue_current_run=0
    local pending_tasks=0

    local effective_batch_size="$batch_size"
    if (( memory_cycle )); then
      if (( memory_cycle_single )); then
        effective_batch_size=1
      else
        effective_batch_size="$batch_size"
      fi
    fi

    while IFS=$'\t' read -r sequence slug story_id story_title epic_id epic_title total_tasks next_task completed status; do
      if [[ -z "${sequence}${slug}${story_id}${story_title}${epic_id}${epic_title}" ]]; then
        continue
      fi

      if ! gc_check_idle_timeout; then
        break
      fi

      iteration_processed_any=1

      if (( throughput_checkpoint_interval > 0 )); then
        now_ts="$(date +%s)"
        if (( throughput_next_checkpoint == 0 || now_ts >= throughput_next_checkpoint )); then
          local throughput_checkpoint_msg=""
          if throughput_checkpoint_msg="$(gc_update_throughput_metrics "$tasks_db" "checkpoint")"; then
            if [[ -n "$throughput_checkpoint_msg" ]]; then
              info "$throughput_checkpoint_msg"
            fi
          else
            warn "Failed to checkpoint throughput metrics."
          fi
          throughput_next_checkpoint=$((now_ts + throughput_checkpoint_interval))
        fi
      fi

    : "${total_tasks:=0}"; : "${next_task:=0}"
    local total_tasks_int=0
    if [[ "$total_tasks" =~ ^[0-9]+$ ]]; then
      total_tasks_int=$((total_tasks))
    fi
    local next_task_int=0
    if [[ "$next_task" =~ ^[0-9]+$ ]]; then
      next_task_int=$((next_task))
    fi

    printf -v story_prefix "%03d" "${sequence:-0}"
    [[ -n "$slug" ]] || slug="story-${story_prefix}"
    local story_run_dir="${run_dir}/story_${story_prefix}_${slug}"
    local story_log_dir="${run_log_dir}/story_${story_prefix}_${slug}"
    mkdir -p "${story_run_dir}/prompts" "${story_run_dir}/out" "${story_run_dir}/reports" "$story_log_dir"
    local report_dir="${story_run_dir}/reports"

    info "Story ${story_prefix} (${story_id:-$slug}) — ${story_title:-Unnamed}"

    if (( total_tasks_int == 0 )); then
      local stats=""
      if stats="$(gc_fetch_story_task_counts "$tasks_db" "$slug" 2>/dev/null)"; then
        local actual_total=0 actual_completed=0
        IFS=$'\t' read -r actual_total actual_completed <<<"$stats"
        if [[ "$actual_total" =~ ^[0-9]+$ ]] && (( actual_total > 0 )); then
          info "  Found ${actual_total} task(s) in backlog despite zero metadata; synchronising."
          total_tasks_int=$actual_total
          completed="${actual_completed}"
          if [[ "$completed" =~ ^[0-9]+$ ]]; then
            (( completed > actual_total )) && completed="$actual_total"
            if (( next_task_int < completed )); then
              next_task_int=$completed
            fi
          fi
          gc_update_work_state "$tasks_db" "$slug" "pending" "$completed" "$total_tasks_int" "$run_stamp"
        fi
      fi
    fi

    if (( total_tasks_int == 0 )); then
      info "  No tasks for this story; marking complete."
      gc_update_work_state "$tasks_db" "$slug" "complete" 0 0 "$run_stamp"
      if (( keep_artifacts == 0 )); then
        rmdir "${story_run_dir}/prompts" 2>/dev/null || true
        rmdir "${story_run_dir}/out" 2>/dev/null || true
      fi
      continue
    fi

    gc_update_work_state "$tasks_db" "$slug" "in-progress" "$next_task_int" "$total_tasks_int" "$run_stamp"

    local task_index
    local story_failed=0
    for (( task_index = next_task_int; task_index < total_tasks_int; task_index++ )); do
      gc_clear_active_task
      if ! gc_check_idle_timeout; then
        break
      fi
      if (( throughput_checkpoint_interval > 0 )); then
        now_ts="$(date +%s)"
        if (( throughput_next_checkpoint == 0 || now_ts >= throughput_next_checkpoint )); then
          local throughput_checkpoint_msg=""
          if throughput_checkpoint_msg="$(gc_update_throughput_metrics "$tasks_db" "checkpoint")"; then
            if [[ -n "$throughput_checkpoint_msg" ]]; then
              info "$throughput_checkpoint_msg"
            fi
          else
            warn "Failed to checkpoint throughput metrics."
          fi
          throughput_next_checkpoint=$((now_ts + throughput_checkpoint_interval))
        fi
      fi
      if (( effective_batch_size > 0 && iteration_processed >= effective_batch_size )); then
        batch_limit_reached=1
        break
      fi
      local task_number
      printf -v task_number "%03d" $((task_index + 1))
      local prompt_path="${story_run_dir}/prompts/task_${task_number}.prompt.md"
      local output_path="${story_run_dir}/out/task_${task_number}.out.md"

      local prompt_meta
      if ! prompt_meta="$(gc_write_task_prompt "$tasks_db" "$slug" "$task_index" "$prompt_path" "$context_tail" "$CODEX_MODEL" "$PROJECT_ROOT" "$STAGING_DIR")"; then
        warn "  Failed to build prompt for task index ${task_index}"
        work_failed=1
        break
      fi

      local task_id="" task_title="" task_story_points=""
      IFS=$'\t' read -r task_id task_title task_story_points <<<"$prompt_meta"
      local banner_task_id="${task_id:-no-id}"
      local task_start_epoch
      task_start_epoch="$(date +%s)"
      local task_tokens_total=0
      local task_duration_seconds=0
      local task_duration_display=""
      local task_tokens_display=""
      local task_story_points_display="—"
      if [[ -n "$task_story_points" ]]; then
        task_story_points_display="$task_story_points"
      fi

      printf '\n'
      gc_render_task_banner "START OF A NEW TASK" "START TASK ID" "$banner_task_id"
      info "  → Working on task ${task_number} (${banner_task_id})"
      info "    ${task_title:-(untitled)}"

      local call_name="story-${slug}-task-${task_number}"
      local codex_ok=0
      local attempt=0
      local max_attempts=2
      gc_touch_progress
      local prompt_augmented=0
      local keep_output=$keep_artifacts
      local parse_error_final=0
      local break_after_update=0
      local task_result_status="in-progress"
      local task_needs_review=0
      local -a task_notes=()
      local -a task_written_paths=()
      local -a task_patched_paths=()
      local -a task_commands=()
      local task_changes_applied=0
      local apply_status="pending"
      local task_report_path="${report_dir}/task_${task_number}.log"
      local task_log_archive_path="${story_log_dir}/task_${task_number}.log"
      gc_update_task_state "$tasks_db" "$slug" "$task_index" "in-progress" "$run_stamp"
      export GC_ACTIVE_TASK_DB="$tasks_db"
      export GC_ACTIVE_TASK_SLUG="$slug"
      export GC_ACTIVE_TASK_INDEX="$task_index"
      export GC_ACTIVE_RUN_STAMP="$run_stamp"
      export GC_ACTIVE_TASK_NUMBER="$task_number"
      export GC_ACTIVE_TASK_ID="$task_id"
      export GC_ACTIVE_TASK_REPORT="$task_report_path"
      export GC_ACTIVE_TASK_ARCHIVE="$task_log_archive_path"
      export GC_ACTIVE_TASK_PROMPT="$prompt_path"
      export GC_ACTIVE_TASK_OUTPUT="$output_path"

      while (( attempt < max_attempts )); do
        (( ++attempt ))
        gc_touch_progress
        if codex_call "$call_name" --prompt "$prompt_path" --output "$output_path"; then
          local attempt_tokens="${GC_CODEX_CALL_TOKEN_ACCUM:-${GC_LAST_CODEX_TOTAL_TOKENS:-0}}"
          if [[ "$attempt_tokens" =~ ^[0-9]+$ ]]; then
            task_tokens_total=$((task_tokens_total + attempt_tokens))
          fi
          if [[ ! -s "$output_path" ]]; then
            warn "  Codex produced no output for ${call_name}; manual review required."
            task_needs_review=1
            manual_followups=1
            keep_output=1
            task_result_status="on-hold"
            task_notes+=("Codex produced no output; manual review required.")
            codex_ok=1
            break
          fi
          local apply_output
          if ! apply_output="$(gc_apply_codex_changes "$output_path" "$PROJECT_ROOT")"; then
            warn "  Failed to apply changes for ${call_name}; manual review required (see ${output_path})."
            task_needs_review=1
            manual_followups=1
            keep_output=1
            task_result_status="on-hold"
            task_notes+=("Codex changes could not be applied automatically; review ${output_path}.")
            codex_ok=1
            break
          fi
          if [[ "$apply_output" == "no-output" || "$apply_output" == "empty-output" ]]; then
            warn "  Codex produced no actionable JSON for ${call_name}; manual review required."
            task_needs_review=1
            manual_followups=1
            keep_output=1
            task_result_status="on-hold"
            task_notes+=("Codex response lacked actionable changes.")
            codex_ok=1
            break
          fi
          apply_status="ok"
          while IFS= read -r change_line; do
            case "$change_line" in
              STATUS\ *)
                apply_status="${change_line#STATUS }"
                ;;
              APPLIED)
                info "    Changes applied."
                ;;
              WRITE\ *)
                local written_path="${change_line#WRITE }"
                info "    Wrote ${written_path}"
                any_changes=1
                task_changes_applied=1
                task_written_paths+=("$written_path")
                ;;
              PATCH\ *)
                local patched_path="${change_line#PATCH }"
                info "    Patched ${patched_path}"
                any_changes=1
                task_changes_applied=1
                task_patched_paths+=("$patched_path")
                ;;
              CMD\ *)
                local suggested_cmd="${change_line#CMD }"
                info "    Suggested command: ${suggested_cmd}"
                task_commands+=("$suggested_cmd")
                ;;
              NOTE\ *)
                local note_text="${change_line#NOTE }"
                warn "    Note: ${note_text}"
                task_notes+=("$note_text")
                if gc_note_requires_followup "$note_text"; then
                  task_needs_review=1
                fi
                ;;
            esac
          done <<<"$apply_output"

          if [[ "$apply_status" == "parse-error" ]]; then
            if (( attempt < max_attempts )); then
              warn "  Codex returned invalid JSON; retrying (attempt $((attempt + 1)) of ${max_attempts})."
              if (( prompt_augmented == 0 )); then
                cat >>"$prompt_path" <<'REM'

## Reminder
- Output a single JSON object exactly as described above; do not include any explanatory text outside the JSON.
- If no changes are required, return the JSON with an empty `changes` array and clear notes explaining why.
- Diff entries must remain valid unified diffs.
REM
                prompt_augmented=1
              fi
              continue
            else
              warn "  Codex output was invalid JSON after retry; manual review required (see ${output_path})."
              task_needs_review=1
              manual_followups=1
              keep_output=1
              task_result_status="on-hold"
              task_notes+=("Codex output remained invalid JSON after retries; inspect ${output_path}.")
              codex_ok=1
              parse_error_final=1
              break_after_update=1
            fi
          fi

          if (( keep_output == 0 )); then
            rm -f "$prompt_path" "$output_path"
          fi
          codex_ok=1
        else
          local codex_status=$?
          local attempt_tokens="${GC_CODEX_CALL_TOKEN_ACCUM:-${GC_LAST_CODEX_TOTAL_TOKENS:-0}}"
          if [[ "$attempt_tokens" =~ ^[0-9]+$ ]]; then
            task_tokens_total=$((task_tokens_total + attempt_tokens))
          fi
          if [[ "${GC_CODEX_USAGE_LIMIT_REACHED:-0}" == "1" ]]; then
            warn "  Codex usage limit reached for ${call_name}; halting further tasks."
            if [[ -n "${GC_CODEX_USAGE_LIMIT_MESSAGE:-}" ]]; then
              warn "    ${GC_CODEX_USAGE_LIMIT_MESSAGE}"
            fi
            task_result_status="blocked"
            task_needs_review=1
            manual_followups=1
            keep_output=1
            local limit_note="${GC_CODEX_USAGE_LIMIT_MESSAGE:-Codex usage limit reached; wait for quota reset before rerunning work-on-tasks.}"
            task_notes+=("$limit_note")
            codex_ok=1
            usage_limit_triggered=1
            work_failed=1
            break
          elif (( codex_status == 124 )); then
            warn "  Codex was idle for ${codex_timeout_seconds}s during ${call_name}."
            if (( attempt < max_attempts )); then
              info "    Retrying task ${task_number} after timeout (attempt $((attempt + 1)) of ${max_attempts})."
              continue
            fi
            task_result_status="on-hold"
            task_needs_review=1
            manual_followups=1
            keep_output=1
            task_notes+=("Codex produced no output for ${codex_timeout_seconds}s; rerun work-on-tasks to continue from this task.")
            codex_ok=1
            break_after_update=1
            work_failed=1
            break
          fi
          warn "  Codex execution failed for ${call_name}; progress saved."
          task_result_status="blocked"
          task_needs_review=1
          manual_followups=1
          keep_output=1
          task_notes+=("Codex execution failed; no automated changes were applied.")
          codex_ok=1
          break
        fi
        break
      done

      if (( task_needs_review )); then
        manual_followups=1
        if [[ "$task_result_status" != "blocked" ]]; then
          task_result_status="on-hold"
        fi
      fi

      if (( codex_ok == 0 )); then
        task_result_status="blocked"
        task_notes+=("Codex execution did not complete; no changes were applied.")
      fi

      if (( codex_ok )) && (( context_lines_current > context_lines_min || context_file_lines_current > context_file_lines_min )); then
        local last_tokens="${GC_LAST_CODEX_TOTAL_TOKENS:-0}"
        local shrink_threshold="$context_auto_shrink_threshold"
        local trigger_shrink=0
        if [[ "$last_tokens" =~ ^[0-9]+$ ]] && (( last_tokens > shrink_threshold )); then
          trigger_shrink=1
        elif (( token_limit > 0 )) && [[ "$last_tokens" =~ ^[0-9]+$ ]] && (( last_tokens >= token_limit )); then
          trigger_shrink=1
        fi
        if (( trigger_shrink )); then
          if (( context_shrink_iterations > 0 )) && (( last_tokens <= context_last_shrink_tokens )); then
            trigger_shrink=0
          fi
        fi
        if (( trigger_shrink )); then
          local new_context_lines="$context_lines_current"
          local new_context_file_lines="$context_file_lines_current"
          local reduced=0
          if (( context_lines_current > context_lines_min )); then
            local decrement_lines=$(( context_lines_current / 3 ))
            (( decrement_lines < 40 )) && decrement_lines=40
            new_context_lines=$(( context_lines_current - decrement_lines ))
            if (( new_context_lines < context_lines_min )); then
              new_context_lines="$context_lines_min"
            fi
            if (( new_context_lines < context_lines_current )); then
              reduced=1
            fi
          fi
          if (( context_file_lines_current > context_file_lines_min )); then
            local decrement_files=$(( context_file_lines_current / 2 ))
            (( decrement_files < 20 )) && decrement_files=20
            new_context_file_lines=$(( context_file_lines_current - decrement_files ))
            if (( new_context_file_lines < context_file_lines_min )); then
              new_context_file_lines="$context_file_lines_min"
            fi
            if (( new_context_file_lines < context_file_lines_current )); then
              reduced=1
            fi
          fi
          if (( reduced )); then
            info "    Token usage ${last_tokens} exceeded budget (~${shrink_threshold}); pruning shared context to ${new_context_lines} lines (per-file ${new_context_file_lines})."
            context_lines_current="$new_context_lines"
            context_file_lines_current="$new_context_file_lines"
            context_last_shrink_tokens="$last_tokens"
            context_shrink_iterations=$((context_shrink_iterations + 1))
            GC_CONTEXT_FILE_LINES="$context_file_lines_current"
            export GC_CONTEXT_FILE_LINES
            GC_CONTEXT_TAIL_LIMIT="$context_lines_current"
            export GC_CONTEXT_TAIL_LIMIT
            if ! gc_build_context_file "$ctx_file" "$STAGING_DIR"; then
              warn "Failed to rebuild shared context after pruning."
            else
              if [[ -n "$context_tail" && -f "$ctx_file" ]]; then
                local new_mode
                new_mode="$(gc_refresh_context_tail "$ctx_file" "$context_tail" "$context_tail_mode" "$context_lines_current")"
                if [[ "$new_mode" == "raw" && "$context_tail_mode" != "raw" ]]; then
                  context_tail="${run_dir}/context_tail.md"
                  new_mode="$(gc_refresh_context_tail "$ctx_file" "$context_tail" "raw" "$context_lines_current")"
                fi
                context_tail_mode="$new_mode"
                GC_CONTEXT_TAIL_MODE="$context_tail_mode"
                export GC_CONTEXT_TAIL_MODE
              fi
            fi
          else
            context_last_shrink_tokens="$last_tokens"
          fi
        fi
      fi

      if [[ "$task_result_status" == "in-progress" ]]; then
        task_result_status="complete"
      fi

      local task_end_epoch
      task_end_epoch="$(date +%s)"
      task_duration_seconds=$((task_end_epoch - task_start_epoch))
      if (( task_duration_seconds < 0 )); then
        task_duration_seconds=0
      fi
      task_duration_display="$(gc_format_duration_compact "$task_duration_seconds")"
      task_tokens_display="$(gc_format_tokens_compact "$task_tokens_total")"

      local story_status_hint="in-progress"
      case "$task_result_status" in
        blocked) story_status_hint="blocked" ;;
        on-hold) story_status_hint="on-hold" ;;
      esac

      local completed_hint="$task_index"
      if [[ "$task_result_status" == "complete" ]]; then
        completed_hint=$((task_index + 1))
      fi

      gc_update_task_state "$tasks_db" "$slug" "$task_index" "$task_result_status" "$run_stamp"
      gc_update_work_state "$tasks_db" "$slug" "$story_status_hint" "$completed_hint" "$total_tasks_int" "$run_stamp"

      if [[ "$task_result_status" == "complete" ]]; then
        local throughput_task_msg=""
        if throughput_task_msg="$(gc_update_throughput_metrics "$tasks_db" "task-complete" "$slug" "$task_index")"; then
          if [[ -n "$throughput_task_msg" ]]; then
            info "  ${throughput_task_msg}"
            now_ts="$(date +%s)"
            throughput_next_checkpoint=$((now_ts + throughput_checkpoint_interval))
          fi
        else
          warn "  Failed to record throughput metrics for task ${task_number}."
        fi
      fi

      local timestamp_utc
      timestamp_utc="$(date -u +%Y-%m-%dT%H:%M:%SZ)"

      local prompt_entry="$prompt_path"
      local output_entry="$output_path"
      local report_entry_path="$task_log_archive_path"
      local report_entry_display="$report_entry_path"
      local report_entry_db=""
      local project_prefix="${PROJECT_ROOT}/"
      if [[ -n "$PROJECT_ROOT" ]]; then
        if [[ "$prompt_entry" == "$project_prefix"* ]]; then
          prompt_entry="${prompt_entry#$project_prefix}"
        fi
        if [[ "$output_entry" == "$project_prefix"* ]]; then
          output_entry="${output_entry#$project_prefix}"
        fi
        if [[ "$report_entry_display" == "$project_prefix"* ]]; then
          report_entry_display="${report_entry_display#$project_prefix}"
        fi
      fi
      report_entry_db="$report_entry_display"
      if [[ ! -f "$prompt_path" ]]; then
        if (( keep_artifacts == 0 )); then
          prompt_entry="(discarded)"
        else
          prompt_entry="(missing)"
        fi
      fi
      if [[ ! -f "$output_path" ]]; then
        if (( keep_output == 0 )); then
          output_entry="(discarded)"
        else
          output_entry="(missing)"
        fi
      fi
      if [[ ! -f "$task_log_archive_path" ]]; then
        report_entry_db=""
        report_entry_display="(missing)"
      fi

      local changes_flag="false"
      if (( task_changes_applied > 0 )); then
        changes_flag="true"
      fi

      local notes_payload=""
      if ((${#task_notes[@]} > 0)); then
        notes_payload="$(printf '%s\n' "${task_notes[@]}")"
      fi
      local written_payload=""
      if ((${#task_written_paths[@]} > 0)); then
        written_payload="$(printf '%s\n' "${task_written_paths[@]}")"
      fi
      local patched_payload=""
      if ((${#task_patched_paths[@]} > 0)); then
        patched_payload="$(printf '%s\n' "${task_patched_paths[@]}")"
      fi
      local commands_payload=""
      if ((${#task_commands[@]} > 0)); then
        commands_payload="$(printf '%s\n' "${task_commands[@]}")"
      fi

      gc_record_task_progress "$tasks_db" "$slug" "$task_index" "$run_stamp" "$task_result_status" "$report_entry_db" "$prompt_entry" "$output_entry" "$attempt" "$task_tokens_total" "$task_duration_seconds" "$apply_status" "$changes_flag" "$notes_payload" "$written_payload" "$patched_payload" "$commands_payload" "$timestamp_utc"

      {
        printf 'task_number: %s\n' "$task_number"
        printf 'task_id: %s\n' "${task_id:-}"
        printf 'task_title: %s\n' "${task_title//$'\n'/ }"
        printf 'story_slug: %s\n' "$slug"
        printf 'status: %s\n' "$task_result_status"
        printf 'timestamp: %s\n' "$timestamp_utc"
        printf 'attempts: %s\n' "$attempt"
        printf 'apply_status: %s\n' "$apply_status"
        printf 'changes_applied: %s\n' "$changes_flag"
        printf 'prompt_path: %s\n' "$prompt_entry"
        printf 'output_path: %s\n' "$output_entry"
        if ((${#task_written_paths[@]} > 0)); then
          printf 'written:\n'
          for path in "${task_written_paths[@]}"; do
            printf '  - %s\n' "$path"
          done
        fi
        if ((${#task_patched_paths[@]} > 0)); then
          printf 'patched:\n'
          for path in "${task_patched_paths[@]}"; do
            printf '  - %s\n' "$path"
          done
        fi
        if ((${#task_commands[@]} > 0)); then
          printf 'commands:\n'
          for cmd in "${task_commands[@]}"; do
            printf '  - %s\n' "$cmd"
          done
        fi
        printf 'notes:\n'
        if ((${#task_notes[@]} > 0)); then
          for note in "${task_notes[@]}"; do
            printf '  - %s\n' "${note//$'\n'/ }"
          done
        else
          printf '  - (none)\n'
        fi
      } >"$task_report_path"
      if ! cp -f "$task_report_path" "$task_log_archive_path"; then
        warn "  Failed to archive task log to ${task_log_archive_path}."
      fi
      gc_clear_active_task

      case "$task_result_status" in
        complete)
          info "  ✓ Task ${task_number} (${task_id:-no-id}) completed with status: ${task_result_status}"
          ;;
        on-hold)
          warn "  Task ${task_number} (${task_id:-no-id}) marked ${task_result_status}; review ${report_entry_display}."
          ;;
        blocked)
          warn "  Task ${task_number} (${task_id:-no-id}) blocked; see ${report_entry_display}."
          ;;
        *)
          info "  Task ${task_number} (${task_id:-no-id}) finished with status: ${task_result_status}"
          ;;
      esac

      local task_status_display="${task_result_status:-unknown}"
      if [[ "$task_status_display" == "complete" ]]; then
        task_status_display="COMPLETED"
      else
        task_status_display="${task_status_display//-/ }"
        task_status_display="$(printf '%s' "$task_status_display" | tr '[:lower:]' '[:upper:]')"
      fi

      printf '\n'
      gc_render_task_banner --header-bottom "END OF THE TASK WORK" \
        "END TASK ID" "$banner_task_id" \
        "REPORT:" \
        "TOKENS USED: ${task_tokens_display}" \
        "STORY POINTS: ${task_story_points_display}" \
        "TIME SPENT: ${task_duration_display}" \
        "STATUS: ${task_status_display}"

      (( ++processed_total ))
      (( ++iteration_processed ))

      if [[ "$task_result_status" == "blocked" ]]; then
        story_failed=1
        break
      fi

      if (( break_after_update )); then
        continue
      fi

      if (( sleep_between > 0 )); then
        sleep "$sleep_between"
      fi

    done

    if (( batch_limit_reached )); then
      break
    fi

    if (( story_failed )); then
      warn "Stopping at story ${slug} due to previous error."
      break
    fi

    if (( idle_timeout_triggered )); then
      break
    fi

    gc_update_work_state "$tasks_db" "$slug" "complete" "$total_tasks_int" "$total_tasks_int" "$run_stamp"
    gc_touch_progress
    if (( keep_artifacts == 0 )); then
      rmdir "${story_run_dir}/prompts" 2>/dev/null || true
      rmdir "${story_run_dir}/out" 2>/dev/null || true
    fi
  done < <(
    python3 - "$tasks_db" "${story_filter}" "$resume_flag" <<'PY'
import sqlite3
import sys
import re

DB_PATH = sys.argv[1]
story_filter = (sys.argv[2] or '').strip().lower()
resume_flag = sys.argv[3] == "1"

conn = sqlite3.connect(DB_PATH)
conn.row_factory = sqlite3.Row
cur = conn.cursor()

stories = cur.execute('SELECT story_slug, story_id, story_title, epic_key, epic_title, sequence, status FROM stories ORDER BY sequence ASC, story_slug ASC').fetchall()

def norm(value):
    return (value or "").strip().lower()

def normalize(value):
    return (value or "").strip()

def slug_norm(value):
    value = norm(value)
    if not value:
        return ""
    return re.sub(r'[^a-z0-9]+', '-', value).strip('-')

start_allowed = not story_filter

for story in stories:
    slug = normalize(story["story_slug"])
    sequence = story["sequence"] or 0
    story_id = normalize(story["story_id"])
    epic_key = normalize(story["epic_key"])
    epic_title = normalize(story["epic_title"])
    story_title = normalize(story["story_title"])

    story_title_clean = story_title.replace('\t', ' ').replace('\n', ' ')
    epic_title_clean = epic_title.replace('\t', ' ').replace('\n', ' ')
    epic_key_clean = epic_key.replace('\t', ' ').replace('\n', ' ')
    story_id_clean = story_id.replace('\t', ' ').replace('\n', ' ')

    if story_filter and not start_allowed:
        keys = {norm(story_id), norm(slug), norm(epic_key), norm(str(sequence))}
        if story_filter in keys:
            start_allowed = True
        else:
            continue

    task_rows = []
    slug_lower = norm(slug)
    if slug_lower:
        task_rows = cur.execute(
            'SELECT position, status FROM tasks WHERE LOWER(COALESCE(story_slug, "")) = ? ORDER BY position ASC',
            (slug_lower,),
        ).fetchall()

    if not task_rows and story_id:
        story_id_lower = norm(story_id)
        if story_id_lower:
            rows = cur.execute(
                'SELECT id, position, status, story_slug FROM tasks WHERE LOWER(COALESCE(story_id, "")) = ? ORDER BY position ASC',
                (story_id_lower,),
            ).fetchall()
            if rows:
                task_rows = [(row["position"], row["status"]) for row in rows]
                if slug_lower:
                    cur.execute(
                        'UPDATE tasks SET story_slug = ? WHERE LOWER(COALESCE(story_id, "")) = ?',
                        (slug, story_id_lower),
                    )
                    conn.commit()

    if not task_rows and slug_lower:
        slug_key = slug_norm(slug)
        if slug_key:
            rows = cur.execute(
                'SELECT position, status FROM tasks WHERE LOWER(COALESCE(story_slug, "")) = ? ORDER BY position ASC',
                (slug_key,),
            ).fetchall()
            task_rows = rows

    total = len(task_rows)
    completed = 0
    next_index = 0
    for row in task_rows:
        status = (row[1] or "").strip().lower()
        if status == "complete":
            completed += 1
            continue
        next_index = row[0] or 0
        break
    else:
        next_index = total

    current_status = (story["status"] or "").strip()

    if resume_flag and not story_filter and current_status.lower() == "complete":
        continue

    if resume_flag:
        if total == 0:
            next_index = 0
        elif completed >= total:
            if story_filter:
                next_index = total
            else:
                continue
    else:
        next_index = 0 if total > 0 else 0

    print("	".join([
        str(sequence),
        slug,
        story_id_clean,
        story_title_clean,
        epic_key_clean,
        epic_title_clean,
        str(total),
        str(next_index),
        str(completed),
        current_status,
    ]))

conn.close()
PY
  )

    if (( idle_timeout_triggered )); then
      break
    fi

    if (( iteration_processed_any )); then
      processed_any_total=1
    else
      if (( processed_any_total == 0 )); then
        info "No stories to process (already complete)."
      fi
      break
    fi

    if (( batch_limit_reached )); then
      remaining_tasks="$(gc_count_pending_tasks "$tasks_db" || echo 0)"
      [[ "$remaining_tasks" =~ ^[0-9]+$ ]] || remaining_tasks=0
      break
    fi

    if (( memory_cycle )); then
      pending_tasks="$(gc_count_pending_tasks "$tasks_db" || echo 0)"
      [[ "$pending_tasks" =~ ^[0-9]+$ ]] || pending_tasks=0
      remaining_tasks="$pending_tasks"
      if (( work_failed == 0 )); then
        if (( iteration_processed > 0 )) && (( pending_tasks > 0 )); then
          gc_trim_memory "memory-cycle"
          info "Memory-cycle paused after ${iteration_processed} task(s); ${pending_tasks} pending."
          memory_cycle_single=1
          continue_current_run=1
        elif (( pending_tasks == 0 )); then
          gc_trim_memory "memory-cycle-final"
        else
          gc_trim_memory "memory-cycle"
        fi
      else
        gc_trim_memory "memory-cycle-error"
      fi
    fi

    if (( continue_current_run == 0 )); then
      remaining_tasks="$(gc_count_pending_tasks "$tasks_db" || echo 0)"
      [[ "$remaining_tasks" =~ ^[0-9]+$ ]] || remaining_tasks=0

      if (( manual_followups > 0 && remaining_tasks > 0 )); then
        warn "Manual follow-ups detected; stopping before rerunning ${remaining_tasks} pending task(s)."
      elif (( work_failed == 0 && manual_followups == 0 && memory_cycle == 0 && batch_limit_reached == 0 && effective_batch_size == 0 && iteration_processed > 0 && remaining_tasks > 0 )); then
        if [[ -n "$story_filter" ]]; then
          info "Remaining tasks detected beyond filtered story; rerun with a broader filter to continue."
        else
          info "${remaining_tasks} task(s) remain; continuing work-on-tasks automatically."
          continue_current_run=1
        fi
      fi
    fi

    if (( continue_current_run )); then
      continue
    fi

    break
  done

  if (( idle_timeout_triggered )); then
    warn "work-on-tasks halted by idle timeout after ${idle_timeout}s without progress."
  fi

  if (( processed_any_total == 0 )); then
    gc_clear_active_task
    return 0
  fi

  gc_clear_active_task

  throughput_msg=""
  if throughput_msg="$(gc_update_throughput_metrics "$tasks_db" "flush")"; then
    if [[ -n "$throughput_msg" ]]; then
      info "$throughput_msg"
    fi
  else
    warn "Failed to finalise throughput metrics."
  fi

  if [[ $work_failed -eq 0 && $batch_limit_reached -eq 0 && $no_verify -eq 0 && $remaining_tasks -eq 0 ]]; then
    if (( any_changes == 0 )); then
      info "No repository changes detected; skipping verify."
    else
      info "Re-running verify after work run"
      if ! cmd_verify all --project "$PROJECT_ROOT"; then
        warn "Verify command reported failures."
        work_failed=1
      fi
    fi
  fi

  if (( batch_limit_reached )); then
    info "Batch size limit hit after ${processed_total} task(s); rerun to continue from the next pending task."
  fi

  if (( usage_limit_triggered )); then
    warn "Codex usage limit detected; halt further work until additional quota is available."
  fi

  if [[ $work_failed -eq 0 ]]; then
    if (( batch_limit_reached )); then
      ok "work-on-tasks paused → ${run_dir}"
    else
      ok "work-on-tasks complete → ${run_dir}"
    fi
    if (( manual_followups )); then
      warn "Manual review needed for some tasks — see notes above and preserved output artifacts."
    fi
  else
    warn "work-on-tasks completed with issues — inspect ${run_dir}"
    return 1
  fi
}


cmd_iterate() {
  warn "'iterate' is deprecated; prefer 'gpt-creator create-tasks' followed by 'gpt-creator work-on-tasks'."

  local root="" jira="" reverify=1
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --jira) jira="$(abs_path "$2")"; shift 2;;
      --no-verify) reverify=0; shift;;
      *) break;;
    esac
  done
  ensure_ctx "$root"
  [[ -n "$jira" ]] || jira="${INPUT_DIR}/jira.md"
  [[ -f "$jira" ]] || die "Jira tasks file not found: ${jira}"

  local tasks_json_local="${PLAN_DIR}/jira-tasks.local.json"
  gc_parse_jira_tasks "$jira" "$tasks_json_local"
  ok "Parsed Jira tasks → ${tasks_json_local}"
  local tasks_json="${tasks_json_local}"

  local codex_parse_prompt="${PLAN_DIR}/iterate-codex-parse.md"
  local codex_raw_json="${PLAN_DIR}/jira-tasks.codex.raw.txt"
  local codex_json="${PLAN_DIR}/jira-tasks.codex.json"
  {
    cat >"$codex_parse_prompt" <<'PROMPT'
# Instruction
You are a structured-data assistant. Convert the following Jira backlog markdown into strict JSON.

## Requirements
- Output **only** valid JSON (no prose, no code fences).
- Structure: { "tasks": [ { "epic_id": str, "epic_title": str, "story_id": str, "story_title": str, "id": str, "title": str, "assignees": [str], "tags": [str], "estimate": str, "description": str, "acceptance_criteria": [str], "dependencies": [str] } ] }.
- Each task begins with a bold identifier such as **T18.5.2**; treat every such block as a separate task and capture its parent story/epic when present.
- Preserve bullet details verbatim inside the description and acceptance criteria lists. Do not repeat metadata (assignee/tags/estimate) inside the description.
- Use empty strings/arrays when information is missing.
- Do not include explanatory text.

## Jira Markdown
PROMPT
    cat "$jira" >>"$codex_parse_prompt"
    cat >>"$codex_parse_prompt" <<'PROMPT'
## End Markdown
PROMPT
  }

  if codex_call "iterate-parse" --prompt "$codex_parse_prompt" --output "$codex_raw_json"; then
    if python3 - <<'PY' "$codex_raw_json" "$codex_json"
import json, pathlib, re, sys
raw_path, out_path = sys.argv[1:3]
text = pathlib.Path(raw_path).read_text().strip()
if not text:
    raise SystemExit(1)
if text.startswith('```'):
    text = re.sub(r'^```[a-zA-Z0-9_-]*\s*', '', text)
    text = re.sub(r'```\s*$', '', text)
data = json.loads(text)
if isinstance(data, list):
    data = {'tasks': data}
elif 'tasks' not in data:
    data = {'tasks': [data]}
pathlib.Path(out_path).write_text(json.dumps(data, indent=2) + '\n')
PY
      then
      ok "Codex parsed Jira tasks → ${codex_json}"
      tasks_json="$codex_json"
    else
      warn "Codex JSON output invalid; falling back to local parser results."
    fi
  else
    warn "Codex parsing step failed; using local parser output."
  fi

  local iterate_dir="${PLAN_DIR}/iterate"
  mkdir -p "$iterate_dir"
  local order_file="${iterate_dir}/tasks-order.txt"

  python3 - <<'PY' "$tasks_json" "$iterate_dir" "$PROJECT_ROOT"
import json, pathlib, sys
source, out_dir, project_root = sys.argv[1:4]
tasks = json.load(open(source)).get('tasks', [])
out = pathlib.Path(out_dir)
out.mkdir(parents=True, exist_ok=True)
index_path = out / 'tasks-order.txt'
with index_path.open('w') as idx:
    for i, task in enumerate(tasks, 1):
        title = (task.get('title') or '').strip() or f'Task {i}'
        task_id = (task.get('id') or '').strip()
        description = (task.get('description') or '').strip() or '(No additional details provided.)'
        estimate = (task.get('estimate') or '').strip()
        tags = ', '.join(task.get('tags') or [])
        assignees = ', '.join(task.get('assignees') or [])
        story_bits = [part for part in [(task.get('story_id') or '').strip(), (task.get('story_title') or '').strip()] if part]
        prompt_path = out / f'task-{i:02d}.md'
        idx.write(str(prompt_path) + '\n')
        lines = []
        if task_id and not title.startswith(task_id):
            lines.append(f"# Task {i}: {task_id} — {title}")
        else:
            lines.append(f"# Task {i}: {title}")
        lines.append('')
        lines.append('## Context')
        lines.append(f'- Working directory: {project_root}')
        if task_id:
            lines.append(f'- Task ID: {task_id}')
        if story_bits:
            lines.append(f"- Story: {' — '.join(story_bits)}")
        if assignees:
            lines.append(f'- Assignees: {assignees}')
        if estimate:
            lines.append(f'- Estimate: {estimate}')
        if tags:
            lines.append(f'- Tags: {tags}')
        lines.append('')
        lines.append('## Description')
        lines.append(description or '(No additional details provided.)')
        lines.append('')
        if task.get('acceptance_criteria'):
            lines.append('## Acceptance Criteria')
            for ac in task['acceptance_criteria']:
                lines.append(f'- {ac}')
            lines.append('')
        if task.get('dependencies'):
            lines.append('## Dependencies')
            for dep in task['dependencies']:
                lines.append(f'- {dep}')
            lines.append('')
        lines.append('')
        lines.append('## Instructions')
        lines.append('- Outline your plan before modifying files.')
        lines.append('- Implement the task in the repository; commits are not required.')
        lines.append('- Show relevant diffs (git snippets) and command results.')
        lines.append('- Verify acceptance criteria for this task.')
        lines.append('- If blocked, explain why and propose next steps.')
        lines.append('')
        lines.append('## Output Format')
        lines.append('- Begin with a heading `Task {i}`.')
        lines.append('- Summarise changes, tests, and outstanding follow-ups.')
        prompt_path.write_text('\n'.join(lines) + '\n')
PY

  if [[ -s "$order_file" ]]; then
    while IFS= read -r prompt_path; do
      [[ -z "$prompt_path" ]] && continue
      local base_name="$(basename "$prompt_path" .md)"
      local output_path="${prompt_path%.md}.output.md"
      info "Running Codex for ${base_name}"
      codex_call "$base_name" --prompt "$prompt_path" --output "$output_path" || warn "Codex task ${base_name} returned non-zero"
    done < "$order_file"
  else
    warn "No Jira tasks to process after parsing."
  fi

  local summary_prompt="${iterate_dir}/summary.md"
  local summary_output="${iterate_dir}/summary.output.md"
  python3 - <<'PY' "$tasks_json" "$order_file" "$summary_prompt"
import json, pathlib, sys
tasks = json.load(open(sys.argv[1])).get('tasks', [])
order_file = pathlib.Path(sys.argv[2])
prompt_path = pathlib.Path(sys.argv[3])
lines = ['# Summary Request', '', 'Summarise the completed Jira work and list follow-up actions.']
lines.append('')
lines.append('## Task Reports')
if order_file.exists():
    for i, prompt in enumerate(order_file.read_text().splitlines(), 1):
        if not prompt:
            continue
        title = tasks[i-1].get('title') if i-1 < len(tasks) else f'Task {i}'
        out_path = pathlib.Path(prompt).with_suffix('.output.md')
        lines.append(f'- Task {i}: {title}')
        if out_path.exists():
            content = out_path.read_text().strip()
            if content:
                snippet = content[:2000]
                lines.append('  ```')
                lines.append(snippet)
                lines.append('  ```')
        else:
            lines.append('  (No output captured)')
else:
    lines.append('- No outputs available.')
lines.append('')
lines.append('## Output Requirements')
lines.append('- Provide an overall summary of work completed.')
lines.append('- List follow-up items or blockers.')
lines.append('- Use markdown headings and bullet lists.')
prompt_path.write_text('\n'.join(lines) + '\n')
PY

  codex_call "iterate-summary" --prompt "$summary_prompt" --output "$summary_output" || warn "Codex summary step returned non-zero"

  if [[ "$reverify" -eq 1 ]]; then
    info "Re-running verify after iteration"
    cmd_verify all --project "$PROJECT_ROOT"
  fi
}

cmd_estimate() {
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project)
        root="$(abs_path "$2")"
        shift 2
        ;;
      -h|--help)
        cat <<'USAGE'
Usage: gpt-creator estimate [--project PATH]

Estimate how long it will take to finish the remaining tasks based on story points (default 15 SP/hour; refined by work-on-tasks throughput).
USAGE
        return 0
        ;;
      *)
        die "Unknown estimate option: $1"
        ;;
    esac
  done

  ensure_ctx "$root"
  local tasks_db="${PLAN_DIR}/tasks/tasks.db"
  if [[ ! -f "$tasks_db" ]]; then
    die "Tasks database not found at ${tasks_db}. Run 'gpt-creator create-tasks' first."
  fi

  python3 - <<'PY' "$tasks_db"
import math
import re
import sqlite3
import sys

db_path = sys.argv[1]
default_rate = 15.0

conn = sqlite3.connect(db_path)
conn.row_factory = sqlite3.Row
cur = conn.cursor()

def parse_points(raw):
    if raw is None:
        return 0.0
    if isinstance(raw, (int, float)):
        return float(raw)
    text = str(raw).strip()
    if not text:
        return 0.0
    normalized = text.lower().replace(",", ".")
    match = re.search(r"-?\d+(?:\.\d+)?", normalized)
    if not match:
        return 0.0
    try:
        return float(match.group(0))
    except ValueError:
        return 0.0

def fetch_rate(cursor):
    rate_value = None
    samples = 0
    try:
        row = cursor.execute(
            "SELECT value FROM metadata WHERE key = ?",
            ("throughput.rate_sp_per_hour",),
        ).fetchone()
        if row and row["value"] not in (None, ""):
            rate_value = float(row["value"])
    except Exception:
        rate_value = None

    try:
        row = cursor.execute(
            "SELECT value FROM metadata WHERE key = ?",
            ("throughput.samples",),
        ).fetchone()
        if row and row["value"] not in (None, ""):
            samples = int(float(row["value"]))
    except Exception:
        samples = 0

    if rate_value is not None and rate_value > 0 and samples > 0:
        return rate_value, samples
    return default_rate, 0

rate, rate_samples = fetch_rate(cur)
try:
    rows = cur.execute("SELECT id, story_points, status FROM tasks").fetchall()
except sqlite3.DatabaseError as exc:
    conn.close()
    raise SystemExit(f"Failed to read tasks: {exc}")

done_statuses = {"complete", "completed", "done"}
total_points = 0.0
remaining_tasks = 0
completed_points = 0.0
task_info = {}

for row in rows:
    status = (row["status"] or "").strip().lower()
    points = parse_points(row["story_points"])
    points = max(points, 0.0)
    task_info[row["id"]] = {"points": points, "status": status}
    if status in done_statuses:
        completed_points += points
        continue
    remaining_tasks += 1
    total_points += points

tokens_total = 0.0
token_samples = 0
covered_points = 0.0

def table_exists(cursor, name):
    try:
        row = cursor.execute(
            "SELECT name FROM sqlite_master WHERE type='table' AND name=?",
            (name,),
        ).fetchone()
        return row is not None
    except sqlite3.DatabaseError:
        return False

if table_exists(cur, "task_progress"):
    token_by_task = {}
    try:
        for progress in cur.execute(
            "SELECT id, task_id, tokens_total FROM task_progress "
            "WHERE tokens_total IS NOT NULL AND tokens_total > 0 "
            "ORDER BY id"
        ):
            task_id = progress["task_id"]
            if task_id is None:
                continue
            token_by_task[task_id] = float(progress["tokens_total"])
    except sqlite3.DatabaseError:
        token_by_task = {}

    token_samples = len(token_by_task)
    for task_id, tokens in token_by_task.items():
        tokens_total += tokens
        info = task_info.get(task_id)
        if info:
            covered_points += info["points"]

conn.close()

if remaining_tasks == 0:
    print("All tasks are complete. No remaining story points.")
    raise SystemExit(0)

if rate <= 0:
    rate = default_rate
total_minutes = math.ceil((total_points / rate) * 60) if total_points > 0 else 0
days, rem_minutes = divmod(total_minutes, 1440)
hours, minutes = divmod(rem_minutes, 60)

parts = []
if days:
    parts.append(f"{days}d")
if hours:
    parts.append(f"{hours}h")
if minutes or not parts:
    parts.append(f"{minutes}m")

def fmt_points(value: float) -> str:
    if math.isclose(value, round(value), rel_tol=1e-9, abs_tol=1e-9):
        return str(int(round(value)))
    return f"{value:.2f}".rstrip("0").rstrip(".")

def fmt_rate(value: float) -> str:
    if math.isclose(value, round(value), rel_tol=1e-9, abs_tol=1e-9):
        return str(int(round(value)))
    return f"{value:.2f}".rstrip("0").rstrip(".")

def fmt_tokens(value: float) -> str:
    return f"{int(round(value)):,}"

def fmt_ratio(value: float) -> str:
    return f"{value:.2f}".rstrip("0").rstrip(".")

estimate = " ".join(parts)
print(f"Remaining tasks: {remaining_tasks}")
print(f"Remaining story points: {fmt_points(total_points)}")
if rate_samples > 0 and rate > 0:
    sample_label = "sample" if rate_samples == 1 else "samples"
    print(f"Measured throughput: {fmt_rate(rate)} SP/hour (based on {rate_samples} {sample_label}).")
else:
    print("Using default throughput assumption: 15 SP/hour.")
print(f"Estimated completion time @{fmt_rate(rate)} SP/hour: {estimate}")

if tokens_total > 0 and token_samples > 0:
    print(f"Tokens observed: {fmt_tokens(tokens_total)} across {token_samples} task(s).")
    if covered_points > 0:
        avg_tokens_per_point = tokens_total / covered_points
        print(
            f"Average tokens per story point: {fmt_ratio(avg_tokens_per_point)} tokens/SP "
            f"(based on {fmt_points(covered_points)} SP)."
        )
        estimated_tokens_hour = avg_tokens_per_point * rate if rate > 0 else 0.0
        if estimated_tokens_hour > 0:
            print(f"Estimated token burn @{fmt_rate(rate)} SP/hour: {fmt_tokens(estimated_tokens_hour)} tokens/hour.")
        projected_remaining = avg_tokens_per_point * total_points if total_points > 0 else 0.0
        if projected_remaining > 0:
            print(f"Projected remaining tokens: {fmt_tokens(projected_remaining)} tokens for {fmt_points(total_points)} SP.")
    else:
        print("Average tokens per story point: insufficient data (no story points recorded on tokenized tasks).")
else:
    print("Token usage data unavailable; run work-on-tasks to capture token telemetry.")
PY
}


cmd_tokens() {
  local root="" details=0 json_output=0
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project)
        root="$(abs_path "$2")"
        shift 2
        ;;
      --details)
        details=1
        shift
        ;;
      --json)
        json_output=1
        shift
        ;;
      -h|--help)
        cat <<'USAGE'
Usage: gpt-creator tokens [--project PATH] [--details] [--json]

Print aggregated Codex token usage captured under .gpt-creator/logs/codex-usage.ndjson.
USAGE
        return 0
        ;;
      *)
        die "Unknown tokens option: $1"
        ;;
    esac
  done

  local project_root=""
  if [[ -n "$root" ]]; then
    project_root="$root"
  elif [[ -n "${PROJECT_ROOT:-}" ]]; then
    project_root="$PROJECT_ROOT"
  else
    project_root="$PWD"
  fi

  local usage_file="${project_root}/.gpt-creator/logs/codex-usage.ndjson"
  if [[ ! -f "$usage_file" ]]; then
    warn "No Codex usage data found at ${usage_file}. Run a codex-enabled command first."
    return 1
  fi

  python3 - <<'PY' "$usage_file" "$details" "$json_output"
import json
import pathlib
import sys
from datetime import datetime

usage_path = pathlib.Path(sys.argv[1])
details = sys.argv[2] == "1"
json_mode = sys.argv[3] == "1"

def as_int(value):
    if isinstance(value, bool):
        return None
    if isinstance(value, int):
        return value
    if isinstance(value, float):
        return int(value)
    if isinstance(value, str):
        stripped = value.strip()
        if stripped.isdigit():
            return int(stripped)
    return None

def parse_timestamp(value):
    if not isinstance(value, str) or not value:
        return None
    cleaned = value
    if cleaned.endswith("Z"):
        cleaned = cleaned[:-1] + "+00:00"
    try:
        return datetime.fromisoformat(cleaned)
    except ValueError:
        return None

def isoformat(dt):
    if dt is None:
        return ""
    text = dt.isoformat()
    if text.endswith("+00:00"):
        text = text[:-6] + "Z"
    return text

raw_lines = usage_path.read_text(encoding="utf-8", errors="ignore").splitlines()
records = []
captured_count = 0
for line in raw_lines:
    line = line.strip()
    if not line:
        continue
    try:
        payload = json.loads(line)
    except Exception:
        continue
    if not isinstance(payload, dict):
        continue
    records.append(payload)
    if payload.get("usage_captured"):
        captured_count += 1

if not records:
    print("No usage entries recorded.")
    sys.exit(0)

fields = ("prompt_tokens", "completion_tokens", "total_tokens", "cached_tokens", "billable_units", "request_units")
totals = {field: 0 for field in fields}
counts = {field: 0 for field in fields}
for entry in records:
    for field in fields:
        value = as_int(entry.get(field))
        if value is not None:
            totals[field] += value
            counts[field] += 1

timestamps = [ts for ts in (parse_timestamp(rec.get("timestamp")) for rec in records) if ts is not None]
first_ts = isoformat(min(timestamps)) if timestamps else ""
last_ts = isoformat(max(timestamps)) if timestamps else ""

summary = {
    "entries": len(records),
    "captured_entries": captured_count,
    "totals": {field: totals[field] for field in fields if counts[field]},
}
if first_ts:
    summary["first_timestamp"] = first_ts
if last_ts:
    summary["last_timestamp"] = last_ts

def clamp(text, limit):
    text = text or ""
    if len(text) <= limit:
        return text
    return text[: max(0, limit - 3)] + "..."

def fmt_int(value):
    if value is None:
        return "-"
    return f"{value:,}"

def fmt_exit_code(value):
    parsed = as_int(value)
    if parsed is None:
        return "-"
    return str(parsed)

sorted_records = sorted(records, key=lambda rec: rec.get("timestamp") or "")

if json_mode:
    if details:
        rows = []
        for rec in sorted_records:
            rows.append({
                "timestamp": rec.get("timestamp"),
                "task": rec.get("task"),
                "model": rec.get("model"),
                "prompt_tokens": as_int(rec.get("prompt_tokens")),
                "completion_tokens": as_int(rec.get("completion_tokens")),
                "total_tokens": as_int(rec.get("total_tokens")),
                "cached_tokens": as_int(rec.get("cached_tokens")),
                "billable_units": as_int(rec.get("billable_units")),
                "request_units": as_int(rec.get("request_units")),
                "exit_code": as_int(rec.get("exit_code")),
                "usage_captured": bool(rec.get("usage_captured")),
            })
        summary["rows"] = rows
    print(json.dumps(summary, indent=2))
    sys.exit(0)

print(f"Codex usage file: {usage_path}")
print(f"Entries: {summary['entries']} (captured={summary['captured_entries']})")
if first_ts and last_ts:
    print(f"Range: {first_ts} → {last_ts}")

label_map = {
    "prompt_tokens": "Prompt tokens",
    "completion_tokens": "Completion tokens",
    "total_tokens": "Total tokens",
    "cached_tokens": "Cached tokens",
    "billable_units": "Billable units",
    "request_units": "Request units",
}

for field, label in label_map.items():
    if field in summary["totals"]:
        print(f"{label}: {summary['totals'][field]:,}")

if not details:
    sys.exit(0)

headers = ["timestamp", "task", "model", "total", "prompt", "completion", "cached", "billable", "request", "exit", "captured"]
rows = []
for rec in sorted_records:
    rows.append([
        rec.get("timestamp") or "",
        clamp(rec.get("task") or "", 32),
        clamp(rec.get("model") or "", 24),
        fmt_int(as_int(rec.get("total_tokens"))),
        fmt_int(as_int(rec.get("prompt_tokens"))),
        fmt_int(as_int(rec.get("completion_tokens"))),
        fmt_int(as_int(rec.get("cached_tokens"))),
        fmt_int(as_int(rec.get("billable_units"))),
        fmt_int(as_int(rec.get("request_units"))),
        fmt_exit_code(rec.get("exit_code")),
        "yes" if rec.get("usage_captured") else "no",
    ])

widths = []
for index, header in enumerate(headers):
    column_values = [len(header)] + [len(row[index]) for row in rows]
    widths.append(max(column_values))

print()
header_line = "  ".join(header.ljust(widths[i]) for i, header in enumerate(headers))
separator = "  ".join("-" * widths[i] for i in range(len(headers)))
print(header_line)
print(separator)
for row in rows:
    print("  ".join(row[i].ljust(widths[i]) for i in range(len(headers))))
PY
}


cmd_reports() {
  local root=""
  local mode="list"
  local slug=""
  local open_editor=0
  local work_branch=""
  local push_after=1
  local prompt_only=0
  local reporter_filter=""
  local close_invalid=0
  local close_comment="Authenticity failed (automated by gpt-creator reports audit)."
  local close_comment_set=0
  local include_closed=0
  local audit_limit=""
  local audit_limit_set=0
  local digests_path=""
  local digests_path_set=0
  local invalid_label=""
  local invalid_label_set=0
  local allow_pairs=()

  while [[ $# -gt 0 ]]; do
    case "$1" in
      list)
        mode="list"
        shift
        ;;
      backlog)
        mode="backlog"
        shift
        ;;
      auto)
        mode="auto"
        shift
        ;;
      audit)
        mode="audit"
        shift
        ;;
      work)
        mode="work"
        shift
        if [[ $# -eq 0 ]]; then
          die "reports work requires a slug identifier"
        fi
        slug="$1"
        shift
        ;;
      show)
        mode="show"
        shift
        if [[ $# -eq 0 ]]; then
          die "reports show requires a slug identifier"
        fi
        slug="$1"
        shift
        ;;
      --project)
        root="$(abs_path "$2")"
        shift 2
        ;;
      --open)
        open_editor=1
        shift
        ;;
      --branch)
        work_branch="$2"
        shift 2
        ;;
      --no-push)
        push_after=0
        shift
        ;;
      --push)
        push_after=1
        shift
        ;;
      --prompt-only)
        prompt_only=1
        shift
        ;;
      --reporter)
        reporter_filter="$2"
        shift 2
        ;;
      --close-invalid)
        close_invalid=1
        shift
        ;;
      --no-close-invalid)
        close_invalid=0
        shift
        ;;
      --comment)
        close_comment="${2:?--comment requires text}"
        close_comment_set=1
        shift 2
        ;;
      --include-closed)
        include_closed=1
        shift
        ;;
      --limit)
        audit_limit="${2:?--limit requires a positive integer}"
        audit_limit_set=1
        shift 2
        ;;
      --digests)
        digests_path="${2:?--digests requires a file path}"
        digests_path_set=1
        shift 2
        ;;
      --allow)
        allow_pairs+=("${2:?--allow requires VERSION=SHA256}")
        shift 2
        ;;
      --label-invalid)
        invalid_label="${2:?--label-invalid requires a value}"
        invalid_label_set=1
        shift 2
        ;;
      --no-label-invalid)
        invalid_label=""
        invalid_label_set=1
        shift
        ;;
      -h|--help)
        cat <<'USAGE'
Usage:
  gpt-creator reports [--project PATH]
  gpt-creator reports list [--project PATH]
  gpt-creator reports backlog [--project PATH]
  gpt-creator reports auto [--project PATH] [--reporter NAME] [--no-push] [--prompt-only]
  gpt-creator reports work [--project PATH] [--branch NAME] [--no-push] [--prompt-only] <slug>
  gpt-creator reports [--project PATH] [--open] <slug>
  gpt-creator reports show [--project PATH] [--open] <slug>
  gpt-creator reports audit [--close-invalid] [--include-closed] [--limit N]
                               [--digests FILE] [--allow VERSION=SHA256]
                               [--label-invalid NAME|--no-label-invalid]
                               [--comment TEXT]

List captured crash/stall reports, show the open backlog, automatically resolve matching issues, assign Codex to resolve a single report, display a specific entry, or audit GitHub auto-reports for authenticity.
USAGE
        return 0
        ;;
      *)
        if [[ -z "$slug" && "$mode" == "list" ]]; then
          mode="show"
          slug="$1"
          shift
        else
          die "Unknown reports argument: $1"
        fi
        ;;
    esac
  done

  if [[ "$mode" == "show" && -z "$slug" ]]; then
    die "reports show requires a slug identifier"
  fi

  if [[ "$mode" == "work" && -z "$slug" ]]; then
    die "reports work requires a slug identifier"
  fi

  if [[ "$mode" == "work" && "$open_editor" -ne 0 ]]; then
    die "--open cannot be combined with reports work"
  fi

  if [[ "$mode" == "auto" && "$open_editor" -ne 0 ]]; then
    die "--open cannot be combined with reports auto"
  fi

  if [[ "$mode" == "auto" && -n "$work_branch" ]]; then
    die "--branch is not supported for reports auto"
  fi

  if [[ "$mode" != "work" && "$mode" != "auto" ]]; then
    if [[ -n "$work_branch" || "$prompt_only" -ne 0 || "$push_after" -ne 1 ]]; then
      die "reports options --branch/--no-push/--prompt-only are only valid with 'work' or 'auto'"
    fi
  fi

  if [[ "$mode" != "auto" && -n "$reporter_filter" ]]; then
    die "--reporter is only supported with reports auto"
  fi

  if [[ -n "$audit_limit" ]]; then
    if [[ ! "$audit_limit" =~ ^[0-9]+$ ]]; then
      die "--limit expects a non-negative integer"
    fi
  fi

  if [[ "$mode" != "audit" ]]; then
    if (( close_invalid != 0 || include_closed != 0 || close_comment_set != 0 || audit_limit_set != 0 || digests_path_set != 0 || invalid_label_set != 0 || ${#allow_pairs[@]} != 0 )); then
      die "reports options --close-invalid/--include-closed/--limit/--digests/--allow/--label-invalid/--comment are only valid with 'audit'"
    fi
  fi

  local pair_entry
  for pair_entry in "${allow_pairs[@]}"; do
    if [[ ! "$pair_entry" =~ ^[^=]+=[0-9a-fA-F]{64}$ ]]; then
      die "--allow expects VERSION=SHA256 (got '${pair_entry}')"
    fi
  done

  if [[ "$mode" != "audit" ]]; then
    if [[ -n "$root" ]]; then
      ensure_ctx "$root"
    else
      ensure_ctx "${PROJECT_ROOT:-$PWD}"
    fi
  elif [[ -n "$root" ]]; then
    ensure_ctx "$root"
  fi

  local reports_dir=""
  if [[ "$mode" != "audit" ]]; then
    reports_dir="$(gc_reports_dir)" || die "Unable to access issue report directory"
  fi

  case "$mode" in
    audit)
      local repo="${GC_GITHUB_REPO:-}"
      local token="${GC_GITHUB_TOKEN:-}"
      if [[ -z "$repo" || -z "$token" ]]; then
        die "reports audit requires GC_GITHUB_REPO and GC_GITHUB_TOKEN to be set"
      fi
      local state="open"
      if (( include_closed )); then
        state="all"
      fi
      if [[ -z "$digests_path" ]]; then
        if [[ -n "${GC_REPORT_DIGESTS_PATH:-}" ]]; then
          digests_path="${GC_REPORT_DIGESTS_PATH}"
        elif [[ -n "${CLI_ROOT:-}" && -f "${CLI_ROOT}/config/release-digests.json" ]]; then
          digests_path="${CLI_ROOT}/config/release-digests.json"
        fi
      fi
      local allow_join=""
      if ((${#allow_pairs[@]} > 0)); then
        allow_join="$(printf '%s\n' "${allow_pairs[@]}")"
      fi
      python3 - <<'PY' "$repo" "$token" "$state" "$close_invalid" "$close_comment" "$audit_limit" "$digests_path" "$invalid_label" "$allow_join"
import json
import os
import sys
import urllib.error
import urllib.parse
import urllib.request
import hashlib
from textwrap import indent

repo, token, state, close_flag, close_comment, limit_value, digests_path, invalid_label, allow_payload = sys.argv[1:10]
close_invalid = close_flag == "1"
limit = None
if limit_value:
    try:
        parsed_limit = int(limit_value)
        if parsed_limit > 0:
            limit = parsed_limit
    except ValueError:
        limit = None

session_headers = {
    "Authorization": f"Bearer {token}",
    "Accept": "application/vnd.github+json",
    "Content-Type": "application/json",
    "X-GitHub-Api-Version": "2022-11-28",
    "User-Agent": "gpt-creator-reports-audit",
}

def github_request(url, method="GET", payload=None):
    data = None
    if payload is not None:
        data = json.dumps(payload).encode("utf-8")
    request = urllib.request.Request(url, data=data, headers=session_headers, method=method)
    with urllib.request.urlopen(request) as resp:
        if resp.status == 204:
            return None
        body = resp.read().decode("utf-8")
        if not body:
            return None
        return json.loads(body)

def load_allowlist(path, inline_pairs):
    mapping = {}
    inline_pairs = [line.strip() for line in inline_pairs.splitlines() if line.strip()]
    for entry in inline_pairs:
        if "=" not in entry:
            continue
        version, digest = entry.split("=", 1)
        version = version.strip()
        digest = digest.strip().lower()
        if not version or len(digest) != 64:
            continue
        mapping.setdefault(version, set()).add(digest)
    if not path:
        return mapping
    try:
        with open(path, "r", encoding="utf-8") as fh:
            data = json.load(fh)
    except FileNotFoundError:
        return mapping
    except Exception as exc:
        print(f"Failed to read digest allowlist '{path}': {exc}", file=sys.stderr)
        return mapping
    if isinstance(data, dict):
        iterable = data.items()
    elif isinstance(data, list):
        iterable = []
        for item in data:
            if isinstance(item, dict):
                version = str(item.get("version") or "").strip()
                if not version:
                    continue
                sha_values = []
                sha_field = item.get("sha256")
                if isinstance(sha_field, str):
                    sha_values = [sha_field]
                elif isinstance(sha_field, list):
                    sha_values = [val for val in sha_field if isinstance(val, str)]
                elif isinstance(sha_field, dict):
                    sha_values = []
                    for val in sha_field.values():
                        if isinstance(val, str):
                            sha_values.append(val)
                        elif isinstance(val, list):
                            sha_values.extend(x for x in val if isinstance(x, str))
                for digest in sha_values:
                    mapping.setdefault(version, set()).add(digest.lower())
            elif isinstance(item, str):
                if "=" in item:
                    version, digest = item.split("=", 1)
                    mapping.setdefault(version.strip(), set()).add(digest.strip().lower())
        return mapping
    else:
        return mapping
    for version, values in iterable:
        if isinstance(values, str):
            mapping.setdefault(str(version), set()).add(values.lower())
        elif isinstance(values, list):
            mapping.setdefault(str(version), set()).update(val.lower() for val in values if isinstance(val, str))
        elif isinstance(values, dict):
            for val in values.values():
                if isinstance(val, str):
                    mapping.setdefault(str(version), set()).add(val.lower())
                elif isinstance(val, list):
                    mapping.setdefault(str(version), set()).update(v.lower() for v in val if isinstance(v, str))
    return mapping

digest_allowlist = load_allowlist(digests_path, allow_payload or "")

def fetch_issues():
    collected = []
    page = 1
    per_page = 100
    while True:
        url = f"https://api.github.com/repos/{repo}/issues?state={urllib.parse.quote(state)}&labels=auto-report&per_page={per_page}&page={page}"
        try:
            items = github_request(url)
        except urllib.error.HTTPError as err:
            text = err.read().decode("utf-8", "ignore")
            print(f"GitHub API error while fetching issues: {err.code} {text}", file=sys.stderr)
            return collected
        except Exception as exc:
            print(f"Failed to fetch GitHub issues: {exc}", file=sys.stderr)
            return collected
        if not items:
            break
        for issue in items:
            if "pull_request" in issue:
                continue
            collected.append(issue)
            if limit is not None and len(collected) >= limit:
                return collected
        if len(items) < per_page:
            break
        page += 1
    return collected

def parse_issue(issue):
    body = issue.get("body") or ""
    metadata = {}
    for line in body.splitlines():
        stripped = line.strip()
        if stripped.startswith("- **") and "**:" in stripped:
            head, value = stripped.split("**:", 1)
            label = head.replace("- **", "").strip()
            metadata[label] = value.strip()
    watermark_token = ""
    start = body.find("<!--")
    while start != -1:
        end = body.find("-->", start + 4)
        if end == -1:
            break
        comment = body[start + 4:end].strip()
        if comment.lower().startswith("gpt-creator:"):
            watermark_token = comment.split(":", 1)[1].strip()
            break
        start = body.find("<!--", end + 3)
    version = metadata.get("CLI Version", "").strip()
    binary_hash = metadata.get("CLI Binary SHA256", "").strip().lower()
    signature = metadata.get("CLI Signature", "").strip().lower()
    expected_signature = ""
    signature_valid = False
    watermark_valid = False
    reasons = []
    if version or binary_hash:
        expected_signature = hashlib.sha256(f"{version}:{binary_hash}".encode("utf-8")).hexdigest()
    if not binary_hash or len(binary_hash) != 64:
        reasons.append("missing CLI binary hash")
    if not version:
        reasons.append("missing CLI version")
    if signature and expected_signature:
        if signature == expected_signature:
            signature_valid = True
        else:
            reasons.append("signature mismatch")
    else:
        reasons.append("missing CLI signature")
    if watermark_token:
        watermark_expected = f"{version or 'unknown'}:{expected_signature}" if expected_signature else ""
        if watermark_expected and watermark_token.lower() == watermark_expected.lower():
            watermark_valid = True
        elif version and signature_valid and watermark_token.lower() == f"{version.lower()}:unsigned":
            watermark_valid = False
            reasons.append("watermark unsigned but signature present")
        else:
            reasons.append("watermark mismatch")
    else:
        reasons.append("missing watermark")
    digest_allowed = True
    if digest_allowlist:
        allowed_hashes = digest_allowlist.get(version, set())
        if allowed_hashes:
            if binary_hash not in allowed_hashes:
                digest_allowed = False
                reasons.append("binary hash not in allowlist")
        else:
            digest_allowed = False
            reasons.append("version not present in allowlist")
    valid = signature_valid and watermark_valid and digest_allowed
    return {
        "number": issue.get("number"),
        "title": issue.get("title") or "",
        "url": issue.get("html_url") or "",
        "state": issue.get("state"),
        "version": version,
        "binary_hash": binary_hash,
        "signature": signature,
        "expected_signature": expected_signature,
        "signature_valid": signature_valid,
        "watermark": watermark_token,
        "watermark_valid": watermark_valid,
        "digest_allowed": digest_allowed,
        "valid": valid,
        "reasons": reasons,
        "metadata": metadata,
    }

def ensure_label(issue_number):
    if not invalid_label:
        return
    try:
        issue = github_request(f"https://api.github.com/repos/{repo}/issues/{issue_number}")
    except Exception:
        return
    if not issue:
        return
    labels = issue.get("labels") or []
    label_names = {lbl["name"] if isinstance(lbl, dict) else str(lbl) for lbl in labels}
    if invalid_label in label_names:
        return
    label_names.add(invalid_label)
    payload = {"labels": sorted(label_names)}
    try:
        github_request(f"https://api.github.com/repos/{repo}/issues/{issue_number}", method="PATCH", payload=payload)
    except Exception:
        pass

def close_issue(item):
    number = item["number"]
    if not close_invalid:
        return
    if str(item.get("state")) == "closed":
        return
    try:
        github_request(
            f"https://api.github.com/repos/{repo}/issues/{number}/comments",
            method="POST",
            payload={"body": close_comment},
        )
    except Exception as exc:
        print(f"Failed to comment on issue #{number}: {exc}", file=sys.stderr)
    try:
        github_request(
            f"https://api.github.com/repos/{repo}/issues/{number}",
            method="PATCH",
            payload={"state": "closed"},
        )
    except Exception as exc:
        print(f"Failed to close issue #{number}: {exc}", file=sys.stderr)
    else:
        ensure_label(number)
        print(f"Closed issue #{number} ({item['title']}) as authenticity failed.")

issues = fetch_issues()
if not issues:
    print("No GitHub issues labelled 'auto-report' found.")
    sys.exit(0)

parsed = [parse_issue(issue) for issue in issues]
valid_count = sum(1 for item in parsed if item["valid"])
invalid_items = [item for item in parsed if not item["valid"]]

print(f"Audited {len(parsed)} auto-report issue(s) [{state}].")
print(f" - Valid:   {valid_count}")
print(f" - Invalid: {len(invalid_items)}")
if digest_allowlist:
    print(f" - Allowlist source: {digests_path or 'inline overrides only'}")

def summarize(item):
    header = f"#{item['number']} [{item['state']}] {item['title']}"
    verdict = "VALID" if item["valid"] else "INVALID"
    print(f"\n{header}\nResult: {verdict}")
    print(f"URL: {item['url']}")
    print(f"Version: {item['version'] or '(missing)'}")
    print(f"Binary SHA256: {item['binary_hash'] or '(missing)'}")
    print(f"Signature: {item['signature'] or '(missing)'}")
    print(f"Watermark: {item['watermark'] or '(missing)'}")
    print(f"Signature OK: {'yes' if item['signature_valid'] else 'no'}")
    print(f"Watermark OK: {'yes' if item['watermark_valid'] else 'no'}")
    if digest_allowlist:
        print(f"Allowlist OK: {'yes' if item['digest_allowed'] else 'no'}")
    if item["reasons"]:
        detail = "\n".join(f"- {reason}" for reason in item["reasons"])
        print("Notes:\n" + indent(detail, "  "))

for item in parsed:
    summarize(item)
    if not item["valid"]:
        close_issue(item)

PY
      return
      ;;
    list|backlog)
      if [[ -z "$(find "$reports_dir" -maxdepth 1 -type f \( -name '*.yml' -o -name '*.yaml' \) -print -quit 2>/dev/null)" ]]; then
        info "No issue reports recorded yet."
        return 0
      fi
      python3 - <<'PY' "$reports_dir" "$mode"
import datetime
import os
import sys

dir_path = sys.argv[1]
mode = sys.argv[2]
entries = []

for name in os.listdir(dir_path):
    lower = name.lower()
    if not (lower.endswith(".yml") or lower.endswith(".yaml")):
        continue
    path = os.path.join(dir_path, name)
    if not os.path.isfile(path):
        continue
    try:
        stat = os.stat(path)
    except OSError:
        continue
    slug = os.path.splitext(name)[0]
    summary = ""
    priority = ""
    timestamp = ""
    issue_type = ""
    status = ""
    likes = 0
    comments = 0
    reporter = ""
    metadata = False
    try:
        with open(path, "r", encoding="utf-8", errors="replace") as fh:
            for line in fh:
                stripped = line.strip()
                if stripped.startswith("summary:") and not summary:
                    value = stripped.split(":", 1)[1].strip()
                    if value.startswith('"') and value.endswith('"') and len(value) >= 2:
                        value = value[1:-1]
                    summary = value
                elif stripped.startswith("priority:") and not priority:
                    priority = stripped.split(":", 1)[1].strip()
                elif stripped.startswith("type:") and not issue_type:
                    issue_type = stripped.split(":", 1)[1].strip()
                elif stripped.startswith("timestamp:") and not timestamp:
                    timestamp = stripped.split(":", 1)[1].strip().strip('"')
                if stripped == "metadata:":
                    metadata = True
                    continue
                if metadata:
                    if not line.startswith("  "):
                        metadata = False
                    else:
                        meta = line.strip()
                        if meta.startswith("timestamp:") and not timestamp:
                            timestamp = meta.split(":", 1)[1].strip().strip('"')
                        elif meta.startswith("status:") and not status:
                            status = meta.split(":", 1)[1].strip()
                        elif meta.startswith("likes:") and likes == 0:
                            try:
                                likes = int(meta.split(":", 1)[1].strip())
                            except ValueError:
                                likes = 0
                        elif meta.startswith("comments:") and comments == 0:
                            try:
                                comments = int(meta.split(":", 1)[1].strip())
                            except ValueError:
                                comments = 0
                        elif meta.startswith("reporter:") and not reporter:
                            reporter = meta.split(":", 1)[1].strip().strip('"')
    except OSError:
        continue
    popularity = likes + comments
    entries.append((stat.st_mtime, slug, summary, priority, timestamp, issue_type, status, popularity, likes, comments, reporter))

entries.sort(key=lambda item: item[0], reverse=True)

printed = False
for mtime, slug, summary, priority, timestamp, issue_type, status, popularity, likes, comments, reporter in entries:
    if mode == "backlog" and status and status.lower() not in ("open", "new", "todo"):
        continue
    summary = summary or "(no summary)"
    priority = priority or "unknown"
    issue_type = issue_type or "unknown"
    status_display = status or "open"
    display_time = timestamp or datetime.datetime.utcfromtimestamp(mtime).strftime("%Y-%m-%dT%H:%M:%SZ")
    print(f"[{slug}] {display_time} {priority} ({issue_type}) status={status_display} pop={popularity} (likes={likes}, comments={comments})")
    if reporter:
        print(f"  reporter={reporter}")
    print(f"  {summary}")
    print()
    printed = True

if mode == "backlog" and not printed:
    print("No open reports found.")
PY
      ;;
    auto)
      local reporter="${reporter_filter:-$(gc_reports_current_user)}"
      info "Auto-processing reports for reporter: ${reporter}"
      local auto_slugs=()
      while IFS= read -r slug_entry; do
        [[ -n "$slug_entry" ]] && auto_slugs+=("$slug_entry")
      done < <(python3 - <<'PY' "$reports_dir" "$reporter"
import os
import sys

dir_path = sys.argv[1]
reporter_filter = sys.argv[2].strip()
entries = []

for name in os.listdir(dir_path):
    lower = name.lower()
    if not (lower.endswith('.yml') or lower.endswith('.yaml')):
        continue
    path = os.path.join(dir_path, name)
    if not os.path.isfile(path):
        continue
    try:
        stat = os.stat(path)
    except OSError:
        continue
    slug = os.path.splitext(name)[0]
    metadata = {}
    metadata_active = False
    try:
        with open(path, 'r', encoding='utf-8', errors='replace') as fh:
            for line in fh:
                stripped = line.strip()
                if stripped == 'metadata:':
                    metadata_active = True
                    continue
                if metadata_active:
                    if line.startswith('  '):
                        if ':' in stripped:
                            k, v = stripped.split(':', 1)
                            metadata[k.strip()] = v.strip().strip('"')
                    else:
                        metadata_active = False
    except OSError:
        continue
    reporter = metadata.get('reporter', '')
    status = metadata.get('status', 'open').lower()
    if reporter_filter and reporter.lower() != reporter_filter.lower():
        continue
    if status not in ('open', 'new', 'todo'):
        continue
    entries.append((stat.st_mtime, slug))

entries.sort(key=lambda item: item[0], reverse=True)

for _, slug in entries:
    print(slug)
PY
)
      if ((${#auto_slugs[@]} == 0)); then
        info "No matching reports for reporter '${reporter}'."
        return 0
      fi
      local slug_entry
      for slug_entry in "${auto_slugs[@]}"; do
        info "Resolving report ${slug_entry}"
        if ! gc_reports_run_work "$slug_entry" "" "$push_after" "$prompt_only" "$reporter"; then
          die "Codex failed while processing report ${slug_entry}"
        fi
      done
      ;;
    show)
      local report_path=""
      if ! report_path="$(gc_reports_resolve_slug "$slug")"; then
        die "No issue report found for slug: ${slug}"
      fi
      info "Report file: ${report_path}"
      printf '\n'
      cat "$report_path"
      printf '\n'
      if (( open_editor )); then
        local editor_cmd="${EDITOR_CMD:-${EDITOR:-vi}}"
        if [[ -z "$editor_cmd" ]]; then
          warn "EDITOR_CMD not set; skipping --open"
        else
          info "Opening report in ${editor_cmd}"
          if ! bash -lc "${editor_cmd} \"${report_path}\""; then
            warn "Failed to launch editor command: ${editor_cmd}"
          fi
        fi
      fi
      ;;
    work)
      if ! gc_reports_run_work "$slug" "$work_branch" "$push_after" "$prompt_only" ""; then
        die "Codex failed to resolve report ${slug}"
      fi
      ;;
  esac
}

cmd_create_project() {
  local template_request="auto"
  local path=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --template)
        template_request="${2:?--template requires a value (template name or 'auto')}"
        shift 2
        ;;
      --skip-template)
        template_request="skip"
        shift
        ;;
      -h|--help)
        cat <<'USAGE'
Usage: gpt-creator create-project [--template NAME|auto|skip] [--skip-template] <path>

Create a new project root, optionally scaffold it from a project template, then
run the full build pipeline (scan → normalize → plan → generate → db → run → verify).
USAGE
        return 0
        ;;
      *)
        if [[ -z "$path" ]]; then
          path="$1"
        else
          die "Unexpected argument: $1"
        fi
        shift
        ;;
    esac
  done

  [[ -n "$path" ]] || die "create-project requires a path"

  local project_root
  project_root="$(abs_path "$path")"
  mkdir -p "$project_root"

  if ! gc_apply_project_template "$project_root" "$template_request"; then
    warn "Project template application reported issues; continuing with base scaffolding."
  fi

  ensure_ctx "$project_root"
  info "Project root: ${PROJECT_ROOT}"

  cmd_scan --project "$PROJECT_ROOT"
  cmd_normalize --project "$PROJECT_ROOT"
  cmd_plan --project "$PROJECT_ROOT"
  cmd_generate all --project "$PROJECT_ROOT"
  cmd_db provision --project "$PROJECT_ROOT" || warn "Database provision step reported an error"
  cmd_run up --project "$PROJECT_ROOT" || warn "Stack start reported an error"
  cmd_verify acceptance --project "$PROJECT_ROOT" || warn "Acceptance checks failing — review stack health."
  ok "Project bootstrap complete"
}

cmd_bootstrap() {
  local template_request="auto"
  local path=""
  local fresh=0
  local rfp_path=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --template)
        template_request="${2:?--template requires a value (template name or 'auto')}"
        shift 2
        ;;
      --skip-template)
        template_request="skip"
        shift
        ;;
      --rfp)
        rfp_path="${2:?--rfp requires a file path}"
        shift 2
        ;;
      --fresh)
        fresh=1
        shift
        ;;
      --fresh)
        fresh=1
        shift
        ;;
      -h|--help)
        cat <<'USAGE'
Usage: gpt-creator bootstrap [--template NAME|auto|skip] [--skip-template] [--rfp FILE] [--fresh] <path>

Generate a full project from an RFP in one step by running:
  • create-pdr → create-sds → create-db-dump → create-jira-tasks → scan/normalize/plan/generate/db/run/verify
Templates from project_templates/ are applied first (auto-selected by default).
USAGE
        return 0
        ;;
      *)
        if [[ -z "$path" ]]; then
          path="$1"
        else
          die "Unexpected argument: $1"
        fi
        shift
        ;;
    esac
  done

  [[ -n "$path" ]] || die "bootstrap requires a path"

  if [[ -n "$rfp_path" ]]; then
    [[ -f "$rfp_path" ]] || die "RFP file not found: ${rfp_path}"
  fi

  local project_root
  project_root="$(abs_path "$path")"
  mkdir -p "$project_root"

  ensure_ctx "$project_root"
  info "Project root: ${PROJECT_ROOT}"

  if (( fresh )); then
    gc_bootstrap_reset_state
  fi

  mkdir -p "$(gc_bootstrap_state_dir)"

  if gc_bootstrap_step_is_done template; then
    info "Step 'template' already completed; skipping."
  else
    if gc_apply_project_template "$PROJECT_ROOT" "$template_request"; then
      gc_bootstrap_mark_step template "done"
    else
      gc_bootstrap_mark_step template "failed"
      die "Project template application failed"
    fi
  fi

  if [[ -n "$rfp_path" ]]; then
    local staged_rfp="${INPUT_DIR}/rfp.md"
    local staged_docs_rfp="${STAGING_DIR}/docs/rfp.md"
    if gc_bootstrap_step_is_done stage-rfp; then
      if [[ ! -f "$staged_rfp" || ! -f "$staged_docs_rfp" ]]; then
        info "Restaging RFP (previous artifacts missing)."
        gc_bootstrap_mark_step stage-rfp "reset"
      fi
    fi
    if ! gc_bootstrap_step_is_done stage-rfp; then
      mkdir -p "${INPUT_DIR}"
      cp "$rfp_path" "$staged_rfp"
      mkdir -p "${STAGING_DIR}/docs"
      cp "$rfp_path" "${STAGING_DIR}/docs/rfp.md"
      gc_bootstrap_mark_step stage-rfp "done"
      info "Staged RFP → ${staged_rfp}"
    else
      info "Step 'stage-rfp' already completed; skipping."
    fi
  fi

  info "[1/10] Scanning documentation"
  if ! gc_bootstrap_run_step scan cmd_scan --project "$PROJECT_ROOT"; then
    die "Bootstrap halted during scan"
  fi

  info "[2/10] Normalizing documentation"
  if ! gc_bootstrap_run_step normalize cmd_normalize --project "$PROJECT_ROOT"; then
    die "Bootstrap halted during normalize"
  fi

  info "[3/10] Generating Product Requirements Document"
  if gc_bootstrap_step_is_done create-pdr; then
    info "Step 'create-pdr' already completed; skipping."
  else
    if ! gc_bootstrap_have_rfp; then
      warn "No RFP found in staging; skipping create-pdr. Provide --rfp or add .gpt-creator/staging/docs/rfp.md to enable this step."
      gc_bootstrap_mark_step create-pdr "done"
    elif bash "$CLI_ROOT/src/cli/create-pdr.sh" --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step create-pdr "done"
    else
      gc_bootstrap_mark_step create-pdr "failed"
      die "create-pdr failed"
    fi
  fi

  info "[4/10] Generating System Design Specification"
  if gc_bootstrap_step_is_done create-sds; then
    info "Step 'create-sds' already completed; skipping."
  else
    if bash "$CLI_ROOT/src/cli/create-sds.sh" --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step create-sds "done"
    else
      gc_bootstrap_mark_step create-sds "failed"
      die "create-sds failed"
    fi
  fi

  info "[5/11] Generating database schema & seed dumps"
  if gc_bootstrap_step_is_done create-db-dump; then
    info "Step 'create-db-dump' already completed; skipping."
  else
    if bash "$CLI_ROOT/src/cli/create-db-dump.sh" --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step create-db-dump "done"
    else
      gc_bootstrap_mark_step create-db-dump "failed"
      die "create-db-dump failed"
    fi
  fi

  info "[6/11] Mining Jira tasks"
  if ! gc_bootstrap_step_is_done create-jira-tasks; then
    if cmd_create_jira_tasks --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step create-jira-tasks "done"
    else
      gc_bootstrap_mark_step create-jira-tasks "failed"
      die "create-jira-tasks failed"
    fi
  else
    info "Step 'create-jira-tasks' already completed; skipping."
  fi

  info "[7/11] Planning build"
  if ! gc_bootstrap_step_is_done plan; then
    if cmd_plan --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step plan "done"
    else
      gc_bootstrap_mark_step plan "failed"
      die "plan step failed"
    fi
  else
    info "Step 'plan' already completed; skipping."
  fi

  if ! gc_bootstrap_step_is_done generate; then
    info "[8/11] Generating stack code"
    if cmd_generate all --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step generate "done"
    else
      gc_bootstrap_mark_step generate "failed"
      die "generate step failed"
    fi
  else
    info "Step 'generate' already completed; skipping."
  fi

  info "[9/11] Provisioning infrastructure"
  if ! gc_bootstrap_step_is_done db-provision; then
    if cmd_db provision --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step db-provision "done"
    else
      gc_bootstrap_mark_step db-provision "failed"
      die "Database provision failed"
    fi
  else
    info "Step 'db-provision' already completed; skipping."
  fi

  info "[10/11] Starting stack"
  if ! gc_bootstrap_step_is_done run-up; then
    if cmd_run up --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step run-up "done"
    else
      gc_bootstrap_mark_step run-up "failed"
      die "Stack start failed"
    fi
  else
    info "Step 'run-up' already completed; skipping."
  fi

  if ! gc_bootstrap_step_is_done verify; then
    if cmd_verify acceptance --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step verify "done"
    else
      gc_bootstrap_mark_step verify "failed"
      die "Acceptance verification failed"
    fi
  else
    info "Step 'verify' already completed; skipping."
  fi

  info "[11/11] Verifying acceptance"
  gc_bootstrap_mark_complete
  ok "Bootstrap complete — code, docs, and backlog generated"
}

cmd_update() {
  local force=0
  local repo_url="${GC_UPDATE_REPO_URL:-https://github.com/bekirdag/gpt-creator.git}"
  local prefix="/usr/local"
  local tmpdir=""

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --force)
        force=1
        shift
        ;;
      -h|--help)
        cat <<EOF
Usage: ${APP_NAME} update [--force]

Fetches the latest gpt-creator sources and reinstalls the CLI.
EOF
        return 0
        ;;
      *)
        die "Unknown argument for update: $1"
        ;;
    esac
  done

  if ! command -v git >/dev/null 2>&1; then
    die "'git' is required for ${APP_NAME} update"
  fi

  tmpdir="$(mktemp -d "${TMPDIR:-/tmp}/gpt-creator-update.XXXXXX")" || die "Unable to create temporary directory"

  info "Cloning repository: $repo_url"
  if ! git clone "$repo_url" "$tmpdir"; then
    rm -rf "$tmpdir"
    die "Failed to clone repository from ${repo_url}"
  fi

  info "Fetching latest changes"
  if ! git -C "$tmpdir" pull --ff-only; then
    rm -rf "$tmpdir"
    die "Failed to update repository in $tmpdir"
  fi

  local install_script="$tmpdir/scripts/install.sh"
  if [[ ! -x "$install_script" ]]; then
    rm -rf "$tmpdir"
    die "Installer not found at $install_script"
  fi

  local -a install_cmd=("$install_script" --prefix "$prefix")
  if (( force )); then
    install_cmd+=("--force")
  fi

  info "Running installer: ${install_cmd[*]}"
  if ! "${install_cmd[@]}"; then
    rm -rf "$tmpdir"
    die "Install script failed"
  fi

  rm -rf "$tmpdir"
  ok "gpt-creator updated successfully"
}

cmd_keys() {
  local action="${1:-list}"
  case "$action" in
    list|status)
      shift || true
      if (($#)); then
        die "keys ${action} does not take additional arguments"
      fi
      gc_api_keys_list
      ;;
    set)
      shift || true
      local target="${1:-}"
      [[ -n "$target" ]] || die "keys set requires a service name or environment variable"
      gc_api_keys_set "$target"
      ;;
    help|-h|--help)
      cat <<EOF
Usage: ${APP_NAME} keys [list|status]
       ${APP_NAME} keys set <service>
       ${APP_NAME} keys <service>

Lists required third-party API keys and stores credentials under your user config directory.
EOF
      ;;
    *)
      if (($# > 1)); then
        die "Unknown keys action: $*"
      fi
      gc_api_keys_set "$action"
      ;;
  esac
}

cmd_tui() {
  ensure_go_runtime
  local go_bin="${GC_GO_BIN:-${GO_BIN:-go}}"
  if ! command -v "$go_bin" >/dev/null 2>&1; then
    die "Go 1.21+ is required to run the gpt-creator TUI. Automatic setup failed; install Go manually and set GO_BIN."
  fi
  local tui_dir="${CLI_ROOT}/tui"
  if [[ ! -d "$tui_dir" ]]; then
    die "TUI sources not found at ${tui_dir}"
  fi
  local skip_tidy="${GC_SKIP_TUI_TIDY:-}"
  info "Launching gpt-creator TUI (preview)"
  (
    cd "$tui_dir"
    local go_dir_readonly=0
    local readonly_path=""
    if [[ ! -w go.mod ]]; then
      go_dir_readonly=1
      readonly_path="${tui_dir}/go.mod"
    elif [[ -e go.sum && ! -w go.sum ]]; then
      go_dir_readonly=1
      readonly_path="${tui_dir}/go.sum"
    fi
    if [[ -z "$skip_tidy" ]]; then
      if (( go_dir_readonly )); then
        warn "Skipping 'go mod tidy' because ${readonly_path} is not writable"
      else
        info "Ensuring TUI Go modules are tidy"
        if ! "$go_bin" mod tidy >/dev/null 2>&1; then
          warn "'go mod tidy' reported issues; retrying with output"
          if ! "$go_bin" mod tidy; then
            die "Failed to tidy Go modules required for the TUI"
          fi
        fi
      fi
    fi
    if (( go_dir_readonly )) && [[ "${GOFLAGS:-}" != *"-mod="* ]]; then
      export GOFLAGS="${GOFLAGS:+${GOFLAGS} }-mod=readonly"
    fi
    "$go_bin" run . "$@"
  )
}

usage() {
cat <<EOF
${APP_NAME} v${VERSION}

Usage:
  ${APP_NAME} [--reports-on|--reports-off] [--reports-idle-timeout SECONDS] <command> [args]
  ${APP_NAME} create-project [--template NAME|auto|skip] [--skip-template] <path>
  ${APP_NAME} bootstrap [--template NAME|auto|skip] [--skip-template] [--rfp FILE] [--fresh] <path>
  ${APP_NAME} scan [--project <path>]
  ${APP_NAME} normalize [--project <path>]
  ${APP_NAME} plan [--project <path>]
  ${APP_NAME} generate <api|web|admin|db|docker|all> [--project <path>]
  ${APP_NAME} db <provision|import|seed> [--project <path>]
  ${APP_NAME} run <up|down|logs|open> [--project <path>]
  ${APP_NAME} refresh-stack [options]
  ${APP_NAME} verify <acceptance|nfr|all> [--project <path>] [--api-url API_BASE] [--api-health URL] [--web-url URL] [--admin-url URL]
  ${APP_NAME} create-sds [--project <path>] [--model NAME] [--dry-run] [--force]
  ${APP_NAME} create-pdr [--project <path>] [--model NAME] [--dry-run] [--force]
  ${APP_NAME} create-jira-tasks [--project <path>] [--model NAME] [--force] [--dry-run]
  ${APP_NAME} create-db-dump [--project <path>] [--model NAME] [--dry-run] [--force]
  ${APP_NAME} migrate-tasks [--project <path>] [--force]
  ${APP_NAME} refine-tasks [--project <path>] [--story SLUG] [--model NAME] [--dry-run]
  ${APP_NAME} create-tasks [--project <path>] [--jira <file>] [--force]
  ${APP_NAME} backlog [--project <path>] [--type epics|stories] [--item-children ID] [--progress] [--task-details ID]
  ${APP_NAME} estimate [--project <path>]
  ${APP_NAME} tokens [--project <path>] [--details] [--json]
  ${APP_NAME} reports [--project <path>] [list|backlog|show|work] [--branch NAME] [--no-push] [--prompt-only] [--open] [slug]
  ${APP_NAME} show-file [--range A:B|--head N|--tail N|--diff|--refresh] <path>
  ${APP_NAME} task-convert [options] (deprecated; runs create-tasks)
  ${APP_NAME} sweep-artifacts [--project <path>] [path...]
  ${APP_NAME} tidy-progress [--project <path>] [path...] (deprecated alias of sweep-artifacts)
  ${APP_NAME} work-on-tasks [--project <path>] [--story ID|SLUG] [--fresh] [--no-verify] [--keep-artifacts]
  ${APP_NAME} iterate [--project <path>] [--jira <file>] [--no-verify] (deprecated)
  ${APP_NAME} keys [list|set <service>]
  ${APP_NAME} tui
  ${APP_NAME} update [--force]
  ${APP_NAME} version
  ${APP_NAME} help

Global flags:
  --reports-on               Enable automatic crash/stall reports for the current invocation
  --reports-off              Disable automatic crash/stall reports (overrides GC_REPORTS_ON=1)
  --reports-idle-timeout <s> Override idle detection threshold in seconds (default: ${GC_REPORTS_IDLE_TIMEOUT})

Environment overrides:
  CODEX_BIN, CODEX_MODEL, DOCKER_BIN, MYSQL_BIN, EDITOR_CMD, GC_API_HEALTH_URL, GC_WEB_URL, GC_ADMIN_URL
EOF
}

main() {
  local cmd="${1:-help}"; shift || true
  local shell_bin="${BASH:-bash}"
  case "$cmd" in
    help|-h|--help) usage ;;
    version|-v|--version) echo "${APP_NAME} ${VERSION}" ;;
    create-project) cmd_create_project "$@" ;;
    bootstrap)      cmd_bootstrap "$@" ;;
    scan)           cmd_scan "$@" ;;
    normalize)      cmd_normalize "$@" ;;
    plan)           cmd_plan "$@" ;;
    generate)       cmd_generate "$@" ;;
    db)             cmd_db "$@" ;;
    run)            cmd_run "$@" ;;
    refresh-stack)  cmd_refresh_stack "$@" ;;
    verify)         cmd_verify "$@" ;;
    migrate-tasks)  cmd_migrate_tasks_json "$@" ;;
    refine-tasks)   cmd_refine_tasks "$@" ;;
    create-sds)     "$shell_bin" "$CLI_ROOT/src/cli/create-sds.sh" "$@" ;;
    create-pdr)     "$shell_bin" "$CLI_ROOT/src/cli/create-pdr.sh" "$@" ;;
    create-jira-tasks) "$CLI_ROOT/src/cli/create-jira-tasks.sh" "$@" ;;
    create-db-dump) "$shell_bin" "$CLI_ROOT/src/cli/create-db-dump.sh" "$@" ;;
    create-tasks)   cmd_create_tasks "$@" ;;
    backlog)        cmd_backlog "$@" ;;
    estimate)       cmd_estimate "$@" ;;
    tokens)         cmd_tokens "$@" ;;
    reports)        cmd_reports "$@" ;;
    task-convert)   cmd_task_convert "$@" ;;
    show-file)      cmd_show_file "$@" ;;
    sweep-artifacts)  cmd_sweep_artifacts "$@" ;;
    tidy-progress)  cmd_tidy_progress "$@" ;;
    work-on-tasks)  cmd_work_on_tasks "$@" ;;
    iterate)        cmd_iterate "$@" ;;
    keys)           cmd_keys "$@" ;;
    tui)            cmd_tui "$@" ;;
    update)         cmd_update "$@" ;;
    *) die "Unknown command: ${cmd}. See '${APP_NAME} help'" ;;
  esac
}

gc_load_api_keys

GC_ORIGINAL_ARGS=("$@")
gc_reports_extract_global_flags "$@"
set -- "${GC_FILTERED_ARGS[@]}"

GC_INVOCATION="$0"
if ((${#GC_ORIGINAL_ARGS[@]})); then
  GC_INVOCATION+=" $(printf '%q ' "${GC_ORIGINAL_ARGS[@]}")"
  GC_INVOCATION="${GC_INVOCATION% }"
fi

if gc_reports_enabled; then
  trap 'gc_reports_activity_trap "$BASH_COMMAND"' DEBUG
fi

trap 'gc_interrupt_handler INT' INT
trap 'gc_interrupt_handler TERM' TERM
trap 'gc_interrupt_handler TSTP' TSTP
trap 'gc_interrupt_handler QUIT' QUIT

trap 'gc_capture_error_context $? "$BASH_COMMAND"' ERR
trap 'gc_exit_handler $?' EXIT

main "$@"
