#!/usr/bin/env bash
# gpt-creator — scaffolding & orchestration CLI
# Aligns with Product Definition & Requirements (PDR v0.2)
# Usage: gpt-creator <command> [args]

set -Eeuo pipefail

if (( BASH_VERSINFO[0] < 4 )); then
  if [[ -z "${GC_BASH_BOOTSTRAP:-}" ]]; then
    bash_candidates=()
    if [[ -n "${GC_PREFERRED_BASH:-}" ]]; then
      bash_candidates+=("${GC_PREFERRED_BASH}")
    fi
    if [[ -n "${GC_BASH:-}" ]]; then
      bash_candidates+=("${GC_BASH}")
    fi
    if command -v brew >/dev/null 2>&1; then
      brew_bash="$(brew --prefix 2>/dev/null)/bin/bash"
      if [[ -x "${brew_bash:-}" ]]; then
        bash_candidates+=("$brew_bash")
      fi
    fi
    bash_candidates+=("/opt/homebrew/bin/bash" "/usr/local/bin/bash")
    for candidate in "${bash_candidates[@]}"; do
      [[ -n "$candidate" ]] || continue
      if [[ "$candidate" != "$BASH" && -x "$candidate" ]]; then
        if "$candidate" -c '[[ ${BASH_VERSINFO[0]} -ge 4 ]]' >/dev/null 2>&1; then
          export GC_BASH_BOOTSTRAP=1
          PATH="$(dirname "$candidate"):$PATH"
          export PATH
          exec "$candidate" "$0" "$@"
        fi
      fi
    done
  fi
  printf 'gpt-creator requires Bash 4 or newer. Install via `brew install bash` and re-run, or set GC_PREFERRED_BASH to a modern shell.\n' >&2
  exit 1
fi

mkdir -p "${PWD}/.gpt-creator/shims" 2>/dev/null || true
export BASH_ENV="${BASH_ENV:-$PWD/.gpt-creator/shims/bash_env.sh}"

if [[ -t 1 && $# -eq 0 && -z "${GC_SKIP_TUI_AUTO:-}" ]]; then
  export GC_SKIP_TUI_AUTO=1
  exec "$0" tui
fi

# Crash logging globals
GC_CRASH_LOGGED=0
GC_LAST_ERROR_CMD=""
GC_LAST_ERROR_STATUS=0
GC_FAIL_LOG_DIR=""
GC_INVOCATION=""

GC_LAST_CRASH_LOG=""
GC_MAIN_PID="$$"

if [[ -n "${GC_REPORTS_ON:-}" && "${GC_REPORTS_ON}" != "0" ]]; then
  GC_REPORTS_ON=1
else
  GC_REPORTS_ON=0
fi
GC_REPORTS_IDLE_TIMEOUT="${GC_REPORTS_IDLE_TIMEOUT:-1800}"
GC_REPORTS_CHECK_INTERVAL="${GC_REPORTS_CHECK_INTERVAL:-60}"
GC_REPORTS_INITIALIZED=0
GC_REPORTS_STORE_DIR=""
GC_REPORTS_ACTIVITY_FILE=""
GC_REPORTS_IDLE_SENTINEL=""
GC_REPORTS_WATCHDOG_PID=""
# shellcheck disable=SC2034
GC_FILTERED_ARGS=()
# shellcheck disable=SC2034
GC_LAST_AUTO_COMMIT_HASH=""
# shellcheck disable=SC2034
GC_LAST_AUTO_COMMIT_STATUS=""


# Ensure docker compose commands have adequate timeouts unless caller overrides
if [[ -n "${GC_DOCKER_COMPOSE_TIMEOUT:-}" ]]; then
  COMPOSE_HTTP_TIMEOUT="$GC_DOCKER_COMPOSE_TIMEOUT"
  DOCKER_CLIENT_TIMEOUT="$GC_DOCKER_COMPOSE_TIMEOUT"
fi
: "${COMPOSE_HTTP_TIMEOUT:=600}"
: "${DOCKER_CLIENT_TIMEOUT:=$COMPOSE_HTTP_TIMEOUT}"
export COMPOSE_HTTP_TIMEOUT DOCKER_CLIENT_TIMEOUT

: "${GC_DOCKER_HEALTH_TIMEOUT:=10}"
: "${GC_DOCKER_HEALTH_INTERVAL:=1}"
: "${GC_PNPM_VERSION:=10.17.1}"
# shellcheck disable=SC2034
GC_CODEX_EXEC_TIMEOUT_INITIAL="${GC_CODEX_EXEC_TIMEOUT-}"
: "${GC_CODEX_EXEC_TIMEOUT:=0}"
: "${GC_CODEX_EXEC_MAX_DURATION:=900}"
: "${GC_CODEX_MAX_TURNS:=180}"
: "${GC_CODEX_MAX_TOKENS_PER_TASK:=0}"
export GC_CODEX_EXEC_MAX_DURATION
export GC_CODEX_MAX_TURNS

GC_LLM_OUTPUT_LIMIT_PLAN_DEFAULT=450
GC_LLM_OUTPUT_LIMIT_STATUS_DEFAULT=350
GC_LLM_OUTPUT_LIMIT_VERIFY_DEFAULT=500
GC_LLM_OUTPUT_LIMIT_PATCH_DEFAULT=7000
GC_LLM_OUTPUT_LIMIT_HARD_CAP_DEFAULT=12000

GC_LLM_OUTPUT_LIMIT_PLAN="${GC_LLM_OUTPUT_LIMIT_PLAN_DEFAULT}"
GC_LLM_OUTPUT_LIMIT_STATUS="${GC_LLM_OUTPUT_LIMIT_STATUS_DEFAULT}"
GC_LLM_OUTPUT_LIMIT_VERIFY="${GC_LLM_OUTPUT_LIMIT_VERIFY_DEFAULT}"
GC_LLM_OUTPUT_LIMIT_PATCH="${GC_LLM_OUTPUT_LIMIT_PATCH_DEFAULT}"
GC_LLM_OUTPUT_LIMIT_HARD_CAP="${GC_LLM_OUTPUT_LIMIT_HARD_CAP_DEFAULT}"

GC_LAST_CODEX_OUTPUT_STEP=""
GC_LAST_CODEX_OUTPUT_LIMIT=0

gc_budget_stage_id() {
  local stage="${1:-}"
  stage="${stage,,}"
  stage="${stage//[^a-z0-9]/_}"
  printf '%s' "$stage"
}

gc_budget_stage_limit_var() {
  local id
  id="$(gc_budget_stage_id "$1")"
  id="${id^^}"
  printf 'GC_BUDGET_STAGE_LIMIT_%s' "$id"
}

gc_budget_stage_total_var() {
  local id
  id="$(gc_budget_stage_id "$1")"
  id="${id^^}"
  printf 'GC_BUDGET_STAGE_TOTAL_%s' "$id"
}

gc_budget_stage_tripped_var() {
  local id
  id="$(gc_budget_stage_id "$1")"
  id="${id^^}"
  printf 'GC_BUDGET_STAGE_TRIPPED_%s' "$id"
}

gc_budget_stage_skip_var() {
  local id
  id="$(gc_budget_stage_id "$1")"
  id="${id^^}"
  printf 'GC_BUDGET_STAGE_SKIP_%s' "$id"
}

gc_budget_stage_reason_var() {
  local id
  id="$(gc_budget_stage_id "$1")"
  id="${id^^}"
  printf 'GC_BUDGET_STAGE_SKIP_REASON_%s' "$id"
}

gc_budget_get_stage_limit() {
  local var
  var="$(gc_budget_stage_limit_var "$1")"
  printf '%s' "${!var:-0}"
}

gc_budget_set_stage_limit() {
  local var
  var="$(gc_budget_stage_limit_var "$1")"
  local value="${2:-0}"
  printf -v "$var" '%s' "$value"
}

gc_budget_stage_tripped() {
  local var
  var="$(gc_budget_stage_tripped_var "$1")"
  [[ "${!var:-0}" == "1" ]]
}

gc_budget_stage_should_skip() {
  local var
  var="$(gc_budget_stage_skip_var "$1")"
  [[ "${!var:-0}" == "1" ]]
}

gc_budget_set_stage_skip() {
  local stage="$1"
  local flag="${2:-0}"
  local reason="${3:-}"
  local skip_var
  skip_var="$(gc_budget_stage_skip_var "$stage")"
  printf -v "$skip_var" '%s' "$flag"
  local reason_var
  reason_var="$(gc_budget_stage_reason_var "$stage")"
  printf -v "$reason_var" '%s' "$reason"
}

gc_budget_stage_skip_reason() {
  local var
  var="$(gc_budget_stage_reason_var "$1")"
  printf '%s' "${!var:-}"
}

gc_budget_reset_stage_tracking() {
  local stages=("retrieve" "plan" "patch" "verify")
  local stage
  for stage in "${stages[@]}"; do
    local total_var
    total_var="$(gc_budget_stage_total_var "$stage")"
    printf -v "$total_var" '%s' "0"
    local trip_var
    trip_var="$(gc_budget_stage_tripped_var "$stage")"
    printf -v "$trip_var" '%s' "0"
    local skip_var
    skip_var="$(gc_budget_stage_skip_var "$stage")"
    printf -v "$skip_var" '%s' "0"
    local reason_var
    reason_var="$(gc_budget_stage_reason_var "$stage")"
    printf -v "$reason_var" '%s' ""
  done
}

gc_budget_collect_tool_actions_json() {
  local helper_path
  helper_path="$(gc_clone_python_tool "gc_budget_collect_tool_actions_json.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "${GC_BUDGET_TOOL_ACTIONS_JSON:-}"
}

gc_env_truthy() {
  local value="${1:-}"
  value="${value#"${value%%[![:space:]]*}"}"
  value="${value%"${value##*[![:space:]]}"}"
  value="${value,,}"
  case "$value" in
    1|true|yes|on) return 0 ;;
  esac
  return 1
}

resolve_cli_root() {
  local source="${BASH_SOURCE[0]}"
  while [[ -L "$source" ]]; do
    local dir
    dir="$(cd "$(dirname "$source")" && pwd)"
    source="$(readlink "$source")"
    [[ "$source" != /* ]] && source="$dir/$source"
  done
  local abs_dir
  abs_dir="$(cd "$(dirname "$source")" && pwd)"
  GC_SELF_PATH="${abs_dir}/$(basename "$source")"
  cd "${abs_dir}/.." && pwd
}

cmd_task_convert() {
  warn "'task-convert' is deprecated; use 'create-tasks' instead. Running create-tasks now."
  cmd_create_tasks "$@"
}

CLI_ROOT="$(resolve_cli_root)"
unset -f resolve_cli_root

gc_configure_tmpdir() {
  local base="${1:-${PROJECT_ROOT:-$PWD}}"
  local dir="${base}/.gpt-creator/tmp"
  if mkdir -p "$dir" 2>/dev/null; then
    GC_TMP_DIR="$dir"
    TMPDIR="$dir"
    export GC_TMP_DIR TMPDIR
    mkdir -p "${base}/tmp" 2>/dev/null || true
  else
    if [[ -n "${GC_TMP_DIR:-}" ]]; then
      :
    elif [[ -n "${TMPDIR:-}" && -d "${TMPDIR}" ]]; then
      GC_TMP_DIR="$TMPDIR"
      export GC_TMP_DIR
    fi
  fi
}

gc_configure_tmpdir

if [[ -z "${GC_COMMAND_FAILURE_CACHE:-}" ]]; then
  if [[ -n "${GC_TMP_DIR:-}" ]]; then
    GC_COMMAND_FAILURE_CACHE="${GC_TMP_DIR}/command-failures.json"
  else
    GC_COMMAND_FAILURE_CACHE="${TMPDIR:-/tmp}/command-failures.json"
  fi
fi
export GC_COMMAND_FAILURE_CACHE
GC_COMMAND_FAILURE_WARN_DIGESTS=""

if [[ -z "${GC_COMMAND_STREAM_CACHE:-}" ]]; then
  if [[ -n "${GC_TMP_DIR:-}" ]]; then
    GC_COMMAND_STREAM_CACHE="${GC_TMP_DIR}/command-stream.json"
  else
    GC_COMMAND_STREAM_CACHE="${TMPDIR:-/tmp}/command-stream.json"
  fi
fi
export GC_COMMAND_STREAM_CACHE
GC_COMMAND_STREAM_WARN_DIGESTS=""

if [[ -z "${GC_COMMAND_FILE_CACHE:-}" ]]; then
  if [[ -n "${GC_TMP_DIR:-}" ]]; then
    GC_COMMAND_FILE_CACHE="${GC_TMP_DIR}/command-file-cache.json"
  else
    GC_COMMAND_FILE_CACHE="${TMPDIR:-/tmp}/command-file-cache.json"
  fi
fi
export GC_COMMAND_FILE_CACHE
GC_COMMAND_FILE_WARN_DIGESTS=""

if [[ -z "${GC_COMMAND_SCAN_CACHE:-}" ]]; then
  if [[ -n "${GC_TMP_DIR:-}" ]]; then
    GC_COMMAND_SCAN_CACHE="${GC_TMP_DIR}/command-scan.json"
  else
    GC_COMMAND_SCAN_CACHE="${TMPDIR:-/tmp}/command-scan.json"
  fi
fi
export GC_COMMAND_SCAN_CACHE
GC_COMMAND_SCAN_WARN_DIGESTS=""
GC_COMMAND_GUARD_WARN_DIGESTS=""
GC_CODEX_USAGE_LIMIT_CONFIRMED=0

if [[ -n "${GC_SCAN_DEDUP_EPOCH:-}" && "${GC_SCAN_DEDUP_EPOCH}" =~ ^[0-9]+$ ]]; then
  :
else
  GC_SCAN_DEDUP_EPOCH=0
fi
export GC_SCAN_DEDUP_EPOCH

gc_note_mutation() {
  if ! [[ "${GC_SCAN_DEDUP_EPOCH:-}" =~ ^[0-9]+$ ]]; then
    GC_SCAN_DEDUP_EPOCH=0
  fi
  GC_SCAN_DEDUP_EPOCH=$((GC_SCAN_DEDUP_EPOCH + 1))
  export GC_SCAN_DEDUP_EPOCH
}

gc_env_file() { echo "${PROJECT_ROOT:-$PWD}/.env"; }

gc_random_string() {
  local helper_path
  helper_path="$(gc_clone_python_tool "random_string.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path"
}

gc_usage_limit_is_provider_signal() {
  local message="${1:-}"
  local lower="${message,,}"
  if [[ "$lower" == *"429"* ]]; then
    return 0
  fi
  if [[ "$lower" == *"insufficient_quota"* || "$lower" == *"insufficient quota"* ]]; then
    return 0
  fi
  if [[ "$lower" == *"rate_limit_exceeded"* || "$lower" == *"rate limit exceeded"* ]]; then
    return 0
  fi
  return 1
}

gc_bootstrap_docs_registry() {
  local db_path="${1:-}"
  [[ -n "$db_path" ]] || return 0

  local repo_root="${PROJECT_ROOT:-$PWD}"
  local candidate_config="${repo_root}/config/bootstrap_docs_catalog.sql"
  local candidate_legacy="${repo_root}/.gpt-creator/staging/plan/tasks/bootstrap_docs_catalog.sql"
  local sql_file_default=""
  if [[ -f "$candidate_config" ]]; then
    sql_file_default="$candidate_config"
  elif [[ -f "$candidate_legacy" ]]; then
    sql_file_default="$candidate_legacy"
  else
    sql_file_default="$candidate_config"
  fi
  local sql_file="${GC_DOCUMENTATION_BOOTSTRAP_SQL:-$sql_file_default}"
  [[ -f "$sql_file" ]] || return 0

  local sqlite_bin="${SQLITE_BIN:-sqlite3}"
  if ! command -v "$sqlite_bin" >/dev/null 2>&1; then
    return 0
  fi

  local db_dir
  db_dir="$(dirname "$db_path")"
  mkdir -p "$db_dir"

  "$sqlite_bin" "$db_path" ".read $sql_file" >/dev/null 2>&1 || true
}

gc_doc_indexer_available() {
  local python_bin="${PYTHON_BIN:-python3}"
  local pkg_root="${CLI_ROOT}/src"
  if ! command -v "$python_bin" >/dev/null 2>&1; then
    return 1
  fi
  if [[ ! -d "$pkg_root" ]]; then
    return 1
  fi
  local helper_path
  helper_path="$(gc_clone_python_tool "doc_indexer_available.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  if PYTHONPATH="${pkg_root}${PYTHONPATH:+:$PYTHONPATH}" \
    "$python_bin" "$helper_path" "$pkg_root"; then
    return 0
  fi
  return 1
}

gc_doc_catalog_ready() {
  local root="${1:-${PROJECT_ROOT:-$PWD}}"
  local runtime_dir="${root}/.gpt-creator"
  local staging_dir="${runtime_dir}/staging"
  local doc_library="${staging_dir}/doc-library.md"
  local doc_index="${staging_dir}/doc-index.md"
  local doc_catalog="${staging_dir}/doc-catalog.json"
  local tasks_db="${staging_dir}/plan/tasks/tasks.db"
  local vector_index="${staging_dir}/plan/tasks/documentation-vector-index.sqlite"

  if [[ ! -s "$doc_library" || ! -s "$doc_index" || ! -s "$doc_catalog" ]]; then
    return 1
  fi
  if [[ ! -f "$tasks_db" ]]; then
    return 1
  fi
  local vector_required=0
  if [[ "${GC_REQUIRE_VECTOR_INDEX:-0}" == "1" ]]; then
    vector_required=1
  elif gc_doc_indexer_available; then
    vector_required=1
  fi
  if (( vector_required )) && [[ ! -s "$vector_index" ]]; then
    return 1
  fi

  if command -v python3 >/dev/null 2>&1; then
    local helper_path
    helper_path="$(gc_clone_python_tool "doc_catalog_ready.py" "${PROJECT_ROOT:-$PWD}")" || return 1
    if ! python3 "$helper_path" "$tasks_db"; then
      return 1
    fi
  else
    return 1
  fi

  return 0
}

gc_setup_doc_catalog_helpers() {
  local helper_path=""
  helper_path="$(gc_clone_python_tool "doc_catalog.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  export GC_DOC_CATALOG_PY="$helper_path"

  helper_path="$(gc_clone_python_tool "doc_registry.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  export GC_DOC_REGISTRY_PY="$helper_path"

  helper_path="$(gc_clone_python_tool "doc_indexer.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  export GC_DOC_INDEXER_PY="$helper_path"
}

gc_populate_doc_catalog_shims() {
  local root="${1:-${PROJECT_ROOT:-$PWD}}"
  local runtime_dir="${root}/.gpt-creator"
  local staging_dir="${runtime_dir}/staging"
  local doc_library="${staging_dir}/doc-library.md"
  local doc_index="${staging_dir}/doc-index.md"
  local doc_catalog="${staging_dir}/doc-catalog.json"
  local plan_docs_dir="${staging_dir}/plan/docs"
  local plan_work_dir="${staging_dir}/plan/work"
  local plan_doc_library="${plan_docs_dir}/doc-library.md"
  local plan_doc_index="${plan_docs_dir}/doc-index.md"
  local plan_doc_catalog="${plan_work_dir}/doc-catalog.json"
  local fallback_library="${root}/docs/doc-library.md"
  local fallback_index="${root}/docs/doc-index.md"

  if [[ -z "$root" ]]; then
    return 0
  fi

  mkdir -p "$(dirname "$doc_library")" "$(dirname "$doc_index")" "$plan_docs_dir" "$plan_work_dir"

  if [[ ! -s "$doc_library" && -s "$fallback_library" ]]; then
    cp -f "$fallback_library" "$doc_library" 2>/dev/null || true
    info "Seeded doc-library.md from docs directory."
  fi
  if [[ ! -s "$doc_index" && -s "$fallback_index" ]]; then
    cp -f "$fallback_index" "$doc_index" 2>/dev/null || true
    info "Seeded doc-index.md from docs directory."
  fi

  if [[ -s "$doc_library" ]]; then
    if [[ ! -f "$plan_doc_library" || "$doc_library" -nt "$plan_doc_library" ]]; then
      cp -f "$doc_library" "$plan_doc_library" 2>/dev/null || true
    fi
  fi

  if [[ -s "$doc_index" ]]; then
    if [[ ! -f "$plan_doc_index" || "$doc_index" -nt "$plan_doc_index" ]]; then
      cp -f "$doc_index" "$plan_doc_index" 2>/dev/null || true
    fi
  fi

  if [[ -s "$doc_catalog" ]]; then
    if [[ ! -f "$plan_doc_catalog" || "$doc_catalog" -nt "$plan_doc_catalog" ]]; then
      cp -f "$doc_catalog" "$plan_doc_catalog" 2>/dev/null || true
    fi
  fi
}

gc_clone_python_tool() {
  local script_name="${1:?python script name required}"
  local root_param="${2:-}"
  local root="${root_param:-${PROJECT_ROOT:-$PWD}}"
  if [[ -z "$root" ]]; then
    die "Unable to determine project root while preparing ${script_name}"
  fi
  local source_path="${CLI_ROOT}/scripts/python/${script_name}"
  if [[ ! -f "$source_path" ]]; then
    die "Python helper missing at ${source_path}"
  fi
  local target_dir="${root}/.gpt-creator/shims/python"
  local target_path="${target_dir}/${script_name}"
  if [[ ! -d "$target_dir" ]]; then
    mkdir -p "$target_dir" || die "Failed to create ${target_dir}"
  fi
  if [[ ! -f "$target_path" || "$source_path" -nt "$target_path" ]]; then
    cp "$source_path" "$target_path" || die "Failed to copy ${script_name} helper"
  fi
  echo "$target_path"
}

gc_require_documentation_catalog() {
  local root="${1:-${PROJECT_ROOT:-$PWD}}"
  ensure_ctx "$root"

  if gc_doc_catalog_ready "$root"; then
    gc_populate_doc_catalog_shims "$root"
    return 0
  fi

  info "Documentation catalog missing or stale; running 'gpt-creator scan'."
  if ! cmd_scan --project "$root"; then
    warn "Documentation scan failed."
    return 1
  fi

  gc_populate_doc_catalog_shims "$root"

  if ! gc_doc_catalog_ready "$root"; then
    warn "Documentation catalog still incomplete after scan; inspect ${root}/.gpt-creator/staging."
    return 1
  fi

  if ! cmd_sweep_artifacts --project "$root"; then
    warn "Artifact sweep encountered issues; run 'gpt-creator sweep-artifacts --project \"$root\"' manually."
  fi

  return 0
}

gc_refresh_documentation_if_needed() {
  local root="${1:-${PROJECT_ROOT:-$PWD}}"
  if [[ "${GC_SKIP_AUTO_DOC_REFRESH:-0}" == "1" ]]; then
    info "Skipping automatic documentation refresh (GC_SKIP_AUTO_DOC_REFRESH=1)."
    return 0
  fi

  if gc_require_documentation_catalog "$root"; then
    ok "Documentation catalog refreshed."
    return 0
  fi

  warn "Automatic scan failed; rerun 'gpt-creator scan' manually."
  return 1
}

gc_parse_int() {
  local value="$1"
  local fallback="$2"
  if [[ "$value" =~ ^[0-9]+$ ]]; then
    printf '%s\n' "$value"
  else
    printf '%s\n' "$fallback"
  fi
}

gc_estimate_tokens_from_bytes() {
  local prompt_file="${1:?prompt path required}"
  if [[ ! -f "$prompt_file" ]]; then
    printf '0\n'
    return 0
  fi
  local bytes
  if ! bytes="$(wc -c <"$prompt_file" 2>/dev/null)"; then
    printf '0\n'
    return 0
  fi
  if ! [[ "$bytes" =~ ^[0-9]+$ ]]; then
    printf '0\n'
    return 0
  fi
  local approx=$(( (bytes + 3) / 4 ))
  printf '%s\n' "$approx"
}

gc_trim_prompt_file() {
  local prompt_file="$1"
  local max_tokens_raw="${GC_MAX_PROMPT_TOKENS:-8000}"
  [[ -f "$prompt_file" ]] || return 0
  if ! [[ "$max_tokens_raw" =~ ^[0-9]+$ ]]; then
    return 0
  fi
  local max_tokens=$((max_tokens_raw))
  if (( max_tokens <= 0 )); then
    return 0
  fi
  local helper_path
  helper_path="$(gc_clone_python_tool "trim_prompt_file.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$prompt_file" "$max_tokens"
}

gc_trim_prompt_file_lean() {
  local prompt_file="$1"
  [[ -f "$prompt_file" ]] || return 0
  local helper_path
  helper_path="$(gc_clone_python_tool "trim_prompt_file_lean.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$prompt_file"
}

gc_write_env_var() {
  local target="$1" key="$2" value="$3"
  local helper_path
  helper_path="$(gc_clone_python_tool "write_env_var.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$target" "$key" "$value"
}

gc_set_env_var() {
  local key="$1" value="$2"
  local env_file
  env_file="$(gc_env_file)"
  gc_write_env_var "$env_file" "$key" "$value"
}

gc_remove_env_var() {
  local target="$1" key="$2"
  [[ -f "$target" ]] || return 0
  local helper_path
  helper_path="$(gc_clone_python_tool "remove_env_var.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$target" "$key"
}

gc_decode_base64() {
  local value="${1:-}"
  local helper_path
  helper_path="$(gc_clone_python_tool "decode_base64.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$value"
}

cmd_show_file() {
  local project="${PROJECT_ROOT:-$PWD}"
  local target_path=""
  local range_spec="" head_lines="" tail_lines="" refresh=0 diff_mode=0
  local max_lines="${GC_SHOW_FILE_MAX_LINES:-400}"

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project)
        project="$(abs_path "$2")"
        shift 2
        ;;
      --range)
        if [[ $# -lt 2 ]]; then
          die "--range requires an argument in the form START:END (e.g. 120:160)"
        fi
        local next_range="$2"
        if [[ -z "$next_range" || "$next_range" == --* ]]; then
          die "--range requires an argument in the form START:END (e.g. 120:160)"
        fi
        range_spec="$next_range"
        shift 2
        ;;
      --head)
        if [[ $# -lt 2 ]]; then
          die "--head requires a positive line count"
        fi
        local next_head="$2"
        if [[ -z "$next_head" || "$next_head" == --* ]]; then
          die "--head requires a positive line count"
        fi
        warn "Tip: prefer --range start:end or --tail N for targeted snippets; --head is retained for compatibility."
        head_lines="$next_head"
        shift 2
        ;;
      --tail)
        if [[ $# -lt 2 ]]; then
          die "--tail requires a positive line count"
        fi
        local next_tail="$2"
        if [[ -z "$next_tail" || "$next_tail" == --* ]]; then
          die "--tail requires a positive line count"
        fi
        tail_lines="$next_tail"
        shift 2
        ;;
      --max-lines)
        if [[ $# -lt 2 ]]; then
          die "--max-lines requires a positive line count"
        fi
        local next_max="$2"
        if [[ -z "$next_max" || "$next_max" == --* ]]; then
          die "--max-lines requires a positive line count"
        fi
        max_lines="$next_max"
        shift 2
        ;;
      --refresh|--force)
        refresh=1
        shift
        ;;
      --diff)
        diff_mode=1
        shift
        ;;
      -h|--help)
        cat <<'EOHELP'
Usage: gpt-creator show-file [options] PATH

Display a cached snippet of PATH without repeatedly streaming large files.

Options:
  --project DIR       Project root (defaults to current PROJECT_ROOT)
  --range A:B         Show inclusive line range A..B (1-based)
  --head N            Show first N lines (legacy; prefer --range/--tail for precise snippets)
  --tail N            Show last N lines
  --max-lines N       Default line count when no range/head/tail is provided (default: 400)
  --diff              Show a unified diff against the cached snapshot (if any)
  --refresh           Re-read the file and update the cache even if unchanged
EOHELP
        return 0
        ;;
      --)
        shift
        break
        ;;
      -*)
        die "Unknown show-file option: ${1}"
        ;;
      *)
        if [[ -z "$target_path" ]]; then
          target_path="$1"
          shift
        else
          break
        fi
        ;;
    esac
  done

  [[ -n "$target_path" ]] || die "show-file requires a path argument"
  local project_abs
  project_abs="$(abs_path "$project")"
  local resolved_path
  resolved_path="$(abs_path "$target_path")"

  if [[ ! -f "$resolved_path" && "$target_path" != /* ]]; then
    local staging_root
    staging_root="$(abs_path "${project_abs}/.gpt-creator/staging")"
    local staging_candidate="${staging_root}/${target_path#./}"
    staging_candidate="$(abs_path "$staging_candidate")"
    if [[ "$staging_candidate" == "${staging_root}"/* && -f "$staging_candidate" ]]; then
      resolved_path="$staging_candidate"
    fi
  fi

  [[ -f "$resolved_path" ]] || die "File not found: ${target_path}"

  [[ -z "$max_lines" || "$max_lines" =~ ^[0-9]+$ ]] || die "--max-lines must be numeric"
  [[ -z "$head_lines" || "$head_lines" =~ ^[0-9]+$ ]] || die "--head value must be numeric"
  [[ -z "$tail_lines" || "$tail_lines" =~ ^[0-9]+$ ]] || die "--tail value must be numeric"

  local cache_dir="${GC_TMP_DIR:-${project_abs}/.gpt-creator/tmp}/view-cache"
  mkdir -p "$cache_dir"

  local rel_path="$resolved_path"
  if [[ "$resolved_path" == "$project_abs"* ]]; then
    rel_path="${resolved_path#$project_abs/}"
  fi

  local helper_path
  helper_path="$(gc_clone_python_tool "show_file.py" "${PROJECT_ROOT:-$PWD}")" || return 1

  GC_SHOW_FILE_PROJECT="$project_abs" \
  GC_SHOW_FILE_PATH="$resolved_path" \
  GC_SHOW_FILE_REL="$rel_path" \
  GC_SHOW_FILE_RANGE="$range_spec" \
  GC_SHOW_FILE_HEAD="$head_lines" \
  GC_SHOW_FILE_TAIL="$tail_lines" \
  GC_SHOW_FILE_MAX_LINES="$max_lines" \
  GC_SHOW_FILE_REFRESH="$refresh" \
  GC_SHOW_FILE_DIFF="$diff_mode" \
  GC_SHOW_FILE_CACHE_DIR="$cache_dir" \
  python3 "$helper_path"
}

gc_env_sync_ports() {
  GC_PORT_RESERVATIONS=""
  GC_DB_HOST_PORT="${GC_DB_HOST_PORT:-${DB_HOST_PORT:-${DB_PORT:-3306}}}"
  DB_NAME="${DB_NAME:-$GC_DB_NAME}"
  DB_USER="${DB_USER:-$GC_DB_USER}"
  DB_PASSWORD="${DB_PASSWORD:-$GC_DB_PASSWORD}"
  DB_ROOT_PASSWORD="${DB_ROOT_PASSWORD:-$GC_DB_ROOT_PASSWORD}"
  DB_HOST_PORT="${DB_HOST_PORT:-$GC_DB_HOST_PORT}"
  GC_API_HOST_PORT="${GC_API_HOST_PORT:-${API_HOST_PORT:-3000}}"
  GC_WEB_HOST_PORT="${GC_WEB_HOST_PORT:-${WEB_HOST_PORT:-5173}}"
  GC_ADMIN_HOST_PORT="${GC_ADMIN_HOST_PORT:-${ADMIN_HOST_PORT:-5174}}"
  GC_PROXY_HOST_PORT="${GC_PROXY_HOST_PORT:-${PROXY_HOST_PORT:-8080}}"
  API_HOST_PORT="${API_HOST_PORT:-$GC_API_HOST_PORT}"
  WEB_HOST_PORT="${WEB_HOST_PORT:-$GC_WEB_HOST_PORT}"
  ADMIN_HOST_PORT="${ADMIN_HOST_PORT:-$GC_ADMIN_HOST_PORT}"
  PROXY_HOST_PORT="${PROXY_HOST_PORT:-$GC_PROXY_HOST_PORT}"
  local api_base_default="http://localhost:${GC_API_HOST_PORT}/api/v1"
  GC_API_BASE_URL="${GC_API_BASE_URL:-$api_base_default}"
  VITE_API_BASE="${VITE_API_BASE:-$GC_API_BASE_URL}"
  local expected_health="${GC_API_BASE_URL%/}/health"
  if [[ -z "${GC_API_HEALTH_URL:-}" ]]; then
    GC_API_HEALTH_URL="$expected_health"
    gc_set_env_var GC_API_HEALTH_URL "$GC_API_HEALTH_URL"
  elif [[ "$GC_API_HEALTH_URL" == "http://localhost:${GC_API_HOST_PORT}/health" && "$expected_health" != "$GC_API_HEALTH_URL" ]]; then
    GC_API_HEALTH_URL="$expected_health"
    gc_set_env_var GC_API_HEALTH_URL "$GC_API_HEALTH_URL"
  fi
  local proxy_origin="http://localhost:${GC_PROXY_HOST_PORT}"
  GC_WEB_URL="${GC_WEB_URL:-${proxy_origin}/}"
  GC_ADMIN_URL="${GC_ADMIN_URL:-${proxy_origin}/admin/}"
  gc_reserve_port db "$GC_DB_HOST_PORT"
  gc_reserve_port api "$GC_API_HOST_PORT"
  gc_reserve_port web "$GC_WEB_HOST_PORT"
  gc_reserve_port admin "$GC_ADMIN_HOST_PORT"
  gc_reserve_port proxy "$GC_PROXY_HOST_PORT"

  if gc_reports_enabled; then
    gc_reports_initialize
    gc_reports_touch_activity "$(date +%s)"
  fi
}

gc_sanitize_env_file() {
  local env_file="$1"
  local helper_path
  helper_path="$(gc_clone_python_tool "sanitize_env_file.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$env_file"
}

gc_load_env() {
  local env_file
  env_file="$(gc_env_file)"
  if [[ -f "$env_file" ]]; then
    gc_sanitize_env_file "$env_file"
    set -a
    # shellcheck disable=SC1090
    source "$env_file"
    set +a
  fi
  GC_DB_NAME="${GC_DB_NAME:-${DB_NAME:-app}}"
  GC_DB_USER="${GC_DB_USER:-${DB_USER:-app}}"
  GC_DB_PASSWORD="${GC_DB_PASSWORD:-${DB_PASSWORD:-app_pass}}"
  GC_DB_ROOT_PASSWORD="${GC_DB_ROOT_PASSWORD:-${DB_ROOT_PASSWORD:-root}}"
  gc_env_sync_ports
}

GC_API_KEYS_METADATA=(
  "openai|OpenAI Codex|OPENAI_API_KEY|AI automation commands (plan, generate, work-on-tasks)|GC_OPENAI_API_KEY,GC_OPENAI_KEY"
  "jira|Jira Automation|JIRA_API_TOKEN|Backlog integrations that sync with Jira (create-jira-tasks)|GC_JIRA_API_TOKEN"
  "github|GitHub Auto Reports|GC_GITHUB_TOKEN|Crash/stall reports published as GitHub issues|"
)

gc_find_api_key_entry() {
  local query input lower entry key_id label primary desc alias_csv
  query="${1:-}"
  [[ -n "$query" ]] || return 1
  lower="$(to_lower "$query")"
  for entry in "${GC_API_KEYS_METADATA[@]}"; do
    IFS='|' read -r key_id label primary desc alias_csv <<<"$entry"
    if [[ "$lower" == "$(to_lower "$key_id")" || "$lower" == "$(to_lower "$primary")" ]]; then
      printf '%s\n' "$entry"
      return 0
    fi
    if [[ -n "$alias_csv" ]]; then
      IFS=',' read -ra input <<<"$alias_csv"
      for alias in "${input[@]}"; do
        alias="${alias//[[:space:]]/}"
        [[ -n "$alias" ]] || continue
        if [[ "$lower" == "$(to_lower "$alias")" ]]; then
          printf '%s\n' "$entry"
          return 0
        fi
      done
    fi
  done
  return 1
}

gc_apply_api_key_aliases() {
  local entry key_id label primary desc alias_csv value alias
  local alias_arr
  for entry in "${GC_API_KEYS_METADATA[@]}"; do
    IFS='|' read -r key_id label primary desc alias_csv <<<"$entry"
    value="${!primary:-}"
    if [[ -z "$value" && -n "$alias_csv" ]]; then
      IFS=',' read -ra alias_arr <<<"$alias_csv"
      for alias in "${alias_arr[@]}"; do
        alias="${alias//[[:space:]]/}"
        [[ -n "$alias" ]] || continue
        if [[ -n "${!alias:-}" ]]; then
          value="${!alias}"
          break
        fi
      done
    fi
    [[ -n "$value" ]] || continue
    export "$primary"="$value"
    if [[ -n "$alias_csv" ]]; then
      IFS=',' read -ra alias_arr <<<"$alias_csv"
      for alias in "${alias_arr[@]}"; do
        alias="${alias//[[:space:]]/}"
        [[ -n "$alias" && "$alias" != "$primary" ]] || continue
        if [[ -z "${!alias:-}" ]]; then
          export "$alias"="$value"
        fi
      done
    fi
  done
}

gc_api_keys_loaded=0

gc_load_api_keys() {
  if (( gc_api_keys_loaded )); then
    gc_apply_api_key_aliases
    return
  fi
  gc_ensure_config_dir
  local keys_file
  keys_file="$(gc_keys_file)"
  if [[ -f "$keys_file" ]]; then
    gc_sanitize_env_file "$keys_file"
    set -a
    # shellcheck disable=SC1090
    source "$keys_file"
    set +a
  fi
  gc_apply_api_key_aliases
  gc_api_keys_loaded=1
}

gc_read_env_file_var() {
  local file="$1" key="$2"
  [[ -f "$file" ]] || return 0
  local helper_path
  helper_path="$(gc_clone_python_tool "read_env_file_var.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$file" "$key"
}

gc_api_keys_list() {
  gc_load_api_keys
  gc_ensure_config_dir
  local keys_file status_header="Status"
  keys_file="$(gc_keys_file)"
  printf "API keys status (storage file: %s)\n\n" "$keys_file"
  printf "%-22s %-20s %-24s %s\n" "Service" "Environment" "$status_header" "Used for"
  printf "%-22s %-20s %-24s %s\n" "-------" "-----------" "------" "-------"
  local entry key_id label primary desc alias_csv value stored_value status alias
  local alias_arr
  for entry in "${GC_API_KEYS_METADATA[@]}"; do
    IFS='|' read -r key_id label primary desc alias_csv <<<"$entry"
    value="${!primary:-}"
    if [[ -z "$value" && -n "$alias_csv" ]]; then
      IFS=',' read -ra alias_arr <<<"$alias_csv"
      for alias in "${alias_arr[@]}"; do
        alias="${alias//[[:space:]]/}"
        [[ -n "$alias" ]] || continue
        if [[ -n "${!alias:-}" ]]; then
          value="${!alias}"
          break
        fi
      done
    fi
    stored_value="$(gc_read_env_file_var "$keys_file" "$primary")"
    if [[ -n "$value" ]]; then
      if [[ -n "$stored_value" ]]; then
        status="configured (stored)"
      else
        status="configured (env)"
      fi
    else
      status="missing"
    fi
    printf "%-22s %-20s %-24s %s\n" "$label" "$primary" "$status" "$desc"
  done
  if [[ -n "${GC_GITHUB_TOKEN:-}" && -z "${GC_GITHUB_REPO:-}" ]]; then
    printf "\nHint: set GC_GITHUB_REPO (owner/name) so GitHub Auto Reports knows where to file issues.\n"
  fi
  printf "\nSet a value with: gpt-creator keys set <service>\n"
}

gc_api_keys_set() {
  local query="${1:-}"
  [[ -n "$query" ]] || die "keys set requires a service name or environment variable"
  local entry
  if ! entry="$(gc_find_api_key_entry "$query")"; then
    die "Unknown API key: ${query}"
  fi
  IFS='|' read -r key_id label primary desc alias_csv <<<"$entry"
  gc_load_api_keys
  gc_ensure_config_dir
  local keys_file value alias
  local alias_arr
  keys_file="$(gc_keys_file)"
  printf "Updating %s (%s)\n" "$label" "$primary"
  printf "Press ENTER without a value to remove the stored credential.\n"
  if [[ -n "$alias_csv" ]]; then
    printf "Aliases: %s\n" "${alias_csv//,/ }"
  fi
  if [[ -t 0 ]]; then
    read -rsp "Enter value: " value
    printf '\n'
  else
    info "Reading ${primary} from stdin (input will be visible)."
    if ! read -r value; then
      die "Failed to read value from stdin"
    fi
  fi
  value="${value//$'\r'/}"
  value="$(printf '%s' "$value" | sed -e 's/^[[:space:]]*//' -e 's/[[:space:]]*$//')"
  if [[ -z "$value" ]]; then
    gc_remove_env_var "$keys_file" "$primary"
    unset "$primary"
    if [[ -n "$alias_csv" ]]; then
      IFS=',' read -ra alias_arr <<<"$alias_csv"
      for alias in "${alias_arr[@]}"; do
        alias="${alias//[[:space:]]/}"
        [[ -n "$alias" ]] || continue
        unset "$alias"
      done
    fi
    ok "Removed stored value for ${label}"
    return 0
  fi
  gc_write_env_var "$keys_file" "$primary" "$value"
  chmod 600 "$keys_file" 2>/dev/null || true
  export "$primary"="$value"
  if [[ -n "$alias_csv" ]]; then
    IFS=',' read -ra alias_arr <<<"$alias_csv"
    for alias in "${alias_arr[@]}"; do
      alias="${alias//[[:space:]]/}"
      [[ -n "$alias" && "$alias" != "$primary" ]] || continue
      export "$alias"="$value"
    done
  fi
  gc_apply_api_key_aliases
  ok "Stored credentials for ${label}"
  printf "Saved to %s\n" "$keys_file"
}

gc_create_env_if_missing() {
  local env_file
  env_file="$(gc_env_file)"
  if [[ -f "$env_file" ]]; then
    return
  fi
  local slug
  slug="$(basename "${PROJECT_ROOT:-$PWD}")"
  slug=$(printf '%s' "$slug" | tr -c '[:alnum:]' '_')
  slug=$(printf '%s' "$slug" | tr '[:upper:]' '[:lower:]')
  slug=$(printf '%.12s' "$slug")
  [[ -n "$slug" ]] || slug="app"
  local db_name="${slug}_db"
  local db_user="gc_${slug}_user"
  local db_password
  db_password="$(gc_random_string)"
  local db_root_password
  db_root_password="$(gc_random_string)"
  cat > "$env_file" <<EOF
# gpt-creator environment
DB_NAME=${db_name}
DB_USER=${db_user}
DB_PASSWORD=${db_password}
DB_ROOT_USER=root
DB_ROOT_PASSWORD=${db_root_password}
DB_HOST=127.0.0.1
DB_PORT=3306
DB_HOST_PORT=3306
API_HOST_PORT=3000
WEB_HOST_PORT=5173
ADMIN_HOST_PORT=5174
PROXY_HOST_PORT=8080
DATABASE_URL=mysql://${db_user}:${db_password}@127.0.0.1:3306/${db_name}
VITE_API_BASE=http://localhost:3000/api/v1
# Optional: GitHub issue reporting
GC_GITHUB_REPO=bekirdag/gpt-creator
GC_GITHUB_TOKEN=
# GC_REPORTER=
# GC_REPORT_ASSIGNEE=
EOF
  chmod 600 "$env_file" || true
}

VERSION="0.2.0"
APP_NAME="gpt-creator"

# Defaults (override via env)
CODEX_BIN="${CODEX_BIN:-codex}"
CODEX_MODEL="${CODEX_MODEL:-gpt-5-codex}"
CODEX_FALLBACK_MODEL="${CODEX_FALLBACK_MODEL:-gpt-5-codex}"

# Stage-aware model selection (defaults fall back to CODEX_MODEL for compatibility)
CODEX_MODEL_LOW="${CODEX_MODEL_LOW:-${CODEX_MODEL}}"
CODEX_MODEL_NON_CODE="${CODEX_MODEL_NON_CODE:-${CODEX_MODEL_LOW}}"
CODEX_MODEL_CODE="${CODEX_MODEL_CODE:-${CODEX_MODEL}}"

# Reasoning effort defaults (non-code work drops to low effort unless overridden)
CODEX_REASONING_EFFORT="${CODEX_REASONING_EFFORT:-high}"
CODEX_REASONING_EFFORT_NON_CODE="${CODEX_REASONING_EFFORT_NON_CODE:-low}"
CODEX_REASONING_EFFORT_CODE="${CODEX_REASONING_EFFORT_CODE:-${CODEX_REASONING_EFFORT}}"

export CODEX_MODEL CODEX_MODEL_LOW CODEX_MODEL_NON_CODE CODEX_MODEL_CODE
export CODEX_FALLBACK_MODEL CODEX_REASONING_EFFORT CODEX_REASONING_EFFORT_NON_CODE CODEX_REASONING_EFFORT_CODE
EDITOR_CMD="${EDITOR_CMD:-code}"
DOCKER_BIN="${DOCKER_BIN:-docker}"
MYSQL_BIN="${MYSQL_BIN:-mysql}"

# Colors (TTY-only)
if [[ -t 1 ]]; then
  c_reset=$'\033[0m'
  c_red=$'\033[31m'; c_yellow=$'\033[33m'; c_cyan=$'\033[36m'; c_green=$'\033[32m'
else
  c_reset=; c_red=; c_yellow=; c_cyan=; c_green=
fi

ts() { date +"%Y-%m-%dT%H:%M:%S"; }
die() { echo "${c_red}✖${c_reset} $*" >&2; exit 1; }
info(){ echo "${c_cyan}➜${c_reset} $*"; }
ok()  { echo "${c_green}✔${c_reset} $*"; }
warn(){ echo "${c_yellow}!${c_reset} $*"; }

gc_format_duration_compact() {
  local total="${1:-0}"
  if [[ ! "$total" =~ ^[0-9]+$ ]]; then
    total=0
  fi
  local hours=$(( total / 3600 ))
  local minutes=$(( (total % 3600) / 60 ))
  local seconds=$(( total % 60 ))
  local parts=()
  if (( hours > 0 )); then
    parts+=("${hours}H")
  fi
  if (( minutes > 0 || hours > 0 )); then
    parts+=("${minutes}M")
  fi
  parts+=("${seconds}S")
  printf '%s' "${parts[*]}"
}

gc_format_tokens_compact() {
  local raw="${1:-0}"
  if [[ ! "$raw" =~ ^[0-9]+$ ]]; then
    raw=0
  fi
  printf '%s' "$(printf '%d' "$raw" | sed ':a;s/\B[0-9]\{3\}\>/,&/;ta')"
}

gc_render_task_banner() {
  local header_position="top"
  case "$1" in
    --header-bottom)
      header_position="bottom"
      shift
      ;;
    --header-top)
      shift
      ;;
  esac

  local header="${1:?header text required}"
  shift
  local -a lines=("$@")

  local min_width=23
  local banner_margin=4
  local header_padding_extra=8
  local inner_width="$min_width"

  local line
  for line in "${lines[@]}"; do
    local length=${#line}
    local candidate=$(( length + banner_margin ))
    if (( candidate > inner_width )); then
      inner_width=$candidate
    fi
  done

  local header_width=$(( inner_width + header_padding_extra ))
  if (( header_width < ${#header} + 2 )); then
    header_width=$(( ${#header} + 2 ))
  fi

  local shade_line
  printf -v shade_line '%*s' "$header_width" ''
  shade_line="${shade_line// /░}"

  local header_pad_left=$(( (header_width - ${#header}) / 2 ))
  local header_pad_right=$(( header_width - header_pad_left - ${#header} ))
  (( header_pad_left < 0 )) && header_pad_left=0
  (( header_pad_right < 0 )) && header_pad_right=0
  local header_line_left header_line_right
  printf -v header_line_left '%*s' "$header_pad_left" ''
  printf -v header_line_right '%*s' "$header_pad_right" ''
  local header_line="${header_line_left// /░}${header}${header_line_right// /░}"

  local border_inner
  printf -v border_inner '%*s' "$inner_width" ''
  local border_line="|${border_inner// /-}|"

  if [[ "$header_position" == "top" ]]; then
    info "$shade_line"
    info "$header_line"
    info "$shade_line"
  fi

  info "$border_line"
  for line in "${lines[@]}"; do
    local line_length=${#line}
    local pad_total=$(( inner_width - line_length ))
    (( pad_total < 0 )) && pad_total=0
    local pad_left=$(( pad_total / 2 ))
    local pad_right=$(( pad_total - pad_left ))
    local padded_line
    printf -v padded_line '|%*s%s%*s|' "$pad_left" '' "$line" "$pad_right" ''
    info "$padded_line"
    info "$border_line"
  done

  if [[ "$header_position" == "bottom" ]]; then
    printf '\n'
    info "$shade_line"
    info "$header_line"
    info "$shade_line"
  fi
}

gc_user_config_root() {
  local helper_path
  helper_path="$(gc_clone_python_tool "user_config_root.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path"
}

gc_config_dir() {
  local root
  root="$(gc_user_config_root)"
  [[ -n "$root" ]] || root="."
  printf '%s\n' "${root%/}/gpt-creator"
}

gc_keys_file() {
  printf '%s/api-keys.env\n' "$(gc_config_dir)"
}

gc_ensure_config_dir() {
  local dir
  dir="$(gc_config_dir)"
  mkdir -p "$dir"
}

abs_path() {
  local target="${1:-}"
  local helper_path
  helper_path="$(gc_clone_python_tool "abs_path.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  local resolved=""
  if resolved="$(python3 "$helper_path" "$target" 2>/dev/null)"; then
    printf '%s\n' "${resolved:-$target}"
    return 0
  fi
  perl -MCwd=abs_path -e 'print abs_path(shift)."\n"' "$target" || echo "$target"
}

to_lower() {
  printf '%s' "$1" | tr '[:upper:]' '[:lower:]'
}

slugify_name() {
  local s="${1:-}"
  s="$(to_lower "$s")"
  s="$(printf '%s' "$s" | tr -cs 'a-z0-9' '-')"
  s="$(printf '%s' "$s" | sed -E 's/-+/-/g; s/^-+//; s/-+$//')"
  printf '%s\n' "${s:-gptcreator}"
}

GC_PORT_RESERVATIONS=""

gc_port_for_service() {
  local service="$1"
  local entry
  for entry in $GC_PORT_RESERVATIONS; do
    local svc="${entry%%:*}"
    if [[ "$svc" == "$service" ]]; then
      printf '%s\n' "${entry#*:}"
      return 0
    fi
  done
  return 1
}

gc_unreserve_port() {
  local service="$1"
  [[ -n "$service" ]] || return 0
  local entry new_list=""
  for entry in $GC_PORT_RESERVATIONS; do
    local svc="${entry%%:*}"
    if [[ "$svc" == "$service" ]]; then
      continue
    fi
    if [[ -z "$new_list" ]]; then
      new_list="$entry"
    else
      new_list+=" $entry"
    fi
  done
  GC_PORT_RESERVATIONS="$new_list"
}

gc_reserve_port() {
  local service="$1" port="$2"
  [[ -n "$service" && -n "$port" ]] || return 0
  gc_unreserve_port "$service"
  if [[ -z "${GC_PORT_RESERVATIONS:-}" ]]; then
    GC_PORT_RESERVATIONS="${service}:${port}"
  else
    GC_PORT_RESERVATIONS+=" ${service}:${port}"
  fi
}

gc_port_is_reserved() {
  local port="$1"
  local entry
  for entry in $GC_PORT_RESERVATIONS; do
    if [[ "${entry#*:}" == "$port" ]]; then
      return 0
    fi
  done
  return 1
}

gc_port_reserved_by_other() {
  local port="$1" service="$2"
  local entry
  for entry in $GC_PORT_RESERVATIONS; do
    local svc="${entry%%:*}"
    local val="${entry#*:}"
    if [[ "$val" == "$port" && "$svc" != "$service" ]]; then
      return 0
    fi
  done
  return 1
}

gc_parse_duration_seconds() {
  local value="${1:-}"
  local default="${2:-0}"
  if [[ -z "$value" ]]; then
    echo "$default"
    return
  fi
  if [[ "$value" =~ ^[0-9]+$ ]]; then
    echo "$value"
    return
  fi
  if [[ "$value" =~ ^([0-9]+)([smhd])$ ]]; then
    local number="${BASH_REMATCH[1]}"
    local unit="${BASH_REMATCH[2]}"
    case "$unit" in
      s) echo "$number" ;;
      m) echo $((number * 60)) ;;
      h) echo $((number * 3600)) ;;
      d) echo $((number * 86400)) ;;
      *) echo "$default" ;;
    esac
    return
  fi
  echo "$default"
}

gc_parse_size_bytes() {
  local value="${1:-}"
  local default="${2:-0}"
  if [[ -z "$value" ]]; then
    echo "$default"
    return
  fi
  if [[ "$value" =~ ^[0-9]+$ ]]; then
    echo "$value"
    return
  fi
  if [[ "$value" =~ ^([0-9]+)([KMG]B?|[kmg]b?)$ ]]; then
    local number="${BASH_REMATCH[1]}"
    local unit="${BASH_REMATCH[2],,}"
    case "$unit" in
      kb|k) echo $((number * 1024)) ;;
      mb|m) echo $((number * 1024 * 1024)) ;;
      gb|g) echo $((number * 1024 * 1024 * 1024)) ;;
      *) echo "$default" ;;
    esac
    return
  fi
  echo "$default"
}

GC_PROGRESS_DIR_MIGRATIONS=(
  "docs/delivery::staging/docs"
  "Design::staging/docs"
  "legal_editor_tmp::artifacts"
)

GC_PROGRESS_FILE_MIGRATIONS=(
  "tmp_*::artifacts/tmp"
  "final_*::artifacts/final"
  "dump_*::artifacts/dump"
  "diff*::artifacts/diff"
  "change_*::artifacts/change"
  "changes_*::artifacts/change"
  "codex_*::artifacts/codex"
  "delivery_plan*::artifacts/delivery"
  "qaDoc.json::artifacts/delivery"
  "backlog.md::artifacts/planning"
  "plan.md::artifacts/planning"
  "tasks.json::staging/plan/legacy"
  "encoded*::artifacts/encoded"
  "session_lifecycle*::artifacts/session"
  "breadcrumbs*.json::artifacts/ui"
  "cli_content.json::artifacts/context"
  "focusTrap.json::artifacts/ui"
  "navStore.json::artifacts/ui"
  "service_content.json::artifacts/context"
  "login_diff.txt::artifacts/diff"
  "login_json.txt::artifacts/context"
  "output_payload.json::artifacts/output"
  "changes_output.json::artifacts/change"
  "changes_strings.txt::artifacts/change"
  "*.patch::artifacts/patches"
  "*_patch.jsonstr::artifacts/patches"
  "*.rej::artifacts/patches"
  "*.rej.orig::artifacts/patches"
  "*.orig::artifacts/patches"
)

GC_PROGRESS_MIGRATION_LAST_COUNT=0

gc_move_progress_artifact() {
  local source_path="$1"
  local dest_dir="$2"
  [[ -e "$source_path" ]] || return 1
  mkdir -p "$dest_dir"
  local base name ext target candidate idx
  base="$(basename "$source_path")"
  name="$base"
  ext=""
  if [[ -f "$source_path" ]]; then
    if [[ "$base" == .* ]]; then
      if [[ "$base" == *.* ]]; then
        ext=".${base##*.}"
        name="${base%$ext}"
        [[ -z "$name" ]] && name="$base"
      fi
    else
      if [[ "$base" == *.* ]]; then
        ext=".${base##*.}"
        name="${base%$ext}"
      fi
    fi
  fi
  target="${dest_dir}/${base}"
  if [[ -e "$target" ]]; then
    idx=2
    while :; do
      if [[ -n "$ext" && "$name" != "$base" ]]; then
        candidate="${dest_dir}/${name}-${idx}${ext}"
      else
        candidate="${dest_dir}/${base}-${idx}"
      fi
      if [[ ! -e "$candidate" ]]; then
        target="$candidate"
        break
      fi
      idx=$((idx + 1))
    done
  fi
  if mv -- "$source_path" "$target"; then
    printf '%s\t%s\n' "$source_path" "$target"
    return 0
  fi
  return 1
}

gc_migrate_progress_artifacts() {
  local project_root="$1"
  local work_dir_name="${GC_WORK_DIR_NAME:-.gpt-creator}"
  local gc_root="${project_root}/${work_dir_name}"
  local skip="${GC_SKIP_PROGRESS_MIGRATION:-0}"
  [[ -d "$project_root" ]] || return 0
  [[ "$skip" == "1" ]] && return 0
  if [[ -n "${GC_ROOT:-}" && "$project_root" == "$GC_ROOT" ]]; then
    return 0
  fi
  GC_PROGRESS_MIGRATION_LAST_COUNT=0
  local python_bin="${PYTHON_BIN:-python3}"
  local -a moved=()
  local entry src_rel dest_rel src_path dest_dir record

  for entry in "${GC_PROGRESS_DIR_MIGRATIONS[@]}"; do
    src_rel="${entry%%::*}"
    dest_rel="${entry#*::}"
    src_path="${project_root}/${src_rel}"
    [[ -d "$src_path" ]] || continue
    dest_dir="${gc_root}/${dest_rel}"
    if record="$(gc_move_progress_artifact "$src_path" "$dest_dir")"; then
      moved+=("$record")
    fi
  done

  shopt -s nullglob dotglob
  for entry in "${GC_PROGRESS_FILE_MIGRATIONS[@]}"; do
    src_rel="${entry%%::*}"
    dest_rel="${entry#*::}"
    local path
    for path in "$project_root"/$src_rel; do
      [[ -e "$path" ]] || continue
      [[ "$path" == "$gc_root"* ]] && continue
      dest_dir="${gc_root}/${dest_rel}"
      if record="$(gc_move_progress_artifact "$path" "$dest_dir")"; then
        moved+=("$record")
      fi
    done
  done
  shopt -u nullglob dotglob

  if ((${#moved[@]} > 0)); then
    local log_dir="${gc_root}/logs"
    mkdir -p "$log_dir"
    local log_file="${log_dir}/progress-migration.log"
    {
      printf -- '--- %s ---\n' "$(date '+%Y-%m-%d %H:%M:%S')"
      local row src_abs dst_abs src_relpath dst_relpath
      for row in "${moved[@]}"; do
        src_abs="${row%%$'\t'*}"
        dst_abs="${row#*$'\t'}"
        src_relpath="${src_abs#$project_root/}"
        dst_relpath="${dst_abs#$project_root/}"
        printf -- '%s -> %s\n' "$src_relpath" "$dst_relpath"
      done
    } >>"$log_file"
    info "Migrated ${#moved[@]} work artifacts into .gpt-creator (see ${log_file#$project_root/})."
  fi
  GC_PROGRESS_MIGRATION_LAST_COUNT=${#moved[@]}

  local tasks_db="${gc_root}/staging/plan/tasks/tasks.db"
  if [[ -f "$tasks_db" ]]; then
    local plan_path="${gc_root}/logs/progress-migration.plan.json"
    local map_path="${gc_root}/logs/progress-migration.map.ndjson"
    local helper_path=""
    if ! helper_path="$(gc_clone_python_tool "progress_migration.py" "$project_root")"; then
      warn "Unable to prepare progress migration helper; skipping task state reconciliation."
      return 0
    fi
    if ! command -v "$python_bin" >/dev/null 2>&1; then
      warn "Skipping task state reconciliation; ${python_bin} not available."
      return 0
    fi
    local plan_output=""
    if plan_output="$("$python_bin" "$helper_path" plan --db "$tasks_db" --output "$plan_path" 2>/dev/null)"; then
      local plan_updates=""
      local plan_stats_helper=""
      if plan_stats_helper="$(gc_clone_python_tool "progress_migration_extract_plan_updates.py" "$project_root")"; then
        plan_updates="$("$python_bin" "$plan_stats_helper" "$plan_output" 2>/dev/null)"
      fi
      if [[ -n "$plan_updates" && "$plan_updates" != "0" ]]; then
        info "Planned task migration updates for ${plan_updates} item(s); applying carry-over."
      fi
      local apply_output=""
      if apply_output="$("$python_bin" "$helper_path" apply --db "$tasks_db" --plan "$plan_path" --map-log "$map_path" 2>/dev/null)"; then
        local apply_stats=""
        local apply_stats_helper=""
        if apply_stats_helper="$(gc_clone_python_tool "progress_migration_extract_apply_stats.py" "$project_root")"; then
          apply_stats="$("$python_bin" "$apply_stats_helper" "$apply_output" 2>/dev/null)"
        fi
        if [[ -n "$apply_stats" ]]; then
          IFS=$'\t' read -r updatedTotal preservedTotal lockedTotal reopenedTotal <<<"$apply_stats"
          info "Carried forward migration state for ${updatedTotal:-0} task(s) (preserved=${preservedTotal:-0}, locked=${lockedTotal:-0}, reopened=${reopenedTotal:-0})."
        fi
      else
        warn "Failed to apply task migration plan; inspect ${plan_path#$project_root/} for details."
      fi
    fi
  fi
}

gc_binder_clear_story() {
  local project_root="${1:?project root required}"
  local epic_slug="${2:-}"
  local story_slug="${3:-}"
  local helper_path
  helper_path="$(gc_clone_python_tool "task_binder.py" "$project_root")" || return 1
  python3 "$helper_path" clear --project "$project_root" --epic "$epic_slug" --story "$story_slug"
}

# Manual command helper to sweep legacy artifacts
cmd_sweep_artifacts() {
  local -a projects=()
  local arg project_path
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project|-p)
        [[ -n "${2:-}" ]] || die "--project requires a directory path"
        project_path="$(abs_path "$2")"
        projects+=("$project_path")
        shift 2
        ;;
      -h|--help)
        cat <<'EOF'
Usage: gpt-creator sweep-artifacts [--project PATH] [PATH...]

Sweep legacy Codex work artifacts (tmp/final/diff/etc.) into the standard
.gpt-creator folder structure. Defaults to the current directory when no path
is supplied. You can pass multiple project directories to tidy them in batch.
EOF
        return 0
        ;;
      --)
        shift
        while [[ $# -gt 0 ]]; do
          projects+=("$(abs_path "$1")")
          shift
        done
        ;;
      -*)
        die "Unknown flag for sweep-artifacts: $1"
        ;;
      *)
        projects+=("$(abs_path "$1")")
        shift
        ;;
    esac
  done

  if ((${#projects[@]} == 0)); then
    projects+=("$(abs_path "${PROJECT_ROOT:-$PWD}")")
  fi

  local root count prev_skip_set prev_skip_val
  for root in "${projects[@]}"; do
    if [[ ! -d "$root" ]]; then
      warn "Skipping missing directory: ${root}"
      continue
    fi
    info "Tidying progress artifacts under ${root}"
    prev_skip_set=0
    prev_skip_val=""
    if [[ ${GC_SKIP_PROGRESS_MIGRATION+x} ]]; then
      prev_skip_set=1
      prev_skip_val="$GC_SKIP_PROGRESS_MIGRATION"
    else
      prev_skip_set=0
      prev_skip_val=""
    fi
    GC_SKIP_PROGRESS_MIGRATION=0
    gc_migrate_progress_artifacts "$root"
    count=${GC_PROGRESS_MIGRATION_LAST_COUNT:-0}
    if (( prev_skip_set )); then
      GC_SKIP_PROGRESS_MIGRATION="$prev_skip_val"
    else
      unset GC_SKIP_PROGRESS_MIGRATION
    fi
    if (( count > 0 )); then
      ok "Relocated ${count} artifact(s) into ${root}/.gpt-creator."
    else
      info "No legacy artifacts found outside .gpt-creator."
    fi
  done

  return 0
}

cmd_tidy_progress() {
  warn "'tidy-progress' has been renamed to 'sweep-artifacts'. Running the renamed command."
  cmd_sweep_artifacts "$@"
}

# Context directories inside project
ensure_ctx() {
  local root="${1:-}" TMP_DIR=""
  if [[ -z "${root}" ]]; then root="${PROJECT_ROOT:-$PWD}"; fi
  PROJECT_ROOT="$(abs_path "$root")"
  GC_DIR="${PROJECT_ROOT}/.gpt-creator"
  STAGING_DIR="${GC_DIR}/staging"
  INPUT_DIR="${STAGING_DIR}/inputs"
  PLAN_DIR="${STAGING_DIR}/plan"
  LOG_DIR="${GC_DIR}/logs"
  ART_DIR="${GC_DIR}/artifacts"
  TMP_DIR="${GC_DIR}/tmp"
  mkdir -p "$GC_DIR" "$STAGING_DIR" "$INPUT_DIR" "$PLAN_DIR" "$LOG_DIR" "$ART_DIR" "$TMP_DIR"
  export INPUT_DIR
  gc_migrate_progress_artifacts "$PROJECT_ROOT"
  gc_configure_tmpdir "$PROJECT_ROOT"
  gc_create_env_if_missing
  gc_load_env
  local base_name
  base_name="$(basename "$PROJECT_ROOT")"
  PROJECT_SLUG="$(slugify_name "${GC_DOCKER_PROJECT_NAME:-$base_name}")"
  GC_DOCKER_PROJECT_NAME="${GC_DOCKER_PROJECT_NAME:-$PROJECT_SLUG}"
  COMPOSE_PROJECT_NAME="${COMPOSE_PROJECT_NAME:-$GC_DOCKER_PROJECT_NAME}"
  GC_FAIL_LOG_DIR="$LOG_DIR"
  export GC_DOCKER_PROJECT_NAME COMPOSE_PROJECT_NAME PROJECT_SLUG

  if gc_reports_enabled; then
    gc_reports_initialize
    gc_reports_touch_activity "$(date +%s)"
  fi
}

gc_project_templates_root() {
  local root="${CLI_ROOT}/project_templates"
  mkdir -p "$root"
  printf '%s\n' "$root"
}

gc_find_primary_rfp() {
  local search_root="${1:-.}"
  find "$search_root" -maxdepth 3 -type f \
    \( -iname 'rfp.md' -o -iname '*rfp*.md' -o -iname '*request*for*proposal*.md' \) \
    | sort | head -n 1
}

gc_bootstrap_state_dir() {
  printf '%s\n' "${PLAN_DIR}/bootstrap"
}

gc_bootstrap_state_file() {
  printf '%s\n' "$(gc_bootstrap_state_dir)/state.json"
}

gc_capture_error_context() {
  local status="${1:-0}"
  local command="${2:-}"
  (( status == 0 )) && return
  GC_LAST_ERROR_STATUS="$status"
  GC_LAST_ERROR_CMD="$command"
}

gc_logs_dir() {
  local dir="${GC_FAIL_LOG_DIR:-}"
  if [[ -z "$dir" ]]; then
    if [[ -n "${PROJECT_ROOT:-}" ]]; then
      dir="${PROJECT_ROOT}/.gpt-creator/logs"
    else
      dir="${PWD}/.gpt-creator/logs"
    fi
  fi
  if ! mkdir -p "$dir" 2>/dev/null; then
    return 1
  fi
  printf '%s\n' "$dir"
}

gc_reports_enabled() {
  (( GC_REPORTS_ON != 0 ))
}

gc_reports_current_user() {
  if [[ -n "${GC_REPORTER:-}" ]]; then
    printf '%s\n' "$GC_REPORTER"
    return 0
  fi
  local name
  name="$(git config user.name 2>/dev/null || true)"
  if [[ -z "$name" ]]; then
    name="${USER:-}"
  fi
  if [[ -z "$name" ]]; then
    name="$(whoami 2>/dev/null || true)"
  fi
  printf '%s\n' "${name:-maintainer}"
}

gc_reports_escape() {
  local s="${1:-}"
  s="${s//\\/\\\\}"
  s="${s//$'\n'/\\n}"
  s="${s//\"/\\\"}"
  printf '%s' "$s"
}

gc_reports_issue_file() {
  local kind="${1:-generic}"
  local dir="${GC_REPORTS_STORE_DIR:-}"
  if [[ -z "$dir" ]]; then
    local base
    base="$(gc_logs_dir)" || return 1
    dir="${base}/issue-reports"
  fi
  if ! mkdir -p "$dir" 2>/dev/null; then
    return 1
  fi
  local stamp random file
  stamp="$(date -u +"%Y%m%dT%H%M%SZ")"
  random="$(printf '%04x%04x' "$RANDOM" "$RANDOM")"
  file="${dir}/${stamp}-${kind}-${random}.yml"
  if [[ -e "$file" ]]; then
    file="${dir}/${stamp}-${kind}-${random}-${RANDOM}.yml"
  fi
  if ! : >"$file" 2>/dev/null; then
    return 1
  fi
  printf '%s\n' "$file"
}

gc_reports_write_issue() {
  local kind="${1:-generic}"
  local summary="${2:-}"
  local definition="${3:-}"
  local priority="${4:-P2-medium}"
  local file
  file="$(gc_reports_issue_file "$kind")" || return 1
  local summary_escaped
  summary_escaped="$(gc_reports_escape "$summary")"
  local reporter
  reporter="$(gc_reports_current_user)"
  local timestamp
  timestamp="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
  {
    printf 'summary: "%s"\n' "$summary_escaped"
    printf 'priority: %s\n' "$priority"
    printf 'issue_definition: |\n'
    if [[ -n "$definition" ]]; then
      while IFS= read -r line || [[ -n "$line" ]]; do
        printf '  %s\n' "$line"
      done <<<"$definition"
    else
      printf '  (no additional details provided)\n'
    fi
    printf 'metadata:\n'
    printf '  type: %s\n' "$kind"
    printf '  timestamp: "%s"\n' "$timestamp"
    printf '  reporter: "%s"\n' "$(gc_reports_escape "$reporter")"
    printf '  command: "%s"\n' "$(gc_reports_escape "${GC_INVOCATION:-$0}")"
    if [[ -n "${GC_LAST_ERROR_STATUS:-}" ]]; then
      printf '  exit_code: %s\n' "${GC_LAST_ERROR_STATUS}"
    fi
    if [[ -n "${GC_LAST_ERROR_CMD:-}" ]]; then
      printf '  last_command: "%s"\n' "$(gc_reports_escape "${GC_LAST_ERROR_CMD}")"
    fi
    printf '  working_dir: "%s"\n' "$(gc_reports_escape "$PWD")"
    printf '  status: open\n'
    printf '  likes: 0\n'
    printf '  comments: 0\n'
  } >"$file"
  printf '%s\n' "$file"
}

gc_reports_sync_github() {
  local report_file="${1:-}"
  local kind="${2:-generic}"
  local summary="${3:-}"
  local definition="${4:-}"
  local priority="${5:-P2-medium}"
  local cli_version="${VERSION:-}"
  local binary_path="${GC_SELF_PATH:-}"

  if [[ -z "$binary_path" || ! -r "$binary_path" ]]; then
    if [[ -n "${CLI_ROOT:-}" && -r "${CLI_ROOT}/bin/${APP_NAME}" ]]; then
      binary_path="${CLI_ROOT}/bin/${APP_NAME}"
    else
      binary_path="$(command -v "${APP_NAME}" 2>/dev/null || true)"
    fi
  fi

  local repo="${GC_GITHUB_REPO:-}"
  local token="${GC_GITHUB_TOKEN:-}"
  if [[ -z "$repo" || -z "$token" ]]; then
    return 0
  fi

  local helper_path
  helper_path="$(gc_clone_python_tool "reports_sync_github.py" "${PROJECT_ROOT:-$PWD}")" || return 1

  local response
  if ! response="$(
    GC_REPORT_SUMMARY="$summary" \
    GC_REPORT_DEFINITION="$definition" \
    GC_REPORT_PRIORITY="$priority" \
    GC_REPORT_KIND="$kind" \
    GC_REPORT_FILE="$report_file" \
    GC_REPORT_COMMAND="${GC_INVOCATION:-}" \
    GC_REPORT_EXIT="${GC_LAST_ERROR_STATUS:-}" \
    GC_REPORT_LAST_CMD="${GC_LAST_ERROR_CMD:-}" \
    GC_REPORT_WORKDIR="$PWD" \
    GC_REPORT_PROJECT="${PROJECT_ROOT:-$PWD}" \
    GC_REPORTER="${GC_REPORTER:-}" \
    GC_REPORT_VERSION="$cli_version" \
    GC_REPORT_BINARY="${binary_path:-}" \
    python3 "$helper_path" "$repo" "$token"
  )"; then
    warn "Failed to create GitHub issue for $(basename "$report_file")."
    return 0
  fi

  local issue_url=""
  local issue_number=""
  IFS=$'\n' read -r issue_url issue_number <<<"$response"
  if [[ -n "$issue_url" ]]; then
    gc_reports_set_metadata_field "$report_file" issue_url "\"$issue_url\""
    if [[ -n "$issue_number" ]]; then
      gc_reports_set_metadata_field "$report_file" issue_number "$issue_number"
    fi
    info "GitHub issue created -> ${issue_url}"
  fi
}

gc_reports_record_issue() {
  local kind="${1:-generic}"
  local summary="${2:-}"
  local definition="${3:-}"
  local priority="${4:-P2-medium}"
  local report_file
  report_file="$(gc_reports_write_issue "$kind" "$summary" "$definition" "$priority")" || return 1
  gc_reports_sync_github "$report_file" "$kind" "$summary" "$definition" "$priority"
  printf '%s\n' "$report_file"
}

gc_reports_touch_activity() {
  local timestamp="${1:-$(date +%s)}"
  local command="${2:-}"
  local file="${GC_REPORTS_ACTIVITY_FILE:-}"
  [[ -n "$file" ]] || return 0
  if [[ -n "$command" ]]; then
    command="${command//$'\n'/ }"
    printf '%s\t%s\n' "$timestamp" "$command" >"$file" 2>/dev/null || true
  else
    printf '%s\n' "$timestamp" >"$file" 2>/dev/null || true
  fi
}

gc_reports_activity_trap() {
  gc_reports_touch_activity "$(date +%s)" "$1"
  return 0
}

gc_reports_handle_crash() {
  local status="${1:-1}"
  gc_reports_enabled || return 0
  local summary
  printf -v summary "Crash (exit %s) while running '%s'" "$status" "${GC_INVOCATION:-$0}"
  local log_dir
  if ! log_dir="$(gc_logs_dir)"; then
    log_dir="<unknown>"
  fi
  local -a lines
  lines=("The CLI exited unexpectedly with status ${status}.")
  if [[ -n "${GC_LAST_ERROR_CMD:-}" ]]; then
    lines+=("Last command observed before exit: ${GC_LAST_ERROR_CMD}")
  fi
  if [[ -n "${GC_LAST_CRASH_LOG:-}" ]]; then
    lines+=("Crash log stored at: ${GC_LAST_CRASH_LOG}")
  fi
  lines+=("Inspect logs under ${log_dir} for further diagnostics.")
  local definition=""
  if ((${#lines[@]})); then
    printf -v definition '%s\n' "${lines[@]}"
    definition="${definition%$'\n'}"
  fi
  local path
  if path="$(gc_reports_record_issue "crash" "$summary" "$definition" "P0-critical")"; then
    warn "Issue report recorded for crash → ${path}"
  fi
}

gc_reports_handle_idle() {
  local idle_seconds="${1:-0}"
  local heartbeat="${2:-}"
  local last_command="${3:-}"
  gc_reports_enabled || return 0
  local summary
  printf -v summary "Idle/stall detected after %ss while running '%s'" "$idle_seconds" "${GC_INVOCATION:-$0}"
  local log_dir
  if ! log_dir="$(gc_logs_dir)"; then
    log_dir="<unknown>"
  fi
  if [[ -n "$last_command" ]]; then
    last_command="${last_command//$'\n'/ }"
  fi
  local -a lines
  lines=("No CLI activity recorded for ${idle_seconds} seconds.")
  if [[ -n "$heartbeat" ]]; then
    lines+=("Heartbeat file: ${heartbeat}")
  fi
  if [[ -n "$last_command" ]]; then
    lines+=("Last command observed: ${last_command}")
  fi
  lines+=("Inspect processes and logs under ${log_dir} to verify whether the command stalled.")
  local definition=""
  if ((${#lines[@]})); then
    printf -v definition '%s\n' "${lines[@]}"
    definition="${definition%$'\n'}"
  fi
  local path
  if path="$(gc_reports_record_issue "idle" "$summary" "$definition" "P1-high")"; then
    warn "Issue report recorded for idle stall → ${path}"
  fi
}

gc_reports_watchdog_loop() {
  local timeout="${1:-0}"
  local interval="${2:-0}"
  local activity_file="${3:-}"
  local main_pid="${4:-0}"
  local sentinel="${5:-}"

  (( timeout > 0 )) || return 0
  (( interval > 0 )) || interval="$timeout"
  [[ -n "$activity_file" && -n "$main_pid" ]] || return 0

  while kill -0 "$main_pid" 2>/dev/null; do
    sleep "$interval" || break
    [[ -f "$activity_file" ]] || continue
    local raw
    raw="$(cat "$activity_file" 2>/dev/null)" || continue
    local payload="$raw"
    local last="${payload%%$'\t'*}"
    local activity_command=""
    if [[ "$payload" == *$'\t'* ]]; then
      activity_command="${payload#*$'\t'}"
    fi
    [[ "$last" =~ ^[0-9]+$ ]] || continue
    local now
    now="$(date +%s)"
    local delta=$(( now - last ))
    if (( delta >= timeout )); then
      if [[ -n "$sentinel" ]]; then
        printf '%s\n' "$now" >"$sentinel" 2>/dev/null || true
      fi
      gc_reports_handle_idle "$delta" "$activity_file" "$activity_command"
      break
    fi
  done
}

gc_reports_start_watchdog() {
  local timeout="${1:-0}"
  local interval="${2:-0}"
  local activity="${3:-}"
  local main_pid="${4:-0}"
  local sentinel="${5:-}"

  if (( timeout <= 0 )); then
    return 0
  fi
  if (( interval <= 0 || interval > timeout )); then
    interval="$timeout"
  fi
  if [[ -z "$activity" || -z "$main_pid" ]]; then
    return 0
  fi
  if [[ -n "${GC_REPORTS_WATCHDOG_PID:-}" ]] && kill -0 "$GC_REPORTS_WATCHDOG_PID" 2>/dev/null; then
    return 0
  fi

  gc_reports_watchdog_loop "$timeout" "$interval" "$activity" "$main_pid" "$sentinel" &
  GC_REPORTS_WATCHDOG_PID=$!
}

gc_reports_initialize() {
  gc_reports_enabled || return 0
  if (( GC_REPORTS_INITIALIZED )); then
    return 0
  fi
  local log_dir
  if ! log_dir="$(gc_logs_dir)"; then
    warn "Unable to determine log directory for issue reporting"
    return 0
  fi
  GC_REPORTS_STORE_DIR="${log_dir}/issue-reports"
  if ! mkdir -p "$GC_REPORTS_STORE_DIR" 2>/dev/null; then
    warn "Unable to prepare reports directory at ${GC_REPORTS_STORE_DIR}"
    return 0
  fi
  GC_REPORTS_ACTIVITY_FILE="${GC_REPORTS_STORE_DIR}/heartbeat-${GC_MAIN_PID}.txt"
  GC_REPORTS_IDLE_SENTINEL="${GC_REPORTS_STORE_DIR}/idle-${GC_MAIN_PID}.flag"
  if ! : >"$GC_REPORTS_ACTIVITY_FILE" 2>/dev/null; then
    warn "Unable to initialize heartbeat tracking for issue reporting"
    return 0
  fi
  GC_REPORTS_INITIALIZED=1
  gc_reports_touch_activity "$(date +%s)"
  gc_reports_start_watchdog "$GC_REPORTS_IDLE_TIMEOUT" "$GC_REPORTS_CHECK_INTERVAL" "$GC_REPORTS_ACTIVITY_FILE" "$GC_MAIN_PID" "$GC_REPORTS_IDLE_SENTINEL"
  return 0
}

gc_reports_cleanup() {
  if [[ -n "${GC_REPORTS_WATCHDOG_PID:-}" ]]; then
    kill "$GC_REPORTS_WATCHDOG_PID" 2>/dev/null || true
    wait "$GC_REPORTS_WATCHDOG_PID" 2>/dev/null || true
    GC_REPORTS_WATCHDOG_PID=""
  fi
}

gc_reports_dir() {
  if [[ -n "${GC_REPORTS_STORE_DIR:-}" ]]; then
    printf '%s\n' "$GC_REPORTS_STORE_DIR"
    return 0
  fi
  local base
  base="$(gc_logs_dir)" || return 1
  local dir="${base}/issue-reports"
  if ! mkdir -p "$dir" 2>/dev/null; then
    return 1
  fi
  printf '%s\n' "$dir"
}

gc_reports_set_metadata_field() {
  local report_file="${1:?report file required}"
  local key="${2:?metadata key required}"
  local value="${3:-}"
  local helper_path
  helper_path="$(gc_clone_python_tool "reports_set_metadata_field.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$report_file" "$key" "$value"
}

gc_reports_resolve_slug() {
  local slug="${1:-}"
  [[ -n "$slug" ]] || return 1
  local dir
  dir="$(gc_reports_dir)" || return 1
  local candidate
  for ext in yml yaml; do
    candidate="${dir}/${slug}.${ext}"
    if [[ -f "$candidate" ]]; then
      printf '%s\n' "$candidate"
      return 0
    fi
  done
  candidate="${dir}/${slug}"
  if [[ -f "$candidate" ]]; then
    printf '%s\n' "$candidate"
    return 0
  fi
  local -a matches=()
  while IFS= read -r file; do
    local base
    base="$(basename "$file")"
    base="${base%.*}"
    if [[ "$base" == "$slug"* ]]; then
      matches+=("$file")
    fi
  done < <(find "$dir" -maxdepth 1 -type f \( -name '*.yml' -o -name '*.yaml' \) -print 2>/dev/null)
  local count="${#matches[@]}"
  if (( count == 1 )); then
    printf '%s\n' "${matches[0]}"
    return 0
  fi
  if (( count > 1 )); then
    warn "Multiple reports match slug '${slug}'."
    local entry
    for entry in "${matches[@]}"; do
      warn "  $(basename "$entry")"
    done
    return 2
  fi
  return 1
}

gc_reports_run_work() {
  local slug="${1:?slug required}"
  local branch_hint="${2:-}"
  local push_after="${3:-1}"
  local prompt_only="${4:-0}"
  local assignee_override="${5:-}"

  local report_path
  if ! report_path="$(gc_reports_resolve_slug "$slug")"; then
    warn "No issue report found for slug: ${slug}"
    return 1
  fi

  local branch="${branch_hint:-report/${slug}}"
  local push_flag="$push_after"
  if [[ "$push_flag" != "0" ]]; then
    push_flag=1
  fi
  local prompt_only_flag="$prompt_only"
  if [[ "$prompt_only_flag" != "0" ]]; then
    prompt_only_flag=1
  fi

  local report_dir="${GC_DIR}/reports/${slug}"
  mkdir -p "$report_dir"
  local prompt_path="${report_dir}/work.md"
  local project_root="${PROJECT_ROOT:-$PWD}"

  local helper_path
  helper_path="$(gc_clone_python_tool reports_run_work.py "${PROJECT_ROOT:-$PWD}")" || return 1

  if ! python3 "$helper_path" "$report_path" "$prompt_path" "$project_root" "$slug" "$branch" "$push_flag"
  then
    warn "Failed to prepare Codex prompt for report ${slug}"
    return 1
  fi

  info "Prepared Codex prompt → ${prompt_path}"

  local now_utc
  now_utc="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
  local assignee="${assignee_override:-${GC_REPORT_ASSIGNEE:-$(gc_reports_current_user)}}"
  if (( prompt_only_flag )); then
    [[ -n "$assignee" ]] && gc_reports_set_metadata_field "$report_path" assigned "\"$(gc_reports_escape "$assignee")\""
    gc_reports_set_metadata_field "$report_path" branch "\"$(gc_reports_escape "$branch")\""
    gc_reports_set_metadata_field "$report_path" status open
    info "Prompt generated (skipping Codex execution due to --prompt-only)."
    info "Run: ${CODEX_BIN:-codex} exec --model ${CODEX_MODEL} --cd \"${PROJECT_ROOT:-$PWD}\" < ${prompt_path}"
    return 0
  fi

  if [[ -n "$assignee" ]]; then
    gc_reports_set_metadata_field "$report_path" assigned "\"$(gc_reports_escape "$assignee")\""
  fi
  gc_reports_set_metadata_field "$report_path" status in-progress
  gc_reports_set_metadata_field "$report_path" branch "\"$(gc_reports_escape "$branch")\""
  gc_reports_set_metadata_field "$report_path" last_started "\"$now_utc\""

  if codex_call "report-${slug}" --step status --prompt "$prompt_path"; then
    local completed_utc
    completed_utc="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
    gc_reports_set_metadata_field "$report_path" status resolved
    gc_reports_set_metadata_field "$report_path" last_completed "\"$completed_utc\""
    ok "Codex resolved report ${slug}"
    return 0
  else
    local failed_utc
    failed_utc="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
    gc_reports_set_metadata_field "$report_path" status open
    gc_reports_set_metadata_field "$report_path" last_failed "\"$failed_utc\""
    warn "Codex failed to resolve report ${slug}"
    return 1
  fi
}

gc_reports_set_idle_timeout() {
  local value="${1:-}"
  if [[ -z "$value" ]]; then
    die "--reports-idle-timeout requires a value in seconds"
  fi
  if ! [[ "$value" =~ ^[0-9]+$ ]]; then
    die "--reports-idle-timeout expects an integer number of seconds (received: $value)"
  fi
  GC_REPORTS_IDLE_TIMEOUT="$value"
}

gc_reports_extract_global_flags() {
  GC_FILTERED_ARGS=()
  local -a args=("$@")
  local idx=0
  while (( idx < ${#args[@]} )); do
    local arg="${args[idx]}"
    case "$arg" in
      --)
        GC_FILTERED_ARGS+=("${args[@]:idx}")
        return 0
        ;;
      --reports-on)
        GC_REPORTS_ON=1
        ;;
      --reports-off)
        GC_REPORTS_ON=0
        ;;
      --reports-idle-timeout=*)
        gc_reports_set_idle_timeout "${arg#*=}"
        ;;
      --reports-idle-timeout)
        (( idx + 1 < ${#args[@]} )) || die "--reports-idle-timeout requires a value in seconds"
        idx=$((idx + 1))
        gc_reports_set_idle_timeout "${args[idx]}"
        ;;
      *)
        GC_FILTERED_ARGS+=("$arg")
        ;;
    esac
    idx=$((idx + 1))
  done
}

gc_write_crash_log() {
  local status="${1:-1}"
  if (( GC_CRASH_LOGGED )); then
    return
  fi

  local log_dir
  log_dir="$(gc_logs_dir)" || return

  local log_file="${log_dir}/crash.log"
  local timestamp
  timestamp="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
  local invocation="${GC_INVOCATION:-$0}"
  local last_command="${GC_LAST_ERROR_CMD:-}"

  {
    printf 'timestamp=%s\n' "$timestamp"
    printf 'command=%s\n' "$invocation"
    printf 'exit_code=%s\n' "$status"
    if [[ -n "$last_command" ]]; then
      printf 'last_command=%s\n' "$last_command"
    fi
    if [[ -n "${PROJECT_ROOT:-}" ]]; then
      printf 'project_root=%s\n' "$PROJECT_ROOT"
    fi
    printf 'working_dir=%s\n' "$PWD"
    printf -- '---\n'
  } >>"$log_file" 2>/dev/null || return

  GC_LAST_CRASH_LOG="$log_file"
  GC_CRASH_LOGGED=1
}

gc_exit_handler() {
  local status="${1:-0}"
  if (( status != 0 )); then
    if [[ -z "${GC_LAST_ERROR_STATUS:-}" || "${GC_LAST_ERROR_STATUS}" -eq 0 ]]; then
      GC_LAST_ERROR_STATUS="$status"
    fi
    gc_write_crash_log "$status"
    if gc_reports_enabled; then
      gc_reports_handle_crash "$status"
    fi
  fi
  gc_reports_cleanup
}

gc_budget_log_stage() {
  local stage="${1:-unknown}"
  local prompt_tokens="${2:-0}"
  local completion_tokens="${3:-0}"
  local total_tokens="${4:-0}"
  local duration_ms="${5:-0}"
  local pruned_items_raw="${6:-[]}"
  local tool_bytes_raw="${7:-{}}"
  local blocked_quota="${8:-false}"
  local note="${9:-}"

  local log_dir="${LOG_DIR:-${PROJECT_ROOT:-$PWD}/.gpt-creator/logs}"
  mkdir -p "$log_dir"
  local usage_file="${log_dir}/codex-usage.ndjson"

  local run_id="${GC_BUDGET_RUN_ID:-manual}"
  local story_ref="${GC_BUDGET_STORY_ID:-}"
  local task_ref="${GC_BUDGET_TASK_ID:-}"
  local model_name="${CODEX_MODEL:-gpt-5-codex}"
  local ts
  ts="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
  local blocked_flag="${blocked_quota:-false}"
  blocked_flag="${blocked_flag,,}"
  if [[ "$blocked_flag" != "true" && "$blocked_flag" != "false" ]]; then
    blocked_flag="false"
  fi

  local stage_key="${stage:-}"
  local total_var
  total_var="$(gc_budget_stage_total_var "$stage_key")"
  local current_total="${!total_var:-0}"
  if [[ "$total_tokens" =~ ^[-+]?[0-9]+$ ]]; then
    local updated_total=$((current_total + total_tokens))
    printf -v "$total_var" '%s' "$updated_total"
    local limit_value
    limit_value="$(gc_budget_get_stage_limit "$stage_key")"
    if [[ "$limit_value" =~ ^[0-9]+$ ]] && (( limit_value > 0 && updated_total > limit_value )); then
      local trip_var
      trip_var="$(gc_budget_stage_tripped_var "$stage_key")"
      printf -v "$trip_var" '%s' "1"
      blocked_flag="true"
    fi
  fi
  local helper_path
  helper_path="$(gc_clone_python_tool "gc_budget_log_stage.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" \
    "$usage_file" "$ts" "$run_id" "$story_ref" "$task_ref" "$stage" "$model_name" \
    "$prompt_tokens" "$completion_tokens" "$total_tokens" "$duration_ms" \
    "$pruned_items_raw" "$tool_bytes_raw" "$blocked_flag" "$note"
}

gc_record_codex_usage() {
  local log_file="${1:-}"
  local task="${2:-}"
  local model="${3:-}"
  local prompt_file="${4:-}"
  local exit_code="${5:-0}"
  local step="${6:-}"
  local max_output="${7:-0}"
  local duration_ms="${8:-0}"

  GC_STAGE_LIMIT_LAST_STEP=""
  GC_STAGE_LIMIT_LAST_LIMIT=""

  GC_LAST_CODEX_PROMPT_TOKENS=0
  GC_LAST_CODEX_COMPLETION_TOKENS=0
  GC_LAST_CODEX_TOTAL_TOKENS=0
  GC_LAST_CODEX_STDOUT_TAIL_HASH="none"
  GC_LAST_CODEX_STDOUT_TAIL_BASE64=""
  GC_LAST_CODEX_TURN_DIFF_HASH=""
  GC_LAST_CODEX_TURN_DIFF_BLOCKS=0

  [[ -n "$log_file" && -f "$log_file" ]] || return 0

  local usage_dir="${LOG_DIR:-${PROJECT_ROOT:-$PWD}/.gpt-creator/logs}"
  mkdir -p "$usage_dir"
  local usage_file="${usage_dir}/codex-usage.ndjson"
  local timestamp
  timestamp="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"

  local py_output=""
  local tmp_output
  tmp_output="$(mktemp "${TMPDIR:-/tmp}/gc-usage-XXXXXX")" || {
    warn "Failed to record Codex usage for task=${task} model=${model}."
    return 1
  }

  local usage_script_py
  usage_script_py="$(mktemp "${TMPDIR:-/tmp}/gc-usage-script-XXXXXX.py")" || {
    warn "Failed to prepare Codex usage recorder."
    rm -f "$tmp_output" || true
    return 1
  }

  local usage_source="${CLI_ROOT}/scripts/python/record_codex_usage.py"
  if [[ ! -f "$usage_source" ]]; then
    warn "Codex usage recorder missing at ${usage_source}"
    rm -f "$tmp_output" "$usage_script_py" || true
    return 1
  fi

  if ! cp "$usage_source" "$usage_script_py"; then
    warn "Failed to copy Codex usage recorder."
    rm -f "$tmp_output" "$usage_script_py" || true
    return 1
  fi


  if GC_USAGE_SUPPRESS_WRITE=1 python3 "$usage_script_py" "$log_file" "$usage_file" "$timestamp" "$task" "$model" "$prompt_file" "$exit_code" "${GC_COMMAND_FAILURE_CACHE:-}" "${GC_COMMAND_STREAM_CACHE:-}" "${GC_COMMAND_FILE_CACHE:-}" "${GC_COMMAND_SCAN_CACHE:-}" "$step" "$max_output" "$duration_ms" >"$tmp_output"
  then
    py_output="$(<"$tmp_output")"
    rm -f "$usage_script_py" || true
    rm -f "$tmp_output" || true
    if [[ -n "$py_output" ]]; then
      while IFS= read -r line; do
        [[ -z "$line" ]] && continue
        case "$line" in
          LIMIT_DETECTED$'\t'*)
            GC_CODEX_USAGE_LIMIT_REACHED=1
            GC_CODEX_USAGE_LIMIT_CONFIRMED=0
            GC_CODEX_USAGE_LIMIT_MESSAGE="${line#LIMIT_DETECTED$'\t'}"
            if gc_usage_limit_is_provider_signal "${GC_CODEX_USAGE_LIMIT_MESSAGE}"; then
              GC_CODEX_USAGE_LIMIT_CONFIRMED=1
            fi
            ;;
          LIMIT_DETECTED)
            GC_CODEX_USAGE_LIMIT_REACHED=1
            GC_CODEX_USAGE_LIMIT_CONFIRMED=0
            GC_CODEX_USAGE_LIMIT_MESSAGE=""
            ;;
          USAGE$'\t'*)
            IFS=$'\t' read -r _ prompt_val completion_val total_val <<<"$line"
            if [[ "$prompt_val" =~ ^[0-9]+$ ]]; then
              GC_LAST_CODEX_PROMPT_TOKENS=$((prompt_val))
            else
              GC_LAST_CODEX_PROMPT_TOKENS=0
            fi
            if [[ "$completion_val" =~ ^[0-9]+$ ]]; then
              GC_LAST_CODEX_COMPLETION_TOKENS=$((completion_val))
            else
              GC_LAST_CODEX_COMPLETION_TOKENS=0
            fi
            if [[ "$total_val" =~ ^[0-9]+$ ]]; then
              GC_LAST_CODEX_TOTAL_TOKENS=$((total_val))
            else
              GC_LAST_CODEX_TOTAL_TOKENS=$((GC_LAST_CODEX_PROMPT_TOKENS + GC_LAST_CODEX_COMPLETION_TOKENS))
            fi
            ;;
          CMDFAIL$'\t'*)
            local rest="${line#CMDFAIL$'\t'}"
            local repeat_flag total_fail _ digest encoded_command encoded_summary
            IFS=$'\t' read -r repeat_flag total_fail _ digest encoded_command encoded_summary <<<"$rest"
            local command_text summary_text
            command_text="$(gc_decode_base64 "${encoded_command:-}")"
            summary_text="$(gc_decode_base64 "${encoded_summary:-}")"
            command_text="${command_text//$'\r'/ }"
            command_text="${command_text//$'\n'/ }"
            summary_text="${summary_text//$'\r'/ }"
            summary_text="${summary_text//$'\n'/ }"
            command_text="${command_text#"${command_text%%[![:space:]]*}"}"
            command_text="${command_text%"${command_text##*[![:space:]]}"}"
            summary_text="${summary_text#"${summary_text%%[![:space:]]*}"}"
            summary_text="${summary_text%"${summary_text##*[![:space:]]}"}"
            if [[ ${#summary_text} -gt 160 ]]; then
              summary_text="${summary_text:0:157}..."
            fi
            if [[ "$repeat_flag" == "1" && -n "$digest" ]]; then
              if [[ "$GC_COMMAND_FAILURE_WARN_DIGESTS" != *"|$digest|"* ]]; then
                if [[ -n "$command_text" ]]; then
                  warn "Repeated command failure detected (x${total_fail}): ${command_text}"
                else
                  warn "Repeated command failure detected (x${total_fail})."
                fi
                if [[ -n "$summary_text" ]]; then
                  warn "  Last failure: ${summary_text}"
                fi
                GC_COMMAND_FAILURE_WARN_DIGESTS+="|$digest|"
              fi
            fi
            ;;
          CMDSTREAM$'\t'*)
            local rest="${line#CMDSTREAM$'\t'}"
            local digest repeat_flag total_seen encoded_summary encoded_advice
            IFS=$'\t' read -r digest repeat_flag total_seen encoded_summary encoded_advice <<<"$rest"
            local summary_text advice_text
            summary_text="$(gc_decode_base64 "${encoded_summary:-}")"
            advice_text="$(gc_decode_base64 "${encoded_advice:-}")"
            summary_text="${summary_text//$'\r'/ }"
            summary_text="${summary_text//$'\n'/ }"
            advice_text="${advice_text//$'\r'/ }"
            advice_text="${advice_text//$'\n'/ }"
            summary_text="${summary_text#"${summary_text%%[![:space:]]*}"}"
            summary_text="${summary_text%"${summary_text##*[![:space:]]}"}"
            advice_text="${advice_text#"${advice_text%%[![:space:]]*}"}"
            advice_text="${advice_text%"${advice_text##*[![:space:]]}"}"
            if [[ "$GC_COMMAND_STREAM_WARN_DIGESTS" != *"|$digest|"* ]]; then
              local repeat_note=""
              if [[ "$repeat_flag" == "1" ]]; then
                repeat_note=" (repeat)"
              fi
              warn "Sequential sed/cat streaming detected${repeat_note}: ${summary_text}"
              if [[ -n "$advice_text" ]]; then
                warn "  Recommendation: ${advice_text}"
              fi
              if [[ "$total_seen" =~ ^[0-9]+$ && "$total_seen" -gt 1 ]]; then
                warn "  Observed ${total_seen} times; pivot to targeted searches to cut token usage."
              fi
              GC_COMMAND_STREAM_WARN_DIGESTS+="|$digest|"
            fi
            ;;
          CMDSCAN$'\t'*)
            local rest="${line#CMDSCAN$'\t'}"
            local digest repeat_flag total_seen encoded_command encoded_message
            IFS=$'\t' read -r digest repeat_flag total_seen encoded_command encoded_message <<<"$rest"
            local command_text message_text
            command_text="$(gc_decode_base64 "${encoded_command:-}")"
            message_text="$(gc_decode_base64 "${encoded_message:-}")"
            command_text="${command_text//$'\r'/ }"
            command_text="${command_text//$'\n'/ }"
            message_text="${message_text//$'\r'/ }"
            message_text="${message_text//$'\n'/ }"
            command_text="${command_text#"${command_text%%[![:space:]]*}"}"
            command_text="${command_text%"${command_text##*[![:space:]]}"}"
            message_text="${message_text#"${message_text%%[![:space:]]*}"}"
            message_text="${message_text%"${message_text##*[![:space:]]}"}"
            if [[ "$GC_COMMAND_SCAN_WARN_DIGESTS" != *"|$digest|"* ]]; then
              local repeat_note=""
              if [[ "$repeat_flag" == "1" ]]; then
                repeat_note=" (repeat)"
              fi
              warn "Directory crawl detected${repeat_note}: ${command_text:-<command unavailable>}"
              if [[ -n "$message_text" ]]; then
                warn "  Reason: ${message_text}"
              fi
              if [[ "$total_seen" =~ ^[0-9]+$ && "$total_seen" -gt 1 ]]; then
                warn "  Observed ${total_seen} times; declare new focus before exploring other areas."
              fi
              GC_COMMAND_SCAN_WARN_DIGESTS+="|$digest|"
            fi
            ;;
          CMDGUARD$'\t'*)
            local rest="${line#CMDGUARD$'\t'}"
            local digest repeat_flag issue_count encoded_command encoded_message
            IFS=$'\t' read -r digest repeat_flag issue_count encoded_command encoded_message <<<"$rest"
            local command_text message_text
            command_text="$(gc_decode_base64 "${encoded_command:-}")"
            message_text="$(gc_decode_base64 "${encoded_message:-}")"
            command_text="${command_text//$'\r'/ }"
            command_text="${command_text//$'\n'/ }"
            message_text="${message_text//$'\r'/ }"
            message_text="${message_text//$'\n'/ }"
            command_text="${command_text#"${command_text%%[![:space:]]*}"}"
            command_text="${command_text%"${command_text##*[![:space:]]}"}"
            message_text="${message_text#"${message_text%%[![:space:]]*}"}"
            message_text="${message_text%"${message_text##*[![:space:]]}"}"
            if [[ "$GC_COMMAND_GUARD_WARN_DIGESTS" != *"|$digest|"* ]]; then
              warn "Guardrails blocked expensive command: ${command_text:-pnpm}"
              if [[ -n "$message_text" ]]; then
                warn "  Fix before retry: ${message_text}"
              fi
              if [[ "$issue_count" =~ ^[0-9]+$ && "$issue_count" -gt 1 ]]; then
                warn "  Multiple pre-check issues detected; update plan/focus once resolved."
              fi
              GC_COMMAND_GUARD_WARN_DIGESTS+="|$digest|"
            fi
            ;;
          CMDFILE$'\t'*)
            local rest="${line#CMDFILE$'\t'}"
            local digest repeat_flag total_seen encoded_summary encoded_excerpt
            IFS=$'\t' read -r digest repeat_flag total_seen encoded_summary encoded_excerpt <<<"$rest"
            local summary_text excerpt_text
            summary_text="$(gc_decode_base64 "${encoded_summary:-}")"
            excerpt_text="$(gc_decode_base64 "${encoded_excerpt:-}")"
            summary_text="${summary_text//$'\r'/ }"
            summary_text="${summary_text//$'\n'/ }"
            excerpt_text="${excerpt_text//$'\r'/ }"
            excerpt_text="${excerpt_text//$'\n'/ }"
            summary_text="${summary_text#"${summary_text%%[![:space:]]*}"}"
            summary_text="${summary_text%"${summary_text##*[![:space:]]}"}"
            excerpt_text="${excerpt_text#"${excerpt_text%%[![:space:]]*}"}"
            excerpt_text="${excerpt_text%"${excerpt_text##*[![:space:]]}"}"
            if [[ "$repeat_flag" == "1" ]]; then
              if [[ "$GC_COMMAND_FILE_WARN_DIGESTS" != *"|$digest|"* ]]; then
                warn "Repeated file read detected: ${summary_text}"
                if [[ -n "$excerpt_text" ]]; then
                  local preview="$excerpt_text"
                  if [[ ${#preview} -gt 140 ]]; then
                    preview="${preview:0:137}..."
                  fi
                  warn "  Refer to cached excerpt instead of re-running cat/sed: \"${preview}\""
                else
                  warn "  Refer to the cached excerpt listed in the prompt instead of re-reading the file."
                fi
                if [[ "$total_seen" =~ ^[0-9]+$ && "$total_seen" -gt 1 ]]; then
                  warn "  Observed ${total_seen} reads for this slice; reuse the cached snippet unless the file changed."
                fi
                GC_COMMAND_FILE_WARN_DIGESTS+="|$digest|"
              fi
            else
              if [[ -n "$summary_text" ]]; then
                info "Cached file excerpt saved: ${summary_text}"
              fi
            fi
            ;;
        esac
      done <<<"$py_output"
      if [[ -n "$step" ]]; then
        # shellcheck disable=SC2034  # tracked for downstream diagnostics
        GC_LAST_CODEX_OUTPUT_STEP="$step"
      fi
      if [[ "$max_output" =~ ^[0-9]+$ ]]; then
        # shellcheck disable=SC2034  # tracked for downstream diagnostics
        GC_LAST_CODEX_OUTPUT_LIMIT="$max_output"
      else
        # shellcheck disable=SC2034  # tracked for downstream diagnostics
        GC_LAST_CODEX_OUTPUT_LIMIT=0
      fi
    fi
    local limit_value="${GC_CODEX_MAX_TOKENS_PER_TASK:-0}"
    if [[ "$limit_value" =~ ^[0-9]+$ ]]; then
      limit_value=$((limit_value))
      if (( limit_value > 0 && GC_LAST_CODEX_TOTAL_TOKENS > limit_value )); then
        # shellcheck disable=SC2034
        GC_CODEX_USAGE_LIMIT_REACHED=1
        # shellcheck disable=SC2034
        GC_CODEX_USAGE_LIMIT_CONFIRMED=1
        if [[ -z "${GC_CODEX_USAGE_LIMIT_MESSAGE:-}" ]]; then
          GC_CODEX_USAGE_LIMIT_MESSAGE="Codex tokens ${GC_LAST_CODEX_TOTAL_TOKENS} exceeded per-task limit ${limit_value}."
        fi
      fi
    fi
    if command -v python3 >/dev/null 2>&1; then
      local tail_info=""
      local tail_helper=""
      if tail_helper="$(gc_clone_python_tool "codex_log_tail.py" "${PROJECT_ROOT:-$PWD}")"; then
        tail_info="$(python3 "$tail_helper" "$log_file")" || tail_info=""
      else
        tail_info=""
      fi
      if [[ -n "$tail_info" ]]; then
        local tail_hash tail_b64 turn_hash turn_blocks
        IFS=$'\n' read -r tail_hash tail_b64 turn_hash turn_blocks <<<"$tail_info"
        # shellcheck disable=SC2034
        [[ -n "$tail_hash" ]] && GC_LAST_CODEX_STDOUT_TAIL_HASH="$tail_hash"
        # shellcheck disable=SC2034
        [[ -n "$tail_b64" ]] && GC_LAST_CODEX_STDOUT_TAIL_BASE64="$tail_b64"
        # shellcheck disable=SC2034
        [[ -n "$turn_hash" ]] && GC_LAST_CODEX_TURN_DIFF_HASH="$turn_hash"
        if [[ "$turn_blocks" =~ ^[0-9]+$ ]]; then
          # shellcheck disable=SC2034
          GC_LAST_CODEX_TURN_DIFF_BLOCKS="$turn_blocks"
        fi
      fi
    fi
    local stage_blocked="false"
    if [[ "${GC_CODEX_USAGE_LIMIT_REACHED:-0}" == "1" ]]; then
      stage_blocked="true"
    fi
    local stage_pruned="${GC_CURRENT_PRUNED_ITEMS:-[]}" 
    if [[ -z "$stage_pruned" ]]; then
      stage_pruned="[]"
    fi
    gc_budget_log_stage "${step:-patch}" "${GC_LAST_CODEX_PROMPT_TOKENS:-0}" "${GC_LAST_CODEX_COMPLETION_TOKENS:-0}" "${GC_LAST_CODEX_TOTAL_TOKENS:-0}" "${duration_ms:-0}" "$stage_pruned" "{}" "$stage_blocked"
    if [[ -n "${step:-}" ]] && gc_budget_stage_tripped "${step}"; then
      GC_STAGE_LIMIT_LAST_STEP="${step}"
      GC_STAGE_LIMIT_LAST_LIMIT="$(gc_budget_get_stage_limit "${step}")"
    fi
    rm -f "$log_file" || true
    return 0
  else
    rm -f "$tmp_output" || true
    warn "Failed to record Codex usage for task=${task} model=${model}."
    return 1
  fi
}

gc_set_llm_output_limit_if_valid() {
  local var_name="$1"
  local candidate="${2//[[:space:]]/}"
  if [[ "$candidate" =~ ^[0-9]+$ ]] && (( candidate > 0 )); then
    printf -v "$var_name" '%s' "$candidate"
  fi
}

gc_load_llm_output_limits() {
  local root="${1:-${PROJECT_ROOT:-$PWD}}"
  local helper=""
  if helper="$(gc_clone_python_tool "load_output_limits.py" "$root")"; then
    :
  else
    return 1
  fi
  local raw_output=""
  if ! raw_output="$(python3 "$helper" "$root" 2>/dev/null)"; then
    return 1
  fi
  while IFS='=' read -r key value; do
    [[ -z "${key// }" ]] && continue
    case "$key" in
      plan) gc_set_llm_output_limit_if_valid GC_LLM_OUTPUT_LIMIT_PLAN "$value" ;;
      status) gc_set_llm_output_limit_if_valid GC_LLM_OUTPUT_LIMIT_STATUS "$value" ;;
      verify) gc_set_llm_output_limit_if_valid GC_LLM_OUTPUT_LIMIT_VERIFY "$value" ;;
      patch) gc_set_llm_output_limit_if_valid GC_LLM_OUTPUT_LIMIT_PATCH "$value" ;;
      hard_cap) gc_set_llm_output_limit_if_valid GC_LLM_OUTPUT_LIMIT_HARD_CAP "$value" ;;
    esac
  done <<<"$raw_output"
  return 0
}

gc_output_limit_default_for_step() {
  local step="${1:-patch}"
  case "${step,,}" in
    plan) printf '%s\n' "$GC_LLM_OUTPUT_LIMIT_PLAN_DEFAULT" ;;
    status) printf '%s\n' "$GC_LLM_OUTPUT_LIMIT_STATUS_DEFAULT" ;;
    verify) printf '%s\n' "$GC_LLM_OUTPUT_LIMIT_VERIFY_DEFAULT" ;;
    patch|*) printf '%s\n' "$GC_LLM_OUTPUT_LIMIT_PATCH_DEFAULT" ;;
  esac
}

gc_output_limit_for_step() {
  local step="${1:-patch}"
  local step_lower="${step,,}"
  local limit
  case "$step_lower" in
    plan) limit="$GC_LLM_OUTPUT_LIMIT_PLAN" ;;
    status) limit="$GC_LLM_OUTPUT_LIMIT_STATUS" ;;
    verify) limit="$GC_LLM_OUTPUT_LIMIT_VERIFY" ;;
    patch) limit="$GC_LLM_OUTPUT_LIMIT_PATCH" ;;
    *) step_lower="patch"; limit="$GC_LLM_OUTPUT_LIMIT_PATCH" ;;
  esac
  if ! [[ "$limit" =~ ^[0-9]+$ ]] || (( limit <= 0 )); then
    limit="$(gc_output_limit_default_for_step "$step_lower")"
  fi
  local hard_cap="$GC_LLM_OUTPUT_LIMIT_HARD_CAP"
  if ! [[ "$hard_cap" =~ ^[0-9]+$ ]] || (( hard_cap <= 0 )); then
    hard_cap="$GC_LLM_OUTPUT_LIMIT_HARD_CAP_DEFAULT"
  fi
  if (( limit > hard_cap )); then
    limit="$hard_cap"
  fi
  printf '%s\n' "$limit"
}

gc_log_blocked_quota() {
  local meta_path="$1"
  local task_id="$2"
  local story_slug="$3"
  local run_stamp="$4"
  local model="$5"

  [[ -f "$meta_path" ]] || return 0

  local log_dir="${LOG_DIR:-${PROJECT_ROOT:-$PWD}/.gpt-creator/logs}"
  local log_path="${log_dir}/work-on-tasks/blocked-quota.ndjson"

  python3 "${CLI_ROOT}/scripts/python/log_blocked_quota.py" \
    --meta "$meta_path" \
    --task-id "$task_id" \
    --story-slug "$story_slug" \
    --run-id "$run_stamp" \
    --model "$model" \
    --log "$log_path"
}

gc_log_task_metrics() {
  local run_id="$1"
  local story_slug="$2"
  local task_number="$3"
  local task_id="$4"
  local status="$5"
  local story_points="$6"
  local tokens_retrieve="${7:-0}"
  local tokens_plan="${8:-0}"
  local tokens_patch="${9:-0}"
  local tokens_verify="${10:-0}"
  local tokens_prompt_estimate="${11:-0}"
  local llm_prompt_tokens="${12:-0}"
  local llm_completion_tokens="${13:-0}"

  local helper_path
  helper_path="$(gc_clone_python_tool "log_task_metrics.py" "${PROJECT_ROOT:-$PWD}")" || return 0
  local log_dir="${LOG_DIR:-${PROJECT_ROOT:-$PWD}/.gpt-creator/logs}/work-on-tasks"
  mkdir -p "$log_dir"
  local log_path="${log_dir}/task-metrics.ndjson"

  python3 "$helper_path" \
    --log "$log_path" \
    --run-id "$run_id" \
    --story "$story_slug" \
    --task-number "$task_number" \
    --task-id "$task_id" \
    --status "$status" \
    --story-points "$story_points" \
    --tokens-retrieve "$tokens_retrieve" \
    --tokens-plan "$tokens_plan" \
    --tokens-patch "$tokens_patch" \
    --tokens-verify "$tokens_verify" \
    --prompt-estimate "$tokens_prompt_estimate" \
    --llm-prompt "$llm_prompt_tokens" \
    --llm-completion "$llm_completion_tokens"
}

gc_bootstrap_reset_state() {
  local file
  file="$(gc_bootstrap_state_file)"
  rm -f "$file"
}

gc_bootstrap_mark_step() {
  local step="${1:?step required}"
  local status="${2:?status required}"
  local file
  file="$(gc_bootstrap_state_file)"
  mkdir -p "$(dirname "$file")"
  local helper_path
  helper_path="$(gc_clone_python_tool "bootstrap_mark_step.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$file" "$step" "$status" || return 1
}

gc_bootstrap_mark_complete() {
  local file
  file="$(gc_bootstrap_state_file)"
  mkdir -p "$(dirname "$file")"
  local helper_path
  helper_path="$(gc_clone_python_tool "bootstrap_mark_complete.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$file" || return 1
}

gc_bootstrap_step_is_done() {
  local step="${1:?step required}"
  local file
  file="$(gc_bootstrap_state_file)"
  [[ -f "$file" ]] || return 1
  local helper_path
  helper_path="$(gc_clone_python_tool "bootstrap_step_is_done.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$file" "$step"
}

gc_bootstrap_run_step() {
  local step="${1:?step required}"
  shift
  if gc_bootstrap_step_is_done "$step"; then
    info "Step '${step}' already completed; skipping."
    return 0
  fi
  if "$@"; then
    gc_bootstrap_mark_step "$step" "done"
    return 0
  else
    gc_bootstrap_mark_step "$step" "failed"
    return 1
  fi
}

gc_bootstrap_have_rfp() {
  local stage_dir="${STAGING_DIR:-}"
  local input_dir="${INPUT_DIR:-}"
  [[ -n "$stage_dir" && -f "$stage_dir/docs/rfp.md" ]] && return 0
  [[ -n "$stage_dir" && -f "$stage_dir/rfp.md" ]] && return 0
  [[ -n "$input_dir" && -f "$input_dir/rfp.md" ]] && return 0
  return 1
}

gc_auto_project_template() {
  local project_root="${1:?project root required}"
  local templates_root="${2:?templates root required}"
  shift 2
  local -a template_dirs=("$@")
  local count=${#template_dirs[@]}
  (( count )) || return 1
  if (( count == 1 )); then
    printf '%s\n' "${template_dirs[0]}"
    return 0
  fi

  local rfp_path
  rfp_path="$(gc_find_primary_rfp "$project_root")"
  [[ -n "$rfp_path" ]] || return 1

  local helper_path
  helper_path="$(gc_clone_python_tool "auto_project_template.py" "$project_root")" || return 1
  python3 "$helper_path" "$rfp_path" "${template_dirs[@]}"
}

gc_copy_project_template() {
  local template_dir="${1:?template directory required}"
  local project_root="${2:?project root required}"
  local helper_path
  helper_path="$(gc_clone_python_tool "copy_project_template.py" "$project_root")" || return 1
  python3 "$helper_path" "$template_dir" "$project_root"
}

gc_apply_project_template() {
  local project_root="${1:?project root required}"
  local template_request="${2:-auto}"
  local templates_root
  templates_root="$(gc_project_templates_root)"

  mapfile -t available_templates < <(find "$templates_root" -mindepth 1 -maxdepth 1 -type d | sort)
  (( ${#available_templates[@]} )) || {
    info "No project templates available under ${templates_root}; continuing without scaffolding."
    return 0
  }

  local chosen=""
  local request_lower
  request_lower="$(to_lower "$template_request")"
  if [[ "$request_lower" == "skip" ]]; then
    info "Skipping project template scaffolding (per flag)."
    return 0
  fi

  if [[ "$request_lower" != "auto" ]]; then
      for tpl in "${available_templates[@]}"; do
        local tpl_name
        tpl_name="$(basename "$tpl")"
      if [[ "$(to_lower "$tpl_name")" == "$request_lower" ]]; then
        chosen="$tpl"
        template_request="$tpl_name"
        break
      fi
    done
    if [[ -z "$chosen" ]]; then
      warn "Template '${template_request}' not found under ${templates_root}; available: $(printf '%s ' "${available_templates[@]##*/}")"
      return 1
    fi
  else
    chosen="$(gc_auto_project_template "$project_root" "$templates_root" "${available_templates[@]}")"
    if [[ -z "$chosen" ]]; then
      info "No matching project template determined automatically; continuing without scaffolding."
      return 0
    fi
  fi

  local template_name
  template_name="$(basename "$chosen")"
  info "Applying project template → ${template_name}"
  if ! gc_copy_project_template "$chosen" "$project_root"; then
    warn "Failed to copy template '${template_name}'"
    return 1
  fi
}

gc_parse_jira_tasks() {
  local jira_file="${1:?jira markdown path required}"
  local out_json="${2:?output json path required}"
  local helper_path
  helper_path="$(gc_clone_python_tool "parse_jira_tasks.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$jira_file" "$out_json"
}

gc_build_tasks_db() {
  local tasks_json="${1:?tasks json path required}"
  local db_path="${2:?sqlite db path required}"
  local force_flag="${3:-0}"
  local helper_path
  helper_path="$(gc_clone_python_tool "build_tasks_db.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$tasks_json" "$db_path" "$force_flag"
}

GC_CONTEXT_SKIP_SUBSTRINGS=(
  "/.gpt-creator/staging/plan/work/runs/"
  "/.gpt-creator/staging/plan/create-jira-tasks/prompts/"
  "/.gpt-creator/staging/plan/create-sds/prompts/"
  "/.gpt-creator/logs/"
  "/.git/"
  "/node_modules/"
  "/dist/"
  "/build/"
  "/docs/automation/prompts/"
)
GC_CONTEXT_SKIP_SUFFIXES=(
  ".meta.json"
  ".log"
  ".log.gz"
)

gc_context_should_skip_path() {
  local path="${1:-}"
  [[ -n "$path" ]] || return 1
  local normalized="${path//\\//}"
  local pattern
  for pattern in "${GC_CONTEXT_SKIP_SUBSTRINGS[@]}"; do
    if [[ "$normalized" == *"$pattern"* ]]; then
      return 0
    fi
  done
  for pattern in "${GC_CONTEXT_SKIP_SUFFIXES[@]}"; do
    if [[ "$normalized" == *"$pattern" ]]; then
      return 0
    fi
  done
  local -a extra_patterns=()
  if declare -p GC_CONTEXT_EXCLUDES >/dev/null 2>&1; then
    extra_patterns=("${GC_CONTEXT_EXCLUDES[@]}")
  elif [[ -n "${GC_CONTEXT_EXCLUDES:-}" ]]; then
    while IFS= read -r _gc_extra_pattern; do
      [[ -n "$_gc_extra_pattern" ]] || continue
      extra_patterns+=("$_gc_extra_pattern")
    done <<<"$(printf '%s\n' "$GC_CONTEXT_EXCLUDES" | tr ':' '\n')"
  fi
  for pattern in "${extra_patterns[@]}"; do
    if [[ "$normalized" == *"$pattern"* ]]; then
      return 0
    fi
  done
  return 1
}

gc_build_context_file() {
  local dest_file="${1:?context destination required}"
  local staging_dir="${2:-$GC_STAGING_DIR}"
  local max_lines="${GC_CONTEXT_FILE_LINES:-200}"
  local allow_ui_dump="${GC_CONTEXT_INCLUDE_UI:-0}"
  local doc_snippet_env="${GC_PROMPT_DOC_SNIPPETS:-}"
  local doc_snippet_mode=0
  if [[ -n "$doc_snippet_env" ]]; then
    case "${doc_snippet_env,,}" in
      0|false|no) doc_snippet_mode=0 ;;
      *) doc_snippet_mode=1 ;;
    esac
  fi
  local pointer_digest_helper=""
  local context_doc_snippet_helper=""
  local context_dump_helper=""
  if (( doc_snippet_mode )); then
    pointer_digest_helper="$(gc_clone_python_tool "context_pointer_digest.py" "${PROJECT_ROOT:-$PWD}")" || return 1
    context_doc_snippet_helper="$(gc_clone_python_tool "context_doc_snippet_dump.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  fi
  context_dump_helper="$(gc_clone_python_tool "context_dump_file.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  local -A gc_context_pointer_seen=()
  local -a skip_patterns=()
  if declare -p GC_CONTEXT_SKIP_PATTERNS >/dev/null 2>&1; then
    skip_patterns=("${GC_CONTEXT_SKIP_PATTERNS[@]}")
  fi
  mkdir -p "$(dirname "$dest_file")"
  {
    echo "# Project Context (auto-generated)"
    echo
    shopt -s nullglob
    shopt -s globstar 2>/dev/null || true
    local f
    local -a patterns=(
      "$staging_dir"/pdr.* 
      "$staging_dir"/sds.* 
      "$staging_dir"/openapi.* 
      "$staging_dir"/*.md 
      "$staging_dir"/*.mdx 
      "$staging_dir"/*.adoc 
      "$staging_dir"/*.mmd 
      "$staging_dir"/*.sql 
      "$staging_dir"/*.yml 
      "$staging_dir"/*.yaml
    )
    if [[ "$allow_ui_dump" == "1" ]]; then
      patterns+=("$staging_dir"/*ui*pages*.* "$staging_dir"/*rfp*.*)
    fi
    for f in "${patterns[@]}"; do
      [[ -f "$f" ]] || continue
      if gc_context_should_skip_path "$f"; then
        continue
      fi
      local base_name
      base_name="$(basename "$f")"

      local pointer_digest=""
      if (( doc_snippet_mode )); then
        pointer_digest="$(GC_CONTEXT_POINTER_FILE="$f" python3 "$pointer_digest_helper")" || pointer_digest=""
        if [[ -n "$pointer_digest" ]]; then
          local existing=""
          if [[ -n "${gc_context_pointer_seen[$pointer_digest]+_}" ]]; then
            existing="${gc_context_pointer_seen[$pointer_digest]}"
          fi
          if [[ -n "$existing" ]]; then
            echo ""
            echo "----- FILE: ${base_name} -----"
            echo "(duplicate staged doc skipped; same initial content as ${existing})"
            continue
          fi
          gc_context_pointer_seen[$pointer_digest]="$base_name"
        fi
      fi

      local skip_file=0
      if ((${#skip_patterns[@]} > 0)); then
        local pattern
        for pattern in "${skip_patterns[@]}"; do
          [[ -n "$pattern" ]] || continue
          if [[ "$base_name" == "$pattern" || "$f" == "$pattern" ]]; then
            skip_file=1
            break
          fi
          # Treat plain patterns without glob characters as substring matches.
          if [[ "$pattern" != *'*'* && "$pattern" != *'?'* ]]; then
            if [[ "$f" == *"$pattern"* ]]; then
              skip_file=1
              break
            fi
          fi
        done
      fi
      (( skip_file )) && continue

      echo ""
      echo "----- FILE: ${base_name} -----"
      local mime=""
      if command -v file >/dev/null 2>&1; then
        mime="$(file -b --mime-type "$f" 2>/dev/null || true)"
      fi
      if [[ -n "$mime" ]]; then
        case "$mime" in
          text/*|application/json|application/vnd.openxmlformats-officedocument*|application/xml|application/xhtml+xml)
            ;;
          *)
            echo "(skipped non-text file; path: $f)"
            continue
            ;;
        esac
      fi
      if (( doc_snippet_mode )); then
        GC_CONTEXT_POINTER_MODE=1 GC_CONTEXT_POINTER_DIGEST="$pointer_digest" GC_DUMP_FILE="$f" GC_MAX_LINES="$max_lines" python3 "$context_doc_snippet_helper"
      else
        GC_DUMP_FILE="$f" GC_MAX_LINES="$max_lines" python3 "$context_dump_helper"
      fi
    done
    shopt -u globstar 2>/dev/null || true
    shopt -u nullglob || true
  } >"$dest_file"
}

gc_discovery_is_stale() {
  local discovery_file="${1:-}"
  shift || true
  local -a required_keys=("$@")
  if [[ -z "$discovery_file" || ! -f "$discovery_file" || ! -s "$discovery_file" ]]; then
    return 0
  fi
  local helper_path
  helper_path="$(gc_clone_python_tool "discovery_is_stale.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$discovery_file" "${required_keys[@]}"
}

gc_discovery_fill_defaults() {
  local discovery_file="${1:-}"
  shift || true
  local -a required_keys=("$@")
  [[ -n "$discovery_file" && -f "$discovery_file" ]] || return 1
  local helper_path
  helper_path="$(gc_clone_python_tool "discovery_fill_defaults.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$discovery_file" "${required_keys[@]}"
}

gc_refresh_discovery_if_needed() {
  local discovery_file="${STAGING_DIR}/discovery.yaml"
  local -a required=("pdr" "sds" "rfp" "jira" "ui_pages" "openapi")
  if gc_discovery_is_stale "$discovery_file" "${required[@]}"; then
    info "Discovery manifest missing context; running scan + normalize."
    if ! cmd_scan --project "$PROJECT_ROOT"; then
      warn "Automatic scan failed; rerun 'gpt-creator scan' manually."
      return
    fi
    if ! cmd_normalize --project "$PROJECT_ROOT"; then
      warn "Automatic normalize failed; rerun 'gpt-creator normalize' manually."
      return
    fi
    if ! gc_discovery_is_stale "$discovery_file" "${required[@]}"; then
      ok "Discovery manifest refreshed."
    else
      warn "Discovery manifest still incomplete after refresh; check project docs."
      if gc_discovery_fill_defaults "$discovery_file" "${required[@]}"; then
        if ! gc_discovery_is_stale "$discovery_file" "${required[@]}"; then
          ok "Discovery manifest supplemented with placeholder entries."
        fi
      fi
    fi
  fi
}

gc_clear_active_task() {
  unset GC_ACTIVE_TASK_DB
  unset GC_ACTIVE_TASK_SLUG
  unset GC_ACTIVE_TASK_INDEX
  unset GC_ACTIVE_RUN_STAMP
  unset GC_ACTIVE_TASK_NUMBER
  unset GC_ACTIVE_TASK_ID
  unset GC_ACTIVE_TASK_REPORT
  unset GC_ACTIVE_TASK_ARCHIVE
  unset GC_ACTIVE_TASK_PROMPT
  unset GC_ACTIVE_TASK_OUTPUT
}

gc_finalize_active_task() {
  local signal="${1:-INT}"
  local reason="${2:-Interrupted}"
  local report_path="${GC_ACTIVE_TASK_REPORT:-}"
  [[ -n "$report_path" ]] || return 0

  local archive_path="${GC_ACTIVE_TASK_ARCHIVE:-}"
  local prompt_path="${GC_ACTIVE_TASK_PROMPT:-}"
  local output_path="${GC_ACTIVE_TASK_OUTPUT:-}"
  local task_number="${GC_ACTIVE_TASK_NUMBER:-unknown}"
  local task_id="${GC_ACTIVE_TASK_ID:-}"
  local slug="${GC_ACTIVE_TASK_SLUG:-unknown}"
  local tasks_db="${GC_ACTIVE_TASK_DB:-}"
  local task_index="${GC_ACTIVE_TASK_INDEX:-}"
  local run_stamp="${GC_ACTIVE_RUN_STAMP:-manual}"
  local timestamp
  timestamp="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"

  mkdir -p "$(dirname "$report_path")"
  local prompt_entry="$prompt_path"
  local output_entry="$output_path"
  local report_entry_display="$report_path"
  if [[ -n "$PROJECT_ROOT" ]]; then
    local project_prefix="${PROJECT_ROOT%/}/"
    if [[ "$prompt_entry" == "$project_prefix"* ]]; then
      prompt_entry="${prompt_entry#$project_prefix}"
    fi
    if [[ "$output_entry" == "$project_prefix"* ]]; then
      output_entry="${output_entry#$project_prefix}"
    fi
    if [[ "$report_entry_display" == "$project_prefix"* ]]; then
      report_entry_display="${report_entry_display#$project_prefix}"
    fi
  fi

  {
    printf 'task_number: %s\n' "$task_number"
    printf 'task_id: %s\n' "${task_id:-}"
    printf 'task_title: %s\n' "(interrupted)"
    printf 'story_slug: %s\n' "$slug"
    printf 'status: %s\n' "interrupted"
    printf 'timestamp: %s\n' "$timestamp"
    printf 'attempts: %s\n' "0"
    printf 'apply_status: %s\n' "interrupted"
    printf 'changes_applied: %s\n' "false"
    printf 'prompt_path: %s\n' "${prompt_entry:-"(none)"}"
    printf 'output_path: %s\n' "${output_entry:-"(none)"}"
    printf 'notes:\n'
    printf '  - %s by signal %s; rerun work-on-tasks to resume.\n' "$reason" "$signal"
  } >"$report_path"

  if [[ -n "$archive_path" ]]; then
    mkdir -p "$(dirname "$archive_path")"
    cp -f "$report_path" "$archive_path" 2>/dev/null || true
  fi

  if [[ -n "$tasks_db" && -n "$slug" && -n "$task_index" ]]; then
    local report_entry_db="$report_entry_display"
    local prompt_db="$prompt_entry"
    local output_db="$output_entry"
    local notes_payload="Interrupted by signal ${signal}; rerun work-on-tasks to resume."
    gc_record_task_progress "$tasks_db" "$slug" "$task_index" "$run_stamp" "on-hold" "$report_entry_db" "$prompt_db" "$output_db" "0" "0" "0" "0" "0" "interrupted" "false" "$notes_payload" "" "" "" "" "$timestamp" "0" "0" "0" "0" "" "" "" "0" "0" "0" "0" "" "" "" "" "" ""
    gc_update_task_state "$tasks_db" "$slug" "$task_index" "on-hold" "$run_stamp"
  fi

  gc_clear_active_task
}

gc_interrupt_handler() {
  local signal="${1:-INT}"
  trap - INT TERM TSTP QUIT
  warn "Received ${signal}; finalising active task and exiting."
  gc_finalize_active_task "$signal" "Interrupted"
  local code=128
  case "$signal" in
    INT) code=130 ;;
    TERM) code=143 ;;
    TSTP) code=148 ;;
    QUIT) code=131 ;;
  esac
  exit "$code"
}

gc_build_context_digest() {
  local context_file="${1:?context file required}"
  local digest_file="${2:?digest destination required}"
  local limit_lines="${3:-400}"
  [[ -f "$context_file" ]] || {
    printf '%s\n' "(warn) context digest skipped; missing source ${context_file}" >&2
    return 1
  }
  local helper_path
  helper_path="$(gc_clone_python_tool "build_context_digest.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$context_file" "$digest_file" "$limit_lines"
}

gc_refresh_context_tail() {
  local context_file="${1:?context file required}"
  local tail_file="${2:?tail file required}"
  local mode="${3:-digest}"
  local limit="${4:-0}"

  [[ -n "$tail_file" ]] || {
    printf '%s\n' "$mode"
    return 0
  }

  if (( limit <= 0 )); then
    : >"$tail_file"
    printf '%s\n' "$mode"
    return 0
  fi

  local effective_mode="$mode"
  case "$mode" in
    digest)
      if gc_build_context_digest "$context_file" "$tail_file" "$limit"; then
        printf '%s\n' "$effective_mode"
        return 0
      fi
      warn "Failed to build context digest (limit=${limit}); falling back to raw tail."
      effective_mode="raw"
      ;;
  esac

  case "$effective_mode" in
    raw)
      if ! tail -n "$limit" "$context_file" >"$tail_file" 2>/dev/null; then
        cp "$context_file" "$tail_file"
      fi
      ;;
    *)
      if ! tail -n "$limit" "$context_file" >"$tail_file" 2>/dev/null; then
        cp "$context_file" "$tail_file"
      fi
      ;;
  esac

  printf '%s\n' "$effective_mode"
}

gc_update_work_state() {
  local db_path="${1:?tasks database path required}"
  local story_slug="${2:?story slug required}"
  local status="${3:?status required}"
  local completed="${4:-0}"
  local total="${5:-0}"
  local run_stamp="${6:-manual}"
  local helper_path
  helper_path="$(gc_clone_python_tool "update_work_state.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$db_path" "$story_slug" "$status" "$completed" "$total" "$run_stamp"
}

gc_update_task_state() {
  local db_path="${1:?tasks database path required}"
  local story_slug="${2:?story slug required}"
  local position="${3:?task position required}"
  local status="${4:?status required}"
  local run_stamp="${5:-manual}"
  local helper_path
  helper_path="$(gc_clone_python_tool "update_task_state.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$db_path" "$story_slug" "$position" "$status" "$run_stamp"
}

gc_mark_task_abandoned_for_budget() {
  local db_path="${1:?tasks database path required}"
  local story_slug="${2:?story slug required}"
  local position="${3:?task position required}"
  local run_stamp="${4:-manual}"
  gc_update_task_state "$db_path" "$story_slug" "$position" "abandoned-for-budget" "$run_stamp"
}

gc_record_task_progress() {
  local db_path="${1:?tasks database path required}"
  local story_slug="${2:?story slug required}"
  local position="${3:?task position required}"
  local run_stamp="${4:-manual}"
  local status="${5:-}"
  local log_path="${6:-}"
  local prompt_path="${7:-}"
  local output_path="${8:-}"
  local attempts="${9:-0}"
  local tokens_total="${10:-0}"
  local tokens_estimate="${11:-0}"
  local llm_prompt_tokens="${12:-0}"
  local llm_completion_tokens="${13:-0}"
  local duration_seconds="${14:-0}"
  local apply_status="${15:-}"
  local changes_applied="${16:-0}"
  local notes_text="${17:-}"
  local written_text="${18:-}"
  local patched_text="${19:-}"
  local commands_text="${20:-}"
  local observation_hash="${21:-}"
  local occurred_at="${22:-}"
  local tokens_retrieve="${23:-0}"
  local tokens_plan="${24:-0}"
  local tokens_patch="${25:-0}"
  local tokens_verify="${26:-0}"
  local story_points="${27:-}"
  local helper_path
  helper_path="$(gc_clone_python_tool "record_task_progress.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" \
    "$db_path" \
    "$story_slug" \
    "$position" \
    "$run_stamp" \
    "$status" \
    "$log_path" \
    "$prompt_path" \
    "$output_path" \
    "$attempts" \
    "$tokens_total" \
    "$tokens_estimate" \
    "$llm_prompt_tokens" \
    "$llm_completion_tokens" \
    "$duration_seconds" \
    "$apply_status" \
    "$changes_applied" \
    "$notes_text" \
    "$written_text" \
    "$patched_text" \
    "$commands_text" \
    "$observation_hash" \
    "$occurred_at" \
    "$tokens_retrieve" \
    "$tokens_plan" \
    "$tokens_patch" \
    "$tokens_verify" \
    "$story_points"
}

gc_make_observation_hash() {
  local seed="${1:-}"
  if [[ -z "$seed" ]]; then
    return 1
  fi
  local helper_path
  helper_path="$(gc_clone_python_tool "gc_make_observation_hash.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$seed"
}

gc_update_throughput_metrics() {
  local db_path="${1:?tasks database path required}"
  local action="${2:?throughput action required}"
  local story_slug="${3:-}"
  local position="${4:-}"
  local helper_path
  helper_path="$(gc_clone_python_tool "update_throughput_metrics.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$db_path" "$action" "$story_slug" "$position"
}

gc_backlog_guard_snapshot() {
  local db_path="${1:?tasks database path required}"
  local output_path="${2:-}"
  local window_days="${3:-}"
  local wip_limit="${4:-}"
  local helper_path
  helper_path="$(gc_clone_python_tool "backlog_guard.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  local -a args=("snapshot" "--db" "$db_path" "--epic" "ADM-01")
  if [[ -n "$window_days" ]]; then
    args+=("--window-days" "$window_days")
  fi
  if [[ -n "$wip_limit" ]]; then
    args+=("--wip-limit" "$wip_limit")
  fi
  if [[ -n "$output_path" ]]; then
    mkdir -p "$(dirname "$output_path")"
    args+=("--output" "$output_path")
  fi
  python3 "$helper_path" "${args[@]}"
}

gc_backlog_guard_compare() {
  local before_path="${1:?before snapshot path required}"
  local after_path="${2:?after snapshot path required}"
  local wip_limit="${3:-}"
  local helper_path
  helper_path="$(gc_clone_python_tool "backlog_guard.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  local -a args=("compare" "--before" "$before_path" "--after" "$after_path" "--epic" "ADM-01")
  if [[ -n "$wip_limit" ]]; then
    args+=("--wip-limit" "$wip_limit")
  fi
  python3 "$helper_path" "${args[@]}"
}

gc_rewind_backlog_from_task() {
  local db_path="${1:?tasks database path required}"
  local task_ref_raw="${2:?task reference required}"
  local story_hint_raw="${3:-}"
  local helper_path
  helper_path="$(gc_clone_python_tool "rewind_backlog_from_task.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$db_path" "$task_ref_raw" "$story_hint_raw"
}

gc_align_task_story_slugs() {
  local db_path="${1:?tasks database path required}"
  local helper_path
  helper_path="$(gc_clone_python_tool "align_task_story_slugs.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$db_path"
}

gc_reset_task_progress() {
  local db_path="${1:?tasks database path required}"
  local helper_path
  helper_path="$(gc_clone_python_tool "reset_task_progress.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$db_path"
}

gc_sync_story_totals() {
  local db_path="${1:?tasks database path required}"
  local helper_path
  helper_path="$(gc_clone_python_tool "sync_story_totals.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$db_path"
}

gc_fetch_story_task_counts() {
  local db_path="${1:?tasks database path required}"
  local story_slug="${2:?story slug required}"
  local helper_path
  helper_path="$(gc_clone_python_tool "fetch_story_task_counts.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$db_path" "$story_slug"
}

gc_fetch_migration_epoch() {
  local db_path="${1:?tasks database path required}"
  local helper_path
  helper_path="$(gc_clone_python_tool "fetch_migration_epoch.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$db_path"
}

gc_trim_memory() {
  local phase="${1:-memory-cycle}"
  info "[memory] Reclaiming resources (${phase})"

  local helper_path
  helper_path="$(gc_clone_python_tool "trim_memory.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  if command -v python3 >/dev/null 2>&1; then
    python3 "$helper_path" >/dev/null 2>&1 || true
  fi
}

gc_count_pending_tasks() {
  local db_path="${1:?tasks database path required}"
  local helper_path
  helper_path="$(gc_clone_python_tool "count_pending_tasks.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$db_path"
}

gc_count_unstarted_tasks() {
  local db_path="${1:?tasks database path required}"
  local helper_path
  helper_path="$(gc_clone_python_tool "count_unstarted_tasks.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$db_path"
}

gc_tasks_db_has_rows() {
  local db_path="${1:?tasks database path required}"
  [[ -f "$db_path" ]] || return 1
  local helper_path
  helper_path="$(gc_clone_python_tool "tasks_db_has_rows.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$db_path"
}

gc_has_legacy_tasks_json() {
  local json_dir="${PLAN_DIR}/create-jira-tasks/json"
  [[ -f "${json_dir}/epics.json" ]] || return 1
  [[ -d "${json_dir}/stories" ]] || return 1
  [[ -d "${json_dir}/tasks" ]] || return 1
  return 0
}

gc_rebuild_tasks_db_from_json() {
  local force_flag="${1:-0}"
  local json_dir="${PLAN_DIR}/create-jira-tasks/json"
  local epics_json="${json_dir}/epics.json"
  local stories_dir="${json_dir}/stories"
  local tasks_dir="${json_dir}/tasks"
  local refined_dir="${json_dir}/refined"
  [[ -f "$epics_json" ]] || return 1
  [[ -d "$stories_dir" ]] || return 1
  [[ -d "$tasks_dir" ]] || return 1

  local payload="${json_dir}/tasks_payload.json"
  python3 "${CLI_ROOT}/src/lib/create-jira-tasks/to_payload.py" \
    "$epics_json" "$stories_dir" "$tasks_dir" "$refined_dir" "$payload" || return 1

  local tasks_workspace="${PLAN_DIR}/tasks"
  mkdir -p "$tasks_workspace"
  local db_path="${tasks_workspace}/tasks.db"
  python3 "${CLI_ROOT}/src/lib/create-jira-tasks/to_sqlite.py" \
    "$payload" "$db_path" "$force_flag" || return 1

  return 0
}

# docker compose helper (prefers "docker compose" then docker-compose)
docker_compose() {
  local project_name="${GC_DOCKER_PROJECT_NAME:-${COMPOSE_PROJECT_NAME:-}}"
  if [[ -z "$project_name" ]]; then
    local base
    base="$(basename "${PROJECT_ROOT:-$PWD}")"
    project_name="$(slugify_name "$base")"
  fi
  local compose_verbose_flag=""
  if [[ -n "${GC_DOCKER_VERBOSE:-}" ]]; then
    compose_verbose_flag="--verbose"
  fi
  if command -v docker >/dev/null 2>&1 && docker compose version >/dev/null 2>&1; then
    if [[ -n "$compose_verbose_flag" ]]; then
      COMPOSE_PROJECT_NAME="$project_name" docker compose "$compose_verbose_flag" "$@"
    else
      COMPOSE_PROJECT_NAME="$project_name" docker compose "$@"
    fi
  elif command -v docker-compose >/dev/null 2>&1; then
    if [[ -n "$compose_verbose_flag" ]]; then
      docker-compose -p "$project_name" "$compose_verbose_flag" "$@"
    else
      docker-compose -p "$project_name" "$@"
    fi
  else
    die "Docker CLI not available. Install Docker Desktop or docker-compose before running this command."
  fi
}

gc_compose_port() {
  local compose_file="$1" service="$2" container_port="${3:-}"
  [[ -f "$compose_file" ]] || return 1
  local output=""
  if command -v docker >/dev/null 2>&1 && docker compose version >/dev/null 2>&1; then
    output="$(COMPOSE_PROJECT_NAME="$GC_DOCKER_PROJECT_NAME" docker compose -f "$compose_file" port "$service" "$container_port" 2>/dev/null | head -n1)"
  fi
  if [[ -z "$output" ]] && command -v docker-compose >/dev/null 2>&1; then
    output="$(docker-compose -p "$GC_DOCKER_PROJECT_NAME" -f "$compose_file" port "$service" "$container_port" 2>/dev/null | head -n1)"
  fi
  [[ -n "$output" ]] || return 1
  output="${output##*:}"
  [[ "$output" =~ ^[0-9]+$ ]] || return 1
  printf '%s\n' "$output"
}

gc_container_name() {
  local service="$1"
  printf '%s-%s\n' "${GC_DOCKER_PROJECT_NAME}" "$service"
}

container_exists() {
  local name="$1"
  docker ps -a --format '{{.Names}}' | grep -Fxq "$name"
}

container_state() {
  local name="$1"
  docker inspect -f '{{.State.Status}}' "$name" 2>/dev/null || echo "absent"
}

gc_start_created_containers() {
  local compose_file="$1"
  shift || true
  local -a services=("$@")
  local service container state any_started=0

  for service in "${services[@]}"; do
    container="$(gc_container_name "$service")"
    if ! container_exists "$container"; then
      continue
    fi
    state="$(container_state "$container")"
    if [[ "$state" == "created" ]]; then
      info "Container ${container} stuck in 'created'; attempting manual start"
      if docker start "$container" >/dev/null 2>&1; then
        any_started=1
      else
        warn "Failed to start ${container}; attempting compose start fallback"
        docker_compose -f "$compose_file" start "$service" >/dev/null 2>&1 || true
      fi
    fi
  done

  if (( any_started == 1 )); then
    local wait_seconds=0
    local timeout="${GC_DOCKER_HEALTH_TIMEOUT:-10}"
    local poll_interval="${GC_DOCKER_HEALTH_INTERVAL:-1}"
    (( poll_interval <= 0 )) && poll_interval=1
    while (( wait_seconds < timeout )); do
      local all_ready=1
      for service in "${services[@]}"; do
        container="$(gc_container_name "$service")"
        if ! container_exists "$container"; then
          continue
        fi
        state="$(container_state "$container")"
        case "$state" in
          running|healthy) continue ;;
          exited|dead)
            warn "Container ${container} exited unexpectedly (state=${state}). Check logs."
            all_ready=0
            ;;
          *)
            all_ready=0
            ;;
        esac
      done
      if (( all_ready == 1 )); then
        break
      fi
      sleep "$poll_interval"
      (( wait_seconds += poll_interval )) || true
    done
  fi
}

port_in_use() {
  local port="$1"
  if command -v lsof >/dev/null 2>&1; then
    lsof -nP -iTCP:"$port" -sTCP:LISTEN >/dev/null 2>&1 && return 0
  elif command -v netstat >/dev/null 2>&1; then
    netstat -an 2>/dev/null | grep -E "\.${port} .*LISTEN" >/dev/null && return 0
  fi
  return 1
}

find_free_port() {
  local start="${1:-3306}"
  local port="$start"; local limit=$((start+100))
  while (( port <= limit )); do
    if ! port_in_use "$port" && ! gc_port_is_reserved "$port"; then
      echo "$port"
      return 0
    fi
    ((port++)) || true
  done
  echo "$start"  # fallback
}

gc_pick_port() {
  local label="$1"
  local default="$2"
  shift 2
  local service_key
  service_key="$(slugify_name "$label")"
  [[ -n "$service_key" ]] || service_key="$label"
  local existing_port=""
  existing_port="$(gc_port_for_service "$service_key" 2>/dev/null || true)"
  gc_unreserve_port "$service_key"
  local port=""
  local env_name value
  for env_name in "$@"; do
    value="${!env_name:-}"
    if [[ -n "$value" ]] && [[ "$value" =~ ^[0-9]+$ ]]; then
      port="$value"
      break
    fi
  done
  [[ -n "$port" ]] || port="$default"
  if [[ ! "$port" =~ ^[0-9]+$ ]] || (( port < 1 || port > 65535 )); then
    port="$default"
  fi
  local original="$port"
  local attempts=0
  local limit=200
  while (( attempts < limit )); do
    if (( port < 1 || port > 65535 )); then
      port="$default"
    fi
    if ! port_in_use "$port" && ! gc_port_reserved_by_other "$port" "$service_key"; then
      break
    fi
    ((port++))
    ((attempts++))
  done
  while gc_port_reserved_by_other "$port" "$service_key"; do
    ((port++))
  done
  if (( attempts >= limit )); then
    warn "Unable to find free port for ${label}; using ${port}" >&2
  elif [[ -n "$original" && "$port" != "$original" ]]; then
    info "Port ${original} in use; remapping ${label} to ${port}" >&2
  elif [[ -z "$existing_port" && "$port" != "$default" ]]; then
    info "Port ${default} in use; remapping ${label} to ${port}" >&2
  fi
  gc_reserve_port "$service_key" "$port"
  echo "$port"
}

wait_for_endpoint() {
  local url="$1" label="$2"
  local max_time="${3:-${GC_DOCKER_HEALTH_TIMEOUT:-10}}"
  local delay="${4:-${GC_DOCKER_HEALTH_INTERVAL:-1}}"
  (( delay <= 0 )) && delay=1
  (( max_time <= 0 )) && max_time=1
  local attempts=$(( (max_time + delay - 1) / delay ))
  (( attempts < 1 )) && attempts=1
  local i=1
  while (( i <= attempts )); do
    if curl -fsS --max-time 2 "$url" >/dev/null 2>&1; then
      ok "${label} ready → ${url}"
      return 0
    fi
    sleep "$delay"
    ((i++)) || true
  done
  warn "${label} not ready after ${max_time}s → ${url}"
  return 1
}

gc_sha256_file() {
  local path="$1"
  [[ -f "$path" ]] || return 1
  local helper_path
  helper_path="$(gc_clone_python_tool "sha256_file.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$path"
}

gc_host_prepare_pnpm() {
  if command -v pnpm >/dev/null 2>&1; then
    return 0
  fi
  if command -v corepack >/dev/null 2>&1; then
    corepack enable pnpm >/dev/null 2>&1 || true
    local version="${GC_PNPM_VERSION:-10.17.1}"
    corepack prepare "pnpm@${version}" --activate >/dev/null 2>&1 || \
      corepack use "pnpm@${version}" >/dev/null 2>&1 || true
  fi
  command -v pnpm >/dev/null 2>&1
}

gc_refresh_stack_prepare_node_modules() {
  [[ "${GC_SKIP_HOST_PNPM_INSTALL:-0}" == "1" ]] && return 0
  local root="${PROJECT_ROOT:-$PWD}"
  local lock_file="${root}/pnpm-lock.yaml"
  local has_manifest=0
  if [[ -f "${root}/pnpm-workspace.yaml" || -f "${root}/package.json" ]]; then
    has_manifest=1
  fi
  (( has_manifest )) || return 0

  local modules_dir="${root}/node_modules/.pnpm"
  local stamp_file="${root}/node_modules/.pnpm-lock.hash"
  local need_install=0
  local lock_hash=""

  if [[ -f "$lock_file" ]]; then
    lock_hash="$(gc_sha256_file "$lock_file" 2>/dev/null || true)"
  fi

  if [[ ! -d "$modules_dir" ]]; then
    need_install=1
  else
    if [[ -z "$lock_hash" ]]; then
      need_install=1
    else
      local stamp_hash=""
      [[ -f "$stamp_file" ]] && stamp_hash="$(cat "$stamp_file" 2>/dev/null || true)"
      if [[ "$lock_hash" != "$stamp_hash" ]]; then
        need_install=1
      fi
    fi
  fi

  if (( need_install )); then
    if [[ ! -f "$lock_file" ]]; then
      info "pnpm lockfile missing; generating via install"
    fi
    info "Installing workspace dependencies via pnpm (host)"
    if ! gc_host_prepare_pnpm; then
      warn "pnpm is not available on the host; skipping host install (containers may retry)."
      return 0
    fi
    local install_rc=0
    if (cd "$root" && CI=1 PNPM_IGNORE_NODE_VERSION=1 pnpm install --frozen-lockfile --unsafe-perm --prefer-offline --engine-strict=false --reporter=append-only); then
      install_rc=0
    else
      warn "pnpm install --frozen-lockfile failed; retrying without frozen lockfile"
      if (cd "$root" && CI=1 PNPM_IGNORE_NODE_VERSION=1 pnpm install --unsafe-perm --prefer-offline --engine-strict=false --no-frozen-lockfile --reporter=append-only); then
        install_rc=0
      else
        install_rc=1
      fi
    fi
    if (( install_rc == 0 )); then
      lock_hash="$(gc_sha256_file "$lock_file" 2>/dev/null || true)"
      if [[ -n "$lock_hash" ]]; then
        mkdir -p "${root}/node_modules"
        printf '%s' "$lock_hash" > "$stamp_file"
      else
        rm -f "$stamp_file"
      fi
      ok "Host dependencies installed"
    else
      warn "Host pnpm install failed; containers will attempt dependency installation."
    fi
  fi
}

render_template_file() {
  local src="$1" dest="$2"
  local db_name="${GC_DB_NAME:-${DB_NAME:-app}}"
  local db_user="${GC_DB_USER:-${DB_USER:-app}}"
  local db_pass="${GC_DB_PASSWORD:-${DB_PASSWORD:-app_pass}}"
  local db_host_port="${GC_DB_HOST_PORT:-${DB_HOST_PORT:-3306}}"
  local db_root_pass="${GC_DB_ROOT_PASSWORD:-${DB_ROOT_PASSWORD:-root}}"
  local project_slug="${GC_DOCKER_PROJECT_NAME:-${PROJECT_SLUG:-$(slugify_name "$(basename "${PROJECT_ROOT:-$PWD}")")}}"
  local api_host_port="${GC_API_HOST_PORT:-${API_HOST_PORT:-3000}}"
  local web_host_port="${GC_WEB_HOST_PORT:-${WEB_HOST_PORT:-5173}}"
  local admin_host_port="${GC_ADMIN_HOST_PORT:-${ADMIN_HOST_PORT:-5174}}"
  local proxy_host_port="${GC_PROXY_HOST_PORT:-${PROXY_HOST_PORT:-8080}}"
  local helper_path
  helper_path="$(gc_clone_python_tool "render_template_file.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$src" "$dest" "$db_name" "$db_user" "$db_pass" "$db_host_port" "$db_root_pass" "$project_slug" "$api_host_port" "$web_host_port" "$admin_host_port" "$proxy_host_port"
}

gc_render_sql() {
  local src="$1" dest="$2" database="$3" app_user="$4" app_pass="$5"
  local helper_path
  helper_path="$(gc_clone_python_tool "render_sql.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$src" "$dest" "$database" "$app_user" "$app_pass"
}

gc_temp_file() {
  local dir="$1" prefix="$2" suffix="$3"
  local helper_path
  helper_path="$(gc_clone_python_tool "temp_file.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$dir" "$prefix" "$suffix"
}

gc_execute_sql() {
  local compose_file="$1" sql_file="$2" database="$3"
  local root_user="$4" root_pass="$5" app_user="$6" app_pass="$7" fallback_init="$8" label="$9"
  local import_ok=0
  [[ -n "$label" ]] || label="operation"
  local container_host="127.0.0.1"

  if [[ -f "$compose_file" ]]; then
    if docker_compose -f "$compose_file" ps >/dev/null 2>&1; then
      info "Using docker-compose db service for ${label}"
      docker_compose -f "$compose_file" up -d db >/dev/null 2>&1 || true
      if [[ -n "$root_pass" ]]; then
        if docker_compose -f "$compose_file" exec -T db mysql -h"${container_host}" -u"${root_user}" -p"${root_pass}" "${database}" < "${sql_file}"; then
          import_ok=1
        else
          warn "Root ${label} failed; retrying as ${app_user}"
        fi
      else
        if docker_compose -f "$compose_file" exec -T db mysql -h"${container_host}" -u"${root_user}" "${database}" < "${sql_file}"; then
          import_ok=1
        fi
      fi
      if [[ "$import_ok" -ne 1 ]]; then
        if docker_compose -f "$compose_file" exec -T db mysql -h"${container_host}" -u"${app_user}" ${app_pass:+-p"${app_pass}"} "${database}" < "${sql_file}"; then
          import_ok=1
        fi
      fi
      if [[ "$import_ok" -ne 1 && -n "$fallback_init" && -f "$fallback_init" ]]; then
        local fallback_output="" fallback_user="" fallback_pass="" fallback_helper=""
        fallback_helper="$(gc_clone_python_tool "fallback_sql_credentials.py" "${PROJECT_ROOT:-$PWD}")" || fallback_helper=""
        if [[ -n "$fallback_helper" ]]; then
          fallback_output="$(python3 "$fallback_helper" "$fallback_init" 2>/dev/null || true)"
        fi
        if [[ -n "$fallback_output" ]]; then
          IFS=$'\n' read -r fallback_user fallback_pass _ <<<"$fallback_output"
          unset IFS
          if [[ -n "$fallback_user" && -n "$fallback_pass" ]]; then
            if docker_compose -f "$compose_file" exec -T db mysql -h"${container_host}" -u"${fallback_user}" ${fallback_pass:+-p"${fallback_pass}"} "${database}" < "${sql_file}"; then
              import_ok=1
            fi
          fi
        fi
      fi
      if [[ "$import_ok" -eq 1 ]]; then
        return 0
      fi
    fi
  fi

  local host="${DB_HOST:-127.0.0.1}"
  local port="${DB_HOST_PORT:-${GC_DB_HOST_PORT:-${DB_PORT:-3306}}}"
  if ${MYSQL_BIN} -h "$host" -P "$port" -u "$root_user" ${root_pass:+-p"${root_pass}"} "$database" < "$sql_file"; then
    return 0
  fi

  if ${MYSQL_BIN} -h "$host" -P "$port" -u "$app_user" ${app_pass:+-p"${app_pass}"} "$database" < "$sql_file"; then
    return 0
  fi

  return 1
}
gc_refresh_stack_collect_sql() {
  local root="$1"
  local helper_path
  helper_path="$(gc_clone_python_tool "refresh_stack_collect_sql.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$root"
}

gc_refresh_stack_exec_mysql() {
  local container_id="$1" sql_file="$2" user="$3" password="$4" database="$5"
  local port="${6:-3306}"

  [[ -n "$container_id" ]] || return 1
  [[ -f "$sql_file" ]] || return 1
  local -a cmd=(docker exec -i "$container_id" mysql --protocol=TCP -h 127.0.0.1 -P "$port" "-u${user}")
  if [[ -n "$password" ]]; then
    cmd+=("-p${password}")
  fi
  if [[ -n "$database" ]]; then
    cmd+=("$database")
  fi
  if ! "${cmd[@]}" <"$sql_file"; then
    return 1
  fi
  return 0
}

gc_refresh_stack_exec_inline_sql() {
  local container_id="$1" user="$2" password="$3" database="$4"
  local port="${5:-3306}"
  local sql_content
  sql_content="$(cat)"
  local -a cmd=(docker exec -i "$container_id" mysql --protocol=TCP -h 127.0.0.1 -P "$port" "-u${user}")
  if [[ -n "$password" ]]; then
    cmd+=("-p${password}")
  fi
  if [[ -n "$database" ]]; then
    cmd+=("$database")
  fi
  if ! printf "%s" "$sql_content" | "${cmd[@]}"; then
    return 1
  fi
  return 0
}

gc_refresh_stack_inspect_containers() {
  local compose_file="${1:?compose file required}"
  local -a container_ids=()
  mapfile -t container_ids < <(docker_compose -f "$compose_file" ps --all -q 2>/dev/null | awk 'NF')
  if (( ${#container_ids[@]} == 0 )); then
    mapfile -t container_ids < <(docker_compose -f "$compose_file" ps -q 2>/dev/null | awk 'NF')
  fi
  if (( ${#container_ids[@]} == 0 )); then
    printf '%s\n' "No containers found for project ${GC_DOCKER_PROJECT_NAME}."
    return 1
  fi

  local inspect_json=""
  if ! inspect_json="$(docker inspect "${container_ids[@]}" 2>/dev/null)"; then
    printf '%s\n' "Failed to inspect Docker containers for project ${GC_DOCKER_PROJECT_NAME}."
    return 1
  fi

  local helper_path
  helper_path="$(gc_clone_python_tool "refresh_stack_inspect_containers.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  INSPECT_JSON="${inspect_json}" python3 "$helper_path"

}


gc_refresh_stack_wait_for_containers() {
  local compose_file="${1:?compose file required}"
  local timeout="${2:-${GC_DOCKER_HEALTH_TIMEOUT:-10}}"
  local interval="${3:-${GC_DOCKER_HEALTH_INTERVAL:-1}}"
  (( interval <= 0 )) && interval=1
  local elapsed=0
  local output rc

  while (( elapsed <= timeout )); do
    output="$(gc_refresh_stack_inspect_containers "$compose_file")"
    rc=$?
    if (( rc == 0 )); then
      while IFS= read -r line; do
        [[ -z "$line" ]] && continue
        info "$line"
      done <<<"$output"
      return 0
    elif (( rc == 2 )); then
      while IFS= read -r line; do
        [[ -z "$line" ]] && continue
        info "$line"
      done <<<"$output"
      sleep "$interval"
      (( elapsed += interval ))
      continue
    else
      while IFS= read -r line; do
        [[ -z "$line" ]] && continue
        warn "$line"
      done <<<"$output"
      return 1
    fi
  done

  warn "Timed out after ${timeout}s waiting for containers to report healthy state."
  output="$(gc_refresh_stack_inspect_containers "$compose_file")"
  rc=$?
  local log_fn=warn
  if (( rc == 0 )); then
    log_fn=info
  fi
  while IFS= read -r line; do
    [[ -z "$line" ]] && continue
    "$log_fn" "$line"
  done <<<"$output"
  (( rc == 0 )) || return 1
  return 0
}

# ---------- Scan helpers ----------
has_pattern() { LC_ALL=C grep -E -i -m 1 -q -- "$1" "$2" 2>/dev/null; }
classify_file() {
  local path="$1"
  local name="${path##*/}"
  local lower
  lower="$(to_lower "$name")"
  local path_norm
  path_norm="$(to_lower "$path")"
  local ext="${lower##*.}"
  local type="" conf=0

  case "$ext" in
    md)
      if [[ "$lower" == *pdr* ]]; then type="pdr"; conf=0.95
      elif [[ "$lower" == *sds* ]]; then type="sds"; conf=0.92
      elif [[ "$lower" == *rfp* ]]; then type="rfp"; conf=0.9
      elif [[ "$lower" == *jira* ]]; then type="jira"; conf=0.88
      elif [[ "$lower" == *ui*pages* || "$lower" == *website*ui*pages* ]]; then type="ui_pages"; conf=0.85
      elif has_pattern '\bJIRA\b|Issue Key' "$path"; then type="jira"; conf=0.6
      fi
      ;;
    yml|yaml|json)
      if has_pattern '^[[:space:]]*openapi[[:space:]]*:[[:space:]]*3' "$path" || has_pattern '"openapi"[[:space:]]*:' "$path" || has_pattern '"swagger"[[:space:]]*:' "$path"; then
        type="openapi"; conf=0.94
      fi
      ;;
    sql)
      type="sql"; conf=0.65
      if has_pattern 'CREATE[[:space:]]+TABLE' "$path"; then conf=0.8; fi
      ;;
    mmd)
      type="mermaid"; conf=0.7
      ;;
    html)
      local is_html=0
      if [[ "$path_norm" == *"page_samples"* || "$path_norm" == *"page-samples"* ]]; then
        is_html=1
      elif echo "$lower" | grep -Eq '(abo|auth|prg|evt|ctn)[0-9]+\.html'; then
        is_html=1
      fi
      if [[ $is_html -eq 1 ]]; then
        type="page_sample_html"; conf=0.7
      fi
      ;;
    css)
      local is_css=0
      if [[ "$path_norm" == *"page_samples"* || "$path_norm" == *"samples"* ]]; then
        is_css=1
      elif [[ "$lower" == *style.css ]]; then
        is_css=1
      fi
      if [[ $is_css -eq 1 ]]; then
        type="page_sample_css"; conf=0.6
      fi
      ;;
  esac

  if [[ -n "$type" ]]; then
    printf '%s|%.2f\n' "$type" "$conf"
  fi
}

cmd_scan() {
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"
  root="${root:-$PROJECT_ROOT}"

  local scan_json="${STAGING_DIR}/scan.json"
  local catalog_dir="${STAGING_DIR}/inputs/catalog"
  mkdir -p "$catalog_dir"

  local -a default_scan_excludes=(
    ".gpt-creator/**"
    ".gpt-creator/staging/plan/work/runs/**"
    ".gpt-creator/**/prompts/**"
    ".gpt-creator/**/*.meta.json"
    ".git/**"
    "node_modules/**"
    "dist/**"
    "build/**"
    "**/__pycache__/**"
    "Library/**"
    "apps/**/dist-tests/**"
    "apps/**/{tests,test}/**"
    "apps/**/cypress/**"
    "apps/**/playwright/**"
    "apps/**/fixtures/**"
    "apps/**/public/**"
    "apps/api/prisma/migrations/**"
    "apps/api/prisma/fixtures/**"
    "apps/api/openapi/**"
    "ops/nginx/rendered/**"
    "ops/systemd/rendered/**"
    "ops/lighthouse/**"
    "qa/**"
    "docs/**/assets/**"
    "docs/**/evidence/**"
    "docs/**/screenshots/**"
    "**/*.lock"
    "apps/web/final_output.json"
  )

  local -a combined_excludes=("${default_scan_excludes[@]}")
  if declare -p GC_CONTEXT_EXCLUDES >/dev/null 2>&1; then
    combined_excludes+=("${GC_CONTEXT_EXCLUDES[@]}")
  elif [[ -n "${GC_CONTEXT_EXCLUDES:-}" ]]; then
    while IFS= read -r _scan_pattern; do
      [[ -n "$_scan_pattern" ]] && combined_excludes+=("$_scan_pattern")
    done <<<"$(printf '%s' "$GC_CONTEXT_EXCLUDES" | tr ':' '\n')"
  fi

  declare -A _scan_seen=()
  local excludes_payload=""
  local pattern
  for pattern in "${combined_excludes[@]}"; do
    [[ -z "$pattern" ]] && continue
    if [[ -n "${_scan_seen[$pattern]:-}" ]]; then
      continue
    fi
    _scan_seen[$pattern]=1
    excludes_payload+="$pattern"$'\n'
  done

  local scan_helper
  scan_helper="$(gc_clone_python_tool "scan_project.py" "${PROJECT_ROOT:-$PWD}")" || return 1

  local run_dir="${GC_RUN_DIR:-$root/.gpt-creator/staging/plan/work/runs/$(date +%Y%m%d%H%M%S)}"
  export GC_RUN_DIR="$run_dir"

  local scan_python="${PYTHON_BIN:-python3}"
  if command -v "$scan_python" >/dev/null 2>&1; then
    if GC_CONTEXT_EXCLUDES="$excludes_payload" "$scan_python" "$scan_helper" \
      --project "$root" --out "$catalog_dir" --scan-json "$scan_json"; then
      info "Catalog scan complete → ${catalog_dir}"
    else
      warn "scan_project.py failed; continuing with legacy scan pipeline outputs."
    fi
  else
    warn "Skipping catalog scan; $scan_python not available."
  fi

  local runtime_dir="$GC_DIR"
  local manifest_dir="${runtime_dir}/manifests"
  mkdir -p "$manifest_dir" "${PLAN_DIR}/tasks"
  local scan_stamp
  scan_stamp="$(date +%Y%m%d-%H%M%S)"
  local manifest="${manifest_dir}/discovery_${scan_stamp}.tsv"
  local manifest_tmp="${manifest}.tmp"
  local python_bin="${PYTHON_BIN:-python3}"
  local python_available=0
  if command -v "$python_bin" >/dev/null 2>&1; then
    python_available=1
  fi

  local -a scan_dirs=()
  if [[ -n "${GC_SCAN_ROOTS:-}" ]]; then
    local IFS=',:'
    read -ra scan_tokens <<<"${GC_SCAN_ROOTS}"
    for token in "${scan_tokens[@]}"; do
      token="${token//[[:space:]]/}"
      [[ -z "$token" ]] && continue
      if [[ -d "$PROJECT_ROOT/$token" ]]; then
        scan_dirs+=("$PROJECT_ROOT/$token")
      elif [[ -d "$token" ]]; then
        scan_dirs+=("$(abs_path "$token")")
      fi
    done
  fi
  if [[ ${#scan_dirs[@]} -eq 0 ]]; then
    local -a defaults=(apps docs db src packages qa tests ops)
    for candidate in "${defaults[@]}"; do
      if [[ -d "$PROJECT_ROOT/$candidate" ]]; then
        scan_dirs+=("$PROJECT_ROOT/$candidate")
      fi
    done
    if [[ ${#scan_dirs[@]} -eq 0 ]]; then
      scan_dirs=("$PROJECT_ROOT")
    fi
  fi

  local -a prune_dirs=(
    ".git"
    "node_modules"
    ".pnpm-store"
    "dist"
    "build"
    ".venv"
    ".gpt-creator"
    "tmp"
    "Library"
    "ansible"
    "docker.bak"
    "vendor"
    ".cache"
  )
  if [[ -n "${GC_SCAN_PRUNE_DIRS:-}" ]]; then
    local IFS=',:'
    read -ra prune_tokens <<<"${GC_SCAN_PRUNE_DIRS}"
    for token in "${prune_tokens[@]}"; do
      token="${token//[[:space:]]/}"
      [[ -n "$token" ]] && prune_dirs+=("$token")
    done
  fi

  info "Scanning project artifacts under: ${scan_dirs[*]}"
  printf "type\tconfidence\tpath\n" > "$manifest_tmp"

  local -a find_prune_expr=()
  if [[ ${#prune_dirs[@]} -gt 0 ]]; then
    find_prune_expr+=( "(" )
    for dir in "${prune_dirs[@]}"; do
      find_prune_expr+=( -name "$dir" -o )
    done
    unset 'find_prune_expr[${#find_prune_expr[@]}-1]'
    find_prune_expr+=( ")" -prune -o )
  fi

  local -a find_args=("${scan_dirs[@]}")
  find_args+=( "${find_prune_expr[@]}" -type f -print0 )

  while IFS= read -r -d '' f; do
    local hit
    hit="$(classify_file "$f")" || true
    if [[ -n "$hit" ]]; then
      local type conf
      IFS='|' read -r type conf <<<"$hit"
      printf "%s\t%.2f\t%s\n" "$type" "$conf" "$f" >> "$manifest_tmp"
    fi
  done < <(find "${find_args[@]}")

  mv "$manifest_tmp" "$manifest"
  if ! cp -f "$manifest" "${runtime_dir}/scan.tsv"; then
    warn "Unable to persist discovery manifest copy at ${runtime_dir}/scan.tsv."
  fi
  info "Discovery TSV → ${manifest}"

  if (( python_available )) && [[ ! -s "$scan_json" ]]; then
    local scan_manifest_helper
    scan_manifest_helper="$(gc_clone_python_tool "scan_manifest_to_json.py" "${PROJECT_ROOT:-$PWD}")" || return 1
    "$python_bin" "$scan_manifest_helper" "$manifest" "$PROJECT_ROOT" "$scan_json"
  else
    [[ -s "$scan_json" ]] || warn "Skipping scan.json export; ${python_bin} not available."
  fi

  if (( python_available )); then
    local doc_registry_tool="${CLI_ROOT}/src/lib/doc_registry.py"
    if [[ -f "$doc_registry_tool" ]]; then
      if "$python_bin" "$doc_registry_tool" sync-scan \
        --project-root "$PROJECT_ROOT" \
        --runtime-dir "$runtime_dir" \
        --scan-tsv "$manifest"; then
        info "Documentation registry synced."
      else
        warn "Documentation registry sync failed; inspect ${runtime_dir}/logs."
      fi
    else
      warn "Skipping documentation registry sync; tool not found at ${doc_registry_tool}."
    fi

    local doc_catalog_tool="${CLI_ROOT}/src/lib/doc_catalog.py"
    local doc_catalog_json="${STAGING_DIR}/doc-catalog.json"
    local doc_catalog_library="${STAGING_DIR}/doc-library.md"
    local doc_catalog_index="${STAGING_DIR}/doc-index.md"
    if [[ -f "$doc_catalog_tool" ]]; then
      if "$python_bin" "$doc_catalog_tool" \
        --project-root "$PROJECT_ROOT" \
        --staging-dir "$STAGING_DIR" \
        --out-json "$doc_catalog_json" \
        --out-library "$doc_catalog_library" \
        --out-index "$doc_catalog_index"; then
        info "Documentation catalog rebuilt."
      else
        warn "Documentation catalog build failed."
      fi
    else
      warn "Skipping documentation catalog build; tool not found at ${doc_catalog_tool}."
    fi

    local doc_pipeline_tool="${CLI_ROOT}/src/lib/doc_pipeline.py"
    if [[ -f "$doc_pipeline_tool" ]]; then
      if "$python_bin" "$doc_pipeline_tool" \
        --project-root "$PROJECT_ROOT" \
        --runtime-dir "$runtime_dir"; then
        info "Documentation summaries refreshed."
      else
        warn "Documentation summaries refresh failed."
      fi
    else
      warn "Skipping documentation summaries refresh; tool not found at ${doc_pipeline_tool}."
    fi

    local doc_indexer_pkg_root="${CLI_ROOT}/src"
    local doc_indexer_ready=0
    local doc_indexer_helper=""
    if doc_indexer_helper="$(gc_clone_python_tool "doc_indexer_ready.py" "${PROJECT_ROOT:-$PWD}")"; then
      if PYTHONPATH="${doc_indexer_pkg_root}${PYTHONPATH:+:$PYTHONPATH}" \
        "$python_bin" "$doc_indexer_helper" "$doc_indexer_pkg_root"; then
        doc_indexer_ready=1
      fi
    fi
    if (( doc_indexer_ready )); then
      if PYTHONPATH="${doc_indexer_pkg_root}${PYTHONPATH:+:$PYTHONPATH}" \
        "$python_bin" -m lib.doc_indexer --runtime-dir "$runtime_dir"; then
        info "Documentation indexes rebuilt."
      else
        warn "Documentation indexing failed."
      fi
    else
      info "Skipping documentation indexing; doc_indexer module not importable."
    fi
  else
    warn "Skipping documentation registry and catalog refresh; ${python_bin} not available."
  fi

  if [[ -f "$scan_json" ]]; then
    ok "Scan manifest → ${scan_json}"
  else
    warn "Scan manifest export missing (${scan_json}); rerun scan after installing ${python_bin}."
  fi
}

cmd_normalize() {
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"

  local scan_json="${STAGING_DIR}/scan.json"
  if [[ ! -f "$scan_json" ]]; then
    warn "No scan.json found, running scan first."
    cmd_scan --project "$PROJECT_ROOT"
  fi

  local python_bin="${PYTHON_BIN:-python3}"
  if ! command -v "$python_bin" >/dev/null 2>&1; then
    die "Python runtime '${python_bin}' not available; cannot normalize inputs."
  fi

  local normalize_helper
  normalize_helper="$(gc_clone_python_tool "normalize_inputs.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  "$python_bin" "$normalize_helper" "$scan_json" "$INPUT_DIR" "$PLAN_DIR"

  ok "Normalized inputs → ${INPUT_DIR}"
}

cmd_plan() {
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"

  local openapi=""
  for cand in "$INPUT_DIR/openapi.yaml" "$INPUT_DIR/openapi.yml" "$INPUT_DIR/openapi.json" "$INPUT_DIR/openapi.src"; do
    [[ -f "$cand" ]] && { openapi="$cand"; break; }
  done
  local sql_dir="$INPUT_DIR/sql"
  local python_bin="${PYTHON_BIN:-python3}"
  if ! command -v "$python_bin" >/dev/null 2>&1; then
    die "Python runtime '${python_bin}' not available; cannot build plan artifacts."
  fi
  local plan_helper
  plan_helper="$(gc_clone_python_tool "generate_plan_artifacts.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  "$python_bin" "$plan_helper" "$openapi" "$sql_dir" "$PLAN_DIR"

  ok "Plan artifacts created under ${PLAN_DIR}"
}

copy_template_tree() {
  local src="$1" dest="$2"
  [[ -d "$src" ]] || die "Template directory not found: $src"
  find "$src" -type d ! -name '.DS_Store' | while IFS= read -r dir; do
    local rel="${dir#$src}"
    mkdir -p "$dest/$rel"
  done
  find "$src" -type f | while IFS= read -r file; do
    local base
    base="$(basename "$file")"
    [[ "$base" == '.DS_Store' ]] && continue
    local rel="${file#$src/}"
    local target="$dest/$rel"
    if [[ "$target" == *.tmpl ]]; then
      target="${target%.tmpl}"
      mkdir -p "$(dirname "$target")"
      render_template_file "$file" "$target"
    else
      mkdir -p "$(dirname "$target")"
      cp "$file" "$target"
    fi
  done
}

cmd_generate() {
  local facet="${1:-}"; shift || true
  [[ -n "$facet" ]] || die "generate requires a facet: api|web|admin|db|docker|all"
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"
  local templates="$CLI_ROOT/templates"

  case "$facet" in
    api)
      local out="$PROJECT_ROOT/apps/api"
      mkdir -p "$out"
      copy_template_tree "$templates/api/nestjs" "$out"
      ok "API scaffolded → ${out}"
      ;;
    web)
      local out="$PROJECT_ROOT/apps/web"
      mkdir -p "$out"
      copy_template_tree "$templates/web/vue3" "$out"
      ok "Web scaffolded → ${out}"
      ;;
    admin)
      local out="$PROJECT_ROOT/apps/admin"
      mkdir -p "$out"
      copy_template_tree "$templates/admin/vue3" "$out"
      ok "Admin scaffolded → ${out}"
      ;;
    db)
      local out="$PROJECT_ROOT/db"
      mkdir -p "$out"
      copy_template_tree "$templates/db/mysql" "$out"
      ok "DB artifacts scaffolded → ${out}"
      ;;
    docker)
      local out="$PROJECT_ROOT/docker"
      mkdir -p "$out"
      local preferred="${GC_DB_HOST_PORT:-${DB_HOST_PORT:-${MYSQL_HOST_PORT:-3306}}}"
      gc_unreserve_port db
      if port_in_use "$preferred"; then
        local next; next="$(find_free_port "$preferred")"
        if [[ "$next" != "$preferred" ]]; then
          info "Port $preferred in use; remapping MySQL to $next"
          preferred="$next"
        fi
      fi
      GC_DB_HOST_PORT="$preferred"
      DB_HOST_PORT="$GC_DB_HOST_PORT"
      MYSQL_HOST_PORT="$GC_DB_HOST_PORT"
      gc_reserve_port db "$GC_DB_HOST_PORT"
      gc_set_env_var DB_HOST_PORT "$GC_DB_HOST_PORT"
      gc_set_env_var MYSQL_HOST_PORT "$GC_DB_HOST_PORT"
      gc_set_env_var GC_DB_HOST_PORT "$GC_DB_HOST_PORT"
      local api_host_port
      api_host_port="$(gc_pick_port "API" 3000 GC_API_HOST_PORT API_HOST_PORT)"
      GC_API_HOST_PORT="$api_host_port"
      API_HOST_PORT="$GC_API_HOST_PORT"
      gc_set_env_var API_HOST_PORT "$API_HOST_PORT"
      gc_set_env_var GC_API_HOST_PORT "$GC_API_HOST_PORT"
      gc_reserve_port api "$GC_API_HOST_PORT"
      local web_host_port
      web_host_port="$(gc_pick_port "Web" 5173 GC_WEB_HOST_PORT WEB_HOST_PORT)"
      GC_WEB_HOST_PORT="$web_host_port"
      WEB_HOST_PORT="$GC_WEB_HOST_PORT"
      gc_set_env_var WEB_HOST_PORT "$WEB_HOST_PORT"
      gc_set_env_var GC_WEB_HOST_PORT "$GC_WEB_HOST_PORT"
      gc_reserve_port web "$GC_WEB_HOST_PORT"
      local admin_host_port
      admin_host_port="$(gc_pick_port "Admin" 5174 GC_ADMIN_HOST_PORT ADMIN_HOST_PORT)"
      GC_ADMIN_HOST_PORT="$admin_host_port"
      ADMIN_HOST_PORT="$GC_ADMIN_HOST_PORT"
      gc_set_env_var ADMIN_HOST_PORT "$ADMIN_HOST_PORT"
      gc_set_env_var GC_ADMIN_HOST_PORT "$GC_ADMIN_HOST_PORT"
      gc_reserve_port admin "$GC_ADMIN_HOST_PORT"
      local proxy_host_port
      proxy_host_port="$(gc_pick_port "Proxy" 8080 GC_PROXY_HOST_PORT PROXY_HOST_PORT)"
      GC_PROXY_HOST_PORT="$proxy_host_port"
      PROXY_HOST_PORT="$GC_PROXY_HOST_PORT"
      gc_set_env_var PROXY_HOST_PORT "$PROXY_HOST_PORT"
      gc_set_env_var GC_PROXY_HOST_PORT "$GC_PROXY_HOST_PORT"
      gc_reserve_port proxy "$GC_PROXY_HOST_PORT"
      local local_url="mysql://${GC_DB_USER}:${GC_DB_PASSWORD}@127.0.0.1:${GC_DB_HOST_PORT}/${GC_DB_NAME}"
      gc_set_env_var DATABASE_URL "$local_url"
      local api_base_url="http://localhost:${GC_API_HOST_PORT}/api/v1"
      gc_set_env_var GC_API_BASE_URL "$api_base_url"
      gc_set_env_var VITE_API_BASE "$api_base_url"
      local api_health_url="${api_base_url%/}/health"
      gc_set_env_var GC_API_HEALTH_URL "$api_health_url"
      local proxy_base="http://localhost:${GC_PROXY_HOST_PORT}"
      gc_set_env_var GC_WEB_URL "${proxy_base}/"
      gc_set_env_var GC_ADMIN_URL "${proxy_base}/admin/"
      gc_load_env
      copy_template_tree "$templates/docker" "$out"
      if [[ -f "$out/pnpm-entry.sh" ]]; then
        chmod +x "$out/pnpm-entry.sh" || true
      fi
      ok "Docker assets scaffolded → ${out}"
      ;;
    all)
      for f in api db web admin docker; do
        cmd_generate "$f" --project "$PROJECT_ROOT"
      done
      return 0
      ;;
    *) die "Unknown facet: ${facet}";;
  esac
}

cmd_db() {
  local action="${1:-}"; shift || true
  [[ -n "$action" ]] || die "db requires: provision|import|seed"
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"
  local compose_file="$PROJECT_ROOT/docker/docker-compose.yml"

  case "$action" in
    provision)
      [[ -f "$compose_file" ]] || die "Compose file not found at ${compose_file}; run 'gpt-creator generate docker'."
      info "Starting database service via docker compose"
      docker_compose -f "$compose_file" up -d db
      ok "MySQL container provisioned"
      ;;
    import)
      local sql_file
      sql_file="$(find "$INPUT_DIR/sql" -maxdepth 2 -type f -name '*.sql' | head -n1 || true)"
      [[ -n "$sql_file" ]] || die "No staged SQL found under ${INPUT_DIR}/sql"
      info "Importing SQL from ${sql_file}"
      local database="${DB_NAME:-$GC_DB_NAME}"
      local root_user="${DB_ROOT_USER:-root}"
      local root_pass="${DB_ROOT_PASSWORD:-${GC_DB_ROOT_PASSWORD:-}}"
      local app_user="${DB_USER:-$GC_DB_USER}"
      local app_pass="${DB_PASSWORD:-$GC_DB_PASSWORD}"
      local cleanup_files=()
      trap 'for f in "${cleanup_files[@]}"; do [[ -n "$f" && -f "$f" ]] && rm -f "$f"; done; trap - RETURN' RETURN
      local rendered_sql
      rendered_sql="$(gc_temp_file "$STAGING_DIR" "import-" ".sql")"
      cleanup_files+=("$rendered_sql")
      gc_render_sql "$sql_file" "$rendered_sql" "$database" "$app_user" "$app_pass"
      local init_sql="${INPUT_DIR}/sql/db/init.sql"
      if gc_execute_sql "$compose_file" "$rendered_sql" "$database" "$root_user" "$root_pass" "$app_user" "$app_pass" "$init_sql" "import"; then
        ok "Database import finished"
      else
        die "Database import failed"
      fi
      ;;
    seed)
      local seed_file="${PROJECT_ROOT}/db/seed.sql"
      [[ -f "$seed_file" ]] || die "Seed file not found: ${seed_file}"
      info "Seeding database from ${seed_file}"
      local database="${DB_NAME:-$GC_DB_NAME}"
      local root_user="${DB_ROOT_USER:-root}"
      local root_pass="${DB_ROOT_PASSWORD:-${GC_DB_ROOT_PASSWORD:-}}"
      local app_user="${DB_USER:-$GC_DB_USER}"
      local app_pass="${DB_PASSWORD:-$GC_DB_PASSWORD}"
      local cleanup_files=()
      trap 'for f in "${cleanup_files[@]}"; do [[ -n "$f" && -f "$f" ]] && rm -f "$f"; done; trap - RETURN' RETURN
      local rendered_seed
      rendered_seed="$(gc_temp_file "$STAGING_DIR" "seed-" ".sql")"
      cleanup_files+=("$rendered_seed")
      gc_render_sql "$seed_file" "$rendered_seed" "$database" "$app_user" "$app_pass"
      local fallback_init="${PROJECT_ROOT}/db/init.sql"
      if [[ ! -f "$fallback_init" ]]; then
        fallback_init="${INPUT_DIR}/sql/db/init.sql"
      fi
      if gc_execute_sql "$compose_file" "$rendered_seed" "$database" "$root_user" "$root_pass" "$app_user" "$app_pass" "$fallback_init" "seed"; then
        ok "Database seed applied"
      else
        die "Database seed failed"
      fi
      ;;
    *) die "Unknown db action: ${action}";;
  esac
}

cmd_run() {
  local action="${1:-}"; shift || true
  [[ -n "$action" ]] || die "run requires: up|down|logs|open"
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"
  local compose_file="$PROJECT_ROOT/docker/docker-compose.yml"

  case "$action" in
    up)
      [[ -f "$compose_file" ]] || die "Compose file not found at ${compose_file}; generate docker assets first."
      gc_refresh_stack_prepare_node_modules
      docker_compose -f "$compose_file" up -d
      ok "Stack is starting (check docker compose ps)"
      local api_base="${GC_API_BASE_URL:-http://localhost:3000/api/v1}"
      local web_url="${GC_WEB_URL:-http://localhost:8080/}"
      local admin_url="${GC_ADMIN_URL:-http://localhost:8080/admin/}"
      local health_timeout="${GC_DOCKER_HEALTH_TIMEOUT:-10}"
      local health_interval="${GC_DOCKER_HEALTH_INTERVAL:-1}"
      wait_for_endpoint "${api_base%/}/health" "API /health" "$health_timeout" "$health_interval" || true
      local web_ping="${web_url%/}/__vite_ping"
      if ! wait_for_endpoint "$web_ping" "Web (vite ping)" "$health_timeout" "$health_interval"; then
        wait_for_endpoint "${web_url%/}/" "Web" "$health_timeout" "$health_interval" || true
      fi
      local admin_ping="${admin_url%/}/__vite_ping"
      if ! wait_for_endpoint "$admin_ping" "Admin (vite ping)" "$health_timeout" "$health_interval"; then
        wait_for_endpoint "${admin_url%/}/" "Admin" "$health_timeout" "$health_interval" || true
      fi
      ;;
    down)
      [[ -f "$compose_file" ]] || die "Compose file not found at ${compose_file}"
      docker_compose -f "$compose_file" down
      ok "Stack shut down"
      ;;
    logs)
      [[ -f "$compose_file" ]] || die "Compose file not found at ${compose_file}"
      docker_compose -f "$compose_file" logs -f
      ;;
    open)
      if command -v open >/dev/null 2>&1; then
        open "http://localhost:8080" || open "http://localhost:5173" || true
      else
        ${EDITOR_CMD} "$PROJECT_ROOT" || true
      fi
      ;;
    *) die "Unknown run action: ${action}";;
  esac
}

cmd_refresh_stack() {
  local root="" compose_override="" sql_override="" seed_override=""
  local skip_import=0 skip_seed=0
  local only_services="" skip_services=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --compose) compose_override="$(abs_path "$2")"; shift 2;;
      --sql) sql_override="$(abs_path "$2")"; shift 2;;
      --seed) seed_override="$(abs_path "$2")"; shift 2;;
      --no-import) skip_import=1; shift;;
      --no-seed) skip_seed=1; shift;;
      --only-services) only_services="${2:-}"; shift 2;;
      --skip-services) skip_services="${2:-}"; shift 2;;
      -h|--help)
        cat <<'EOHELP'
Usage: gpt-creator refresh-stack [options]

Tear down, rebuild, and prime the local Docker stack (schema + seeds).

Options:
  --project PATH   Project root (defaults to current directory)
  --compose FILE   Override docker-compose file
  --sql FILE       Explicit SQL dump to import (auto-discovered if omitted)
  --seed FILE      Seed SQL file to apply after import
  --only-services LIST  Comma/space separated subset of services to start (e.g. "web,api")
  --skip-services LIST  Comma/space separated services to skip when starting (e.g. "db,admin")
  --no-import      Skip schema import step
  --no-seed        Skip seeding step
  -h, --help       Show this help
EOHELP
        return 0
        ;;
      *) break;;
    esac
  done

  ensure_ctx "$root"

  local -a refresh_sql_init_files=() refresh_sql_schema_files=() refresh_sql_seed_files=() refresh_sql_all_files=()
  local refresh_sql_default_db_name="" refresh_sql_default_db_user="" refresh_sql_default_db_password="" refresh_sql_default_user_host=""
  eval "$(gc_refresh_stack_collect_sql "$PROJECT_ROOT")"

  if [[ -n "$sql_override" ]]; then
    refresh_sql_schema_files=("$sql_override")
  fi
  if [[ -n "$seed_override" ]]; then
    refresh_sql_seed_files=("$seed_override")
  fi

  local env_updated=0
  if [[ -n "$refresh_sql_default_db_name" && "$refresh_sql_default_db_name" != "$GC_DB_NAME" ]]; then
    gc_set_env_var DB_NAME "$refresh_sql_default_db_name"
    gc_set_env_var GC_DB_NAME "$refresh_sql_default_db_name"
    env_updated=1
  fi
  if [[ -n "$refresh_sql_default_db_user" && "$refresh_sql_default_db_user" != "$GC_DB_USER" ]]; then
    gc_set_env_var DB_USER "$refresh_sql_default_db_user"
    gc_set_env_var GC_DB_USER "$refresh_sql_default_db_user"
    env_updated=1
  fi
  if [[ -n "$refresh_sql_default_db_password" && "$refresh_sql_default_db_password" != "$GC_DB_PASSWORD" ]]; then
    gc_set_env_var DB_PASSWORD "$refresh_sql_default_db_password"
    gc_set_env_var GC_DB_PASSWORD "$refresh_sql_default_db_password"
    env_updated=1
  fi
  if (( env_updated )); then
    gc_load_env
    local host_port="${GC_DB_HOST_PORT:-${DB_HOST_PORT:-3306}}"
    local database_url="mysql://${GC_DB_USER}:${GC_DB_PASSWORD}@127.0.0.1:${host_port}/${GC_DB_NAME}"
    gc_set_env_var DATABASE_URL "$database_url"
  fi

  info "Using database '${GC_DB_NAME}' with user '${GC_DB_USER}'"

  local compose_file="$compose_override"
  if [[ -n "$compose_file" ]]; then
    compose_file="$(abs_path "$compose_file")"
  else
    info "Rendering docker assets from templates"
    if ! cmd_generate docker --project "$PROJECT_ROOT"; then
      die "Failed to generate docker assets"
    fi
    if [[ -f "${PROJECT_ROOT}/docker/compose.yaml" ]]; then
      compose_file="${PROJECT_ROOT}/docker/compose.yaml"
    elif [[ -f "${PROJECT_ROOT}/docker/docker-compose.yml" ]]; then
      compose_file="${PROJECT_ROOT}/docker/docker-compose.yml"
    elif [[ -f "${PROJECT_ROOT}/docker-compose.yml" ]]; then
      compose_file="${PROJECT_ROOT}/docker-compose.yml"
    else
      die "Compose file not found after generation. Expected docker/compose.yaml or docker-compose.yml"
    fi
  fi

  info "Refreshing Docker stack for ${GC_DOCKER_PROJECT_NAME}"

  info "Stopping existing containers (removing volumes)"
  docker_compose -f "$compose_file" down -v --remove-orphans || true

  local slug="$GC_DOCKER_PROJECT_NAME"
  local -a stale_containers=(
    "${slug}-db"
    "${slug}-api"
    "${slug}-web"
    "${slug}-admin"
    "${slug}-proxy"
    "${slug}_db"
    "${slug}_api"
    "${slug}_web"
    "${slug}_admin"
    "${slug}_proxy"
  )
  local container
  for container in "${stale_containers[@]}"; do
    if docker ps -a --format '{{.Names}}' | grep -Fxq "$container"; then
      info "Removing leftover container ${container}"
      docker rm -f "$container" >/dev/null 2>&1 || true
    fi
  done

  if (( ${#refresh_sql_all_files[@]} > 0 )); then
    info "Discovered SQL assets:"
    local listed
    for listed in "${refresh_sql_all_files[@]}"; do
      if [[ "$listed" == "$PROJECT_ROOT/"* ]]; then
        info "  - ${listed#$PROJECT_ROOT/}"
      else
        info "  - ${listed}"
      fi
    done
  else
    info "No SQL assets discovered automatically."
  fi

  local -a all_services=(db api web admin proxy)
  local -a services_to_start=()
  if [[ -n "$only_services" ]]; then
    local normalized_only="${only_services//,/ }"
    read -r -a services_to_start <<< "$normalized_only"
  else
    services_to_start=("${all_services[@]}")
  fi
  if [[ -n "$skip_services" ]]; then
    local -a skip_list=()
    local normalized_skip="${skip_services//,/ }"
    read -r -a skip_list <<< "$normalized_skip"
    if (( ${#skip_list[@]} > 0 )); then
      local -a filtered=()
      local svc skip_flag skip_item
      for svc in "${services_to_start[@]}"; do
        skip_flag=0
        for skip_item in "${skip_list[@]}"; do
          [[ -z "$skip_item" ]] && continue
          if [[ "$svc" == "$skip_item" ]]; then
            skip_flag=1
            break
          fi
        done
        if (( skip_flag == 0 )); then
          filtered+=("$svc")
        fi
      done
      services_to_start=("${filtered[@]}")
    fi
  fi
  if (( ${#services_to_start[@]} > 0 )); then
    # Deduplicate and drop empties
    local -a deduped=()
    local svc seen_services=""
    for svc in "${services_to_start[@]}"; do
      [[ -z "$svc" ]] && continue
      case " $seen_services " in
        *" $svc "*) continue ;;
      esac
      deduped+=("$svc")
      seen_services+=" $svc"
    done
    services_to_start=("${deduped[@]}")
  fi

  if (( ${#services_to_start[@]} == 0 )); then
    warn "No services selected to start; skipping docker compose up."
  else
    info "Building and starting containers (${services_to_start[*]})"
    GC_DOCKER_VERBOSE="${GC_DOCKER_VERBOSE:-1}"
    gc_refresh_stack_prepare_node_modules
    docker_compose -f "$compose_file" up -d --build "${services_to_start[@]}"
    gc_start_created_containers "$compose_file" "${services_to_start[@]}"
  fi

  local db_requested=0
  local svc
  for svc in "${services_to_start[@]}"; do
    if [[ "$svc" == "db" ]]; then
      db_requested=1
      break
    fi
  done

  local db_container=""
  if (( db_requested )); then
    db_container="$(docker_compose -f "$compose_file" ps -q db || true)"
    if [[ -n "$db_container" ]]; then
      info "Waiting for MySQL to be ready…"
      local waited=0
      local mysql_timeout="${GC_DOCKER_HEALTH_TIMEOUT:-10}"
      local sleep_interval="${GC_DOCKER_HEALTH_INTERVAL:-1}"
      (( sleep_interval <= 0 )) && sleep_interval=1
      while (( waited < mysql_timeout )); do
        if docker exec -i "$db_container" sh -lc 'mysqladmin ping -h 127.0.0.1 --silent' >/dev/null 2>&1; then
          info "MySQL is ready."
          break
        fi
        sleep "$sleep_interval"
        ((waited += sleep_interval)) || true
      done
      if (( waited >= mysql_timeout )); then
        warn "MySQL readiness timeout after ${mysql_timeout}s (continuing)."
      fi
    else
      warn "Database container did not start; SQL import will be skipped."
    fi
  else
    info "Database service excluded from start; skipping readiness wait."
  fi

  docker_compose -f "$compose_file" ps

  local db_port="3306"
  local root_user="${DB_ROOT_USER:-root}"
  local root_pass="${DB_ROOT_PASSWORD:-${GC_DB_ROOT_PASSWORD:-}}"
  local app_user="${DB_USER:-$GC_DB_USER}"
  local app_pass="${DB_PASSWORD:-$GC_DB_PASSWORD}"
  local db_name="${DB_NAME:-$GC_DB_NAME}"
  local app_host="${refresh_sql_default_user_host:-%}"
  local python_bin="${PYTHON_BIN:-python3}"

  local import_rc=0 seed_rc=0
  local schema_attempted=0 seed_attempted=0

  if [[ -z "$db_container" ]]; then
    if (( skip_import == 0 )); then
      import_rc=1
    fi
    if (( skip_seed == 0 )); then
      seed_rc=1
    fi
  else
    if (( skip_import == 0 || skip_seed == 0 )); then
      local ensure_sql=""
      if command -v "$python_bin" >/dev/null 2>&1; then
        local ensure_helper
        ensure_helper="$(gc_clone_python_tool "gc_refresh_stack_ensure_sql.py" "${PROJECT_ROOT:-$PWD}")" || return 1
        ensure_sql="$("$python_bin" "$ensure_helper" "$db_name" "$app_user" "$app_pass" "$app_host")"
      else
        warn "Skipping database/user ensure; ${python_bin} not available."
      fi
      if [[ -n "$ensure_sql" ]]; then
        if ! gc_refresh_stack_exec_inline_sql "$db_container" "$root_user" "$root_pass" "" "$db_port" <<<"$ensure_sql"; then
          warn "Failed to ensure database or user; continuing with imports."
        else
          info "Ensured database ${db_name} and user ${app_user}"
        fi
      fi
    fi

    if (( skip_import == 0 )) && (( ${#refresh_sql_init_files[@]} + ${#refresh_sql_schema_files[@]} == 0 )); then
      info "No schema SQL files found; skipping import."
      skip_import=1
    fi
    if (( skip_seed == 0 )) && (( ${#refresh_sql_seed_files[@]} == 0 )); then
      info "No seed SQL files found; skipping seeding."
      skip_seed=1
    fi

    if (( skip_import == 0 )); then
      local file display
      for file in "${refresh_sql_init_files[@]}"; do
        [[ -f "$file" ]] || { warn "Init SQL not found: $file"; import_rc=1; continue; }
        display="$file"
        [[ "$display" == "$PROJECT_ROOT/"* ]] && display="${display#$PROJECT_ROOT/}"
        info "Applying init SQL: ${display}"
        ((schema_attempted++))
        if ! gc_refresh_stack_exec_mysql "$db_container" "$file" "$root_user" "$root_pass" "" "$db_port"; then
          warn "Init SQL failed as ${root_user}; retrying as ${app_user}"
          if ! gc_refresh_stack_exec_mysql "$db_container" "$file" "$app_user" "$app_pass" "" "$db_port"; then
            warn "Init SQL failed: ${display}"
            import_rc=1
            continue
          fi
        fi
      done

      for file in "${refresh_sql_schema_files[@]}"; do
        [[ -f "$file" ]] || { warn "Schema SQL not found: $file"; import_rc=1; continue; }
        display="$file"
        [[ "$display" == "$PROJECT_ROOT/"* ]] && display="${display#$PROJECT_ROOT/}"
        info "Importing schema SQL: ${display}"
        ((schema_attempted++))
        if ! gc_refresh_stack_exec_mysql "$db_container" "$file" "$root_user" "$root_pass" "$db_name" "$db_port"; then
          warn "Schema import failed as ${root_user}; retrying as ${app_user}"
          if ! gc_refresh_stack_exec_mysql "$db_container" "$file" "$app_user" "$app_pass" "$db_name" "$db_port"; then
            warn "Schema SQL failed: ${display}"
            import_rc=1
            continue
          fi
        fi
      done
    else
      info "Skipping schema import (--no-import)"
    fi

    if (( skip_seed == 0 )); then
      local seed_file seed_display
      for seed_file in "${refresh_sql_seed_files[@]}"; do
        [[ -f "$seed_file" ]] || { warn "Seed SQL not found: $seed_file"; seed_rc=1; continue; }
        seed_display="$seed_file"
        [[ "$seed_display" == "$PROJECT_ROOT/"* ]] && seed_display="${seed_display#$PROJECT_ROOT/}"
        info "Applying seed SQL: ${seed_display}"
        ((seed_attempted++))
        if ! gc_refresh_stack_exec_mysql "$db_container" "$seed_file" "$root_user" "$root_pass" "$db_name" "$db_port"; then
          warn "Seed import failed as ${root_user}; retrying as ${app_user}"
          if ! gc_refresh_stack_exec_mysql "$db_container" "$seed_file" "$app_user" "$app_pass" "$db_name" "$db_port"; then
            warn "Seed SQL failed: ${seed_display}"
            seed_rc=1
            continue
          fi
        fi
      done
    else
      info "Skipping seeding (--no-seed)"
    fi
  fi

  info "Verifying Docker service health"
  local stack_health_rc=0
  if gc_refresh_stack_wait_for_containers "$compose_file" "${GC_DOCKER_HEALTH_TIMEOUT:-10}" "${GC_DOCKER_HEALTH_INTERVAL:-1}"; then
    ok "Docker services healthy"
  else
    stack_health_rc=1
    warn "Docker services reported issues; inspect compose logs for details."
  fi

  local status=0
  if (( import_rc != 0 )); then
    status=1
  elif (( skip_import == 0 && schema_attempted > 0 )); then
    ok "Database schema imported"
  fi
  if (( seed_rc != 0 )); then
    status=1
  elif (( skip_seed == 0 && seed_attempted > 0 )); then
    ok "Database seeds applied"
  fi
  if (( stack_health_rc != 0 )); then
    status=1
  fi

  if (( status == 0 )); then
    ok "Stack refreshed successfully"
  else
    warn "Stack refresh completed with issues; inspect logs above."
  fi
  return $status
}


cmd_verify() {
  local kind="${1:-all}"; shift || true
  local root=""
  local api_base="${GC_API_BASE_URL:-http://localhost:3000/api/v1}"
  local api_health="${GC_API_HEALTH_URL:-}"
  local web_url="${GC_WEB_URL:-http://localhost:8080/}"
  local admin_url="${GC_ADMIN_URL:-http://localhost:8080/admin/}"
  local api_base_override=0 api_health_override=0 web_url_override=0 admin_url_override=0

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --api-url) api_base="$2"; api_base_override=1; shift 2;;
      --api-health) api_health="$2"; api_health_override=1; shift 2;;
      --web-url) web_url="$2"; web_url_override=1; shift 2;;
      --admin-url) admin_url="$2"; admin_url_override=1; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"

  kind="$(printf '%s' "$kind" | tr '[:upper:]' '[:lower:]')"

  local compose_file="${PROJECT_ROOT}/docker/docker-compose.yml"
  local ports_updated=0
  if [[ -f "$compose_file" ]]; then
    local detected
    if detected="$(gc_compose_port "$compose_file" api 3000)"; then
      if [[ -n "$detected" && "$detected" != "$GC_API_HOST_PORT" ]]; then
        GC_API_HOST_PORT="$detected"; API_HOST_PORT="$detected"; ports_updated=1
      fi
    fi
    if detected="$(gc_compose_port "$compose_file" web 5173)"; then
      if [[ -n "$detected" && "$detected" != "$GC_WEB_HOST_PORT" ]]; then
        GC_WEB_HOST_PORT="$detected"; WEB_HOST_PORT="$detected"; ports_updated=1
      fi
    fi
    if detected="$(gc_compose_port "$compose_file" admin 5173)"; then
      if [[ -n "$detected" && "$detected" != "$GC_ADMIN_HOST_PORT" ]]; then
        GC_ADMIN_HOST_PORT="$detected"; ADMIN_HOST_PORT="$detected"; ports_updated=1
      fi
    fi
    if detected="$(gc_compose_port "$compose_file" proxy 80)"; then
      if [[ -n "$detected" && "$detected" != "$GC_PROXY_HOST_PORT" ]]; then
        GC_PROXY_HOST_PORT="$detected"; PROXY_HOST_PORT="$detected"; ports_updated=1
      fi
    fi
  fi
  (( ports_updated )) && gc_env_sync_ports

  if (( api_base_override == 0 )); then
    api_base="${GC_API_BASE_URL:-$api_base}"
  fi
  if (( web_url_override == 0 )); then
    web_url="${GC_WEB_URL:-$web_url}"
  fi
  if (( admin_url_override == 0 )); then
    admin_url="${GC_ADMIN_URL:-$admin_url}"
  fi
  if (( api_health_override == 0 )); then
    api_health="${GC_API_HEALTH_URL:-$api_health}"
  fi

  local trimmed_base="${api_base%/}"
  api_health="${api_health:-${trimmed_base}/health}"

  local verify_root="$CLI_ROOT/verify"
  [[ -d "$verify_root" ]] || die "verify scripts directory missing at ${verify_root}"

  case "$kind" in
    program_filters) kind="program-filters" ;;
    program-filters|acceptance|openapi|a11y|lighthouse|consent|telemetry|nfr|all) ;;
    *) die "Unknown verify target: ${kind}";;
  esac

  local -a check_names
  case "$kind" in
    acceptance) check_names=(acceptance) ;;
    openapi|a11y|lighthouse|consent|program-filters|telemetry)
      check_names=("$kind")
      ;;
    nfr)
      check_names=(openapi a11y lighthouse consent program-filters telemetry)
      ;;
    all)
      check_names=(acceptance openapi a11y lighthouse consent program-filters telemetry)
      ;;
  esac

  local summary_dir="${PROJECT_ROOT}/.gpt-creator/staging/verify"
  local logs_dir="${summary_dir}/logs"
  mkdir -p "$summary_dir" "$logs_dir"

  local summary_path="${summary_dir}/summary.json"
  local python_available=0
  local python_bin=""
  if command -v python3 >/dev/null 2>&1; then
    python_available=1
    python_bin="$(command -v python3)"
  fi
  local check_order="acceptance,openapi,lighthouse,a11y,consent,program-filters,telemetry"

  local pass=0 fail=0 skip=0

  # verification summary handled inline

  run_check() {
    local name="$1"; shift
    local label="$1"; shift
    local -a cmd=("$@")
    local timestamp
    timestamp="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"
    local stamp
    stamp="$(date -u +"%Y%m%d-%H%M%S")"
    local log_file="${logs_dir}/${stamp}-${name}.log"
    SECONDS=0
    set +e
    "${cmd[@]}" 2>&1 | tee "$log_file"
    local exit_status=${PIPESTATUS[0]}
    set -e
    local duration="$SECONDS"
    local status message
    case "$exit_status" in
      0)
        status="pass"
        message="${label} checks passed."
        ((pass++))
        ;;
      3)
        status="skip"
        message="${label} check skipped (missing dependency)."
        ((skip++))
        warn "${label} check skipped (missing dependency)"
        ;;
      *)
        status="fail"
        message="${label} check failed (exit ${exit_status})."
        ((fail++))
        warn "${label} check failed (exit ${exit_status})"
        ;;
    esac
    cp -f "$log_file" "${logs_dir}/${name}-latest.log" 2>/dev/null || true
    local log_rel="${log_file#$PROJECT_ROOT/}"
    log_rel="${log_rel#./}"
    if [[ "$python_available" -eq 1 ]]; then
      local event=""
      local py_cmd=$'import json, os, sys, datetime
path = sys.argv[1]
root = os.environ.get("PROJECT_ROOT", "")
name = os.environ.get("CHECK_NAME", "")
if not name:
    sys.exit(0)
label = os.environ.get("CHECK_LABEL", name.title())
status = os.environ.get("CHECK_STATUS", "unknown")
message = os.environ.get("CHECK_MESSAGE", "")
log_path = os.environ.get("CHECK_LOG", "")
report_path = os.environ.get("CHECK_REPORT", "")
score_raw = os.environ.get("CHECK_SCORE", "")
duration_raw = os.environ.get("CHECK_DURATION", "")
run_kind = os.environ.get("CHECK_RUN_KIND", "")
timestamp = os.environ.get("CHECK_TIMESTAMP", datetime.datetime.utcnow().replace(microsecond=0).isoformat() + "Z")
order_raw = os.environ.get("CHECK_ORDER", "")

def relify(path_value):
    if not path_value:
        return ""
    if not os.path.isabs(path_value) and root:
        abs_path = os.path.normpath(os.path.join(root, path_value))
    else:
        abs_path = os.path.normpath(path_value)
    if root:
        try:
            rel = os.path.relpath(abs_path, root)
        except Exception:
            rel = abs_path
    else:
        rel = abs_path
    return rel.replace(os.sep, "/")

score = None
if score_raw:
    try:
        score = float(score_raw)
    except Exception:
        score = None

duration_value = None
if duration_raw:
    try:
        duration_value = float(duration_raw)
    except Exception:
        duration_value = None

log_path = relify(log_path)
report_path = relify(report_path)

summary = {
    "name": name,
    "label": label,
    "status": status,
    "message": message,
    "log": log_path,
    "report": report_path,
    "score": score,
    "duration": duration_value,
    "run_kind": run_kind,
    "timestamp": timestamp,
}

order = [item.strip() for item in order_raw.split(",") if item.strip()]

try:
    with open(path, "r", encoding="utf-8") as fh:
        data = json.load(fh)
except Exception:
    data = {}

if order:
    data["order"] = order
data.setdefault("runs", [])
data["runs"].append(summary)

with open(path, "w", encoding="utf-8") as fh:
    json.dump(data, fh, ensure_ascii=False, indent=2)

print(json.dumps(summary, ensure_ascii=False))'
      event="$(
        CHECK_NAME="$name" \
        CHECK_STATUS="$status" \
        CHECK_LABEL="$label" \
        CHECK_MESSAGE="$message" \
        CHECK_LOG="$log_rel" \
        CHECK_REPORT="" \
        CHECK_SCORE="" \
        CHECK_DURATION="$duration" \
        CHECK_RUN_KIND="$kind" \
        CHECK_TIMESTAMP="$timestamp" \
        CHECK_ORDER="$check_order" \
        PROJECT_ROOT="$PROJECT_ROOT" \
        "$python_bin" -c "$py_cmd" "$summary_path" 2>/dev/null
      )"
      if [[ -n "$event" ]]; then
        printf '::verify::%s\n' "$event"
      fi
    fi
    return 0
  }

  find_openapi_candidate() {
    local spec=""
    for cand in "$INPUT_DIR/openapi.yaml" "$INPUT_DIR/openapi.yml" "$INPUT_DIR/openapi.json"; do
      if [[ -f "$cand" ]]; then
        spec="$cand"
        break
      fi
    done
    printf '%s' "$spec"
  }

  for name in "${check_names[@]}"; do
    case "$name" in
      acceptance)
        run_check "acceptance" "Acceptance" \
          env PROJECT_ROOT="$PROJECT_ROOT" GC_COMPOSE_FILE="$compose_file" \
          bash "$verify_root/acceptance.sh" "${api_base}" "${web_url}" "${admin_url}" "${api_health}"
        ;;
      openapi)
        run_check "openapi" "OpenAPI" \
          bash "$verify_root/check-openapi.sh" "$(find_openapi_candidate)"
        ;;
      a11y)
        run_check "a11y" "Accessibility" \
          bash "$verify_root/check-a11y.sh" "${web_url}" "${admin_url}"
        ;;
      lighthouse)
        run_check "lighthouse" "Lighthouse" \
          bash "$verify_root/check-lighthouse.sh" "${web_url}" "${admin_url}"
        ;;
      consent)
        run_check "consent" "Consent" \
          bash "$verify_root/check-consent.sh" "${web_url}"
        ;;
      program-filters)
        run_check "program-filters" "Program Filters" \
          bash "$verify_root/check-program-filters.sh" "${api_base}"
        ;;
      telemetry)
        run_check "telemetry" "Telemetry" \
          bash "$verify_root/check-telemetry.sh"
        ;;
    esac
  done

  if (( fail > 0 )); then
    die "Verify failed — pass=${pass} fail=${fail} skip=${skip}"
  fi
  ok "Verify complete — pass=${pass} skip=${skip}"
}

gc_exec_with_timeout() {
  local timeout="${1:-0}"
  local stdin_file="${2:-}"
  local log_file="${3:-}"
  shift 3 || true
  local -a cmd=("$@")

  local python_bin="${PYTHON_BIN:-python3}"
  if (( ${#cmd[@]} == 0 )); then
    return 1
  fi

  if ! command -v "$python_bin" >/dev/null 2>&1; then
    warn "Python runtime '$python_bin' not available for gc_exec_with_timeout"
    return 1
  fi

  local helper_path
  helper_path="$(gc_clone_python_tool "gc_exec_with_timeout.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  "$python_bin" "$helper_path" "$timeout" "$stdin_file" "$log_file" "${cmd[@]}"
  return "$?"
}

codex_call() {
  local task="${1:?task}"; shift || true
  local prompt_dir="${GC_DIR}/prompts"
  mkdir -p "$prompt_dir"
  GC_CODEX_CALL_TOKEN_ACCUM=0

  local prompt_file=""
  local output_file=""
  local call_step="patch"
  local call_max_output=0

  if [[ $# -gt 0 && -f "$1" ]]; then
    prompt_file="$1"
    shift || true
  fi

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --prompt) prompt_file="$2"; shift 2;;
      --output) output_file="$2"; shift 2;;
      --step)
        call_step="${2,,}"
        shift 2
        ;;
      *) break;;
    esac
  done

  if [[ -z "$prompt_file" ]]; then
    prompt_file="${prompt_dir}/${task}.md"
    if [[ ! -f "$prompt_file" ]]; then
      cat >"$prompt_file" <<'PROMPT'
# Instruction
You are Codex (gpt-5-codex) assisting the gpt-creator pipeline. Apply requested changes deterministically.
PROMPT
    fi
  fi

  if [[ -n "$prompt_file" && -f "$prompt_file" ]]; then
    gc_trim_prompt_file "$prompt_file"
  fi

  call_step="${call_step,,}"
  [[ -n "$call_step" ]] || call_step="patch"
  call_max_output="$(gc_output_limit_for_step "$call_step")"
  if ! [[ "$call_max_output" =~ ^[0-9]+$ ]]; then
    call_max_output=0
  fi
  # shellcheck disable=SC2034  # exposed for observability in external tooling
  GC_CURRENT_CODEX_STEP="$call_step"
  # shellcheck disable=SC2034  # exposed for observability in external tooling
  GC_CURRENT_CODEX_MAX_OUT="$call_max_output"

  local codex_model_for_step=""
  local codex_reasoning_for_step=""
  case "$call_step" in
    patch|apply|code|implement)
      codex_model_for_step="${CODEX_MODEL_CODE:-${CODEX_MODEL}}"
      if [[ -n "${CODEX_REASONING_EFFORT_CODE:-}" ]]; then
        codex_reasoning_for_step="${CODEX_REASONING_EFFORT_CODE}"
      fi
      ;;
    *)
      codex_model_for_step="${CODEX_MODEL_NON_CODE:-${CODEX_MODEL}}"
      if [[ -n "${CODEX_REASONING_EFFORT_NON_CODE:-}" ]]; then
        codex_reasoning_for_step="${CODEX_REASONING_EFFORT_NON_CODE}"
      fi
      ;;
  esac
  [[ -n "$codex_model_for_step" ]] || codex_model_for_step="$CODEX_MODEL"

  if [[ "${GC_CODEX_USAGE_LIMIT_REACHED:-0}" == "1" && "${GC_CODEX_USAGE_LIMIT_CONFIRMED:-0}" == "1" ]]; then
    warn "Codex usage limit previously reached; skipping ${task}."
    return 95
  fi

  if command -v "$CODEX_BIN" >/dev/null 2>&1; then
    local fallback_model="${CODEX_FALLBACK_MODEL:-$codex_model_for_step}"
    info "Codex ${task} (step=${call_step}) → model=${codex_model_for_step} (reasoning=${codex_reasoning_for_step:-default})"
    # shellcheck disable=SC2034  # local function assigned below for lexical scoping
    local run_codex_model
    run_codex_model() {
      local model="$1"
      shift || true
      local step_reasoning="${1:-}"
      shift || true
      local args=(exec --model "$model")
      if [[ -n "${CODEX_PROFILE:-}" ]]; then
        args+=(--profile "$CODEX_PROFILE")
      fi
      if [[ -n "${PROJECT_ROOT:-}" ]]; then
        args+=(--cd "$PROJECT_ROOT")
      fi
      if [[ -n "$step_reasoning" ]]; then
        args+=(-c "model_reasoning_effort=\"${step_reasoning}\"")
      elif [[ -n "${CODEX_REASONING_EFFORT:-}" ]]; then
        args+=(-c "model_reasoning_effort=\"${CODEX_REASONING_EFFORT}\"")
      fi
      if (( call_max_output > 0 )); then
        args+=(-c "model_max_output_tokens=${call_max_output}")
      fi
      args+=(--full-auto --sandbox workspace-write --skip-git-repo-check)
      if [[ -n "$output_file" ]]; then
        mkdir -p "$(dirname "$output_file")"
        args+=(--output-last-message "$output_file")
      fi
      local usage_dir="${LOG_DIR:-${PROJECT_ROOT:-$PWD}/.gpt-creator/logs}"
      mkdir -p "$usage_dir"
      local task_slug
      task_slug="$(printf '%s' "$task" | tr '[:upper:]' '[:lower:]')"
      task_slug="$(printf '%s' "$task_slug" | tr -c 'a-z0-9' '_')"
      [[ -n "$task_slug" ]] || task_slug="codex"
      local model_slug
      model_slug="$(printf '%s' "$model" | tr '[:upper:]' '[:lower:]')"
      model_slug="$(printf '%s' "$model_slug" | tr -c 'a-z0-9' '_')"
      [[ -n "$model_slug" ]] || model_slug="model"
      local codex_log=""
      if ! codex_log="$(mktemp "${usage_dir}/codex-${task_slug}-${model_slug}.XXXXXX.log" 2>/dev/null)"; then
        codex_log="$(mktemp 2>/dev/null)" || codex_log=""
      fi
      local exec_timeout=0
      if [[ "${GC_CODEX_EXEC_TIMEOUT:-}" =~ ^[0-9]+$ ]]; then
        exec_timeout=$((GC_CODEX_EXEC_TIMEOUT))
      fi
      if [[ -n "$codex_log" ]]; then
        local cmd_status=0
        local exec_start_time exec_end_time exec_duration_ms
        exec_start_time="$(date +%s)"
        if gc_exec_with_timeout "$exec_timeout" "$prompt_file" "$codex_log" "$CODEX_BIN" "${args[@]}"; then
          cmd_status=0
        else
          cmd_status=$?
        fi
        exec_end_time="$(date +%s)"
        exec_duration_ms=$(( (exec_end_time - exec_start_time) * 1000 ))
        (( exec_duration_ms < 0 )) && exec_duration_ms=0
        gc_record_codex_usage "$codex_log" "$task" "$model" "$prompt_file" "$cmd_status" "$call_step" "$call_max_output" "$exec_duration_ms"
        local attempt_tokens="${GC_LAST_CODEX_TOTAL_TOKENS:-0}"
        if [[ "$attempt_tokens" =~ ^[0-9]+$ ]]; then
          GC_CODEX_CALL_TOKEN_ACCUM=$((GC_CODEX_CALL_TOKEN_ACCUM + attempt_tokens))
        fi
        if [[ "${GC_STAGE_LIMIT_LAST_STEP:-}" == "$call_step" ]]; then
          skip_codex=1
          skip_codex_reason="stage-limit"
          blocked_stop_run=1
          run_blocked_quota=1
          task_result_status="blocked-quota"
          codex_ok=1
          task_notes+=("Stage ${call_step} exceeded budget limit ${GC_STAGE_LIMIT_LAST_LIMIT:-}")
        fi
        if [[ "${GC_CODEX_USAGE_LIMIT_REACHED:-0}" == "1" ]]; then
          return 95
        fi
        return "$cmd_status"
      else
        local cmd_status=0
        if gc_exec_with_timeout "$exec_timeout" "$prompt_file" "" "$CODEX_BIN" "${args[@]}"; then
          cmd_status=0
        else
          cmd_status=$?
        fi
        if [[ "${GC_CODEX_USAGE_LIMIT_REACHED:-0}" == "1" ]]; then
          return 95
        fi
        return "$cmd_status"
      fi
    }
    if run_codex_model "$codex_model_for_step" "$codex_reasoning_for_step"; then
      return 0
    fi
    local primary_status="$?"
    if [[ "${GC_CODEX_USAGE_LIMIT_REACHED:-0}" == "1" ]]; then
      return "$primary_status"
    fi
    if [[ "$codex_model_for_step" != "$fallback_model" ]]; then
      warn "Codex model ${codex_model_for_step} failed; retrying with ${fallback_model}."
      if run_codex_model "$fallback_model" "$codex_reasoning_for_step"; then
        return 0
      fi
      primary_status="$?"
      if [[ "${GC_CODEX_USAGE_LIMIT_REACHED:-0}" == "1" ]]; then
        return "$primary_status"
      fi
      warn "Codex invocation returned non-zero."
      return "$primary_status"
    fi
    warn "Codex invocation returned non-zero."
    return "$primary_status"
  else
    warn "Codex binary (${CODEX_BIN}) not found — skipping ${task}."
  fi
}

ensure_go_runtime() {
  local min_major=1
  local min_minor=21
  local target_version="1.22.4"
  local requested_bin="${GO_BIN:-}"
  local resolved_bin=""

  if [[ -n "$requested_bin" ]]; then
    if command -v "$requested_bin" >/dev/null 2>&1; then
      resolved_bin="$(command -v "$requested_bin")"
    else
      warn "GO_BIN is set to '${requested_bin}' but that binary was not found on PATH."
    fi
  fi

  if [[ -z "$resolved_bin" ]] && command -v go >/dev/null 2>&1; then
    resolved_bin="$(command -v go)"
  fi

  if [[ -n "$resolved_bin" ]]; then
    local raw_version
    raw_version="$("$resolved_bin" version 2>/dev/null | awk '{print $3}')"
    local version="${raw_version#go}"
    version="${version%%[^0-9.]*}"
    local IFS=.
    read -r version_major version_minor _ <<<"${version}"
    version_major="${version_major:-0}"
    version_minor="${version_minor:-0}"
    if (( 10#$version_major > min_major )) || { (( 10#$version_major == min_major )) && (( 10#$version_minor >= min_minor )); }; then
      GC_GO_BIN="$resolved_bin"
      export GO_BIN="$GC_GO_BIN"
      return
    fi
    warn "Found Go ${version:-unknown} at ${resolved_bin}, but need ≥ ${min_major}.${min_minor}."
  fi

  local os_name arch_name go_os go_arch
  os_name="$(uname -s)"
  arch_name="$(uname -m)"
  case "$os_name" in
    Linux) go_os="linux" ;;
    Darwin) go_os="darwin" ;;
    *)
      die "Unsupported OS (${os_name}) for automatic Go installation. Install Go ${min_major}.${min_minor}+ and set GO_BIN."
      ;;
  esac

  case "$arch_name" in
    x86_64|amd64) go_arch="amd64" ;;
    arm64|aarch64) go_arch="arm64" ;;
    *)
      die "Unsupported architecture (${arch_name}) for automatic Go installation. Install Go ${min_major}.${min_minor}+ and set GO_BIN."
      ;;
  esac

  local base_dir
  if [[ -n "${PLAN_DIR:-}" ]]; then
    base_dir="$PLAN_DIR"
  else
    base_dir="${XDG_CACHE_HOME:-$HOME/.cache}/gpt-creator"
    mkdir -p "$base_dir"
  fi
  local runtime_root="${base_dir}/.runtime/go"
  local archive_basename="go${target_version}.${go_os}-${go_arch}"
  local target_dir="${runtime_root}/${archive_basename}"
  local go_root="${target_dir}/go"
  local go_binary="${go_root}/bin/go"

  mkdir -p "$runtime_root"

  if [[ ! -x "$go_binary" ]]; then
    info "Installing Go ${target_version} (${go_os}-${go_arch})"
    local url="https://go.dev/dl/${archive_basename}.tar.gz"
    local tmp_archive
    tmp_archive="$(mktemp "${runtime_root}/go-${target_version}.XXXXXX")"
    if ! curl -fsSL "$url" -o "$tmp_archive"; then
      rm -f "$tmp_archive"
      die "Failed to download Go toolchain from ${url}"
    fi
    rm -rf "$target_dir"
    mkdir -p "$target_dir"
    if ! tar -xzf "$tmp_archive" -C "$target_dir"; then
      rm -f "$tmp_archive"
      die "Failed to extract Go toolchain archive (${url})"
    fi
    rm -f "$tmp_archive"
  fi

  if [[ ! -x "$go_binary" ]]; then
    die "Go toolchain installation incomplete; expected executable at ${go_binary}"
  fi

  local gopath="${target_dir}/gopath"
  local cache_dir="${target_dir}/cache"
  mkdir -p "${gopath}/bin" "${gopath}/pkg/mod" "${cache_dir}"
  export PATH="${go_root}/bin:${PATH}"
  export GOROOT="${go_root}"
  export GOPATH="${gopath}"
  export GOMODCACHE="${gopath}/pkg/mod"
  export GOCACHE="${cache_dir}"
  GC_GO_BIN="$go_binary"
  export GO_BIN="$GC_GO_BIN"
  ok "Go runtime pinned to ${target_version}"
}

ensure_node_runtime() {
  local root_dir="${1:-$PROJECT_ROOT}"
  local target_version="20.10.0"
  local runtime_root="${PLAN_DIR}/.runtime"

  # If current node already satisfies v20.x with sufficient minor, skip.
  if command -v node >/dev/null 2>&1; then
    local current
    current="$(node -v 2>/dev/null | sed 's/^v//')"
    if [[ "$current" == 20.* ]]; then
      local minor="${current#20.}"
      minor="${minor%%.*}"
      local patch="${current#20.${minor}.}"
      [[ -z "$patch" ]] && patch=0
      if (( 10#${minor:-0} > 10 )) || { (( 10#${minor:-0} == 10 )) && (( 10#${patch:-0} >= 0 )); }; then
        return
      fi
    fi
  fi

  mkdir -p "$runtime_root"

  local os_name
  os_name="$(uname -s)"
  local arch_name
  arch_name="$(uname -m)"
  local os_tag="" arch_tag="" ext="tar.xz"

  case "$os_name" in
    Darwin)
      os_tag="darwin"
      ext="tar.gz"
      ;;
    Linux)
      os_tag="linux"
      ;;
    *)
      warn "Unsupported OS (${os_name}) for automatic Node runtime; continuing with system Node."
      return
      ;;
  esac

  case "$arch_name" in
    x86_64|amd64)
      arch_tag="x64"
      ;;
    arm64|aarch64)
      arch_tag="arm64"
      ;;
    *)
      warn "Unsupported CPU architecture (${arch_name}) for automatic Node runtime; continuing with system Node."
      return
      ;;
  esac

  local archive_name="node-v${target_version}-${os_tag}-${arch_tag}"
  local target_dir="${runtime_root}/${archive_name}"

  if [[ ! -d "$target_dir" ]]; then
    local url="https://nodejs.org/dist/v${target_version}/${archive_name}.${ext}"
    info "Downloading Node.js ${target_version} (${os_tag}-${arch_tag})"
    local tmp_archive
    tmp_archive="$(mktemp "${runtime_root}/node-${target_version}.XXXXXX")"
    if ! curl -fsSL "$url" -o "$tmp_archive"; then
      warn "Failed to download Node.js runtime from ${url}; continuing with system Node."
      rm -f "$tmp_archive"
      return
    fi
    mkdir -p "$runtime_root"
    if [[ "$ext" == "tar.gz" ]]; then
      tar -xzf "$tmp_archive" -C "$runtime_root"
    else
      tar -xJf "$tmp_archive" -C "$runtime_root"
    fi
    rm -f "$tmp_archive"
  fi

  if [[ ! -d "$target_dir" ]]; then
    warn "Node runtime directory ${target_dir} missing after download; continuing with system Node."
    return
  fi

  export PATH="${target_dir}/bin:${PATH}"
  export NODE_HOME="$target_dir"
  export npm_config_cache="${runtime_root}/npm-cache"
  export PNPM_HOME="${runtime_root}/pnpm-home"
  mkdir -p "$npm_config_cache" "$PNPM_HOME"
  export PATH="${PNPM_HOME}:${PATH}"
  export GC_NODE_RUNTIME="${target_dir}"
  ok "Node.js runtime pinned to ${target_version}"
}

ensure_node_dependencies() {
  local root_dir="${1:-$PROJECT_ROOT}"
  local sentinel="${PLAN_DIR}/.deps-installed"

  ensure_node_runtime "$root_dir"

  if [[ -f "$sentinel" ]]; then
    return
  fi

  local bootstrap_script="${PROJECT_ROOT}/scripts/bootstrap_dependencies.sh"
  local default_bootstrap="${CLI_ROOT}/scripts/bootstrap_dependencies.sh"
  if [[ -f "$default_bootstrap" ]]; then
    mkdir -p "${PROJECT_ROOT}/scripts"
    if [[ ! -f "$bootstrap_script" ]]; then
      if cp "$default_bootstrap" "$bootstrap_script"; then
        chmod +x "$bootstrap_script"
        info "Seeded dependency bootstrap script at ${bootstrap_script}."
      fi
    elif ! cmp -s "$default_bootstrap" "$bootstrap_script"; then
      if cp "$default_bootstrap" "$bootstrap_script"; then
        chmod +x "$bootstrap_script"
        info "Updated dependency bootstrap script at ${bootstrap_script}."
      fi
    fi
  fi

  if [[ ! -x "$bootstrap_script" ]]; then
    warn "Dependency bootstrap script missing at ${bootstrap_script}; skipping automatic install."
    return
  fi

  if "$bootstrap_script" "$root_dir"; then
    touch "$sentinel"
    return
  fi

  die "Dependency bootstrap failed; see the bootstrap log for details."
}

gc_wot_normalize_sleep_between() {
  local python_bin="${1:-python3}"
  local raw_value="${2:-0}"

  if [[ -z "$raw_value" ]]; then
    raw_value="0"
  fi

  if ! command -v "$python_bin" >/dev/null 2>&1; then
    if [[ "$raw_value" =~ ^[0-9]+([.][0-9]+)?$ ]]; then
      printf '%s\n' "$raw_value"
      return 0
    fi
    return 1
  fi

  local helper_path
  helper_path="$(gc_clone_python_tool "normalize_sleep_between.py" "${PROJECT_ROOT:-$PWD}")" || return 1

  local normalized=""
  if ! normalized="$("$python_bin" "$helper_path" "$raw_value" 2>/dev/null)"; then
    return 1
  fi

  printf '%s\n' "$normalized"
}

gc_wot_run_verify_if_needed() {
  local reason="${1:-completion}"
  local project_root="${2:?project root required}"
  local any_changes="${3:-0}"
  local work_failed="${4:-0}"

  local reason_label=""
  local skip_message=""
  case "$reason" in
    completion)
      reason_label="work run"
      skip_message="No repository changes detected; skipping verify."
      ;;
    batch-limit)
      reason_label="batch pause"
      skip_message="Batch limit reached with no repository changes; skipping verify."
      ;;
    *)
      reason_label="$reason"
      skip_message="No repository changes detected; skipping verify."
      ;;
  esac

  if (( any_changes == 0 )); then
    info "$skip_message"
    printf '%s\n' "$work_failed"
    return 0
  fi

  local verify_start_ts verify_duration_ms verify_failed=0
  info "Re-running verify after ${reason_label}"
  verify_start_ts="$(date +%s)"
  if ! cmd_verify all --project "$project_root"; then
    verify_failed=1
    warn "Verify command reported failures."
  fi
  verify_duration_ms=$(( ( $(date +%s) - verify_start_ts ) * 1000 ))
  gc_budget_log_stage "verify" 0 0 0 "$verify_duration_ms" "[]" "{}" "false"

  if (( verify_failed )); then
    if (( ${GC_WOT_SOFT_VERIFY:-0} )); then
      GC_WOT_SOFT_VERIFY_TRIGGERED=1
      info "Verify failure tolerated via soft mode."
      printf '%s\n' "$work_failed"
    else
      printf '%s\n' "1"
    fi
  else
    printf '%s\n' "$work_failed"
  fi
}

gc_apply_codex_changes() {
  local output_file="${1:?output file required}"
  local project_root="${2:?project root required}"
  local _artifact_path="${3:-}" # reserved for future patch artifact support
  : "${_artifact_path:=}"

  local helper_root="${project_root}"
  local extract_py="${GC_WOT_EXTRACT_HELPER:-}"
  local validate_py="${GC_WOT_VALIDATE_HELPER:-}"
  local apply_py="${GC_WOT_APPLY_HELPER:-}"
  local placeholders_py="${GC_WOT_PLACEHOLDERS_HELPER:-}"
  local externalize_py="${GC_WOT_EXTERNALIZE_HELPER:-}"
  local sha_helper="${GC_WOT_SHA_HELPER:-}"

  if [[ -z "$extract_py" ]]; then
    extract_py="$(gc_clone_python_tool "wot_extract_first_json.py" "$helper_root")" || return 1
    GC_WOT_EXTRACT_HELPER="$extract_py"
  fi
  if [[ -z "$validate_py" ]]; then
    validate_py="$(gc_clone_python_tool "wot_validate_envelope.py" "$helper_root")" || return 1
    GC_WOT_VALIDATE_HELPER="$validate_py"
  fi
  if [[ -z "$apply_py" ]]; then
    apply_py="$(gc_clone_python_tool "wot_apply_changes.py" "$helper_root")" || return 1
    GC_WOT_APPLY_HELPER="$apply_py"
  fi
  if [[ -z "$placeholders_py" ]]; then
    placeholders_py="$(gc_clone_python_tool "wot_ensure_placeholders.py" "$helper_root")" || return 1
    GC_WOT_PLACEHOLDERS_HELPER="$placeholders_py"
  fi
  if [[ -z "$externalize_py" ]]; then
    externalize_py="$(gc_clone_python_tool "wot_externalize_blobs.py" "$helper_root")" || return 1
    GC_WOT_EXTERNALIZE_HELPER="$externalize_py"
  fi
  if [[ -z "$sha_helper" ]]; then
    if sha_helper="$(gc_clone_python_tool "sha256_file.py" "$helper_root" 2>/dev/null)"; then
      GC_WOT_SHA_HELPER="$sha_helper"
    else
      sha_helper=""
    fi
  fi

  local run_dir="${RUN_DIR:-${project_root}/.gpt-creator/staging/plan/work/runs/$(date +%Y%m%d%H%M%S)}"
  mkdir -p "$run_dir"

  local payload_raw="${run_dir}/payload.raw"
  local payload_json="${run_dir}/payload.json"
  cp -f "$output_file" "$payload_raw" || return 1

  if ! python3 "$extract_py" <"$payload_raw" >"$payload_json"; then
    return 2
  fi
  if ! python3 "$externalize_py" "$payload_json" "$run_dir" "$project_root"; then
    return 2
  fi
  local envelope_meta="${run_dir}/payload.meta.json"
  local validate_status=0
  if ! GC_WOT_META_OUTPUT="$envelope_meta" python3 "$validate_py" "$payload_json"; then
    validate_status=$?
  fi
  if (( validate_status == 3 )); then
    return 13
  elif (( validate_status != 0 )); then
    return 2
  fi

  while IFS= read -r placeholder_path; do
    [[ -z "$placeholder_path" ]] && continue
    (
      cd "$project_root" || exit 0
      "$CLI_ROOT/scripts/create-doc-placeholder.sh" \
        "$placeholder_path" \
        --owner "Work-on-Tasks" \
        --summary "Auto-created placeholder"
    ) || true
  done < <(python3 "$placeholders_py" "$payload_json")

  local envelope_sha=""
  if [[ -n "$sha_helper" ]]; then
    envelope_sha="$(python3 "$sha_helper" "$payload_json" 2>/dev/null || true)"
  fi
  if [[ -z "$envelope_sha" ]]; then
    if command -v shasum >/dev/null 2>&1; then
      envelope_sha="$(shasum -a 256 "$payload_json" 2>/dev/null | awk '{print $1}')"
    elif command -v sha256sum >/dev/null 2>&1; then
      envelope_sha="$(sha256sum "$payload_json" 2>/dev/null | awk '{print $1}')"
    fi
  fi

  local meta_file="${run_dir}/.applied.json"
  touch "$meta_file"
  local identical_envelope=0
  if [[ -n "$envelope_sha" ]] && grep -q "$envelope_sha" "$meta_file"; then
    identical_envelope=1
  fi

  if (( identical_envelope == 0 )); then
    if ! python3 "$apply_py" "$payload_json" "$project_root"; then
      return 12
    fi
    if [[ -n "$envelope_sha" ]]; then
      printf '%s %s\n' "$envelope_sha" "$(date -u +%FT%TZ)" >>"$meta_file"
    fi
  fi

  local summary_helper
  summary_helper="$(gc_clone_python_tool "wot_summarize_envelope.py" "$helper_root")" || return 1

  local summary
  if ! summary="$(WOT_IDENTICAL="$identical_envelope" WOT_META_PATH="$envelope_meta" python3 "$summary_helper" "$payload_json" "$project_root")"; then
    return 2
  fi

  printf '%s\n' "$summary"
}

cmd_create_jira_tasks() {
  local args=()
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project)
        args+=(--project "$(abs_path "$2")")
        shift 2
        ;;
      --model)
        args+=(--model "$2")
        shift 2
        ;;
      --force|--dry-run)
        args+=("$1")
        shift
        ;;
      -h|--help)
        # Let the dedicated CLI script handle help output.
        args+=("$1")
        shift
        ;;
      *)
        args+=("$1")
        shift
        ;;
    esac
  done

  local shell_bin="${BASH:-bash}"
  "$shell_bin" "$CLI_ROOT/src/cli/create-jira-tasks.sh" "${args[@]}"
}

cmd_create_tasks() {
  local root="" jira="" force=0
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --jira) jira="$(abs_path "$2")"; shift 2;;
      --force) force=1; shift;;
      -h|--help)
        cat <<'EOUSAGE'
Usage: gpt-creator create-tasks [--project PATH] [--jira FILE] [--force]

Convert Jira markdown tasks into a project-scoped SQLite database stored under .gpt-creator/staging/plan/tasks.

Options:
  --project PATH  Project root (defaults to current directory)
  --jira FILE     Jira markdown source (defaults to staging/inputs/jira.md)
  --force         Rebuild the tasks database without restoring prior progress metadata
EOUSAGE
        return 0
        ;;
      *) break;;
    esac
  done

  ensure_ctx "$root"
  [[ -n "$jira" ]] || jira="${INPUT_DIR}/jira.md"
  [[ -f "$jira" ]] || die "Jira tasks file not found: ${jira}"

  local tasks_dir="${PLAN_DIR}/tasks"
  local parsed_local="${tasks_dir}/parsed.local.json"
  local tasks_db="${tasks_dir}/tasks.db"
  mkdir -p "$tasks_dir"
  local intake_lock_path="${tasks_dir}/.intake-frozen"
  if [[ -f "$intake_lock_path" && "$force" -ne 1 ]]; then
    warn "Backlog intake frozen (see ${intake_lock_path#${PROJECT_ROOT:-$PWD}/}); rerun with --force after resolving duplicates."
    return 1
  fi

  info "Parsing Jira backlog → ${parsed_local}"
  gc_parse_jira_tasks "$jira" "$parsed_local"

  info "Building tasks database → ${tasks_db}"
  local db_stats
  if ! db_stats="$(gc_build_tasks_db "$parsed_local" "$tasks_db" "$force")"; then
    die "Failed to build Jira tasks SQLite database"
  fi

  local story_count=0 task_count=0 restored_stories=0 restored_tasks=0
  while IFS= read -r line; do
    case "$line" in
      STORIES\ *) story_count="${line#STORIES }" ;;
      TASKS\ *) task_count="${line#TASKS }" ;;
      RESTORED_STORIES\ *) restored_stories="${line#RESTORED_STORIES }" ;;
      RESTORED_TASKS\ *) restored_tasks="${line#RESTORED_TASKS }" ;;
    esac
  done <<<"$db_stats"

  info "Stories: ${story_count:-0} (restored: ${restored_stories:-0})"
  info "Tasks: ${task_count:-0} (restored statuses: ${restored_tasks:-0})"
  ok "Tasks database updated → ${tasks_db}"

  local legacy_manifest="${tasks_dir}/manifest.json"
  local legacy_stories="${tasks_dir}/stories"
  if [[ -f "$legacy_manifest" || -d "$legacy_stories" ]]; then
    warn "Legacy task manifest/JSON artifacts detected under ${tasks_dir}; they are no longer used now that tasks live in SQLite. Remove them when convenient to avoid confusion."
  fi
}

cmd_backlog() {
  local root="" type_arg="" item_children="" show_progress=0 task_details=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project|--root)
        root="$(abs_path "$2")"
        shift 2
        ;;
      --type)
        type_arg="$2"
        shift 2
        ;;
      --item-children)
        item_children="$2"
        shift 2
        ;;
      --progress)
        show_progress=1
        shift
        ;;
      --task-details)
        task_details="$2"
        shift 2
        ;;
      -h|--help)
        cat <<'EOUSAGE'
Usage: gpt-creator backlog [--project PATH|--root PATH]
                           [--type epics|stories]
                           [--item-children ID]
                           [--progress]
                           [--task-details ID]

Inspect the backlog stored in .gpt-creator/staging/plan/tasks/tasks.db without an interactive prompt.

  --type epics|stories     List backlog summaries (defaults to 'epics' when no other flag is provided).
  --item-children ID       Show the direct children of the epic/story identified by ID (slug, key, or ID).
  --progress               Print an overall progress bar summarising task completion.
  --task-details ID        Show a detailed view for the matching task ID (case-insensitive).

Pass any combination of the flags above; each requested view is printed sequentially.
EOUSAGE
        return 0
        ;;
      *)
        die "Unknown argument for backlog: $1"
        ;;
    esac
  done

  if [[ -z "$type_arg" && -z "$item_children" && "$show_progress" -eq 0 && -z "$task_details" ]]; then
    type_arg="epics"
  fi

  ensure_ctx "$root"
  local tasks_db="${PLAN_DIR}/tasks/tasks.db"
  if [[ ! -f "$tasks_db" ]]; then
    die "Tasks database not found at ${tasks_db}. Run 'gpt-creator create-tasks' first."
  fi

  local backlog_helper
  backlog_helper="$(gc_clone_python_tool "fetch_stories.py" "${PROJECT_ROOT:-$PWD}")" || die "Failed to prepare backlog helper script"
  python3 "$backlog_helper" "$tasks_db" "${type_arg:-}" "${item_children:-}" "$show_progress" "${task_details:-}"
}

cmd_dag() {
  local root="" action="" story=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project|-p)
        root="$(abs_path "$2")"
        shift 2
        ;;
      validate)
        action="validate"
        shift
        break
        ;;
      -h|--help)
        cat <<'EOUSAGE'
Usage: gpt-creator dag validate [--project PATH] [--story SLUG]

Validate story-level DAG definitions under .gpt-creator/dag.
EOUSAGE
        return 0
        ;;
      *)
        die "Unknown argument for dag: $1"
        ;;
    esac
  done

  [[ -n "$action" ]] || die "dag requires a subcommand (e.g. 'dag validate')"
  ensure_ctx "$root"

  case "$action" in
    validate)
      while [[ $# -gt 0 ]]; do
        case "$1" in
          --story)
            story="$2"
            shift 2
            ;;
          --help|-h)
            cat <<'EOUSAGE'
Usage: gpt-creator dag validate [--project PATH] [--story SLUG]
EOUSAGE
            return 0
            ;;
          *)
            die "Unknown argument for dag validate: $1"
            ;;
        esac
      done
      local dag_helper
      dag_helper="$(gc_clone_python_tool "dag_inspect.py" "${PROJECT_ROOT:-$PWD}")" || return 1
      if [[ -n "$story" ]]; then
        python3 "$dag_helper" validate --project-root "${PROJECT_ROOT:-$PWD}" --story "$story"
      else
        python3 "$dag_helper" validate --project-root "${PROJECT_ROOT:-$PWD}"
      fi
      ;;
    *)
      die "Unknown dag subcommand: ${action}"
      ;;
  esac
}

cmd_next() {
  local root="" story=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project|-p)
        root="$(abs_path "$2")"
        shift 2
        ;;
      --story)
        story="$2"
        shift 2
        ;;
      -h|--help)
        cat <<'EOUSAGE'
Usage: gpt-creator next [--project PATH] [--story SLUG]

List the next ready task(s) per story with dependency gate reasons.
EOUSAGE
        return 0
        ;;
      *)
        die "Unknown argument for next: $1"
        ;;
    esac
  done

  ensure_ctx "$root"
  local tasks_db="${PLAN_DIR}/tasks/tasks.db"
  [[ -f "$tasks_db" ]] || die "Tasks database not found at ${tasks_db}. Run 'gpt-creator create-tasks' first."

  local story_plan_helper
  story_plan_helper="$(gc_clone_python_tool "story_scheduler.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  "$python_bin" "$story_plan_helper" "$tasks_db" "${story:-}" "1" >/dev/null 2>&1 || true

  local dag_helper
  dag_helper="$(gc_clone_python_tool "dag_inspect.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  local -a helper_args=(next --project-root "${PROJECT_ROOT:-$PWD}" --db "$tasks_db")
  if [[ -n "$story" ]]; then
    helper_args+=(--story "$story")
  fi
  python3 "$dag_helper" "${helper_args[@]}"
}

cmd_why() {
  local root="" story="" task=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project|-p)
        root="$(abs_path "$2")"
        shift 2
        ;;
      --story)
        story="$2"
        shift 2
        ;;
      --task)
        task="$2"
        shift 2
        ;;
      -h|--help)
        cat <<'EOUSAGE'
Usage: gpt-creator why --task TASK_ID [--project PATH] [--story SLUG]

Explain why a task is blocked, including dependency and readiness gates.
EOUSAGE
        return 0
        ;;
      *)
        die "Unknown argument for why: $1"
        ;;
    esac
  done

  [[ -n "$task" ]] || die "--task is required"
  ensure_ctx "$root"
  local tasks_db="${PLAN_DIR}/tasks/tasks.db"
  [[ -f "$tasks_db" ]] || die "Tasks database not found at ${tasks_db}. Run 'gpt-creator create-tasks' first."

  local story_plan_helper
  story_plan_helper="$(gc_clone_python_tool "story_scheduler.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$story_plan_helper" "$tasks_db" "${story:-}" "1" >/dev/null 2>&1 || true

  local dag_helper
  dag_helper="$(gc_clone_python_tool "dag_inspect.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$dag_helper" why --project-root "${PROJECT_ROOT:-$PWD}" --db "$tasks_db" --task "$task"
}

cmd_migrate_tasks_json() {
  local root="" force=0
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --force) force=1; shift;;
      -h|--help)
        cat <<'EOUSAGE'
Usage: gpt-creator migrate-tasks [--project PATH] [--force]

Populate the tasks SQLite database from the JSON outputs produced by
`gpt-creator create-jira-tasks` (under plan/create-jira-tasks/json).

Options:
  --project PATH  Project root (defaults to current directory)
  --force         Rebuild the database without restoring prior task status metadata
EOUSAGE
        return 0
        ;;
      *) break;;
    esac
  done

  ensure_ctx "$root"

  local pipeline_dir="${PLAN_DIR}/create-jira-tasks"
  local json_dir="${pipeline_dir}/json"
  local epics_json="${json_dir}/epics.json"
  local stories_dir="${json_dir}/stories"
  local tasks_dir="${json_dir}/tasks"

  [[ -f "$epics_json" ]] || die "Epics JSON not found: ${epics_json}"
  [[ -d "$stories_dir" ]] || die "Stories JSON directory not found: ${stories_dir}"
  [[ -d "$tasks_dir" ]] || die "Tasks JSON directory not found: ${tasks_dir}"

  local payload="${json_dir}/tasks_payload.json"
  local tasks_workspace="${PLAN_DIR}/tasks"
  mkdir -p "$tasks_workspace"
  local db_path="${tasks_workspace}/tasks.db"
  local intake_lock_path="${tasks_workspace}/.intake-frozen"
  if [[ -f "$intake_lock_path" && "$force" -ne 1 ]]; then
    warn "Backlog intake frozen (see ${intake_lock_path#${PROJECT_ROOT:-$PWD}/}); rerun with --force after resolving duplicates."
    return 1
  fi

  info "Building tasks payload from JSON → ${payload}"
  info "Updating tasks database → ${db_path}"

  if ! gc_rebuild_tasks_db_from_json "$force"; then
    die "Failed to rebuild tasks database from JSON payload"
  fi

  ok "Tasks database updated → ${db_path}"
}

cmd_refine_tasks() {
  local root="" story_filter="" model_override="" dry_run=0 force=0
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --story) story_filter="$2"; shift 2;;
      --model) model_override="$2"; shift 2;;
      --dry-run) dry_run=1; shift;;
      --force) force=1; shift;;
      -h|--help)
        cat <<'EOUSAGE'
Usage: gpt-creator refine-tasks [--project PATH] [--story SLUG] [--model NAME] [--dry-run] [--force]

Refine tasks stored in the SQLite backlog using Codex, updating each task
record and the refined JSON artifacts as soon as a task is enriched.

Options:
  --project PATH  Project root (defaults to current directory)
  --story SLUG    Limit refinement to a single story slug (optional)
  --model NAME    Override Codex model (defaults to CODEX_MODEL/GC defaults)
  --dry-run       Build prompts without invoking Codex
  --force         Reset refinement progress and reprocess every task
EOUSAGE
        return 0
        ;;
      *) break;;
    esac
  done

  ensure_ctx "$root"

  local tasks_db="${PLAN_DIR}/tasks/tasks.db"
  [[ -f "$tasks_db" ]] || die "Tasks database not found: ${tasks_db}"

  local python_bin="${PYTHON_BIN:-python3}"
  if ! command -v "$python_bin" >/dev/null 2>&1; then
    die "Python runtime '$python_bin' not available; cannot refine tasks."
  fi

  local pipeline_dir="${PLAN_DIR}/create-jira-tasks"
  local json_tasks_dir="${pipeline_dir}/json/tasks"
  [[ -d "$json_tasks_dir" ]] || die "Tasks JSON directory not found: ${json_tasks_dir}"

  local have_refined
  local refine_init_helper
  refine_init_helper="$(gc_clone_python_tool "refine_tasks_init_db.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  have_refined="$($python_bin "$refine_init_helper" "$tasks_db")"

  local summary
  local refine_summary_helper
  refine_summary_helper="$(gc_clone_python_tool "refine_tasks_summary.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  summary="$($python_bin "$refine_summary_helper" "$tasks_db" "$story_filter")" || die "Failed to summarise tasks backlog"

  local total_tasks refined_tasks pending_tasks total_stories pending_stories
  read -r total_tasks refined_tasks pending_tasks total_stories pending_stories <<<"$summary"

  if (( force )); then
    local refine_reset_helper
    refine_reset_helper="$(gc_clone_python_tool "refine_tasks_reset.py" "${PROJECT_ROOT:-$PWD}")" || return 1
    "$python_bin" "$refine_reset_helper" "$tasks_db"
    refined_tasks=0
    pending_tasks=$total_tasks
  fi

  info "Backlog summary → tasks: total=${total_tasks}, refined=${refined_tasks}, pending=${pending_tasks}; stories: total=${total_stories}, pending=${pending_stories}${story_filter:+ (filter='${story_filter}')}."

  local codex_cmd="${CODEX_BIN:-${CODEX_CMD:-codex}}"
  if (( dry_run == 0 )) && ! command -v "$codex_cmd" >/dev/null 2>&1; then
    warn "Codex CLI '$codex_cmd' not found; switching to --dry-run."
    dry_run=1
  fi

  local model_name="${model_override:-${CODEX_MODEL:-$GC_DEFAULT_MODEL}}"

  # shellcheck source=src/lib/create-jira-tasks/pipeline.sh
  source "${CLI_ROOT}/src/lib/create-jira-tasks/pipeline.sh"

  local force_flag=0
  local skip_refine=0
  local dry_flag="$dry_run"
  cjt::init "$PROJECT_ROOT" "$model_name" "$force_flag" "$skip_refine" "$dry_flag"
  CJT_DOC_FILES=()
  cjt::build_context_files

  CJT_SYNC_DB=1
  CJT_TASKS_DB_PATH="$tasks_db"
  CJT_IGNORE_REFINE_STATE=1
  CJT_REFINE_FORCE=$force
  CJT_HAVE_REFINED_COLUMN="$have_refined"
  CJT_REFINE_TOTAL_TASKS="$total_tasks"
  CJT_REFINE_REFINED_TASKS="$refined_tasks"
  CJT_REFINE_PENDING_TASKS="$pending_tasks"
  CJT_REFINE_TOTAL_STORIES="$total_stories"
  CJT_REFINE_PENDING_STORIES="$pending_stories"
  if [[ -n "$story_filter" ]]; then
    CJT_ONLY_STORY_SLUG="$story_filter"
  fi

  cjt::refine_tasks
  ok "Task refinement complete"
}

gc_write_task_prompt() {
  local db_path="${1:?tasks db path required}"
  local story_slug="${2:?story slug required}"
  local task_index="${3:?task index required}"
  local prompt_path="${4:?prompt path required}"
  local context_tail="${5:-}"
  local model_name="${6:-$CODEX_MODEL}"
  local project_root="${7:-$PROJECT_ROOT}"
  local staging_dir="${8:-$GC_STAGING_DIR}"
  local task_binder_helper
  task_binder_helper="$(gc_clone_python_tool "task_binder.py" "$project_root")" || return 1
  : "$task_binder_helper"
  local compose_helper
  compose_helper="$(gc_clone_python_tool "compose_sections.py" "$project_root")" || return 1
  : "$compose_helper"
  local prompt_helper
  prompt_helper="$(gc_clone_python_tool "document_index.py" "$project_root")" || return 1
  python3 "$prompt_helper" "$db_path" "$story_slug" "$task_index" "$prompt_path" "$context_tail" "$model_name" "$project_root" "$staging_dir"
}

# Return success (0) when note language implies manual follow-up is required.
gc_note_requires_followup() {
  local note_text="${1:-}"
  local note_lower="${note_text,,}"

  [[ -n "$note_lower" ]] || return 1

  if [[ "$note_lower" == *"parse-error"* ]]; then
    return 0
  fi

  if [[ "$note_lower" == *"no manual"* || "$note_lower" == *"no manual steps"* || "$note_lower" == *"no manual verification"* || "$note_lower" == *"no manual review"* || "$note_lower" == *"no review needed"* || "$note_lower" == *"no review required"* || "$note_lower" == *"no action required"* || "$note_lower" == *"no follow-up"* || "$note_lower" == *"no follow up"* ]]; then
    return 1
  fi

  if [[ "$note_lower" == *"optional"* || "$note_lower" == *"recommended"* || "$note_lower" == *"informational"* || "$note_lower" == *"for reference"* ]]; then
    if [[ "$note_lower" != *"requires"* && "$note_lower" != *"required"* && "$note_lower" != *"must"* && "$note_lower" != *"need"* && "$note_lower" != *"needs"* && "$note_lower" != *"needed"* && "$note_lower" != *"blocked"* && "$note_lower" != *"blocker"* && "$note_lower" != *"pending"* ]]; then
      return 1
    fi
  fi

  if [[ "$note_lower" == *"error"* || "$note_lower" == *"failure"* || "$note_lower" == *"failed"* ]]; then
    if [[ "$note_lower" == *"no error"* || "$note_lower" == *"no errors"* || "$note_lower" == *"without error"* || "$note_lower" == *"error free"* || "$note_lower" == *"error-free"* || "$note_lower" == *"errors resolved"* || "$note_lower" == *"errors addressed"* || "$note_lower" == *"failure resolved"* || "$note_lower" == *"failure addressed"* ]]; then
      :
    else
      return 0
    fi
  fi

  local contains_manual=0
  if [[ "$note_lower" == *"manual"* || "$note_lower" == *"manually"* ]]; then
    contains_manual=1
  fi

  if (( contains_manual )); then
    if [[ "$note_lower" == *"manual steps optional"* || "$note_lower" == *"manual testing optional"* || "$note_lower" == *"manual qa optional"* || "$note_lower" == *"manual testing recommended"* || "$note_lower" == *"manual qa recommended"* || "$note_lower" == *"manual review optional"* || "$note_lower" == *"manual verification optional"* ]]; then
      contains_manual=0
    fi
  fi

  if (( contains_manual )); then
    local -a manual_triggers=(
      "requires"
      "required"
      "require"
      "must"
      "need"
      "needs"
      "needed"
      "manual follow-up"
      "manual follow up"
      "manual followup"
      "manual verification"
      "manual review"
      "manual steps"
      "manual patch"
      "manual fix"
      "manual merge"
      "manual deploy"
      "manual migration"
      "manual intervention"
      "manual action"
      "apply manually"
      "manually apply"
      "manually patch"
      "manually merge"
      "manually verify"
      "could not"
      "can't"
      "cannot"
      "unable"
      "failed"
      "failure"
      "todo"
      "tbd"
      "pending"
      "block"
      "blocked"
      "follow-up"
      "follow up"
      "followup"
    )
    for trigger in "${manual_triggers[@]}"; do
      if [[ "$note_lower" == *"$trigger"* ]]; then
        return 0
      fi
    done
  fi

  if [[ "$note_lower" == *"review"* ]]; then
    if [[ "$note_lower" == *"no review"* || "$note_lower" == *"reviewed"* ]]; then
      return 1
    fi
    local -a review_triggers=(
      "needs review"
      "need review"
      "required review"
      "requires review"
      "review required"
      "pending review"
      "awaiting review"
      "please review"
      "for review"
      "manual review"
      "review manually"
      "review and apply"
      "review this change"
    )
    for trigger in "${review_triggers[@]}"; do
      if [[ "$note_lower" == *"$trigger"* ]]; then
        return 0
      fi
    done
  fi

  return 1
}

gc_generate_auto_review() {
  local story_run_dir="${1:-}"
  local story_log_dir="${2:-}"
  local banner_task_id="${3:-}"
  local task_id="${4:-}"
  local task_status="${5:-}"
  local task_number="${6:-}"
  local tokens_used="${7:-}"
  local project_root="${8:-${PROJECT_ROOT:-}}"

  [[ -n "$story_run_dir" ]] || return 1

  local review_dir="${story_run_dir}/review"
  if [[ ! -d "$review_dir" ]]; then
    mkdir -p "$review_dir" || return 1
  fi

  local slug="${banner_task_id:-${task_id:-task_${task_number}}}"
  slug="${slug//[^A-Za-z0-9_.-]/_}"
  [[ -n "$slug" ]] || slug="task_${task_number}"

  local review_md="${review_dir}/${slug}.md"
  local review_json="${review_dir}/${slug}.json"

  if [[ -s "$review_md" ]]; then
    printf '%s\n' "$review_md"
    return 0
  fi

  local run_dir="${story_run_dir%/*}"
  if [[ -z "$run_dir" || "$run_dir" == "$story_run_dir" ]]; then
    run_dir="$(dirname "$story_run_dir")"
  fi
  local run_id="$(basename "$run_dir")"
  local timestamp_utc
  timestamp_utc="$(date -u +"%Y-%m-%dT%H:%M:%SZ")"

  local log_path="${story_log_dir}/task_${task_number}.log"
  local log_rel=""
  if [[ -f "$log_path" ]]; then
    if [[ -n "$project_root" && "$log_path" == "$project_root/"* ]]; then
      log_rel="${log_path#$project_root/}"
    else
      log_rel="$log_path"
    fi
  fi

  cat >"$review_md" <<EOF
# Automated Review — ${banner_task_id:-${task_id:-Task ${task_number}}}

Generated automatically because GC_AUTO_REVIEW=1.

- Status: ${task_status:-unknown}
- Tokens used: ${tokens_used:-unknown}
- Run: ${run_id:-unknown}
- Generated: ${timestamp_utc}
EOF

  if [[ -n "$log_rel" ]]; then
    printf '\nSource log: %s\n' "$log_rel" >>"$review_md"
  fi

  local artifact_rel="$review_md"
  if [[ -n "$project_root" && "$artifact_rel" == "$project_root/"* ]]; then
    artifact_rel="${artifact_rel#$project_root/}"
  fi

  cat >"$review_json" <<EOF
{
  "taskId": "${task_id:-${banner_task_id:-task_${task_number}}}",
  "status": "${task_status:-unknown}",
  "tokensUsed": "${tokens_used:-unknown}",
  "runId": "${run_id:-unknown}",
  "generatedAt": "${timestamp_utc}",
  "autoGenerated": true,
  "artifact": "${artifact_rel}"
}
EOF

  printf '%s\n' "$review_md"
}

cmd_work_on_tasks() {
  local root="" resume=1 story_filter="" start_task_ref="" no_verify=0 keep_artifacts=0 memory_cycle=0 force_reset=0
  local batch_size=0 sleep_between=0 sleep_between_positive=0 context_lines=400 context_file_lines=200 prompt_compact=1 sample_lines=0 doc_snippets=1
  local -a context_skip_patterns=()
  local override_max_tokens=""
  local override_soft_limit=""
  local override_reserved_output=""
  local override_stop_on_overbudget=""
  local -a stage_limit_overrides=()
  local auto_abandon_override=""
  local idle_timeout="${GC_WORK_ON_TASKS_IDLE_TIMEOUT:-0}"
  local throughput_checkpoint_interval=300
  local throughput_next_checkpoint=0
  local diff_repeat_limit_override=""
  local override_plan_max_out=""
  local override_status_max_out=""
  local override_verify_max_out=""
  local override_patch_max_out=""
  local override_hard_cap=""
  local verify_mode="hard"
  local complete_on_followup=0
  local backlog_guard_window_days="${GC_BACKLOG_GUARD_WINDOW_DAYS:-7}"
  local backlog_guard_wip_limit="${GC_BACKLOG_GUARD_WIP_LIMIT:-12}"
  local backlog_snapshot_before_path=""
  local backlog_snapshot_after_path=""
  local backlog_guard_enabled=0
  local backlog_guard_window_value=""
  local prompt_slim_helper=""
  local prompt_safeguard_helper=""

  local auto_review_enabled=1
  if [[ -n "${GC_AUTO_REVIEW:-}" ]]; then
    case "${GC_AUTO_REVIEW,,}" in
      0|false|no|off) auto_review_enabled=0 ;;
      1|true|yes|on) auto_review_enabled=1 ;;
    esac
  fi

  local binder_enabled=1
  local binder_ttl_seconds=604800
  local binder_max_size_bytes=209715200
  local binder_clear_on_migration=0
  local python_bin="${PYTHON_BIN:-python3}"
  prompt_safeguard_helper="$(gc_clone_python_tool "prompt_safeguard.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  GC_PY_HELPERS_DIR="$(dirname "$prompt_safeguard_helper")"
  export GC_PY_HELPERS_DIR
  if [[ -n "${GC_BINDER_ENABLED:-}" ]]; then
    case "${GC_BINDER_ENABLED,,}" in
      0|false|no|off) binder_enabled=0;;
      *) binder_enabled=1;;
    esac
  fi
  if [[ -n "${GC_BINDER_TTL_SECONDS:-}" ]]; then
    binder_ttl_seconds=$(gc_parse_duration_seconds "${GC_BINDER_TTL_SECONDS}" "$binder_ttl_seconds")
  fi
  if [[ -n "${GC_BINDER_MAX_BYTES:-}" ]]; then
    binder_max_size_bytes=$(gc_parse_size_bytes "${GC_BINDER_MAX_BYTES}" "$binder_max_size_bytes")
  fi
  if [[ -n "${GC_BINDER_CLEAR_ON_MIGRATION:-}" ]]; then
    case "${GC_BINDER_CLEAR_ON_MIGRATION,,}" in
      1|true|yes|on) binder_clear_on_migration=1;;
      *) binder_clear_on_migration=0;;
    esac
  fi

  local codex_timeout_default=600
  local codex_timeout_value="${GC_CODEX_EXEC_TIMEOUT:-}"
  if [[ -z "$codex_timeout_value" ]]; then
    GC_CODEX_EXEC_TIMEOUT="$codex_timeout_default"
  elif [[ "$codex_timeout_value" =~ ^[0-9]+$ ]]; then
    if (( codex_timeout_value <= 0 )); then
      if [[ -n "${GC_CODEX_EXEC_TIMEOUT_INITIAL:-}" ]]; then
        warn "GC_CODEX_EXEC_TIMEOUT (idle timeout) must be greater than zero; defaulting to ${codex_timeout_default}s."
      fi
      GC_CODEX_EXEC_TIMEOUT="$codex_timeout_default"
    fi
  else
    warn "GC_CODEX_EXEC_TIMEOUT ('${codex_timeout_value}') is not numeric; defaulting idle timeout to ${codex_timeout_default}s."
    GC_CODEX_EXEC_TIMEOUT="$codex_timeout_default"
  fi
  local codex_timeout_seconds="${GC_CODEX_EXEC_TIMEOUT}"

  local loop_guard_triggered=0
  local loop_guard_exit_code="${GC_LOOP_GUARD_EXIT_CODE:-72}"
  if ! [[ "$loop_guard_exit_code" =~ ^[0-9]+$ ]]; then
    loop_guard_exit_code=72
  fi

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --story|--from-story) story_filter="$2"; shift 2;;
      --from-task|--fresh-from|--task)
        start_task_ref="${2:-}"
        [[ -n "$start_task_ref" ]] || die "--from-task requires a task id or story:position reference"
        shift 2
        ;;
      --fresh) resume=0; shift;;
      --force)
        resume=0
        force_reset=1
        shift
        ;;
      --no-verify) no_verify=1; shift;;
      --soft-verify|--verify-soft|--verify-soft-fail)
        verify_mode="soft"
        shift
        ;;
      --verify-mode)
        verify_mode="${2:-}"
        shift 2
        ;;
      --verify-mode=*)
        verify_mode="${1#--verify-mode=}"
        shift
        ;;
      --complete-on-followup|--keep-complete-on-followup)
        complete_on_followup=1
        shift
        ;;
      --auto-push)
        export GC_AUTO_PUSH=1
        shift
        ;;
      --auto-push=*)
        export GC_AUTO_PUSH=1
        local auto_push_arg="${1#--auto-push=}"
        if [[ "$auto_push_arg" == *":"* ]]; then
          export GC_AUTO_PUSH_REMOTE="${auto_push_arg%%:*}"
          export GC_AUTO_PUSH_BRANCH="${auto_push_arg#*:}"
        elif [[ "$auto_push_arg" == */* ]]; then
          export GC_AUTO_PUSH_REMOTE="${auto_push_arg%%/*}"
          export GC_AUTO_PUSH_BRANCH="${auto_push_arg#*/}"
        else
          export GC_AUTO_PUSH_REMOTE="$auto_push_arg"
        fi
        shift
        ;;
      --keep-artifacts) keep_artifacts=1; shift;;
      --memory-cycle) memory_cycle=1; shift;;
      --batch-size) batch_size="${2:-0}"; shift 2;;
      --sleep-between) sleep_between="${2:-0}"; shift 2;;
      --context-lines)
        context_lines="${2:-}"
        shift 2
        ;;
      --context-none)
        context_lines=0
        shift
        ;;
      --context-file-lines)
        context_file_lines="${2:-}"
        shift 2
        ;;
      --context-skip)
        context_skip_patterns+=("$2")
        shift 2
        ;;
      --prompt-compact)
        prompt_compact=1
        shift
        ;;
      --prompt-expanded)
        prompt_compact=0
        shift
        ;;
      --context-doc-snippets|--doc-snippets)
        doc_snippets=1
        shift
        ;;
      --no-context-doc-snippets|--no-doc-snippets)
        doc_snippets=0
        shift
        ;;
      --plan-max-out)
        override_plan_max_out="${2:-}"
        shift 2
        ;;
      --status-max-out)
        override_status_max_out="${2:-}"
        shift 2
        ;;
      --verify-max-out)
        override_verify_max_out="${2:-}"
        shift 2
        ;;
      --patch-max-out)
        override_patch_max_out="${2:-}"
        shift 2
        ;;
      --out-hard-cap)
        override_hard_cap="${2:-}"
        shift 2
        ;;
      --sample-lines)
        sample_lines="${2:-}"
        shift 2
        ;;
      --max-tokens)
        override_max_tokens="${2:-}"
        shift 2
        ;;
      --soft-limit)
        override_soft_limit="${2:-}"
        shift 2
        ;;
      --reserve-output)
        override_reserved_output="${2:-}"
        shift 2
        ;;
      --stop-on-overbudget)
        override_stop_on_overbudget="true"
        shift
        ;;
      --stop-on-overbudget=*)
        override_stop_on_overbudget="${1#*=}"
        shift
        ;;
      --no-stop-on-overbudget)
        override_stop_on_overbudget="false"
        shift
        ;;
      --max-tokens-per-stage)
        stage_limit_overrides+=("${2:-}")
        shift 2
        ;;
      --auto-abandon-top-offenders)
        auto_abandon_override="true"
        shift
        ;;
      --no-auto-abandon-top-offenders)
        auto_abandon_override="false"
        shift
        ;;
      --binder-ttl)
        binder_ttl_seconds=$(gc_parse_duration_seconds "${2:-}" "$binder_ttl_seconds")
        shift 2
        ;;
      --binder-max-size)
        binder_max_size_bytes=$(gc_parse_size_bytes "${2:-}" "$binder_max_size_bytes")
        shift 2
        ;;
      --binder-clear-on-migration)
        binder_clear_on_migration=1
        shift
        ;;
      --no-binder)
        binder_enabled=0
        shift
        ;;
      --binder-enabled)
        binder_enabled=1
        shift
        ;;
      --diff-repeat-limit)
        diff_repeat_limit_override="${2:-}"
        [[ -n "$diff_repeat_limit_override" ]] || die "--diff-repeat-limit requires a value"
        [[ "$diff_repeat_limit_override" =~ ^[0-9]+$ ]] || die "Invalid --diff-repeat-limit value: ${diff_repeat_limit_override}"
        export GC_CODEX_DIFF_REPEAT_LIMIT="$diff_repeat_limit_override"
        shift 2
        ;;
      --idle-timeout)
        idle_timeout="${2:-}"
        shift 2
        ;;
      --)
        shift
        break
        ;;
      -h|--help)
        cat <<'EOHELP'
Usage: gpt-creator work-on-tasks [options]

Execute tasks from the project SQLite backlog using Codex, with resumable progress.

Options:
  --project PATH       Project root (defaults to current directory)
  --story ID|SLUG      Start from the matching story id or slug (inclusive)
  --from-task REF      Resume from task REF (task id or story-slug:position) and continue forward
  --fresh              Ignore saved progress and start from the first story
  --no-verify          Skip running gpt-creator verify after tasks complete
  --soft-verify        Allow verify failures but continue the run (soft gate)
  --verify-mode MODE   Set verify policy: hard (default), soft, or skip
  --complete-on-followup
                        Keep task status as complete even when follow-up notes are recorded
  --auto-push[=REMOTE/BRANCH]
                        Push commits after each successful task (defaults to upstream or origin/current)
  --keep-artifacts     Retain Codex prompt/output files for each task (default cleans up)
  --memory-cycle       Process one task at a time, prune caches, and auto-resume to limit memory usage
  --batch-size NUM     Process at most NUM tasks this run, then pause (default: unlimited)
  --sleep-between SEC  Sleep SEC seconds between tasks (default: 0)
  --context-lines NUM  Include the last NUM lines of shared context in each prompt (default: 400)
  --context-none       Skip attaching shared context to task prompts
  --context-file-lines NUM
                        Limit each shared-context file to NUM lines before summarising (default: 200)
  --context-skip GLOB  Ignore matching files when building shared context (repeatable)
  --prompt-compact     Use the compact instruction/schema block (default setting)
  --prompt-expanded    Restore the legacy verbose instruction/schema block
  --context-doc-snippets
                        (default) Pull scoped excerpts for referenced docs/endpoints when available
  --no-context-doc-snippets
                        Disable doc-snippet mode and include staged docs verbatim
  --plan-max-out NUM    Max output tokens for planning/status updates (default: 450; respects config/overrides)
  --status-max-out NUM  Max output tokens for per-task status notes (default: 350)
  --verify-max-out NUM  Max output tokens for verification summaries (default: 500)
  --patch-max-out NUM   Max output tokens for code/diff responses (default: 7000)
  --out-hard-cap NUM    Absolute ceiling for any Codex response (default: 12000)
  --sample-lines NUM   Include at most NUM chunks of minified sample payloads (default: 0; increase to view raw content)
  --max-tokens NUM     Override per-task hard token limit (defaults to model context minus reserved output)
  --soft-limit RATIO   Override soft budget ratio (0 < ratio ≤ 1, default: 0.85)
  --reserve-output NUM Reserve NUM tokens for model output (default: 1024)
  --stop-on-overbudget[=true|false]
                        Halt the run when a prompt exceeds the hard budget (default: true)
  --max-tokens-per-stage stage=NUM
                        Override a specific stage budget (e.g. --max-tokens-per-stage patch=15000)
  --auto-abandon-top-offenders
                        Force auto-abandon when prior runs exceeded stage budgets regardless of config
  --no-auto-abandon-top-offenders
                        Disable auto-abandon for offender stages even if configured
  --binder-ttl DUR     Retain binder cache for duration (e.g. 7d, 12h; default: 7d)
  --binder-max-size SIZE
                        Maximum on-disk size for binder cache (supports KB/MB/GB suffixes; default: 200MB)
  --binder-clear-on-migration
                        Drop cached binders when the task migration epoch changes
  --no-binder          Disable binder cache reuse for this run
  --diff-repeat-limit NUM
                        Override Codex diff repeat guard threshold (default: GC_CODEX_DIFF_REPEAT_LIMIT or 6)
  --idle-timeout SEC   Abort the run if no forward progress occurs for SEC seconds (default: 0 → disabled)
EOHELP
        return 0
        ;;
      *)
        die "Unknown work-on-tasks option: ${1}"
        ;;
    esac
  done

  case "${verify_mode,,}" in
    ""|hard)
      verify_mode="hard"
      ;;
    soft)
      ;;
    skip)
      no_verify=1
      verify_mode="skip"
      ;;
    *)
      die "Invalid --verify-mode value: ${verify_mode}"
      ;;
  esac

  GC_WOT_SOFT_VERIFY=0
  if [[ "$verify_mode" == "soft" ]]; then
    GC_WOT_SOFT_VERIFY=1
  fi
  GC_WOT_SOFT_VERIFY_TRIGGERED=0
  GC_WOT_COMPLETE_ON_FOLLOWUP="$complete_on_followup"

  GC_COMPLETE_ON_VERIFY="${GC_COMPLETE_ON_VERIFY:-1}"
  GC_ALLOW_EMPTY_COMMIT="${GC_ALLOW_EMPTY_COMMIT:-1}"
  GC_AUTO_PUSH_MAIN="${GC_AUTO_PUSH_MAIN:-1}"
  GC_RETRY_PUSH_MAX="${GC_RETRY_PUSH_MAX:-3}"
  GC_FORCE_DIFF_ON_VERIFY_FAIL="${GC_FORCE_DIFF_ON_VERIFY_FAIL:-1}"
  GC_LAST_VERIFY_STATUS="skipped"
  GC_LAST_VERIFY_SUMMARY=""
  GC_LAST_VERIFY_REPORT=""
  GC_LAST_VERIFY_DETAILS=""
  GC_LAST_AUTO_PUSH_ERROR=""
  if [[ -z "${GC_VERIFY_CONFIG:-}" ]]; then
    GC_VERIFY_CONFIG=""
  fi

  if [[ "${GC_AUTO_PUSH_MAIN:-1}" == "1" && -z "${GC_AUTO_PUSH_BRANCH:-}" ]]; then
    GC_AUTO_PUSH_BRANCH="main"
  fi

  if (( binder_enabled )); then
    export GC_BINDER_ENABLED=1
  else
    export GC_BINDER_ENABLED=0
  fi
  export GC_BINDER_TTL_SECONDS="$binder_ttl_seconds"
  export GC_BINDER_MAX_BYTES="$binder_max_size_bytes"
  export GC_BINDER_CLEAR_ON_MIGRATION="$binder_clear_on_migration"

  if [[ $# -gt 0 ]]; then
    die "Unexpected argument for work-on-tasks: ${1}"
  fi

  [[ "$batch_size" =~ ^[0-9]+$ ]] || die "Invalid --batch-size value: ${batch_size}"
  if ! sleep_between="$(gc_wot_normalize_sleep_between "$python_bin" "$sleep_between")"; then
    die "Invalid --sleep-between value: ${sleep_between}"
  fi
  [[ "$context_lines" =~ ^[0-9]+$ ]] || die "Invalid --context-lines value: ${context_lines}"
  [[ "$context_file_lines" =~ ^[0-9]+$ ]] || die "Invalid --context-file-lines value: ${context_file_lines}"
  [[ "$sample_lines" =~ ^-?[0-9]+$ ]] || die "Invalid --sample-lines value: ${sample_lines}"
  if [[ -z "$idle_timeout" ]]; then
    idle_timeout=0
  elif [[ "$idle_timeout" =~ ^[0-9]+$ ]]; then
    :
  else
    die "Invalid --idle-timeout value: ${idle_timeout}"
  fi
  batch_size=$((batch_size))
  context_lines=$((context_lines))
  context_file_lines=$((context_file_lines))
  sample_lines=$((sample_lines))
  idle_timeout=$((idle_timeout))
  (( sample_lines >= 0 )) || die "--sample-lines must be zero or positive (got ${sample_lines})"
  if [[ "$sleep_between" != "0" ]]; then
    sleep_between_positive=1
  fi

  backlog_guard_window_value="$backlog_guard_window_days"
  if [[ -z "$backlog_guard_window_value" ]]; then
    backlog_guard_window_value="7"
  elif [[ "$backlog_guard_window_value" =~ ^[0-9]+([.][0-9]+)?$ ]]; then
    :
  else
    warn "GC_BACKLOG_GUARD_WINDOW_DAYS ('${backlog_guard_window_days}') is invalid; defaulting to 7."
    backlog_guard_window_value="7"
  fi
  if [[ -z "$backlog_guard_wip_limit" || ! "$backlog_guard_wip_limit" =~ ^[0-9]+$ ]]; then
    warn "GC_BACKLOG_GUARD_WIP_LIMIT ('${backlog_guard_wip_limit}') is invalid; defaulting to 12."
    backlog_guard_wip_limit="12"
  fi

  if [[ -n "$override_max_tokens" && ! "$override_max_tokens" =~ ^[0-9]+$ ]]; then
    die "Invalid --max-tokens value: ${override_max_tokens}"
  fi
  if [[ -n "$override_soft_limit" && ! "$override_soft_limit" =~ ^[0-9]*\.?[0-9]+$ ]]; then
    die "Invalid --soft-limit value: ${override_soft_limit}"
  fi
  if [[ -n "$override_reserved_output" && ! "$override_reserved_output" =~ ^[0-9]+$ ]]; then
    die "Invalid --reserve-output value: ${override_reserved_output}"
  fi
  if ((${#stage_limit_overrides[@]})); then
    local entry
    for entry in "${stage_limit_overrides[@]}"; do
      [[ -n "$entry" && "$entry" == *"="* ]] || die "--max-tokens-per-stage expects stage=value (received: ${entry})"
      local stage_name="${entry%%=*}"
      local stage_value="${entry#*=}"
      [[ -n "$stage_name" ]] || die "--max-tokens-per-stage requires a stage name"
      [[ "$stage_value" =~ ^[0-9]+$ ]] || die "Invalid stage limit '${entry}'; value must be numeric"
    done
  fi
  if [[ -n "$override_plan_max_out" && ! "$override_plan_max_out" =~ ^[0-9]+$ ]]; then
    die "Invalid --plan-max-out value: ${override_plan_max_out}"
  fi
  if [[ -n "$override_status_max_out" && ! "$override_status_max_out" =~ ^[0-9]+$ ]]; then
    die "Invalid --status-max-out value: ${override_status_max_out}"
  fi
  if [[ -n "$override_verify_max_out" && ! "$override_verify_max_out" =~ ^[0-9]+$ ]]; then
    die "Invalid --verify-max-out value: ${override_verify_max_out}"
  fi
  if [[ -n "$override_patch_max_out" && ! "$override_patch_max_out" =~ ^[0-9]+$ ]]; then
    die "Invalid --patch-max-out value: ${override_patch_max_out}"
  fi
  if [[ -n "$override_hard_cap" && ! "$override_hard_cap" =~ ^[0-9]+$ ]]; then
    die "Invalid --out-hard-cap value: ${override_hard_cap}"
  fi
  local stop_override_normalized=""
  if [[ -n "$override_stop_on_overbudget" ]]; then
    stop_override_normalized="${override_stop_on_overbudget,,}"
    case "$stop_override_normalized" in
      true|false|0|1|yes|no|on|off|t|f|y|n) ;;
      *) die "Invalid --stop-on-overbudget value: ${override_stop_on_overbudget}" ;;
    esac
  fi

  if [[ -n "$override_max_tokens" ]]; then
    export GC_PER_TASK_HARD_LIMIT_OVERRIDE="$override_max_tokens"
  else
    unset GC_PER_TASK_HARD_LIMIT_OVERRIDE
  fi
  if [[ -n "$override_soft_limit" ]]; then
    export GC_PER_TASK_SOFT_RATIO_OVERRIDE="$override_soft_limit"
  else
    unset GC_PER_TASK_SOFT_RATIO_OVERRIDE
  fi
  if [[ -n "$override_reserved_output" ]]; then
    export GC_PER_TASK_MIN_OUTPUT_OVERRIDE="$override_reserved_output"
  else
    unset GC_PER_TASK_MIN_OUTPUT_OVERRIDE
  fi
  if [[ -n "$stop_override_normalized" ]]; then
    export GC_STOP_ON_OVERBUDGET_OVERRIDE="$stop_override_normalized"
  else
    unset GC_STOP_ON_OVERBUDGET_OVERRIDE
  fi
  if (( idle_timeout > 0 )); then
    info "Idle watchdog active → ${idle_timeout}s without progress."
  fi
  if [[ -n "$diff_repeat_limit_override" ]]; then
    info "Codex diff repeat guard limit → ${diff_repeat_limit_override} diff(s)."
  fi

  local diff_guard_stall_limit="${GC_DIFF_GUARD_STALL_LIMIT:-1}"
  local diff_guard_token_threshold="${GC_DIFF_GUARD_TOKEN_THRESHOLD:-12}"
  local diff_guard_stdout_slice="${GC_DIFF_GUARD_STDOUT_SLICE:-2048}"
  local diff_guard_file_cooldown="${GC_DIFF_GUARD_FILE_COOLDOWN:-2}"
  local diff_guard_file_min_bytes="${GC_DIFF_GUARD_FILE_MIN_BYTES:-120}"
  local diff_guard_turn_repeat_limit="${GC_DIFF_GUARD_TURN_REPEAT_LIMIT:-3}"
  local diff_guard_history_limit="${GC_DIFF_GUARD_HISTORY_LIMIT:-200}"
  if ! [[ "$diff_guard_stall_limit" =~ ^-?[0-9]+$ ]]; then
    diff_guard_stall_limit=1
  fi
  if ! [[ "$diff_guard_token_threshold" =~ ^[0-9]+$ ]]; then
    diff_guard_token_threshold=12
  fi
  if ! [[ "$diff_guard_stdout_slice" =~ ^[0-9]+$ ]]; then
    diff_guard_stdout_slice=2048
  fi
  if ! [[ "$diff_guard_file_cooldown" =~ ^[0-9]+$ ]]; then
    diff_guard_file_cooldown=2
  fi
  if ! [[ "$diff_guard_file_min_bytes" =~ ^[0-9]+$ ]]; then
    diff_guard_file_min_bytes=120
  fi
  if ! [[ "$diff_guard_turn_repeat_limit" =~ ^[0-9]+$ ]]; then
    diff_guard_turn_repeat_limit=3
  fi
  if ! [[ "$diff_guard_history_limit" =~ ^[0-9]+$ ]]; then
    diff_guard_history_limit=200
  fi
  export GC_DIFF_GUARD_STDOUT_SLICE="$diff_guard_stdout_slice"
  local diff_guard_history=""
  local diff_guard_global_attempt=0

  if [[ -n "$start_task_ref" && $resume -eq 0 ]]; then
    info "--fresh ignored when --from-task is provided; resuming from the specified task instead."
    resume=1
  fi

  if (( memory_cycle )); then
    if (( batch_size == 0 || batch_size > 1 )); then
      info "Memory-cycle enabled; forcing --batch-size 1 for iterative runs."
    fi
    batch_size=1
  fi

  gc_auto_clean_dirty_tree() {
    local git_root="${PROJECT_ROOT:-$PWD}"
    if ! command -v git >/dev/null 2>&1; then
      warn "Auto-clean skipped: git command unavailable."
      return 1
    fi
    if [[ -z "$git_root" || ! -d "$git_root" ]]; then
      warn "Auto-clean skipped: project root unavailable."
      return 1
    fi
    if ! (cd "$git_root" && git rev-parse --is-inside-work-tree >/dev/null 2>&1); then
      warn "Auto-clean skipped: not inside a git repository."
      return 1
    fi

    local dirty_status
    dirty_status="$(cd "$git_root" && git status --porcelain=v1 --untracked-files=all 2>/dev/null || true)"
    if [[ -z "$dirty_status" ]]; then
      return 0
    fi

    local snapshot_timestamp snapshot_label stash_output post_status
    snapshot_timestamp="$(date -u +%Y-%m-%dT%H:%M:%SZ)"
    snapshot_label="work-on-tasks auto snapshot ${snapshot_timestamp}"

    local commit_message="chore(gpt-creator): auto snapshot before work-on-tasks ${snapshot_timestamp}"
    info "Dirty working tree detected; attempting auto snapshot commit before run."
    if (cd "$git_root" && git add --all >/dev/null 2>&1); then
      if (cd "$git_root" && git diff --cached --quiet); then
        info "Working tree clean after staging; nothing to snapshot."
        return 0
      fi
      if (cd "$git_root" && GIT_AUTHOR_NAME="gpt-creator automation" GIT_AUTHOR_EMAIL="automation@gpt-creator" GIT_COMMITTER_NAME="gpt-creator automation" GIT_COMMITTER_EMAIL="automation@gpt-creator" git commit -m "$commit_message" >/dev/null 2>&1); then
        local commit_hash
        commit_hash="$(cd "$git_root" && git rev-parse HEAD 2>/dev/null | head -n1)"
        if [[ -n "$commit_hash" ]]; then
          info "Auto commit created (${commit_message}) [${commit_hash:0:7}]."
        else
          info "Auto commit created (${commit_message})."
        fi
        return 0
      fi
      warn "Auto commit snapshot failed; falling back to stash."
      if (cd "$git_root" && git rev-parse --verify HEAD >/dev/null 2>&1); then
        (cd "$git_root" && git reset --mixed HEAD >/dev/null 2>&1 || true)
      else
        (cd "$git_root" && git reset --mixed >/dev/null 2>&1 || true)
      fi
    else
      warn "Auto snapshot staging failed; falling back to stash."
    fi

    info "Attempting auto stash before run."
    if stash_output="$(cd "$git_root" && git stash push --include-untracked --message "$snapshot_label" 2>&1)"; then
      post_status="$(cd "$git_root" && git status --porcelain=v1 --untracked-files=all 2>/dev/null || true)"
      if [[ -z "$post_status" ]]; then
        info "Auto stash created (${snapshot_label})."
        return 0
      fi
      warn "Auto stash incomplete; working tree still dirty."
    else
      stash_output="${stash_output//$'\n'/ }"
      warn "Auto stash failed; ${stash_output}"
    fi
    return 1
  }

  gc_auto_sync_i18n() {
    local project_root="${PROJECT_ROOT:-$PWD}"
    if [[ -z "$project_root" || ! -d "$project_root" ]]; then
      warn "Auto locale sync skipped: project root unavailable."
      return 1
    fi

    local sync_cmd_output=""

    if [[ -f "${project_root}/package.json" ]] && grep -q '"i18n:sync"' "${project_root}/package.json"; then
      if command -v pnpm >/dev/null 2>&1; then
        info "Attempting auto locale sync via 'pnpm run i18n:sync'."
        if sync_cmd_output="$(cd "$project_root" && pnpm run i18n:sync 2>&1)"; then
          info "Locale sync completed (pnpm)."
          return 0
        fi
        warn "Locale sync via pnpm failed: ${sync_cmd_output//$'\n'/ }"
      fi

      if command -v npm >/dev/null 2>&1; then
        info "Attempting auto locale sync via 'npm run i18n:sync'."
        if sync_cmd_output="$(cd "$project_root" && npm run i18n:sync --silent 2>&1)"; then
          info "Locale sync completed (npm)."
          return 0
        fi
        warn "Locale sync via npm failed: ${sync_cmd_output//$'\n'/ }"
      fi
    fi

    local i18n_script="${CLI_ROOT}/scripts/i18n-sync.js"
    if [[ -f "$i18n_script" ]] && command -v node >/dev/null 2>&1; then
      info "Attempting auto locale sync via local i18n-sync.js."
      if sync_cmd_output="$(cd "$project_root" && node "$i18n_script" 2>&1)"; then
        info "Locale sync completed (node)."
        return 0
      fi
      warn "Locale sync via node script failed: ${sync_cmd_output//$'\n'/ }"
    fi

    warn "Auto locale sync failed; tooling unavailable or commands returned errors."
    return 1
  }

  ensure_ctx "$root"

  local preflight_script="${CLI_ROOT}/scripts/preflight_git_blockers.sh"
  local auto_conflict_script="${CLI_ROOT}/scripts/auto_resolve_conflicts.sh"
  local preflight_conflict_autofix_attempted=0
  local preflight_dirty_tree_autofix_attempts=0
  local preflight_dirty_tree_autofix_limit="${GC_PREFLIGHT_DIRTY_TREE_AUTO_FIX_LIMIT:-3}"
  if ! [[ "$preflight_dirty_tree_autofix_limit" =~ ^[0-9]+$ ]]; then
    preflight_dirty_tree_autofix_limit=3
  fi
  if [[ -x "$preflight_script" ]]; then
    while true; do
      local preflight_output="" preflight_code=0 preflight_outcome=""
      if [[ -n "${PROJECT_ROOT:-}" ]]; then
        set +e
        preflight_output="$(cd "$PROJECT_ROOT" && "$preflight_script")"
        preflight_code=$?
        set -e
      else
        set +e
        preflight_output="$("$preflight_script")"
        preflight_code=$?
        set -e
      fi
      preflight_outcome="${preflight_output//[$'\r\n']}"
      if [[ -z "$preflight_outcome" ]]; then
        if (( preflight_code == 3 )); then
          preflight_outcome="blocked-merge-conflict"
        elif (( preflight_code == 2 )); then
          preflight_outcome="blocked-dirty-tree"
        else
          preflight_outcome="ok"
        fi
      fi
      case "$preflight_outcome" in
        blocked-merge-conflict)
          if (( preflight_conflict_autofix_attempted == 0 )) && [[ -x "$auto_conflict_script" ]]; then
            info "Merge artifacts detected; attempting automatic cleanup."
            if [[ -n "${PROJECT_ROOT:-}" ]]; then
              (cd "$PROJECT_ROOT" && "$auto_conflict_script" "$PROJECT_ROOT") || true
            else
              "$auto_conflict_script" "$PWD" || true
            fi
            preflight_conflict_autofix_attempted=1
            continue
          fi
          warn "Git merge conflicts detected; resolve before running work-on-tasks."
          printf '%s\n' "$preflight_outcome"
          return 3
          ;;
        blocked-dirty-tree)
          if (( preflight_dirty_tree_autofix_attempts < preflight_dirty_tree_autofix_limit )); then
            ((preflight_dirty_tree_autofix_attempts+=1))
            if gc_auto_clean_dirty_tree; then
              continue
            fi
          fi
          warn "Dirty working tree detected; commit or stash changes before running work-on-tasks."
          printf '%s\n' "$preflight_outcome"
          return 2
          ;;
        ok)
          break
          ;;
        *)
          if (( preflight_code != 0 )); then
            warn "Preflight git blockers returned '${preflight_outcome:-?}' (exit ${preflight_code}); treating as dirty tree."
            printf '%s\n' "${preflight_outcome:-blocked-dirty-tree}"
            return 2
          fi
          break
          ;;
      esac
    done
  fi

  local i18n_guard_script="${CLI_ROOT}/scripts/preflight_i18n_guard.sh"
  if [[ -x "$i18n_guard_script" ]]; then
    local i18n_guard_output="" i18n_guard_code=0 i18n_guard_outcome=""
    local i18n_guard_autofix_attempts=0
    local i18n_guard_autofix_limit="${GC_PREFLIGHT_I18N_AUTO_SYNC_LIMIT:-1}"
    if ! [[ "$i18n_guard_autofix_limit" =~ ^[0-9]+$ ]]; then
      i18n_guard_autofix_limit=1
    fi
    while true; do
      if [[ -n "${PROJECT_ROOT:-}" ]]; then
        set +e
        i18n_guard_output="$(cd "$PROJECT_ROOT" && "$i18n_guard_script")"
        i18n_guard_code=$?
        set -e
      else
        set +e
        i18n_guard_output="$("$i18n_guard_script")"
        i18n_guard_code=$?
        set -e
      fi
      i18n_guard_outcome="${i18n_guard_output//[$'\r\n']}"
      if [[ -z "$i18n_guard_outcome" ]]; then
        if (( i18n_guard_code == 6 )); then
          i18n_guard_outcome="blocked-dependency(i18n_sync_required)"
        elif (( i18n_guard_code == 3 )); then
          i18n_guard_outcome="blocked-merge-conflict"
        else
          i18n_guard_outcome="ok"
        fi
      fi
      case "$i18n_guard_outcome" in
        blocked-merge-conflict)
          warn "Locale merge conflicts detected; resolve locale .rej files before running work-on-tasks."
          printf '%s\n' "$i18n_guard_outcome"
          return 3
          ;;
        'blocked-dependency(i18n_sync_required)')
          if (( i18n_guard_autofix_attempts < i18n_guard_autofix_limit )); then
            ((i18n_guard_autofix_attempts+=1))
            if gc_auto_sync_i18n; then
              continue
            fi
          fi
          warn "Locale files out of sync; run 'pnpm i18n:sync' to regenerate translations."
          printf '%s\n' "$i18n_guard_outcome"
          return 6
          ;;
        blocked-i18n-guard-error)
          warn "i18n preflight guard failed; inspect scripts/preflight_i18n_guard.sh output."
          printf '%s\n' "$i18n_guard_outcome"
          return 6
          ;;
        ok)
          break
          ;;
        *)
          if (( i18n_guard_code != 0 )); then
            warn "i18n preflight guard returned '${i18n_guard_outcome:-?}' (exit ${i18n_guard_code}); treating as guard failure."
            printf '%s\n' "blocked-i18n-guard-error"
            return 6
          fi
          break
          ;;
      esac
    done
  fi

  local schema_guard_script="${CLI_ROOT}/scripts/preflight_prisma_guard.sh"
  if [[ -x "$schema_guard_script" ]]; then
    local schema_guard_output="" schema_guard_code=0 schema_guard_outcome=""
    if [[ -n "${PROJECT_ROOT:-}" ]]; then
      set +e
      schema_guard_output="$(cd "$PROJECT_ROOT" && "$schema_guard_script")"
      schema_guard_code=$?
      set -e
    else
      set +e
      schema_guard_output="$("$schema_guard_script")"
      schema_guard_code=$?
      set -e
    fi
    schema_guard_outcome="${schema_guard_output//[$'\r\n']}"
    if [[ -z "$schema_guard_outcome" ]]; then
      if (( schema_guard_code == 4 )); then
        schema_guard_outcome="blocked-schema-drift"
      elif (( schema_guard_code == 5 )); then
        schema_guard_outcome="blocked-schema-guard-error"
      else
        schema_guard_outcome="ok"
      fi
    fi
    case "$schema_guard_outcome" in
      blocked-schema-drift)
        warn "Prisma schema drift detected; align prisma/schema.prisma with migrations before running work-on-tasks."
        printf '%s\n' "$schema_guard_outcome"
        return 4
        ;;
      blocked-schema-guard-error)
        warn "Prisma schema guard failed; inspect prisma migrate diff output and rerun."
        printf '%s\n' "$schema_guard_outcome"
        return 4
        ;;
      ok)
        ;;
      *)
        if (( schema_guard_code != 0 )); then
          warn "Prisma schema guard returned '${schema_guard_outcome:-?}' (exit ${schema_guard_code}); treating as guard failure."
          printf '%s\n' "blocked-schema-guard-error"
          return 4
        fi
        ;;
    esac
  fi

  local budget_cfg_json="{}"
  local budget_loader
  if budget_loader="$(gc_clone_python_tool "load_budget_config.py" "$PROJECT_ROOT" 2>/dev/null)"; then
    budget_cfg_json="$("$python_bin" "$budget_loader" "$PROJECT_ROOT" 2>/dev/null || echo '{}')"
  fi

  local budget_stage_json
  local budget_stage_helper
  if budget_stage_helper="$(gc_clone_python_tool "budget_stage_from_config.py" "${PROJECT_ROOT:-$PWD}")"; then
    budget_stage_json="$("$python_bin" "$budget_stage_helper" "$budget_cfg_json" 2>/dev/null || echo '{}')"
  else
    budget_stage_json="{}"
  fi

  if ((${#stage_limit_overrides[@]} > 0)); then
    local budget_stage_override_helper
    if budget_stage_override_helper="$(gc_clone_python_tool "budget_stage_apply_overrides.py" "${PROJECT_ROOT:-$PWD}")"; then
      budget_stage_json="$("$python_bin" "$budget_stage_override_helper" "$budget_stage_json" "${stage_limit_overrides[@]}" 2>/dev/null || echo '{}')"
    fi
  fi

  local budget_stage_iter_helper
  budget_stage_iter_helper="$(gc_clone_python_tool "budget_stage_iter.py" "${PROJECT_ROOT:-$PWD}")" || budget_stage_iter_helper=""

  if [[ -n "$budget_stage_iter_helper" ]]; then
    while IFS=$'\t' read -r stage_name stage_limit_value; do
      gc_budget_set_stage_limit "$stage_name" "$stage_limit_value"
    done < <("$python_bin" "$budget_stage_iter_helper" "$budget_stage_json")
  fi

  gc_budget_reset_stage_tracking

  local budget_off_json="{}"
  local budget_off_helper
  if budget_off_helper="$(gc_clone_python_tool "budget_offenders_from_config.py" "${PROJECT_ROOT:-$PWD}")"; then
    budget_off_json="$("$python_bin" "$budget_off_helper" "$budget_cfg_json" 2>/dev/null || echo '{}')"
  fi
  [[ -n "$budget_off_json" ]] || budget_off_json="{}"

  local budget_offender_window=10
  local budget_offender_top_k=3
  local budget_offender_dom=0.5
  local budget_auto_abandon_default=1
  local budget_offender_actions_json="{}"
  local budget_offender_meta_helper
  if budget_offender_meta_helper="$(gc_clone_python_tool "budget_offenders_meta.py" "${PROJECT_ROOT:-$PWD}")"; then
    local offender_meta
    offender_meta="$("$python_bin" "$budget_offender_meta_helper" "$budget_off_json" 2>/dev/null)"
    if [[ -n "$offender_meta" ]]; then
      IFS=$'\t' read -r budget_offender_window budget_offender_top_k budget_offender_dom budget_auto_abandon_default budget_offender_actions_json <<<"$offender_meta"
    fi
  fi
  budget_offender_window=${budget_offender_window:-10}
  budget_offender_top_k=${budget_offender_top_k:-3}
  budget_offender_dom=${budget_offender_dom:-0.5}
  budget_auto_abandon_default=${budget_auto_abandon_default:-1}
  [[ -n "$budget_offender_actions_json" ]] || budget_offender_actions_json="{}"

  local auto_abandon_flag=$budget_auto_abandon_default
  if [[ "$auto_abandon_override" == "true" ]]; then
    auto_abandon_flag=1
  elif [[ "$auto_abandon_override" == "false" ]]; then
    auto_abandon_flag=0
  fi
  if gc_env_truthy "${GC_DISABLE_AUTO_ABANDON:-}"; then
    auto_abandon_flag=0
  fi

  local budget_usage_file="${LOG_DIR:-${PROJECT_ROOT:-$PWD}/.gpt-creator/logs}/codex-usage.ndjson"
  local budget_offenders_json="{}"
  local offenders_helper
  if offenders_helper="$(gc_clone_python_tool "budget_offenders.py" "$PROJECT_ROOT" 2>/dev/null)"; then
    local -a offender_args=("$offenders_helper" --usage-file "$budget_usage_file" --per-stage-json "$budget_stage_json" --actions-json "$budget_offender_actions_json" --window-runs "$budget_offender_window" --top-k "$budget_offender_top_k" --dominance-threshold "$budget_offender_dom")
    if (( auto_abandon_flag )); then
      offender_args+=(--auto-abandon)
    else
      offender_args+=(--no-auto-abandon)
    fi
    budget_offenders_json="$("$python_bin" "${offender_args[@]}" 2>/dev/null || echo '{}')"
  fi

  local last_offender_run=""
  local offenders_auto_flag=$auto_abandon_flag
  if gc_env_truthy "${GC_DISABLE_AUTO_ABANDON:-}"; then
    offenders_auto_flag=0
  fi
  if [[ -n "$budget_offenders_json" && "$budget_offenders_json" != "{}" ]]; then
    local budget_off_iter_helper
    budget_off_iter_helper="$(gc_clone_python_tool "budget_offenders_iter.py" "${PROJECT_ROOT:-$PWD}")" || budget_off_iter_helper=""
    if [[ -n "$budget_off_iter_helper" ]]; then
      while IFS=$'\t' read -r kind value1 value2 value3 value4; do
        case "$kind" in
          AUTO)
            offenders_auto_flag="${value1:-0}"
            ;;
          RUN)
            last_offender_run="$value1"
            ;;
          STAGE)
            if [[ "$offenders_auto_flag" == "1" ]]; then
              gc_budget_set_stage_skip "$value1" 1 "auto-abandon:${last_offender_run:-previous}"
            fi
            ;;
          TOOL)
            local tool_key
            tool_key="$(gc_budget_stage_id "$value1")"
            local tool_action_var="GC_BUDGET_TOOL_ACTION_${tool_key^^}"
            printf -v "$tool_action_var" '%s' "$value4"
            local tool_bytes_var="GC_BUDGET_TOOL_BYTES_${tool_key^^}"
            printf -v "$tool_bytes_var" '%s' "$value2"
            local tool_share_var="GC_BUDGET_TOOL_SHARE_${tool_key^^}"
            printf -v "$tool_share_var" '%s' "$value3"
            ;;
        esac
      done < <("$python_bin" "$budget_off_iter_helper" "$budget_offenders_json")
    fi
  fi

  if [[ "${GC_BUDGET_TOOL_ACTION_SHOW_FILE:-}" == "range-only" ]]; then
    export GC_SHOW_FILE_FORCE_RANGE=1
  fi
  if [[ "${GC_BUDGET_TOOL_ACTION_RG:-}" == "narrow" ]]; then
    export GC_RG_NARROW=1
  fi
  if [[ "${GC_BUDGET_TOOL_ACTION_TESTS:-}" == "summary" ]]; then
    export GC_TESTS_SUMMARY=1
  fi

  export GC_BUDGET_STAGE_LIMITS_JSON="$budget_stage_json"
  export GC_BUDGET_TOOL_ACTIONS_JSON="$budget_offender_actions_json"

  gc_load_llm_output_limits "${PROJECT_ROOT:-$PWD}" || true
  if [[ -n "$override_hard_cap" ]]; then
    gc_set_llm_output_limit_if_valid GC_LLM_OUTPUT_LIMIT_HARD_CAP "$override_hard_cap"
  fi
  if [[ -n "$override_plan_max_out" ]]; then
    gc_set_llm_output_limit_if_valid GC_LLM_OUTPUT_LIMIT_PLAN "$override_plan_max_out"
  fi
  if [[ -n "$override_status_max_out" ]]; then
    gc_set_llm_output_limit_if_valid GC_LLM_OUTPUT_LIMIT_STATUS "$override_status_max_out"
  fi
  if [[ -n "$override_verify_max_out" ]]; then
    gc_set_llm_output_limit_if_valid GC_LLM_OUTPUT_LIMIT_VERIFY "$override_verify_max_out"
  fi
  if [[ -n "$override_patch_max_out" ]]; then
    gc_set_llm_output_limit_if_valid GC_LLM_OUTPUT_LIMIT_PATCH "$override_patch_max_out"
  fi

  info "LLM output limits → plan=${GC_LLM_OUTPUT_LIMIT_PLAN}, status=${GC_LLM_OUTPUT_LIMIT_STATUS}, verify=${GC_LLM_OUTPUT_LIMIT_VERIFY}, patch=${GC_LLM_OUTPUT_LIMIT_PATCH}, hard_cap=${GC_LLM_OUTPUT_LIMIT_HARD_CAP}"

  local tasks_dir="${PLAN_DIR}/tasks"
  mkdir -p "$tasks_dir"
  local canonical_tasks_dir="${PROJECT_ROOT:-$PWD}/.gpt-creator/staging/plan/tasks"
  mkdir -p "$canonical_tasks_dir"
  local tasks_db="${canonical_tasks_dir}/tasks.db"
  local intake_lock_path="${canonical_tasks_dir}/.intake-frozen"
  local doc_vector_path="${canonical_tasks_dir}/documentation-vector-index.sqlite"
  export GC_DOCUMENTATION_DB_PATH="$tasks_db"
  export GC_DOC_VECTOR_INDEX_PATH="$doc_vector_path"
  export GC_DOCUMENTATION_INDEX_PATH="$doc_vector_path"
  if command -v sqlite3 >/dev/null 2>&1; then
    if ! sqlite3 "$GC_DOCUMENTATION_DB_PATH" 'SELECT count(*) FROM documentation;' >/dev/null 2>&1; then
      die "Documentation database missing or invalid at ${GC_DOCUMENTATION_DB_PATH}; run 'gpt-creator scan --project \"${PROJECT_ROOT:-$PWD}\"' and retry."
    fi
  else
    warn "sqlite3 not available; unable to verify documentation database integrity."
  fi
  if ! gc_require_documentation_catalog "$PROJECT_ROOT"; then
    local catalog_root="${PROJECT_ROOT:-$PWD}"
    die "Failed to prepare documentation catalog. Run 'gpt-creator scan --project \"${catalog_root}\"' and retry."
  fi
  gc_bootstrap_docs_registry "$tasks_db"

  if ! gc_tasks_db_has_rows "$tasks_db"; then
    die "Task database missing or empty. Run 'gpt-creator create-tasks' (or create-jira-tasks + migrate-tasks) before work-on-tasks."
  fi

  local now_ts
  now_ts="$(date +%s)"
  throughput_next_checkpoint=$((now_ts + throughput_checkpoint_interval))

  local throughput_msg=""
  if throughput_msg="$(gc_update_throughput_metrics "$tasks_db" "flush")"; then
    if [[ -n "$throughput_msg" ]]; then
      info "$throughput_msg"
    fi
  else
    warn "Failed to prime throughput metrics (flush)."
  fi
  if ! gc_update_throughput_metrics "$tasks_db" "init" >/dev/null; then
    warn "Failed to start throughput metrics window."
  fi

  gc_align_task_story_slugs "$tasks_db"
  gc_sync_story_totals "$tasks_db"

  if (( force_reset )); then
    info "Resetting backlog progress to pending (--force)."
    gc_reset_task_progress "$tasks_db"
    gc_sync_story_totals "$tasks_db"
  fi

  local migration_epoch_initial=0
  if migration_epoch_initial="$(gc_fetch_migration_epoch "$tasks_db" 2>/dev/null)"; then
    :
  else
    migration_epoch_initial=0
  fi
  local migration_epoch_baseline="$migration_epoch_initial"
  local migration_transition_triggered=0

  local start_task_story_slug="" start_task_story_title="" start_task_position="" start_task_id="" start_task_title=""
  if [[ -n "$start_task_ref" ]]; then
    info "Rewinding backlog starting from task reference '${start_task_ref}'."
    local rewind_info=""
    local original_story_filter="$story_filter"
    if ! rewind_info="$(gc_rewind_backlog_from_task "$tasks_db" "$start_task_ref" "$original_story_filter")"; then
      die "Unable to rewind backlog from task reference '${start_task_ref}'."
    fi
    IFS=$'\t' read -r start_task_story_slug start_task_story_title start_task_position start_task_id start_task_title <<<"$rewind_info"
    if [[ -z "$start_task_story_slug" || -z "$start_task_position" ]]; then
      die "Invalid response while rewinding backlog; aborting."
    fi
    if [[ -n "$original_story_filter" && "${original_story_filter,,}" != "${start_task_story_slug,,}" ]]; then
      info "Normalizing story filter '${original_story_filter}' to story slug '${start_task_story_slug}'."
    fi
    story_filter="$start_task_story_slug"
    gc_sync_story_totals "$tasks_db"
    local story_display="$start_task_story_slug"
    if [[ -n "$start_task_story_title" && "${start_task_story_title,,}" != "${start_task_story_slug,,}" ]]; then
      story_display+=" — ${start_task_story_title}"
    fi
    info "Starting from task ${start_task_position} (${start_task_id:-no-id}) in story ${story_display}."
    if [[ -n "$start_task_title" ]]; then
      info "  ${start_task_title}"
    fi
  fi

  ensure_node_dependencies "$PROJECT_ROOT"
  gc_refresh_discovery_if_needed
  gc_clear_active_task

  if (( prompt_compact )); then
    export GC_PROMPT_COMPACT=1
  else
    unset GC_PROMPT_COMPACT
  fi
  if (( doc_snippets )); then
    export GC_PROMPT_DOC_SNIPPETS=1
  else
    unset GC_PROMPT_DOC_SNIPPETS
  fi

  local state_dir="${PLAN_DIR}/work"
  local runs_dir="${state_dir}/runs"
  local work_logs_root="${LOG_DIR}/work-on-tasks"
  mkdir -p "$runs_dir" "$work_logs_root"

  GC_CONTEXT_FILE_LINES="$context_file_lines"
  if ((${#context_skip_patterns[@]} > 0)); then
    GC_CONTEXT_SKIP_PATTERNS=("${context_skip_patterns[@]}")
  else
    unset GC_CONTEXT_SKIP_PATTERNS
  fi

  export GC_PROMPT_SAMPLE_LINES="$sample_lines"

  local run_stamp
  run_stamp="$(date +%Y%m%d_%H%M%S)"
  export GC_BUDGET_RUN_ID="$run_stamp"
  local run_dir="${runs_dir}/${run_stamp}"
  local run_log_dir="${work_logs_root}/${run_stamp}"
  mkdir -p "$run_dir" "$run_log_dir"

  RUN_DIR="${RUN_DIR:-$run_dir}"
  export RUN_DIR
  mkdir -p "$RUN_DIR"

  if [[ -z "${TMUX:-}${STY:-}" ]]; then
    echo "W: consider running inside tmux/screen" >&2
  fi

  export GC_HARD_TOKENS_PER_TASK="${GC_HARD_TOKENS_PER_TASK:-60000}"
  WOT_EXCLUDE_STATUSES="${WOT_EXCLUDE_STATUSES:-skip-already-complete,blocked-*,test-env-failed}"

  local backlog_guard_snapshot_output=""
  backlog_snapshot_before_path="${run_dir}/backlog-before.json"
  if backlog_guard_snapshot_output="$(gc_backlog_guard_snapshot "$tasks_db" "" "$backlog_guard_window_value" "$backlog_guard_wip_limit" 2>/dev/null)"; then
    if [[ -n "$backlog_guard_snapshot_output" ]]; then
      printf '%s\n' "$backlog_guard_snapshot_output" >"$backlog_snapshot_before_path"
      backlog_guard_enabled=1
    fi
  else
    backlog_guard_enabled=0
  fi
  local doc_catalog_path="${state_dir}/doc-catalog.json"
  if [[ ! -f "$doc_catalog_path" ]]; then
    mkdir -p "$(dirname "$doc_catalog_path")"
    printf '{"version":1,"documents":{}}\n' >"$doc_catalog_path"
  fi
  export GC_DOC_CATALOG_PATH="$doc_catalog_path"

  if gc_setup_doc_catalog_helpers; then
    export GC_DOC_CATALOG_HELPER="${GC_DOC_CATALOG_PY}"
    export GC_DOC_REGISTRY_HELPER="${GC_DOC_REGISTRY_PY}"
    export GC_DOC_INDEXER_HELPER="${GC_DOC_INDEXER_PY}"
    export doc_catalog="${GC_DOC_CATALOG_PY}"
    export doc_registry="${GC_DOC_REGISTRY_PY}"
    export doc_indexer="${GC_DOC_INDEXER_PY}"
  else
    warn "Doc helpers unavailable; header will fall back to raw sqlite3."
    unset GC_DOC_CATALOG_PY GC_DOC_REGISTRY_PY GC_DOC_INDEXER_PY
    unset GC_DOC_CATALOG_HELPER GC_DOC_REGISTRY_HELPER GC_DOC_INDEXER_HELPER
    unset doc_catalog doc_registry doc_indexer
  fi

  local last_progress_ts
  local idle_timeout_triggered=0
  local ctx_file="${run_dir}/context.md"
  gc_build_context_file "$ctx_file" "$STAGING_DIR"
  local context_tail=""
  local context_tail_mode="none"
  export GC_CONTEXT_TAIL_LIMIT="$context_lines"
  if (( context_lines > 0 )); then
    context_tail_mode="digest"
    context_tail="${run_dir}/context_digest.md"
    if ! gc_build_context_digest "$ctx_file" "$context_tail" "$context_lines"; then
      warn "Failed to build context digest; falling back to raw tail."
      context_tail_mode="raw"
      context_tail="${run_dir}/context_tail.md"
      if ! tail -n "$context_lines" "$ctx_file" >"$context_tail" 2>/dev/null; then
        cp "$ctx_file" "$context_tail"
      fi
    fi
  fi
  GC_CONTEXT_TAIL_MODE="$context_tail_mode"
  export GC_CONTEXT_TAIL_MODE

  local context_lines_current="$context_lines"
  local context_file_lines_current="$context_file_lines"
  local context_lines_min=0
  local context_file_lines_min=0
  local context_lines_min_default="${GC_CONTEXT_MIN_LINES:-80}"
  local context_file_lines_min_default="${GC_CONTEXT_MIN_FILE_LINES:-60}"
  if ! [[ "$context_lines_min_default" =~ ^[0-9]+$ ]]; then
    context_lines_min_default=80
  fi
  if ! [[ "$context_file_lines_min_default" =~ ^[0-9]+$ ]]; then
    context_file_lines_min_default=60
  fi
  if (( context_lines_current > 0 )); then
    context_lines_min="$context_lines_min_default"
    if (( context_lines_current < context_lines_min )); then
      context_lines_min="$context_lines_current"
    fi
  fi
  if (( context_file_lines_current > 0 )); then
    context_file_lines_min="$context_file_lines_min_default"
    if (( context_file_lines_current < context_file_lines_min )); then
      context_file_lines_min="$context_file_lines_current"
    fi
  fi
  local context_shrink_iterations=0
  local context_last_shrink_tokens=0
  local context_auto_shrink_threshold="${GC_CONTEXT_AUTO_SHRINK_THRESHOLD:-60000}"
  if ! [[ "$context_auto_shrink_threshold" =~ ^[0-9]+$ ]]; then
    context_auto_shrink_threshold=60000
  fi
  info "Work run directory → ${run_dir}"

  local resume_flag=1
  [[ $resume -eq 1 ]] || resume_flag=0

  local work_failed=0
  local any_changes=0
  local manual_followups=0
  local usage_limit_triggered=0
  local batch_limit_reached=0
  local run_blocked_quota=0

  gc_touch_progress() {
    last_progress_ts="$(date +%s)"
  }

  gc_check_idle_timeout() {
    if (( idle_timeout > 0 )); then
      local now_ts
      now_ts="$(date +%s)"
      if (( now_ts - last_progress_ts >= idle_timeout )); then
        if (( idle_timeout_triggered == 0 )); then
          idle_timeout_triggered=1
          manual_followups=1
          work_failed=1
          warn "Idle timeout reached (${idle_timeout}s without progress); halting run."
        fi
        return 1
      fi
    fi
    return 0
  }

  gc_auto_commit_task() {
    local commit_message="${1:-}"
    shift || true
    local -a raw_paths=("$@")
    GC_LAST_AUTO_COMMIT_HASH=""
    GC_LAST_AUTO_COMMIT_STATUS="skipped"

    local helper_path
    helper_path="$(gc_clone_python_tool "gc_auto_commit_task.py" "${PROJECT_ROOT:-$PWD}")" || return 0

    local helper_output=""
    if ! helper_output="$("${python_bin:-python3}" "$helper_path" "$commit_message" "${raw_paths[@]}")"; then
      warn "    Auto-commit helper failed."
      return 0
    fi

    local status_received=0
    while IFS=$'	' read -r kind field1 rest; do
      [[ -z "$kind" ]] && continue
      case "$kind" in
        MESSAGE)
          local message="$rest"
          case "$field1" in
            info) info "    ${message}" ;;
            warn) warn "    ${message}" ;;
            *) info "    ${message}" ;;
          esac
          ;;
        RESULT)
          GC_LAST_AUTO_COMMIT_STATUS="${field1:-skipped}"
          GC_LAST_AUTO_COMMIT_HASH="${rest:-}"
          status_received=1
          ;;
      esac
    done <<<"$helper_output"

    if (( status_received == 0 )); then
      GC_LAST_AUTO_COMMIT_STATUS="skipped"
      GC_LAST_AUTO_COMMIT_HASH=""
    fi
    return 0
  }

  gc_run_task_verify() {
    local task_ref="${1:-task}"
    local report_dir="${2:-${RUN_DIR:-${PROJECT_ROOT:-$PWD}/.gpt-creator/staging/plan/work}}"
    local project_root="${PROJECT_ROOT:-$PWD}"
    local helper_path
    helper_path="$(gc_clone_python_tool "task_verify.py" "$project_root")" || return 1
    local report_path="${report_dir}/verify_${task_ref//[^A-Za-z0-9._-]/_}.json"
    local config_arg=()
    if [[ -n "${GC_VERIFY_CONFIG:-}" ]]; then
      config_arg=(--config "${GC_VERIFY_CONFIG}")
    fi
    local verify_output=""
    if ! verify_output="$("$python_bin" "$helper_path" --project "$project_root" "${config_arg[@]}" --task-ref "$task_ref" --output "$report_path")"; then
      GC_LAST_VERIFY_STATUS="fail"
      GC_LAST_VERIFY_SUMMARY="Verification helper failed."
      GC_LAST_VERIFY_REPORT="$report_path"
      GC_LAST_VERIFY_DETAILS=""
      return 1
    fi

    GC_LAST_VERIFY_REPORT="$report_path"
    GC_LAST_VERIFY_STATUS="inconclusive"
    GC_LAST_VERIFY_SUMMARY=""
    GC_LAST_VERIFY_DETAILS=""
    if command -v "$python_bin" >/dev/null 2>&1; then
      local parsed=""
      local py_cmd=$'import json, sys
payload = json.loads(sys.argv[1])
status = str(payload.get("status", "inconclusive")).strip().lower()
if status not in {"pass", "fail", "inconclusive"}:
    status = "inconclusive"
summary = str(payload.get("summary", "") or "")
summary = " ".join(summary.split())
details = json.dumps(payload.get("details", []), ensure_ascii=False)
print(status)
print(summary)
print(details)'
      if parsed="$("$python_bin" -c "$py_cmd" "$verify_output" 2>/dev/null)"; then
        IFS=$'\n' read -r GC_LAST_VERIFY_STATUS GC_LAST_VERIFY_SUMMARY GC_LAST_VERIFY_DETAILS <<<"$parsed"
      fi
    fi
    return 0
  }

  gc_validator_effective_status() {
    local raw_status="${1:-inconclusive}"
    local raw_summary="${2:-}"
    local lowered_summary="${raw_summary,,}"
    case "$raw_status" in
      pass)
        printf '%s\n' "pass"
        ;;
      fail)
        printf '%s\n' "fail"
        ;;
      *)
        if [[ -z "$lowered_summary" ]]; then
          printf '%s\n' "pass"
        elif [[ "$lowered_summary" == *"no verification adapters configured"* ]]; then
          printf '%s\n' "pass"
        elif [[ "$lowered_summary" == *"verification config not found"* ]]; then
          printf '%s\n' "pass"
        elif [[ "$lowered_summary" == *"verification config"* && "$lowered_summary" == *"not found"* ]]; then
          printf '%s\n' "pass"
        else
          printf '%s\n' "fail"
        fi
        ;;
    esac
  }

  gc_compose_validator_guidance() {
    local status="${1:-}"
    local summary="${2:-}"
    local report="${3:-}"
    local details_json="${4:-[]}"
    "$python_bin" - "$status" "$summary" "$report" "$details_json" <<'PYCODE' 2>/dev/null
import json
import sys

status = sys.argv[1]
summary = sys.argv[2]
report = sys.argv[3]
try:
    details = json.loads(sys.argv[4])
except Exception:
    details = []

failures = []
for entry in details:
    if not isinstance(entry, dict):
        continue
    check_status = str(entry.get("status", "")).lower()
    if check_status not in {"fail", "error"}:
        continue
    label = entry.get("name") or entry.get("kind") or "check"
    message = entry.get("message") or ""
    failures.append(f"{label}: {message}".strip())

if not failures and summary:
    failures.append(summary.strip())

if not failures:
    sys.exit(0)

print("Only output JSON. Make minimal edits so these validators pass:")
for failure in failures:
    line = failure.replace("\n", " ").strip()
    if line:
        print(f"- {line}")
if report:
    print(f"- Review validator report: {report}")
print("Do not plan; produce diffs in the `changes` array.")
PYCODE
  }

  gc_auto_push_helper() {
    local label="${1:-}"
    shift || true
    local -a change_records=("$@")

    GC_LAST_AUTO_COMMIT_STATUS="failed"
    GC_LAST_AUTO_COMMIT_HASH=""
    GC_LAST_AUTO_PUSH_STATUS="skipped"
    GC_LAST_AUTO_PUSH_REMOTE=""
    GC_LAST_AUTO_PUSH_BRANCH=""
    GC_LAST_AUTO_PUSH_ERROR=""

    if [[ "${GC_AUTO_PUSH:-}" != "1" ]]; then
      return 1
    fi

    local project_root="${PROJECT_ROOT:-$PWD}"
    local node_bin="${NODE_BIN:-node}"
    local helper_path="${CLI_ROOT}/src/lib/autoPush.js"

    if [[ -z "$project_root" ]]; then
      warn "    Auto-push skipped: project root unavailable."
      return 1
    fi
    if [[ ! -f "$helper_path" ]]; then
      warn "    Auto-push skipped: helper missing at ${helper_path}."
      return 1
    fi
    if ! command -v "$node_bin" >/dev/null 2>&1; then
      warn "    Auto-push skipped: node runtime not found."
      return 1
    fi
    if ! command -v git >/dev/null 2>&1; then
      warn "    Auto-push skipped: git command not available."
      return 1
    fi
    if ! git -C "$project_root" rev-parse --is-inside-work-tree >/dev/null 2>&1; then
      warn "    Auto-push skipped: not a git repository."
      return 1
    fi

    local summary_path
    summary_path="$(mktemp)" || return 1
    local summary_py=$'import json, sys
repo = sys.argv[1]
label = sys.argv[2]
records = []
for raw in sys.argv[3:]:
    if "\t" in raw:
        op, path = raw.split("\t", 1)
    else:
        op, path = "?", raw
    op = op or "?"
    records.append({"op": op, "path": path})
summary = {"cwd": repo, "files": records}
if label:
    summary["label"] = label
print(json.dumps(summary))'
    if ! "$python_bin" -c "$summary_py" "$project_root" "$label" "${change_records[@]}" >"$summary_path"; then
      rm -f "$summary_path"
      warn "    Auto-push skipped: failed to prepare summary payload."
      return 1
    fi

    local autopush_stdout=""
    if ! autopush_stdout="$(GC_AUTOPUSH_EXPECT_JSON=1 "$node_bin" "$helper_path" "$summary_path" 2>&1)"; then
      rm -f "$summary_path"
      GC_LAST_AUTO_PUSH_STATUS="failed"
      GC_LAST_AUTO_PUSH_ERROR="$autopush_stdout"
      return 1
    fi
    rm -f "$summary_path"

    if [[ -z "$autopush_stdout" ]]; then
      autopush_stdout='{}'
    fi

    local parse_output=""
    local autopush_parse_py=$'import json, sys
try:
    payload = json.loads(sys.argv[1])
except Exception:
    payload = {}
commit_status = payload.get("commitStatus", "skipped")
commit_sha = payload.get("commitSha") or ""
push_status = payload.get("pushStatus", "skipped")
remote = payload.get("remote") or ""
branch = payload.get("branch") or ""
error = payload.get("error") or ""
print(commit_status)
print(commit_sha)
print(push_status)
print(remote)
print(branch)
print(error)'
    if ! parse_output="$("$python_bin" -c "$autopush_parse_py" "$autopush_stdout" 2>/dev/null)"; then
      parse_output=$'failed\n\nfailed\n\n\n'
    fi

    IFS=$'\n' read -r GC_LAST_AUTO_COMMIT_STATUS GC_LAST_AUTO_COMMIT_HASH GC_LAST_AUTO_PUSH_STATUS GC_LAST_AUTO_PUSH_REMOTE GC_LAST_AUTO_PUSH_BRANCH GC_LAST_AUTO_PUSH_ERROR <<<"$parse_output"

    if [[ "$GC_LAST_AUTO_COMMIT_STATUS" == "failed" || "$GC_LAST_AUTO_PUSH_STATUS" == "failed" ]]; then
      return 1
    fi

    return 0
  }

  gc_auto_push_only() {
    GC_LAST_AUTO_PUSH_STATUS="skipped"
    GC_LAST_AUTO_PUSH_REMOTE=""
    GC_LAST_AUTO_PUSH_BRANCH=""
    GC_LAST_AUTO_PUSH_ERROR=""

    if [[ "${GC_AUTO_PUSH:-}" != "1" ]]; then
      return 0
    fi

    local project_root="${PROJECT_ROOT:-$PWD}"
    if [[ -z "$project_root" ]]; then
      warn "    Auto-push skipped: project root unavailable."
      return 1
    fi
    if ! command -v git >/dev/null 2>&1; then
      warn "    Auto-push skipped: git command not available."
      return 1
    fi
    if ! git -C "$project_root" rev-parse --is-inside-work-tree >/dev/null 2>&1; then
      warn "    Auto-push skipped: not a git repository."
      return 1
    fi

    local current_branch
    current_branch="$(git -C "$project_root" rev-parse --abbrev-ref HEAD 2>/dev/null || true)"
    local resolved_branch="${GC_AUTO_PUSH_BRANCH:-$current_branch}"
    local resolved_remote="${GC_AUTO_PUSH_REMOTE:-}"

    if [[ -z "$resolved_remote" && -n "$resolved_branch" ]]; then
      resolved_remote="$(git -C "$project_root" config --get "branch.${resolved_branch}.remote" 2>/dev/null || true)"
    fi
    if [[ -z "$resolved_remote" && -n "$current_branch" ]]; then
      resolved_remote="$(git -C "$project_root" config --get "branch.${current_branch}.remote" 2>/dev/null || true)"
    fi
    if [[ -z "$resolved_remote" ]]; then
      resolved_remote="origin"
    fi

    local push_branch="$resolved_branch"
    if [[ -z "$push_branch" || "$push_branch" == "HEAD" ]]; then
      push_branch="$current_branch"
    fi

    GC_LAST_AUTO_PUSH_REMOTE="$resolved_remote"
    GC_LAST_AUTO_PUSH_BRANCH="$push_branch"

    if [[ "$resolved_remote" == "__skip__" ]]; then
      GC_LAST_AUTO_PUSH_STATUS="skipped"
      return 0
    fi
    if ! git -C "$project_root" remote get-url "$resolved_remote" >/dev/null 2>&1; then
      warn "    Auto-push skipped: remote ${resolved_remote} not configured."
      GC_LAST_AUTO_PUSH_STATUS="skipped"
      return 1
    fi

    local -a push_args=("$resolved_remote")
    if [[ -n "$push_branch" && "$push_branch" != "HEAD" ]]; then
      push_args+=("$push_branch")
    fi

    if ! git -C "$project_root" push "${push_args[@]}"; then
      warn "    Auto-push failed: git push ${push_args[*]} returned non-zero."
      GC_LAST_AUTO_PUSH_STATUS="failed"
      return 1
    fi

    GC_LAST_AUTO_PUSH_STATUS="pushed"
    return 0
  }


  gc_diff_fingerprint() {
    if [[ -z "${PROJECT_ROOT:-}" ]]; then
      printf 'no-project-%s
' "$RANDOM$RANDOM$$"
      return 0
    fi
    if ! command -v git >/dev/null 2>&1; then
      printf 'nogit-%s
' "$RANDOM$RANDOM$$"
      return 0
    fi
    local diff_payload
    if ! diff_payload="$( (git -C "$PROJECT_ROOT" diff --shortstat --no-ext-diff || true; printf '
'; git -C "$PROJECT_ROOT" diff --numstat --no-ext-diff || true; printf '
'; git -C "$PROJECT_ROOT" diff --name-only --no-ext-diff || true) 2>/dev/null )"; then
      printf 'diff-error-%s
' "$RANDOM$RANDOM$$"
      return 0
    fi
    if command -v "$python_bin" >/dev/null 2>&1; then
      local diff_hash_helper
      if diff_hash_helper="$(gc_clone_python_tool "diff_payload_hash.py" "${PROJECT_ROOT:-$PWD}")"; then
        "$python_bin" "$diff_hash_helper" "$diff_payload"
        return 0
      fi
    fi
    if command -v shasum >/dev/null 2>&1; then
      printf '%s' "$diff_payload" | shasum -a 1 2>/dev/null | awk '{print $1}'
    elif command -v sha1sum >/dev/null 2>&1; then
      printf '%s' "$diff_payload" | sha1sum 2>/dev/null | awk '{print $1}'
    else
      printf 'hash-%s
' "$(printf '%s' "$diff_payload" | wc -c | awk '{print $1}')"
    fi
  }


  gc_create_empty_apply_checkpoint() {
    local story_slug="$1"
    local task_id="$2"
    local root="${PROJECT_ROOT:-$PWD}"
    [[ -n "$story_slug" ]] || story_slug="story"
    [[ -n "$task_id" ]] || task_id="task"
    local checkpoint_dir="${root}/.gpt-creator/logs/empty-apply/${story_slug}/${task_id}"
    local checkpoint_file="${checkpoint_dir}/checkpoint.md"
    mkdir -p "$checkpoint_dir" || return 1
    if [[ ! -f "$checkpoint_file" ]]; then
      cat >"$checkpoint_file" <<EOF
# Checkpoint — ${story_slug}/${task_id}

Generated automatically because the agent response contained no actionable 'changes'.

- Timestamp: $(date -u +%FT%TZ)
- Runner: gc-empty-apply-fallback

## What was attempted
- Review the raw agent output under .gpt-creator/staging/plan/work/runs for detailed context.
EOF
    fi
    local checkpoint_rel
    local checkpoint_helper
    checkpoint_helper="$(gc_clone_python_tool "checkpoint_relative_path.py" "${PROJECT_ROOT:-$PWD}")" || checkpoint_helper=""
    if [[ -n "$checkpoint_helper" ]]; then
      checkpoint_rel="$("$python_bin" "$checkpoint_helper" "$checkpoint_file" "$root")"
    else
      checkpoint_rel="$checkpoint_file"
    fi
    printf '%s\n' "$checkpoint_rel"
  }

  gc_touch_progress

  local processed_total=0
  local processed_any_total=0
  local remaining_tasks=0
  local memory_cycle_single=0
  local story_plan_helper=""
  story_plan_helper="$(gc_clone_python_tool "story_scheduler.py" "${PROJECT_ROOT:-$PWD}")" || return 1

  while :; do
    if gc_check_idle_timeout; then :; else break; fi
    local iteration_processed_any=0
    local iteration_processed=0
    local continue_current_run=0
    local pending_tasks=0

    local effective_batch_size="$batch_size"
    if (( memory_cycle )); then
      if (( memory_cycle_single )); then
        effective_batch_size=1
      else
        effective_batch_size="$batch_size"
      fi
    fi

    while IFS=$'\t' read -r sequence slug story_id story_title epic_id epic_title total_tasks next_task completed status; do
      if [[ -z "${sequence}${slug}${story_id}${story_title}${epic_id}${epic_title}" ]]; then
        continue
      fi

      if ! gc_check_idle_timeout; then
        break
      fi

      iteration_processed_any=1

      if (( throughput_checkpoint_interval > 0 )); then
        now_ts="$(date +%s)"
        if (( throughput_next_checkpoint == 0 || now_ts >= throughput_next_checkpoint )); then
          local throughput_checkpoint_msg=""
          if throughput_checkpoint_msg="$(gc_update_throughput_metrics "$tasks_db" "checkpoint")"; then
            if [[ -n "$throughput_checkpoint_msg" ]]; then
              info "$throughput_checkpoint_msg"
            fi
          else
            warn "Failed to checkpoint throughput metrics."
          fi
          throughput_next_checkpoint=$((now_ts + throughput_checkpoint_interval))
        fi
      fi

    : "${total_tasks:=0}"; : "${next_task:=0}"
    local total_tasks_int=0
    if [[ "$total_tasks" =~ ^[0-9]+$ ]]; then
      total_tasks_int=$((total_tasks))
    fi
    local next_task_int=0
    if [[ "$next_task" =~ ^[0-9]+$ ]]; then
      next_task_int=$((next_task))
    fi

    if (( migration_transition_triggered )); then
      break
    fi

    local migration_epoch_current=""
    if migration_epoch_current="$(gc_fetch_migration_epoch "$tasks_db" 2>/dev/null)"; then
      if [[ "$migration_epoch_current" != "$migration_epoch_baseline" ]]; then
        migration_transition_triggered=1
        work_failed=1
        manual_followups=1
        local block_position="$next_task_int"
        if (( block_position >= total_tasks_int )); then
          if (( total_tasks_int > 0 )); then
            block_position=$((total_tasks_int - 1))
          else
            block_position=0
          fi
        fi
        gc_update_task_state "$tasks_db" "$slug" "$block_position" "blocked-migration-transition" "$run_stamp" 2>/dev/null || true
        if (( binder_clear_on_migration )); then
          gc_binder_clear_story "$PROJECT_ROOT" "${epic_id:-}" "$slug"
          info "  Cleared binder cache for story ${slug} due to migration epoch change."
        fi
        info "  Migration epoch changed ($migration_epoch_baseline → $migration_epoch_current); deferring remaining tasks to a fresh run."
        migration_epoch_baseline="$migration_epoch_current"
        break
      fi
    fi

    printf -v story_prefix "%03d" "${sequence:-0}"
    [[ -n "$slug" ]] || slug="story-${story_prefix}"
    local story_run_dir="${run_dir}/story_${story_prefix}_${slug}"
    local story_log_dir="${run_log_dir}/story_${story_prefix}_${slug}"
    mkdir -p "${story_run_dir}/prompts" "${story_run_dir}/out" "${story_run_dir}/reports" "$story_log_dir"
    local report_dir="${story_run_dir}/reports"

    info "Story ${story_prefix} (${story_id:-$slug}) — ${story_title:-Unnamed}"
    export GC_BUDGET_STORY_ID="${story_id:-$slug}"

    if (( total_tasks_int == 0 )); then
      local stats=""
      if stats="$(gc_fetch_story_task_counts "$tasks_db" "$slug" 2>/dev/null)"; then
        local actual_total=0 actual_completed=0
        IFS=$'\t' read -r actual_total actual_completed <<<"$stats"
        if [[ "$actual_total" =~ ^[0-9]+$ ]] && (( actual_total > 0 )); then
          info "  Found ${actual_total} task(s) in backlog despite zero metadata; synchronising."
          total_tasks_int=$actual_total
          completed="${actual_completed}"
          if [[ "$completed" =~ ^[0-9]+$ ]]; then
            (( completed > actual_total )) && completed="$actual_total"
            if (( next_task_int < completed )); then
              next_task_int=$completed
            fi
          fi
          gc_update_work_state "$tasks_db" "$slug" "pending" "$completed" "$total_tasks_int" "$run_stamp"
        fi
      fi
    fi

    if (( total_tasks_int == 0 )); then
      info "  No tasks for this story; marking complete."
      gc_update_work_state "$tasks_db" "$slug" "complete" 0 0 "$run_stamp"
      if (( keep_artifacts == 0 )); then
        rmdir "${story_run_dir}/prompts" 2>/dev/null || true
        rmdir "${story_run_dir}/out" 2>/dev/null || true
      fi
      continue
    fi

    gc_update_work_state "$tasks_db" "$slug" "in-progress" "$next_task_int" "$total_tasks_int" "$run_stamp"

    local task_index
    info "  Preparing prompts and context…"
    local story_failed=0
    for (( task_index = next_task_int; task_index < total_tasks_int; task_index++ )); do
      gc_clear_active_task
      if ! gc_check_idle_timeout; then
        break
      fi
      if (( throughput_checkpoint_interval > 0 )); then
        now_ts="$(date +%s)"
        if (( throughput_next_checkpoint == 0 || now_ts >= throughput_next_checkpoint )); then
          local throughput_checkpoint_msg=""
          if throughput_checkpoint_msg="$(gc_update_throughput_metrics "$tasks_db" "checkpoint")"; then
            if [[ -n "$throughput_checkpoint_msg" ]]; then
              info "$throughput_checkpoint_msg"
            fi
          else
            warn "Failed to checkpoint throughput metrics."
          fi
          throughput_next_checkpoint=$((now_ts + throughput_checkpoint_interval))
        fi
      fi
      if (( effective_batch_size > 0 && iteration_processed >= effective_batch_size )); then
        batch_limit_reached=1
        break
      fi
      local task_number
      printf -v task_number "%03d" $((task_index + 1))
      local prompt_path="${story_run_dir}/prompts/task_${task_number}.prompt.md"
      local output_path="${story_run_dir}/out/task_${task_number}.out.md"
      local stage_baseline_retrieve="${GC_BUDGET_STAGE_TOTAL_RETRIEVE:-0}"
      local stage_baseline_plan="${GC_BUDGET_STAGE_TOTAL_PLAN:-0}"
      local stage_baseline_patch="${GC_BUDGET_STAGE_TOTAL_PATCH:-0}"
      local stage_baseline_verify="${GC_BUDGET_STAGE_TOTAL_VERIFY:-0}"

      local prompt_meta
      if ! prompt_meta="$(gc_write_task_prompt "$tasks_db" "$slug" "$task_index" "$prompt_path" "$context_tail" "$CODEX_MODEL_CODE" "$PROJECT_ROOT" "$STAGING_DIR")"; then
        warn "  Failed to build prompt for task index ${task_index}"
        work_failed=1
        break
      fi
      if [[ -z "$prompt_slim_helper" ]]; then
        prompt_slim_helper="$(gc_clone_python_tool "wot_slim_prompt.py" "${PROJECT_ROOT:-$PWD}")" || return 1
      fi
      "$python_bin" "$prompt_slim_helper" "$prompt_path"

      local prompt_est_tokens_raw
      prompt_est_tokens_raw="$(gc_estimate_tokens_from_bytes "$prompt_path")"
      if ! [[ "$prompt_est_tokens_raw" =~ ^[0-9]+$ ]]; then
        prompt_est_tokens_raw=0
      fi

      local task_id="" task_title="" task_story_points="" task_status_current="" task_locked_flag="0" task_status_reason=""
      IFS=$'\t' read -r task_id task_title task_story_points task_status_current task_locked_flag task_status_reason <<<"$prompt_meta"
      local banner_task_id="${task_id:-no-id}"
      local task_start_epoch
      task_start_epoch="$(date +%s)"
      local task_tokens_total=0
      local task_llm_prompt_tokens=0
      local task_llm_completion_tokens=0
      local task_prompt_estimate=0
      local task_duration_seconds=0
      local task_duration_display=""
      local task_tokens_display=""
      local task_story_points_display="—"
      if [[ -n "$task_story_points" ]]; then
        task_story_points_display="$task_story_points"
      fi

      local task_status_original="$task_status_current"
      local task_status_lower="${task_status_original,,}"
      local task_locked_migration=0
      if [[ "$task_locked_flag" =~ ^[0-9]+$ ]] && (( task_locked_flag > 0 )); then
        task_locked_migration=1
      fi
      local task_status_reason_current="$task_status_reason"
      local terminal_locked_regex='^(complete|completed|completed-no-changes|blocked-budget|blocked-quota|blocked-merge-conflict|blocked-schema-drift|blocked-schema-guard-error|blocked-dependency\([^)]+\)|skipped-already-complete)$'
      if [[ "$task_status_lower" == blocked-dependency\(* ]]; then
        local blocked_reason="${task_status_reason_current:-${task_status_original}}"
        warn "  Task ${task_number} (${banner_task_id}) blocked by dependency: ${blocked_reason}"
        manual_followups=1
        story_failed=1
        break
      fi

      printf '\n'
      gc_render_task_banner "START OF A NEW TASK" "START TASK ID" "$banner_task_id"
      info "  → Working on task ${task_number} (${banner_task_id})"
      info "    ${task_title:-(untitled)}"

      local call_name="story-${slug}-task-${task_number}"
      local codex_ok=0
      local attempt=0
      local max_attempts=2
      gc_touch_progress
      local prompt_augmented=0 prompt_change_reminder=0
      local contract_guard_injected=0
      local keep_output=$keep_artifacts
      local break_after_update=0
      local task_result_status="in-progress"
      local task_needs_review=0
      local task_verify_status="skipped"
      local task_verify_summary=""
      local task_verify_details=""
      local task_verify_report=""
      local validator_cached=0
      local validator_status="inconclusive"
      local validator_summary=""
      local validator_details="[]"
      local validator_report=""
      local delta_prompt_injected=0
      local task_meta_plan_flag=0
      local task_meta_focus_flag=0
      local task_meta_no_changes_flag=0
      local task_meta_already_flag=0
      local -a task_notes=()
      local -a task_written_paths=()
      local -a task_patched_paths=()
      local task_ref_for_verify="${task_id:-${banner_task_id:-${slug:-story}-${task_number}}}"
      if [[ -z "$task_ref_for_verify" ]]; then
        task_ref_for_verify="${slug:-story}-${task_number}"
      fi
      local -a task_commands=()
      local -a task_auto_push_records=()
      local task_changes_applied=0
      local apply_status="pending"
      local skip_codex_reason="prompt-blocked"
      local task_report_path="${report_dir}/task_${task_number}.log"
      local task_log_archive_path="${story_log_dir}/task_${task_number}.log"
      local task_change_sizes=""
      local diff_last_transition=""
      local diff_prev_after_sig=""
      local diff_prev_prev_after_sig=""
      local diff_stall_count=0
      local turn_diff_prev_hash=""
      local turn_diff_repeat_count=0
      local diff_guard_enabled=1
      if (( diff_guard_stall_limit <= 0 && diff_guard_file_cooldown <= 0 && diff_guard_turn_repeat_limit <= 0 )); then
        diff_guard_enabled=0
      fi
      local empty_checkpoint_created=0
      local attempt_tokens=0
      local codex_attempted=0
      local skip_codex=0
      local blocked_stop_run=0
      local prompt_status="ok"
      local prompt_soft_limit_value=0
      local prompt_hard_limit_value=0
      local prompt_stop_on_overbudget="true"
      local prompt_token_estimate=0
      local prompt_pruned_bytes=0
      local prompt_pruned_items="[]"
      local prompt_binder_status=""
      local prompt_binder_reason=""
      gc_update_task_state "$tasks_db" "$slug" "$task_index" "in-progress" "$run_stamp"
      export GC_ACTIVE_TASK_DB="$tasks_db"
      export GC_ACTIVE_TASK_SLUG="$slug"
      export GC_ACTIVE_TASK_INDEX="$task_index"
      export GC_ACTIVE_RUN_STAMP="$run_stamp"
      export GC_ACTIVE_TASK_NUMBER="$task_number"
      export GC_ACTIVE_TASK_ID="$task_id"
      export GC_ACTIVE_TASK_REPORT="$task_report_path"
      export GC_ACTIVE_TASK_ARCHIVE="$task_log_archive_path"
      export GC_ACTIVE_TASK_PROMPT="$prompt_path"
      export GC_ACTIVE_TASK_OUTPUT="$output_path"
      export GC_BUDGET_TASK_ID="${task_id:-$banner_task_id}"

      local prompt_meta_path="${prompt_path}.meta.json"
      if [[ -f "$prompt_meta_path" ]]; then
        local prompt_meta_helper
        prompt_meta_helper="$(gc_clone_python_tool "prompt_meta_summary.py" "${PROJECT_ROOT:-$PWD}")" || prompt_meta_helper=""
        if [[ -n "$prompt_meta_helper" ]]; then
          local _
          read -r prompt_status prompt_soft_limit_value prompt_hard_limit_value prompt_stop_on_overbudget prompt_token_estimate prompt_pruned_bytes prompt_pruned_items _ prompt_binder_status prompt_binder_reason < <(
            "$python_bin" "$prompt_meta_helper" "$prompt_meta_path"
          )
        fi
      fi
      GC_CURRENT_PRUNED_ITEMS="$prompt_pruned_items"
      if [[ "$prompt_token_estimate" =~ ^[0-9]+$ ]]; then
        task_prompt_estimate=$((prompt_token_estimate))
      else
        task_prompt_estimate=0
      fi
      local retrieve_tokens=0
      if [[ "$prompt_pruned_bytes" =~ ^[0-9]+$ && prompt_pruned_bytes -gt 0 ]]; then
        retrieve_tokens=$((prompt_pruned_bytes / 4))
      fi
      local retrieve_tool_bytes="{}"
      if [[ "$prompt_pruned_bytes" =~ ^[0-9]+$ && prompt_pruned_bytes -gt 0 ]]; then
        retrieve_tool_bytes="{\"show-file\": ${prompt_pruned_bytes}}"
      fi
      gc_budget_log_stage "retrieve" "$retrieve_tokens" 0 "$retrieve_tokens" 0 "$prompt_pruned_items" "$retrieve_tool_bytes" "false"
      if gc_budget_stage_tripped "retrieve"; then
        local retrieve_limit_value
        retrieve_limit_value="$(gc_budget_get_stage_limit "retrieve")"
        task_notes+=("Retrieve stage exceeded budget limit (${retrieve_tokens}/${retrieve_limit_value})")
        skip_codex=1
        codex_ok=1
        manual_followups=0
        keep_output=0
        task_result_status="blocked-quota"
        apply_status="prompt-blocked"
        skip_codex_reason="stage-limit"
        blocked_stop_run=1
        run_blocked_quota=1
      else
        gc_budget_log_stage "plan" "$prompt_token_estimate" 0 "$prompt_token_estimate" 0 "$prompt_pruned_items" "{}" "false"
        local guard_safe_limit="${GC_SAFE_PROMPT_TOKENS:-8000}"
        if ! [[ "$guard_safe_limit" =~ ^[0-9]+$ ]]; then
          guard_safe_limit=8000
        fi
        guard_safe_limit=$((guard_safe_limit))
        local guard_prompt_estimate=0
        if [[ "$prompt_token_estimate" =~ ^[0-9]+$ ]]; then
          guard_prompt_estimate=$((prompt_token_estimate))
        elif [[ "$prompt_est_tokens_raw" =~ ^[0-9]+$ ]]; then
          guard_prompt_estimate=$((prompt_est_tokens_raw))
        fi
        if gc_budget_stage_should_skip "patch"; then
          local guard_skip_reason
          guard_skip_reason="$(gc_budget_stage_skip_reason "patch")"
          [[ -z "$guard_skip_reason" ]] && guard_skip_reason="auto-abandon"
          if [[ "$guard_skip_reason" == auto-abandon* ]] && (( guard_prompt_estimate >= 0 && guard_prompt_estimate <= guard_safe_limit )); then
            if ! gc_env_truthy "${GC_DISABLE_AUTO_ABANDON:-}"; then
              export GC_DISABLE_AUTO_ABANDON=1
            fi
            gc_budget_set_stage_skip "patch" 0 ""
            offenders_auto_flag=0
          fi
        fi
        if gc_budget_stage_should_skip "patch"; then
          skip_codex=1
          codex_ok=1
          task_needs_review=0
          manual_followups=0
          keep_output=0
          task_result_status="abandoned-for-budget"
          apply_status="auto-abandon"
          skip_codex_reason="$(gc_budget_stage_skip_reason "patch")"
          [[ -z "$skip_codex_reason" ]] && skip_codex_reason="auto-abandon"
        fi
      fi

      : "${GC_TASK_TOKEN_HARD_LIMIT:=60000}"
      local prompt_token_hard_cap
      prompt_token_hard_cap="$(gc_parse_int "${GC_TASK_TOKEN_HARD_LIMIT}" 60000)"
      if ! [[ "$prompt_token_hard_cap" =~ ^[0-9]+$ ]]; then
        prompt_token_hard_cap=60000
      else
        prompt_token_hard_cap=$((prompt_token_hard_cap))
      fi
      local prompt_estimate_tokens=$((prompt_est_tokens_raw))
      if (( prompt_token_hard_cap > 0 )) && (( prompt_estimate_tokens > prompt_token_hard_cap )) && (( skip_codex == 0 )); then
        warn "    Prompt ~${prompt_estimate_tokens} tokens exceeds hard cap ${prompt_token_hard_cap}; skipping task (abandoned-for-budget)."
        gc_mark_task_abandoned_for_budget "$tasks_db" "$slug" "$task_index" "$run_stamp" || true
        skip_codex=1
        codex_ok=1
        manual_followups=1
        keep_output=0
        task_result_status="abandoned-for-budget"
        apply_status="prompt-blocked"
        skip_codex_reason="hard-cap"
        prompt_status="blocked-quota"
        prompt_hard_limit_value="$prompt_token_hard_cap"
        prompt_token_estimate="$prompt_estimate_tokens"
        prompt_stop_on_overbudget="true"
        run_blocked_quota=1
        task_prompt_estimate=$((prompt_estimate_tokens))
        task_notes+=("Prompt estimated at ~${prompt_estimate_tokens} tokens exceeds hard cap ${prompt_token_hard_cap}; not sent to LLM.")
      fi

      if [[ "${prompt_status,,}" != "blocked-quota" ]]; then
        if [[ "$prompt_hard_limit_value" =~ ^[0-9]+$ ]] && (( prompt_hard_limit_value > 0 )) && [[ "$prompt_token_estimate" =~ ^[0-9]+$ ]] && (( prompt_token_estimate > prompt_hard_limit_value )); then
          skip_codex=1
          codex_ok=1
          task_needs_review=0
          manual_followups=0
          keep_output=0
          task_result_status="blocked-quota"
          apply_status="prompt-blocked"
          skip_codex_reason="hard-cap"
          local budget_note="${prompt_token_estimate}/${prompt_hard_limit_value}"
          task_notes+=("Prompt estimate ${budget_note} exceeds hard token budget; halting before Codex call.")
          gc_log_blocked_quota "$prompt_meta_path" "${task_id:-}" "$slug" "$run_stamp" "$codex_model_for_step"
          run_blocked_quota=1
          if [[ "$prompt_stop_flag" != "false" ]]; then
            blocked_stop_run=1
          else
            manual_followups=1
          fi
        fi
      fi

      local prompt_stop_flag="${prompt_stop_on_overbudget,,}"
      if [[ -n "$prompt_binder_status" ]]; then
        case "${prompt_binder_status,,}" in
          hit)
            info "    Binder hit — cached context reused."
            ;;
          stale)
            info "    Binder stale (${prompt_binder_reason:-reason unknown}); rebuilding context."
            ;;
          miss)
            info "    Binder cache miss (${prompt_binder_reason:-})"
            ;;
        esac
      fi

      if [[ "${prompt_status,,}" == "blocked-quota" ]]; then
        skip_codex=1
        codex_ok=1
        task_needs_review=0
        manual_followups=0
        keep_output=0
        task_result_status="blocked-quota"
        apply_status="prompt-blocked"
        local budget_note="${prompt_token_estimate}"
        if [[ "$prompt_hard_limit_value" =~ ^[0-9]+$ ]] && (( prompt_hard_limit_value > 0 )); then
          budget_note="${prompt_token_estimate}/${prompt_hard_limit_value}"
        elif [[ "$prompt_soft_limit_value" =~ ^[0-9]+$ ]] && (( prompt_soft_limit_value > 0 )); then
          budget_note="${prompt_token_estimate}/${prompt_soft_limit_value}"
        fi
        task_notes+=("Prompt exceeded token budgets after deterministic pruning; estimated ${budget_note} tokens.")
        gc_log_blocked_quota "$prompt_meta_path" "${task_id:-}" "$slug" "$run_stamp" "$codex_model_for_step"
        if [[ "$prompt_stop_flag" != "false" ]]; then
          blocked_stop_run=1
          run_blocked_quota=1
        else
          manual_followups=1
        fi
      fi

      if (( skip_codex == 1 )); then
        attempt=$max_attempts
        gc_budget_log_stage "patch" 0 0 0 0 "$prompt_pruned_items" "{}" "true" "$skip_codex_reason"
      fi

      while (( attempt < max_attempts )); do
        (( ++attempt ))
        (( ++diff_guard_global_attempt ))
        local diff_guard_current_step="$diff_guard_global_attempt"
        gc_touch_progress
        GC_LAST_APPLY_META=""
        codex_attempted=1
        local diff_before=""
        task_change_sizes=""
        if (( diff_guard_enabled )); then
          diff_before="$(gc_diff_fingerprint)"
        fi
        if codex_call "$call_name" --step patch --prompt "$prompt_path" --output "$output_path"; then
          attempt_tokens="${GC_CODEX_CALL_TOKEN_ACCUM:-${GC_LAST_CODEX_TOTAL_TOKENS:-0}}"
          if [[ "$attempt_tokens" =~ ^[0-9]+$ ]]; then
            task_tokens_total=$((task_tokens_total + attempt_tokens))
          fi
          local attempt_prompt_tokens="${GC_LAST_CODEX_PROMPT_TOKENS:-0}"
          local attempt_completion_tokens="${GC_LAST_CODEX_COMPLETION_TOKENS:-0}"
          if [[ "$attempt_prompt_tokens" =~ ^[0-9]+$ ]]; then
            task_llm_prompt_tokens=$((task_llm_prompt_tokens + attempt_prompt_tokens))
          fi
          if [[ "$attempt_completion_tokens" =~ ^[0-9]+$ ]]; then
            task_llm_completion_tokens=$((task_llm_completion_tokens + attempt_completion_tokens))
          fi
          if [[ ! -s "$output_path" ]]; then
            local checkpoint_story="${story_id:-$slug}"
            local checkpoint_task="${task_id:-$banner_task_id}"
            local checkpoint_rel=""
            if (( empty_checkpoint_created == 0 )) && checkpoint_rel="$(gc_create_empty_apply_checkpoint "$checkpoint_story" "$checkpoint_task")"; then
              empty_checkpoint_created=1
              info "    Recorded empty-apply checkpoint ${checkpoint_rel} (no repository changes)."
              task_notes+=("Codex returned no output; recorded checkpoint ${checkpoint_rel} for follow-up.")
            fi
            if (( task_locked_migration )) && [[ "$task_status_lower" =~ $terminal_locked_regex ]]; then
              warn "  Codex produced no output for ${call_name}; preserving locked status ${task_status_original:-unknown}."
              task_result_status="${task_status_original:-complete}"
              apply_status="locked-no-output"
              task_notes+=("Migration lock active; retained status ${task_result_status} despite empty output.")
              codex_ok=1
            else
              warn "  Codex produced no output for ${call_name}; marking task as apply-failed-migration-context."
              task_needs_review=1
              manual_followups=1
              keep_output=1
              task_result_status="apply-failed-migration-context"
              apply_status="no-output"
              task_notes+=("No agent output; requires manual follow-up.")
              codex_ok=1
            fi
            break
          fi
          local apply_output
          local patch_artifact_dir="${PROJECT_ROOT}/.gpt-creator/artifacts/patches"
          local patch_label="${banner_task_id:-${slug:-story}-${task_number}}"
          patch_label="${patch_label//[^A-Za-z0-9_.-]/_}"
          local patch_artifact_path="${patch_artifact_dir}/${patch_label}.patch"
          local apply_rc=0
          if ! apply_output="$(gc_apply_codex_changes "$output_path" "$PROJECT_ROOT" "$patch_artifact_path")"; then
            apply_rc=$?
            if (( apply_rc == 13 )); then
              warn "  Agent response violated contract (plan/focus without actionable changes)."
              task_notes+=("Response contract violated: plan/focus present without corresponding changes or notes.")
              apply_status="contract-violation"
              if (( attempt < max_attempts )); then
                if (( contract_guard_injected == 0 )); then
                  cat >>"$prompt_path" <<'REM'

## Critical constraint
- Provide at least one concrete repository change (diff or file write) or explicitly document a blocker with evidence that proves no change is required.
- Do not repeat the previous plan without actionable modifications.
REM
                  contract_guard_injected=1
                fi
                continue
              fi
              task_needs_review=1
              manual_followups=1
              keep_output=1
              task_result_status="blocked"
              task_notes+=("Repeated contract violations after retries; marking task blocked.")
              codex_ok=1
              break_after_update=1
              break
            fi
            warn "  Failed to apply changes for ${call_name}; manual review required (see ${output_path})."
            task_needs_review=1
            manual_followups=1
            keep_output=1
            task_result_status="blocked"
            task_notes+=("Codex changes could not be applied automatically; review ${output_path}.")
            codex_ok=1
            break
          fi
          if [[ "$apply_output" == "no-output" || "$apply_output" == "empty-output" ]]; then
            warn "  Codex produced no actionable JSON for ${call_name}."
            if (( task_locked_migration )) && [[ "$task_status_lower" =~ $terminal_locked_regex ]]; then
              task_result_status="${task_status_original:-complete}"
              task_notes+=("Migration lock active; retained status ${task_result_status} after empty apply result.")
              apply_status="locked-no-output"
              codex_ok=1
            else
              task_needs_review=1
              manual_followups=1
              keep_output=1
              task_result_status="apply-failed-migration-context"
              apply_status="empty-apply"
              task_notes+=("No actionable JSON returned; marked apply-failed-migration-context.")
              codex_ok=1
            fi
            break
          fi
          apply_status="ok"
          local task_apply_meta_path=""
          while IFS= read -r change_line; do
            case "$change_line" in
              STATUS\ *)
                apply_status="${change_line#STATUS }"
                ;;
              APPLIED)
                info "    Changes applied."
                ;;
              WRITE\ *)
                local written_path="${change_line#WRITE }"
                info "    Wrote ${written_path}"
                any_changes=1
                task_changes_applied=1
                task_written_paths+=("$written_path")
                task_auto_push_records+=("M\t${written_path}")
                ;;
              PATCH\ *)
                local patched_path="${change_line#PATCH }"
                info "    Patched ${patched_path}"
                any_changes=1
                task_changes_applied=1
                task_patched_paths+=("$patched_path")
                task_auto_push_records+=("M\t${patched_path}")
                ;;
              DELETE\ *)
                local deleted_path="${change_line#DELETE }"
                info "    Deleted ${deleted_path}"
                any_changes=1
                task_changes_applied=1
                task_auto_push_records+=("D\t${deleted_path}")
                ;;
              RENAME\ *)
                local rename_entry="${change_line#RENAME }"
                info "    Renamed ${rename_entry}"
                any_changes=1
                task_changes_applied=1
                task_auto_push_records+=("R\t${rename_entry}")
                ;;
              MOVE\ *)
                local move_entry="${change_line#MOVE }"
                info "    Moved ${move_entry}"
                any_changes=1
                task_changes_applied=1
                task_auto_push_records+=("R\t${move_entry}")
                ;;
              ARTIFACT\ *)
                local artifact_entry="${change_line#ARTIFACT }"
                local artifact_path="" artifact_hunks="" artifact_lines=""
                IFS=$'\t' read -r artifact_path artifact_hunks artifact_lines <<<"$artifact_entry"
                info "    Patch artifact: ${artifact_path} (hunks=${artifact_hunks:-0}, lines=${artifact_lines:-0})"
                ;;
              SIZE\ *)
                local size_entry="${change_line#SIZE }"
                local size_path="${size_entry%%$'\t'*}"
                local size_bytes="${size_entry#*$'\t'}"
                if [[ -z "$size_path" || "$size_path" == "$size_bytes" ]]; then
                  size_bytes="${size_entry##* }"
                  size_path="${size_entry%% *}"
                fi
                task_change_sizes+="${size_path}"$'\t'"${size_bytes}"$'\n'
                ;;
              NOOP\ *)
                local noop_path="${change_line#NOOP }"
                warn "    No-op: ${noop_path}"
                task_notes+=("Codex proposed change already applied: ${noop_path}")
                ;;
              CMD\ *)
                local suggested_cmd="${change_line#CMD }"
                info "    Suggested command: ${suggested_cmd}"
                task_commands+=("$suggested_cmd")
                ;;
              NOTE\ *)
                local note_text="${change_line#NOTE }"
                warn "    Note: ${note_text}"
                task_notes+=("$note_text")
                if gc_note_requires_followup "$note_text"; then
                  task_needs_review=1
                fi
                ;;
              META\ *)
                task_apply_meta_path="${change_line#META }"
                ;;
            esac
          done <<<"$apply_output"
          if [[ -n "$task_apply_meta_path" ]]; then
            if [[ "$task_apply_meta_path" != /* ]]; then
              task_apply_meta_path="${PROJECT_ROOT}/${task_apply_meta_path#./}"
            fi
            GC_LAST_APPLY_META="$task_apply_meta_path"
          else
            GC_LAST_APPLY_META=""
          fi
          if [[ -n "${GC_LAST_APPLY_META:-}" && -f "$GC_LAST_APPLY_META" ]]; then
            local meta_flags=""
            local meta_parse_py=$'import json, sys
from pathlib import Path
path = Path(sys.argv[1])
try:
    payload = json.loads(path.read_text(encoding="utf-8"))
except Exception:
    payload = {}

def flag(name):
    value = payload.get(name)
    return "1" if value else "0"

print(flag("plan_has_items"))
print(flag("focus_has_items"))
print(flag("notes_contains_no_changes"))
print(flag("notes_contains_already_satisfied"))'
            if meta_flags="$("$python_bin" -c "$meta_parse_py" "$GC_LAST_APPLY_META" 2>/dev/null)"; then
              IFS=$'\n' read -r task_meta_plan_flag task_meta_focus_flag task_meta_no_changes_flag task_meta_already_flag <<<"$meta_flags"
            fi
          fi

          if (( task_changes_applied == 0 )); then
            if (( empty_checkpoint_created == 0 )); then
              local checkpoint_story="${story_id:-$slug}"
              local checkpoint_task="${task_id:-$banner_task_id}"
              local checkpoint_rel=""
              if checkpoint_rel="$(gc_create_empty_apply_checkpoint "$checkpoint_story" "$checkpoint_task")"; then
                empty_checkpoint_created=1
                info "    Recorded empty-apply checkpoint ${checkpoint_rel} (no repository changes)."
                task_notes+=("Agent returned no actionable changes; recorded checkpoint ${checkpoint_rel} for follow-up.")
              fi
            fi

            if (( validator_cached == 0 )); then
              if gc_run_task_verify "$task_ref_for_verify" "$story_run_dir"; then
                validator_status="${GC_LAST_VERIFY_STATUS:-inconclusive}"
                validator_summary="${GC_LAST_VERIFY_SUMMARY:-}"
                validator_details="${GC_LAST_VERIFY_DETAILS:-[]}"
                validator_report="${GC_LAST_VERIFY_REPORT:-}"
              else
                validator_status="inconclusive"
                validator_summary="${GC_LAST_VERIFY_SUMMARY:-Verification helper failure}"
                validator_details="${GC_LAST_VERIFY_DETAILS:-[]}"
                validator_report="${GC_LAST_VERIFY_REPORT:-}"
              fi
              validator_cached=1
            fi

            local validator_report_display="$validator_report"
            if [[ -n "$validator_report_display" && -n "$PROJECT_ROOT" && "$validator_report_display" == "$PROJECT_ROOT/"* ]]; then
              validator_report_display="${validator_report_display#$PROJECT_ROOT/}"
            fi

            local validator_effective
            validator_effective="$(gc_validator_effective_status "$validator_status" "$validator_summary")"

            if [[ "$validator_effective" == "pass" ]]; then
              info "    Acceptance validators passed with no repository changes; marking task complete."
              apply_status="noop-accepted"
              task_result_status="complete"
              task_needs_review=0
              manual_followups=0
              keep_output=0
              codex_ok=1
              task_verify_status="$validator_effective"
              task_verify_summary="$validator_summary"
              task_verify_details="$validator_details"
              task_verify_report="$validator_report"
              task_notes+=("Acceptance validators passed with no repository changes (COMPLETED_NOOP).")
              if [[ -n "$validator_summary" ]]; then
                task_notes+=("Validator summary: ${validator_summary}")
              fi
              if [[ -n "$validator_report_display" ]]; then
                task_notes+=("Validator report stored at ${validator_report_display}.")
              fi
              break
            fi

            if (( attempt < max_attempts )); then
              warn "  Acceptance validators still failing; guiding Codex to produce minimal diffs."
              if (( delta_prompt_injected == 0 )); then
                local guidance_output=""
                guidance_output="$(gc_compose_validator_guidance "$validator_status" "$validator_summary" "$validator_report_display" "$validator_details")"
                if [[ -n "$guidance_output" ]]; then
                  cat >>"$prompt_path" <<REM

## Validator remediation
$guidance_output
REM
                fi
                delta_prompt_injected=1
                task_notes+=("Acceptance validators reported outstanding issues; retrying with delta-only guidance.")
              fi
              apply_status="needs-delta"
              task_commands=()
              task_written_paths=()
              task_patched_paths=()
              validator_cached=0
              validator_status="inconclusive"
              validator_summary=""
              validator_details="[]"
              validator_report=""
              continue
            fi

            warn "  Acceptance validators remained failing without repository changes."
            if (( task_locked_migration )) && [[ "$task_status_lower" =~ $terminal_locked_regex ]]; then
              task_result_status="${task_status_original:-complete}"
              apply_status="locked-no-output"
              task_notes+=("Migration lock active; retained status ${task_result_status} despite empty diff.")
              codex_ok=1
            else
              task_needs_review=1
              manual_followups=1
              keep_output=1
              task_result_status="blocked"
              apply_status="validators-failed"
              if [[ -n "$validator_summary" ]]; then
                task_notes+=("Validators failing with no repository changes: ${validator_summary}")
              else
                task_notes+=("Validators failing with no repository changes; manual follow-up required.")
              fi
              if [[ -n "$validator_report_display" ]]; then
                task_notes+=("Validator report stored at ${validator_report_display}.")
              fi
              codex_ok=1
            fi
            break
          fi

          if [[ "$apply_status" == "parse-error" ]]; then
            if (( attempt < max_attempts )); then
              warn "  Codex returned invalid JSON; retrying (attempt $((attempt + 1)) of ${max_attempts})."
              if (( prompt_augmented == 0 )); then
                cat >>"$prompt_path" <<'REM'

## Reminder
- Output a single JSON object exactly as described above; do not include any explanatory text outside the JSON.
- If no changes are required, return the JSON with an empty `changes` array and clear notes explaining why.
- Diff entries must remain valid unified diffs.
REM
                prompt_augmented=1
              fi
              continue
            else
              warn "  Codex output was invalid JSON after retry; manual review required (see ${output_path})."
              task_needs_review=1
              manual_followups=1
              keep_output=1
              task_result_status="on-hold"
              task_notes+=("Codex output remained invalid JSON after retries; inspect ${output_path}.")
              codex_ok=1
              break_after_update=1
            fi
          fi

          if (( task_changes_applied > 0 )); then
            gc_note_mutation
          fi

          local stdout_hash="${GC_LAST_CODEX_STDOUT_TAIL_HASH:-none}"
          local stdout_base64="${GC_LAST_CODEX_STDOUT_TAIL_BASE64:-}"
          local turn_hash="${GC_LAST_CODEX_TURN_DIFF_HASH:-}"
          local turn_blocks="${GC_LAST_CODEX_TURN_DIFF_BLOCKS:-0}"
          local diff_after=""
          if (( diff_guard_enabled )); then
            diff_after="$(gc_diff_fingerprint)"
          fi
          local after_signature="${diff_after:-}:${stdout_hash}"
          local attempt_tokens_value=0
          if [[ "$attempt_tokens" =~ ^[0-9]+$ ]]; then
            attempt_tokens_value=$((attempt_tokens))
          fi
          local progress_sig="${diff_before:-}:${diff_after:-}:${stdout_hash}:${attempt_tokens_value}"
          local guard_reason=""
          if (( diff_guard_enabled )); then
            if (( attempt_tokens_value <= diff_guard_token_threshold )) && [[ "$progress_sig" == "$diff_last_transition" ]]; then
              diff_stall_count=$((diff_stall_count + 1))
            else
              diff_stall_count=0
            fi
            if (( diff_guard_stall_limit > 0 )) && (( diff_stall_count >= diff_guard_stall_limit )); then
              guard_reason="no-progress"
            fi
          fi
          if [[ -z "$guard_reason" && -n "$diff_prev_prev_after_sig" && "$after_signature" == "$diff_prev_prev_after_sig" && "$diff_prev_after_sig" != "$after_signature" ]]; then
            guard_reason="oscillation"
          fi
          if (( diff_guard_enabled )) && [[ -z "$guard_reason" ]]; then
            if [[ -n "$turn_hash" && "$turn_blocks" -gt 0 ]]; then
              if [[ "$turn_hash" == "$turn_diff_prev_hash" ]]; then
                turn_diff_repeat_count=$((turn_diff_repeat_count + 1))
              else
                turn_diff_repeat_count=1
              fi
            else
              turn_diff_repeat_count=0
            fi
            turn_diff_prev_hash="$turn_hash"
            if [[ -n "$turn_hash" ]] && (( diff_guard_turn_repeat_limit > 0 )) && (( turn_diff_repeat_count >= diff_guard_turn_repeat_limit )); then
              guard_reason="turn-diff"
            fi
          fi
          if [[ -z "$guard_reason" ]] && (( diff_guard_file_cooldown > 0 )) && [[ -n "$task_change_sizes" ]]; then
            while IFS=$'	' read -r base_path size_bytes; do
              [[ -z "$base_path" ]] && continue
              if ! [[ "$size_bytes" =~ ^[0-9]+$ ]]; then
                continue
              fi
              local change_size=$((size_bytes))
              if (( change_size >= diff_guard_file_min_bytes )); then
                continue
              fi
              local last_step=""
              while IFS=$'	' read -r hist_path hist_step _hist_bytes; do
                [[ -z "$hist_path" ]] && continue
                if [[ "$hist_path" == "$base_path" ]]; then
                  last_step="$hist_step"
                  break
                fi
              done <<<"$diff_guard_history"
              if [[ -n "$last_step" ]]; then
                local step_delta=$((diff_guard_current_step - last_step))
                if (( step_delta <= diff_guard_file_cooldown )); then
                  guard_reason="file-cooldown:${base_path}"
                  break
                fi
              fi
            done <<<"$task_change_sizes"
          fi
          if (( diff_guard_enabled )); then
            diff_prev_prev_after_sig="$diff_prev_after_sig"
            diff_prev_after_sig="$after_signature"
            diff_last_transition="$progress_sig"
          fi
          if [[ -n "$task_change_sizes" ]]; then
            local new_history=""
            while IFS=$'\t' read -r size_path size_bytes; do
              [[ -z "$size_path" ]] && continue
              new_history+="${size_path}"$'\t'"${diff_guard_current_step}"$'\t'"${size_bytes}"$'\n'
            done <<<"$task_change_sizes"
            if [[ -n "$new_history" ]]; then
              diff_guard_history="${new_history}${diff_guard_history}"
              diff_guard_history="$(printf '%s' "$diff_guard_history" | head -n "$diff_guard_history_limit")"
            fi
          fi
          if [[ -n "$guard_reason" ]]; then
            warn "    Loop guard (${guard_reason}) detected; halting task."
            task_needs_review=1
            manual_followups=1
            keep_output=1
            task_result_status="on-hold"
            apply_status="loop-guard"
            codex_ok=1
            break_after_update=1
            story_failed=1
            loop_guard_triggered=1
            work_failed=1
            local guard_log="${story_run_dir}/loop_guard_${task_number}_${attempt}.log"
            {
              printf 'LOOP_GUARD_TRIPPED\n'
              printf 'reason: %s\n' "$guard_reason"
              printf 'timestamp: %s\n' "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
              printf 'attempt_tokens: %s\n' "$attempt_tokens_value"
              printf 'diff_before: %s\n' "${diff_before:-}"
              printf 'diff_after: %s\n' "${diff_after:-}"
              printf 'stdout_hash: %s\n' "$stdout_hash"
              printf 'turn_hash: %s\n' "$turn_hash"
              printf 'turn_blocks: %s\n' "$turn_blocks"
              printf '\n-- git diff --stat --\n'
              git -C "$PROJECT_ROOT" diff --stat || true
              printf '\n-- git diff --numstat --\n'
              git -C "$PROJECT_ROOT" diff --numstat || true
              printf '\n-- stdout tail (latest %s bytes) --\n' "$diff_guard_stdout_slice"
              if [[ -n "$stdout_base64" ]]; then
                if command -v "$python_bin" >/dev/null 2>&1; then
                  local decode_helper
                  decode_helper="$(gc_clone_python_tool "decode_base64_stdout.py" "${PROJECT_ROOT:-$PWD}")" || decode_helper=""
                  if [[ -n "$decode_helper" ]]; then
                    "$python_bin" "$decode_helper" "$stdout_base64"
                  else
                    printf '(python helper unavailable to decode base64 tail)\n'
                  fi
                else
                  printf '(python3 unavailable to decode base64 tail)\n'
                fi
              else
                printf '(unavailable)\n'
              fi
            } >"$guard_log" 2>&1 || true
            local guard_rel="$guard_log"
            if [[ -n "$PROJECT_ROOT" ]]; then
              guard_rel="${guard_rel#$PROJECT_ROOT/}"
            fi
            task_notes+=("Loop guard tripped (${guard_reason}); see ${guard_rel}.")
            task_notes+=("LOOP_GUARD_TRIPPED exit=${loop_guard_exit_code}")
            break
          fi

          if (( keep_output == 0 )); then
            rm -f "$prompt_path" "$output_path"
          fi
          codex_ok=1
        else
          local codex_status=$?
          local attempt_tokens="${GC_CODEX_CALL_TOKEN_ACCUM:-${GC_LAST_CODEX_TOTAL_TOKENS:-0}}"
          if [[ "$attempt_tokens" =~ ^[0-9]+$ ]]; then
            task_tokens_total=$((task_tokens_total + attempt_tokens))
          fi
          local attempt_prompt_tokens="${GC_LAST_CODEX_PROMPT_TOKENS:-0}"
          local attempt_completion_tokens="${GC_LAST_CODEX_COMPLETION_TOKENS:-0}"
          if [[ "$attempt_prompt_tokens" =~ ^[0-9]+$ ]]; then
            task_llm_prompt_tokens=$((task_llm_prompt_tokens + attempt_prompt_tokens))
          fi
          if [[ "$attempt_completion_tokens" =~ ^[0-9]+$ ]]; then
            task_llm_completion_tokens=$((task_llm_completion_tokens + attempt_completion_tokens))
          fi
          if [[ "${GC_CODEX_USAGE_LIMIT_REACHED:-0}" == "1" ]]; then
            local limit_message="${GC_CODEX_USAGE_LIMIT_MESSAGE:-}"
            local tokens_this_task="$task_tokens_total"
            if ! [[ "$tokens_this_task" =~ ^[0-9]+$ ]]; then
              tokens_this_task=0
            fi
            local usage_limit_confirmed="${GC_CODEX_USAGE_LIMIT_CONFIRMED:-0}"

            if (( tokens_this_task == 0 )); then
              warn "  Quota suspicion but tokens=0 for ${call_name} — continuing (soft gate)."
              if [[ -n "$limit_message" ]]; then
                warn "    $limit_message"
              fi
              GC_CODEX_USAGE_LIMIT_REACHED=0
              GC_CODEX_USAGE_LIMIT_MESSAGE=""
              GC_CODEX_USAGE_LIMIT_CONFIRMED=0
              continue
            fi

            if (( usage_limit_confirmed == 0 )); then
              warn "  Codex quota warning for ${call_name} not confirmed by provider — continuing."
              if [[ -n "$limit_message" ]]; then
                warn "    $limit_message"
              fi
              GC_CODEX_USAGE_LIMIT_REACHED=0
              GC_CODEX_USAGE_LIMIT_MESSAGE=""
              GC_CODEX_USAGE_LIMIT_CONFIRMED=0
              continue
            fi

            local wait_seconds
            local wait_helper
            wait_helper="$(gc_clone_python_tool "parse_wait_seconds.py" "${PROJECT_ROOT:-$PWD}")" || wait_helper=""
            if [[ -n "$wait_helper" ]]; then
              wait_seconds="$("$python_bin" "$wait_helper" "${limit_message}")"
            else
              wait_seconds=3600
            fi
            if ! [[ "$wait_seconds" =~ ^[0-9]+$ ]]; then
              wait_seconds=3600
            fi
            local wait_minutes=$((wait_seconds / 60))
            if (( wait_minutes > 0 )); then
              warn "  Codex usage limit confirmed; pausing for approximately ${wait_minutes} minute(s) before retry."
            else
              warn "  Codex usage limit confirmed; pausing for ${wait_seconds} second(s) before retry."
            fi
            if [[ -n "$limit_message" ]]; then
              warn "    $limit_message"
            fi
            sleep "$wait_seconds" || true
            GC_CODEX_USAGE_LIMIT_REACHED=0
            GC_CODEX_USAGE_LIMIT_MESSAGE=""
            GC_CODEX_USAGE_LIMIT_CONFIRMED=0
            (( attempt-- ))
            continue
          elif (( codex_status == 124 )); then
            warn "  Codex was idle for ${codex_timeout_seconds}s during ${call_name}."
            if (( attempt < max_attempts )); then
              info "    Retrying task ${task_number} after timeout (attempt $((attempt + 1)) of ${max_attempts})."
              continue
            fi
            task_result_status="on-hold"
            task_needs_review=1
            manual_followups=1
            keep_output=1
            task_notes+=("Codex produced no output for ${codex_timeout_seconds}s; rerun work-on-tasks to continue from this task.")
            codex_ok=1
            break_after_update=1
            work_failed=1
            break
          elif (( codex_status == 125 )); then
            warn "  Codex loop guard detected repeated diffs/oscillation; halting task."
            task_result_status="on-hold"
            task_needs_review=1
            manual_followups=1
            keep_output=1
            task_notes+=("Codex loop guard triggered (repeated diffs); inspect Codex log for details.")
            apply_status="loop-guard"
            codex_ok=1
            break_after_update=1
            story_failed=1
            loop_guard_triggered=1
            work_failed=1
            break
          elif (( codex_status == 126 )); then
            warn "  Codex turn limit reached for ${call_name}; stopping task."
            task_result_status="on-hold"
            task_needs_review=1
            manual_followups=1
            keep_output=1
            task_notes+=("Codex turn limit reached; adjust focus or retry after inspecting Codex log.")
            apply_status="loop-guard"
            codex_ok=1
            break_after_update=1
            story_failed=1
            loop_guard_triggered=1
            work_failed=1
            break
          fi
          warn "  Codex execution failed for ${call_name}; progress saved."
          task_result_status="blocked"
          task_needs_review=1
          manual_followups=1
          keep_output=1
          task_notes+=("Codex execution failed; no automated changes were applied.")
          codex_ok=1
          break
        fi
        break
      done

      local should_run_verify=0
      if [[ "${GC_COMPLETE_ON_VERIFY:-1}" == "1" ]]; then
        should_run_verify=1
      elif (( task_changes_applied == 0 )); then
        should_run_verify=1
      elif (( task_meta_plan_flag == 1 || task_meta_focus_flag == 1 )); then
        should_run_verify=1
      fi
      if (( should_run_verify )); then
        if (( validator_cached )); then
          task_verify_status="$validator_status"
          task_verify_summary="$validator_summary"
          task_verify_details="$validator_details"
          task_verify_report="$validator_report"
        elif gc_run_task_verify "$task_ref_for_verify" "$story_run_dir"; then
          task_verify_status="${GC_LAST_VERIFY_STATUS:-inconclusive}"
          task_verify_summary="${GC_LAST_VERIFY_SUMMARY:-}"
          task_verify_details="${GC_LAST_VERIFY_DETAILS:-}"
          task_verify_report="${GC_LAST_VERIFY_REPORT:-}"
        else
          task_verify_status="inconclusive"
          task_verify_summary="${GC_LAST_VERIFY_SUMMARY:-Verification helper failure}"
          task_verify_details="${GC_LAST_VERIFY_DETAILS:-}"
          task_verify_report="${GC_LAST_VERIFY_REPORT:-}"
        fi
        local verify_report_display="$task_verify_report"
        if [[ -n "$verify_report_display" && -n "$PROJECT_ROOT" && "$verify_report_display" == "$PROJECT_ROOT/"* ]]; then
          verify_report_display="${verify_report_display#$PROJECT_ROOT/}"
        fi
        if [[ -n "$task_verify_summary" ]]; then
          task_notes+=("Verify ${task_verify_status}: ${task_verify_summary}")
        else
          task_notes+=("Verify ${task_verify_status}.")
        fi
        if [[ -n "$verify_report_display" ]]; then
          task_notes+=("Verification report stored at ${verify_report_display}.")
        fi
      fi

      if [[ "$task_verify_status" == "pass" && $task_changes_applied -eq 0 && "${GC_COMPLETE_ON_VERIFY:-1}" == "1" ]]; then
        if [[ "$task_result_status" == "in-progress" || "$task_result_status" == "complete" ]]; then
          task_result_status="complete"
          task_needs_review=0
          manual_followups=0
          keep_output=0
        fi
      elif [[ "$task_verify_status" == "fail" ]]; then
        task_needs_review=1
        manual_followups=1
        keep_output=1
        task_result_status="blocked"
        if [[ -n "$task_verify_report" ]]; then
          task_notes+=("Verification failed; inspect ${task_verify_report}.")
        fi
      elif [[ "$task_verify_status" == "inconclusive" && $task_changes_applied -eq 0 ]]; then
        if [[ "$task_result_status" == "in-progress" ]]; then
          task_needs_review=1
          manual_followups=1
          task_result_status="blocked"
          task_notes+=("Verification inconclusive with zero repository changes; follow up required.")
        fi
      fi

      if (( task_needs_review )) && (( auto_review_enabled )) && (( task_changes_applied > 0 )); then
        local auto_review_path=""
        if auto_review_path="$(gc_generate_auto_review "$story_run_dir" "$story_log_dir" "$banner_task_id" "$task_id" "$task_result_status" "$task_number" "$task_tokens_total" "$PROJECT_ROOT")"; then
          task_needs_review=0
          manual_followups=0
          if [[ "$task_result_status" == "on-hold" || "$task_result_status" == "apply-failed-migration-context" ]]; then
            task_result_status="complete"
          fi
          local auto_review_rel="$auto_review_path"
          if [[ -n "$PROJECT_ROOT" && "$auto_review_rel" == "$PROJECT_ROOT/"* ]]; then
            auto_review_rel="${auto_review_rel#$PROJECT_ROOT/}"
          fi
          task_notes+=("Auto-generated review artifact: ${auto_review_rel}")
          info "    Auto review generated at ${auto_review_rel}"
        else
          warn "    GC_AUTO_REVIEW enabled but review artifact generation failed."
        fi
      fi

      if (( task_needs_review )) && (( task_changes_applied == 0 )) && [[ "${GC_COMPLETE_ON_EMPTY:-0}" == "1" ]]; then
        local _gc_note_text=""
        local _gc_note_lower=""
        local _gc_complete_on_empty=0
        for _gc_note_text in "${task_notes[@]}"; do
          _gc_note_lower="${_gc_note_text,,}"
          if [[ "$_gc_note_lower" == *"already satisfies"* ]] || [[ "$_gc_note_lower" == *"no repository edits required"* ]] || [[ "$_gc_note_lower" == *"no changes needed"* ]] || [[ "$_gc_note_lower" == *"acceptance criteria met"* ]]; then
            _gc_complete_on_empty=1
            break
          fi
        done
        if (( _gc_complete_on_empty )); then
          task_needs_review=0
          manual_followups=0
          keep_output=0
          task_result_status="completed-no-changes"
          if [[ -z "$apply_status" || "$apply_status" == "pending" || "$apply_status" == "no-changes" ]]; then
            apply_status="completed-no-changes"
          fi
          task_notes+=("Marked completed-no-changes via GC_COMPLETE_ON_EMPTY; existing artifacts satisfy the task.")
        fi
      fi

      if (( task_needs_review )); then
        manual_followups=1
        if [[ "$task_result_status" != "blocked" ]]; then
          if (( ${GC_WOT_COMPLETE_ON_FOLLOWUP:-0} )); then
            case "$task_result_status" in
              complete|completed-no-changes)
                task_notes+=("Follow-up required; status remains complete due to --complete-on-followup.")
                ;;
              apply-failed-migration-context|on-hold|in-progress)
                task_result_status="on-hold"
                ;;
              *)
                task_result_status="on-hold"
                ;;
            esac
          else
            task_result_status="on-hold"
          fi
        fi
      fi

      if (( codex_ok == 0 )); then
        task_result_status="blocked"
        task_notes+=("Codex execution did not complete; no changes were applied.")
      fi

      if (( codex_attempted )) && (( codex_ok )) && (( context_lines_current > context_lines_min || context_file_lines_current > context_file_lines_min )); then
        local last_tokens="${GC_LAST_CODEX_TOTAL_TOKENS:-0}"
        local shrink_threshold="$context_auto_shrink_threshold"
        if [[ "$prompt_soft_limit_value" =~ ^[0-9]+$ ]] && (( prompt_soft_limit_value > 0 )); then
          shrink_threshold="$prompt_soft_limit_value"
        fi
        local trigger_shrink=0
        if [[ "$last_tokens" =~ ^[0-9]+$ ]] && (( last_tokens > shrink_threshold )); then
          trigger_shrink=1
        fi
        if (( trigger_shrink )); then
          if (( context_shrink_iterations > 0 )) && (( last_tokens <= context_last_shrink_tokens )); then
            trigger_shrink=0
          fi
        fi
        if (( trigger_shrink )); then
          local new_context_lines="$context_lines_current"
          local new_context_file_lines="$context_file_lines_current"
          local reduced=0
          if (( context_lines_current > context_lines_min )); then
            local decrement_lines=$(( context_lines_current / 3 ))
            (( decrement_lines < 40 )) && decrement_lines=40
            new_context_lines=$(( context_lines_current - decrement_lines ))
            if (( new_context_lines < context_lines_min )); then
              new_context_lines="$context_lines_min"
            fi
            if (( new_context_lines < context_lines_current )); then
              reduced=1
            fi
          fi
          if (( context_file_lines_current > context_file_lines_min )); then
            local decrement_files=$(( context_file_lines_current / 2 ))
            (( decrement_files < 20 )) && decrement_files=20
            new_context_file_lines=$(( context_file_lines_current - decrement_files ))
            if (( new_context_file_lines < context_file_lines_min )); then
              new_context_file_lines="$context_file_lines_min"
            fi
            if (( new_context_file_lines < context_file_lines_current )); then
              reduced=1
            fi
          fi
          if (( reduced )); then
            info "    Token usage ${last_tokens} exceeded budget (~${shrink_threshold}); pruning shared context to ${new_context_lines} lines (per-file ${new_context_file_lines})."
            context_lines_current="$new_context_lines"
            context_file_lines_current="$new_context_file_lines"
            context_last_shrink_tokens="$last_tokens"
            context_shrink_iterations=$((context_shrink_iterations + 1))
            GC_CONTEXT_FILE_LINES="$context_file_lines_current"
            export GC_CONTEXT_FILE_LINES
            GC_CONTEXT_TAIL_LIMIT="$context_lines_current"
            export GC_CONTEXT_TAIL_LIMIT
            if ! gc_build_context_file "$ctx_file" "$STAGING_DIR"; then
              warn "Failed to rebuild shared context after pruning."
            else
              if [[ -n "$context_tail" && -f "$ctx_file" ]]; then
                local new_mode
                new_mode="$(gc_refresh_context_tail "$ctx_file" "$context_tail" "$context_tail_mode" "$context_lines_current")"
                if [[ "$new_mode" == "raw" && "$context_tail_mode" != "raw" ]]; then
                  context_tail="${run_dir}/context_tail.md"
                  new_mode="$(gc_refresh_context_tail "$ctx_file" "$context_tail" "raw" "$context_lines_current")"
                fi
                context_tail_mode="$new_mode"
                GC_CONTEXT_TAIL_MODE="$context_tail_mode"
                export GC_CONTEXT_TAIL_MODE
              fi
            fi
          else
            context_last_shrink_tokens="$last_tokens"
          fi
        fi
      fi

      # Register documentation changes in the registry and guard against rejects.
      if (( ${#task_written_paths[@]} > 0 || ${#task_patched_paths[@]} > 0 )); then
        local -A gc_doc_changed=()
        local gc_doc_path=""
        for gc_doc_path in "${task_written_paths[@]}" "${task_patched_paths[@]}"; do
          [[ -z "$gc_doc_path" ]] && continue
          gc_doc_path="${gc_doc_path%% (*}"
          if [[ "$gc_doc_path" == docs/* ]]; then
            gc_doc_changed["$gc_doc_path"]=1
          fi
        done
        if (( ${#gc_doc_changed[@]} > 0 )); then
          local doc_registry_python="${PYTHON_BIN:-python3}"
          local doc_registry_tool="${CLI_ROOT}/src/lib/doc_registry.py"
          if command -v "$doc_registry_python" >/dev/null 2>&1 && [[ -f "$doc_registry_tool" ]]; then
            local gc_doc_register_ok=1
            local -a gc_doc_sorted=()
            while IFS= read -r gc_doc_line; do
              gc_doc_sorted+=("$gc_doc_line")
            done < <(printf '%s\n' "${!gc_doc_changed[@]}" | LC_ALL=C sort)
            local gc_doc_helper=""
            gc_doc_helper="$(gc_clone_python_tool "doc_registry_compute_id.py" "${PROJECT_ROOT:-$PWD}")" || gc_doc_helper=""
            if [[ -z "$gc_doc_helper" ]]; then
              gc_doc_register_ok=0
              task_notes+=("Doc registry helper unavailable; register manually for: ${gc_doc_sorted[*]}")
            else
              local gc_doc_reg_path=""
              for gc_doc_reg_path in "${gc_doc_sorted[@]}"; do
                local gc_doc_abs="${PROJECT_ROOT}/${gc_doc_reg_path}"
                if [[ ! -f "$gc_doc_abs" ]]; then
                  task_notes+=("Doc registry skipped ${gc_doc_reg_path} (file missing after apply).")
                  continue
                fi
                local gc_doc_id=""
                if ! gc_doc_id="$("$doc_registry_python" "$gc_doc_helper" "$gc_doc_abs")"; then
                  gc_doc_register_ok=0
                  task_notes+=("Failed to compute documentation id for ${gc_doc_reg_path}; register manually.")
                  continue
                fi
                if ! "$doc_registry_python" "$doc_registry_tool" register "$gc_doc_id" "$gc_doc_abs" "updated trace link(s)"; then
                  gc_doc_register_ok=0
                  task_notes+=("Doc registry update failed for ${gc_doc_reg_path}; rerun `python3 src/lib/doc_registry.py register ${gc_doc_id} ${gc_doc_reg_path}`.")
                fi
              done
            fi
            if (( gc_doc_register_ok == 0 )); then
              task_needs_review=1
              manual_followups=1
            fi
          else
            local -a gc_doc_missing=()
            while IFS= read -r gc_doc_line; do
              gc_doc_missing+=("$gc_doc_line")
            done < <(printf '%s\n' "${!gc_doc_changed[@]}" | LC_ALL=C sort)
            task_notes+=("Doc registry tooling unavailable; queue manual register for: ${gc_doc_missing[*]}")
          fi
        fi
      fi

      local -a gc_doc_rejects=()
      if [[ -d "${PROJECT_ROOT}/docs" ]]; then
        while IFS= read -r -d '' gc_reject; do
          gc_reject="${gc_reject#"${PROJECT_ROOT}/"}"
          gc_doc_rejects+=("$gc_reject")
        done < <(find "${PROJECT_ROOT}/docs" -type f -name '*.rej' -print0 2>/dev/null)
      fi
      if (( ${#gc_doc_rejects[@]} > 0 )); then
        task_result_status="blocked-merge-conflict"
        apply_status="blocked-merge-conflict"
        task_needs_review=1
        manual_followups=1
        keep_output=1
        task_notes+=("Documentation patch rejects present: ${gc_doc_rejects[*]}. Resolve before rerunning work-on-tasks.")
      fi

      if [[ "$task_result_status" == "in-progress" ]]; then
        task_result_status="complete"
      fi

      local task_end_epoch
      task_end_epoch="$(date +%s)"
      task_duration_seconds=$((task_end_epoch - task_start_epoch))
      if (( task_duration_seconds < 0 )); then
        task_duration_seconds=0
      fi
      task_duration_display="$(gc_format_duration_compact "$task_duration_seconds")"
      task_tokens_display="$(gc_format_tokens_compact "$task_tokens_total")"
      local task_prompt_estimate_display
      task_prompt_estimate_display="$(gc_format_tokens_compact "$task_prompt_estimate")"

      local story_status_hint="in-progress"
      case "$task_result_status" in
        blocked|blocked-budget|blocked-schema-drift|blocked-schema-guard-error|blocked-dependency\(*\)|retryable|blocked-push) story_status_hint="blocked" ;;
        on-hold) story_status_hint="on-hold" ;;
      esac

      local completed_hint="$task_index"
      if [[ "$task_result_status" == "complete" || "$task_result_status" == "completed-no-changes" ]]; then
        completed_hint=$((task_index + 1))
      fi

      gc_update_task_state "$tasks_db" "$slug" "$task_index" "$task_result_status" "$run_stamp"
      gc_update_work_state "$tasks_db" "$slug" "$story_status_hint" "$completed_hint" "$total_tasks_int" "$run_stamp"

      if [[ "$task_result_status" == "complete" || "$task_result_status" == "completed-no-changes" ]]; then
        local commit_label="${banner_task_id}: ${task_title:-Task ${task_number}}"
        local original_auto_push="${GC_AUTO_PUSH:-0}"
        local original_remote="${GC_AUTO_PUSH_REMOTE:-}"
        local original_branch="${GC_AUTO_PUSH_BRANCH:-}"
        local auto_push_enabled="${GC_AUTO_PUSH:-0}"
        if [[ "$auto_push_enabled" != "1" ]]; then
          auto_push_enabled=0
          GC_AUTO_PUSH=1
          GC_AUTO_PUSH_REMOTE="__skip__"
        else
          auto_push_enabled=1
        fi
        export GC_AUTOPUSH_TASK_REF="$task_ref_for_verify"
        export GC_AUTOPUSH_VERIFY_STATUS="$task_verify_status"
        export GC_ALLOW_EMPTY_COMMIT="${GC_ALLOW_EMPTY_COMMIT:-1}"

        local finalize_attempt=1
        local finalize_success=0
        local finalize_error=""
        local retry_cap="${GC_RETRY_PUSH_MAX:-3}"
        if ! [[ "$retry_cap" =~ ^[0-9]+$ ]]; then
          retry_cap=3
        fi

        while (( finalize_attempt <= retry_cap )); do
          if gc_auto_push_helper "$commit_label" "${task_auto_push_records[@]}"; then
            finalize_success=1
            break
          fi
          finalize_error="${GC_LAST_AUTO_PUSH_ERROR:-auto-push failure}"
          sleep $((finalize_attempt * 2))
          ((finalize_attempt++))
        done

        if (( auto_push_enabled == 0 )); then
          GC_AUTO_PUSH="$original_auto_push"
          GC_AUTO_PUSH_REMOTE="$original_remote"
          GC_AUTO_PUSH_BRANCH="$original_branch"
        fi

        if (( finalize_success )); then
          if [[ "${GC_LAST_AUTO_COMMIT_STATUS:-}" == "committed" ]]; then
            local commit_hash="${GC_LAST_AUTO_COMMIT_HASH:0:7}"
            local push_desc=""
            if [[ "${GC_LAST_AUTO_PUSH_STATUS:-}" == "pushed" ]]; then
              push_desc="pushed to ${GC_LAST_AUTO_PUSH_REMOTE:-origin}/${GC_LAST_AUTO_PUSH_BRANCH:-HEAD}"
            elif [[ "${GC_LAST_AUTO_PUSH_STATUS:-}" == "failed" ]]; then
              push_desc="push failed"
            elif [[ "${GC_LAST_AUTO_PUSH_STATUS:-}" == "skipped" ]]; then
              push_desc="push skipped"
            else
              push_desc="commit recorded"
            fi
            task_notes+=("Auto-finalized commit ${commit_hash} (${push_desc}).")
          fi
        else
          task_result_status="blocked-push"
          manual_followups=1
          task_notes+=("Auto-push failed after ${retry_cap} attempt(s): ${finalize_error}.")
          keep_output=1
        fi

        local finalize_script="${PROJECT_ROOT}/scripts/auto_finalize_task.sh"
        if (( finalize_success )) && [[ -x "$finalize_script" ]]; then
          if ! bash "$finalize_script"; then
            warn "  Auto finalize failed; inspect git status."
          fi
        fi
      fi

      if (( blocked_stop_run )); then
        story_failed=1
        work_failed=1
      fi

      local timestamp_utc
      timestamp_utc="$(date -u +%Y-%m-%dT%H:%M:%SZ)"

      local prompt_entry="$prompt_path"
      local output_entry="$output_path"
      local report_entry_path="$task_log_archive_path"
      local report_entry_display="$report_entry_path"
      local report_entry_db=""
      local project_prefix="${PROJECT_ROOT}/"
      if [[ -n "$PROJECT_ROOT" ]]; then
        if [[ "$prompt_entry" == "$project_prefix"* ]]; then
          prompt_entry="${prompt_entry#$project_prefix}"
        fi
        if [[ "$output_entry" == "$project_prefix"* ]]; then
          output_entry="${output_entry#$project_prefix}"
        fi
        if [[ "$report_entry_display" == "$project_prefix"* ]]; then
          report_entry_display="${report_entry_display#$project_prefix}"
        fi
      fi
      report_entry_db="$report_entry_display"
      if [[ ! -f "$prompt_path" ]]; then
        if (( keep_artifacts == 0 )); then
          prompt_entry="(discarded)"
        else
          prompt_entry="(missing)"
        fi
      fi
      if [[ ! -f "$output_path" ]]; then
        if (( keep_output == 0 )); then
          output_entry="(discarded)"
        else
          output_entry="(missing)"
        fi
      fi
      if [[ ! -f "$task_log_archive_path" ]]; then
        report_entry_db=""
        report_entry_display="(missing)"
      fi

      local changes_flag="false"
      if (( task_changes_applied > 0 )); then
        changes_flag="true"
      fi

      local notes_payload=""
      if ((${#task_notes[@]} > 0)); then
        notes_payload="$(printf '%s\n' "${task_notes[@]}")"
      fi
      local written_payload=""
      if ((${#task_written_paths[@]} > 0)); then
        written_payload="$(printf '%s\n' "${task_written_paths[@]}")"
      fi
      local patched_payload=""
      if ((${#task_patched_paths[@]} > 0)); then
        patched_payload="$(printf '%s\n' "${task_patched_paths[@]}")"
      fi
      local commands_payload=""
      if ((${#task_commands[@]} > 0)); then
        commands_payload="$(printf '%s\n' "${task_commands[@]}")"
      fi

      local observation_hash=""
      if (( task_tokens_total > 0 )); then
        local observation_seed="${task_id:-}:${stdout_hash:-}:${apply_status:-}:${task_result_status:-}:${task_tokens_total:-0}"
        if observation_hash="$(gc_make_observation_hash "$observation_seed" 2>/dev/null)"; then
          :
        else
          observation_hash=""
        fi
      fi
      local stage_retrieve_after_int stage_plan_after_int stage_patch_after_int stage_verify_after_int
      stage_retrieve_after_int="$(gc_parse_int "${GC_BUDGET_STAGE_TOTAL_RETRIEVE:-0}" 0)"
      stage_plan_after_int="$(gc_parse_int "${GC_BUDGET_STAGE_TOTAL_PLAN:-0}" 0)"
      stage_patch_after_int="$(gc_parse_int "${GC_BUDGET_STAGE_TOTAL_PATCH:-0}" 0)"
      stage_verify_after_int="$(gc_parse_int "${GC_BUDGET_STAGE_TOTAL_VERIFY:-0}" 0)"
      local stage_retrieve_start_int stage_plan_start_int stage_patch_start_int stage_verify_start_int
      stage_retrieve_start_int="$(gc_parse_int "$stage_baseline_retrieve" 0)"
      stage_plan_start_int="$(gc_parse_int "$stage_baseline_plan" 0)"
      stage_patch_start_int="$(gc_parse_int "$stage_baseline_patch" 0)"
      stage_verify_start_int="$(gc_parse_int "$stage_baseline_verify" 0)"
      local task_stage_tokens_retrieve=$((stage_retrieve_after_int - stage_retrieve_start_int))
      local task_stage_tokens_plan=$((stage_plan_after_int - stage_plan_start_int))
      local task_stage_tokens_patch=$((stage_patch_after_int - stage_patch_start_int))
      local task_stage_tokens_verify=$((stage_verify_after_int - stage_verify_start_int))
      (( task_stage_tokens_retrieve < 0 )) && task_stage_tokens_retrieve=0
      (( task_stage_tokens_plan < 0 )) && task_stage_tokens_plan=0
      (( task_stage_tokens_patch < 0 )) && task_stage_tokens_patch=0
      (( task_stage_tokens_verify < 0 )) && task_stage_tokens_verify=0

      gc_record_task_progress "$tasks_db" "$slug" "$task_index" "$run_stamp" "$task_result_status" "$report_entry_db" "$prompt_entry" "$output_entry" "$attempt" "$task_tokens_total" "$task_prompt_estimate" "$task_llm_prompt_tokens" "$task_llm_completion_tokens" "$task_duration_seconds" "$apply_status" "$changes_flag" "$notes_payload" "$written_payload" "$patched_payload" "$commands_payload" "$observation_hash" "$timestamp_utc" "$task_stage_tokens_retrieve" "$task_stage_tokens_plan" "$task_stage_tokens_patch" "$task_stage_tokens_verify" "$task_story_points" "$task_verify_status" "$task_verify_summary" "$task_verify_report" "$task_verify_details" "$task_meta_plan_flag" "$task_meta_focus_flag" "$task_meta_no_changes_flag" "$task_meta_already_flag" "${GC_LAST_AUTO_COMMIT_HASH:-}" "${GC_LAST_AUTO_COMMIT_STATUS:-}" "${GC_LAST_AUTO_PUSH_STATUS:-}" "${GC_LAST_AUTO_PUSH_REMOTE:-}" "${GC_LAST_AUTO_PUSH_BRANCH:-}" "${GC_LAST_AUTO_PUSH_ERROR:-}"

      gc_log_task_metrics "$run_stamp" "$slug" "$task_number" "$banner_task_id" "$task_result_status" "$task_story_points" "$task_stage_tokens_retrieve" "$task_stage_tokens_plan" "$task_stage_tokens_patch" "$task_stage_tokens_verify" "$task_prompt_estimate" "$task_llm_prompt_tokens" "$task_llm_completion_tokens"

      if [[ "$task_result_status" == "complete" || "$task_result_status" == "completed-no-changes" ]]; then
        local throughput_task_msg=""
        if throughput_task_msg="$(gc_update_throughput_metrics "$tasks_db" "task-complete" "$slug" "$task_index")"; then
          if [[ -n "$throughput_task_msg" ]]; then
            info "  ${throughput_task_msg}"
            now_ts="$(date +%s)"
            throughput_next_checkpoint=$((now_ts + throughput_checkpoint_interval))
          fi
        else
          warn "  Failed to record throughput metrics for task ${task_number}."
        fi
      fi

      {
        printf 'task_number: %s\n' "$task_number"
        printf 'task_id: %s\n' "${task_id:-}"
        printf 'task_title: %s\n' "${task_title//$'\n'/ }"
        printf 'story_slug: %s\n' "$slug"
        printf 'status: %s\n' "$task_result_status"
        printf 'timestamp: %s\n' "$timestamp_utc"
        printf 'attempts: %s\n' "$attempt"
        printf 'apply_status: %s\n' "$apply_status"
        printf 'changes_applied: %s\n' "$changes_flag"
        printf 'tokens_used: %s\n' "$task_tokens_total"
        printf 'llm_prompt_tokens: %s\n' "$task_llm_prompt_tokens"
        printf 'llm_completion_tokens: %s\n' "$task_llm_completion_tokens"
        printf 'prompt_tokens_estimate: %s\n' "$task_prompt_estimate"
        printf 'prompt_path: %s\n' "$prompt_entry"
        printf 'output_path: %s\n' "$output_entry"
        if ((${#task_written_paths[@]} > 0)); then
          printf 'written:\n'
          for path in "${task_written_paths[@]}"; do
            printf '  - %s\n' "$path"
          done
        fi
        if ((${#task_patched_paths[@]} > 0)); then
          printf 'patched:\n'
          for path in "${task_patched_paths[@]}"; do
            printf '  - %s\n' "$path"
          done
        fi
        if ((${#task_commands[@]} > 0)); then
          printf 'commands:\n'
          for cmd in "${task_commands[@]}"; do
            printf '  - %s\n' "$cmd"
          done
        fi
        printf 'notes:\n'
        if ((${#task_notes[@]} > 0)); then
          for note in "${task_notes[@]}"; do
            printf '  - %s\n' "${note//$'\n'/ }"
          done
        else
          printf '  - (none)\n'
        fi
      } >"$task_report_path"
      if ! cp -f "$task_report_path" "$task_log_archive_path"; then
        warn "  Failed to archive task log to ${task_log_archive_path}."
      fi
      gc_clear_active_task

      case "$task_result_status" in
        complete)
          info "  ✓ Task ${task_number} (${task_id:-no-id}) completed with status: ${task_result_status}"
          ;;
        on-hold)
          warn "  Task ${task_number} (${task_id:-no-id}) marked ${task_result_status}; review ${report_entry_display}."
          ;;
        blocked|blocked-budget|blocked-quota|blocked-migration-transition|blocked-schema-drift|blocked-schema-guard-error|blocked-dependency\(*\))
          warn "  Task ${task_number} (${task_id:-no-id}) blocked; see ${report_entry_display}."
          ;;
        apply-failed-migration-context)
          warn "  Task ${task_number} (${task_id:-no-id}) flagged apply-failed-migration-context; inspect ${report_entry_display}."
          ;;
        *)
          info "  Task ${task_number} (${task_id:-no-id}) finished with status: ${task_result_status}"
          ;;
      esac

      local task_status_display="${task_result_status:-unknown}"
      if [[ "$task_status_display" == "complete" ]]; then
        task_status_display="COMPLETED"
      else
        task_status_display="${task_status_display//-/ }"
        task_status_display="${task_status_display//_/ }"
        task_status_display="$(printf '%s' "$task_status_display" | tr '[:lower:]' '[:upper:]')"
      fi

      printf '\n'
      gc_render_task_banner --header-bottom "END OF THE TASK WORK" \
        "END TASK ID" "$banner_task_id" \
        "REPORT:" \
        "TOKENS USED: ${task_tokens_display}" \
        "EST. TOKENS (PROMPT): ${task_prompt_estimate_display}" \
        "STORY POINTS: ${task_story_points_display}" \
        "TIME SPENT: ${task_duration_display}" \
        "STATUS: ${task_status_display}"

      (( ++processed_total ))
      (( ++iteration_processed ))

      case "$task_result_status" in
        blocked|blocked-budget|blocked-quota|blocked-schema-drift|blocked-schema-guard-error|blocked-dependency\(*\))
          story_failed=1
          break
          ;;
      esac

      if (( break_after_update )); then
        continue
      fi

      if (( sleep_between_positive )); then
        sleep "$sleep_between"
      fi

    done

    if (( batch_limit_reached )); then
      break
    fi

    if (( story_failed )); then
      warn "Stopping at story ${slug} due to previous error."
      break
    fi

    if (( idle_timeout_triggered )); then
      break
    fi

    gc_update_work_state "$tasks_db" "$slug" "complete" "$total_tasks_int" "$total_tasks_int" "$run_stamp"
    gc_touch_progress
    if (( keep_artifacts == 0 )); then
      rmdir "${story_run_dir}/prompts" 2>/dev/null || true
      rmdir "${story_run_dir}/out" 2>/dev/null || true
    fi
  done < <("$python_bin" "$story_plan_helper" "$tasks_db" "${story_filter}" "$resume_flag")

    if (( migration_transition_triggered )); then
      break
    fi

    if (( idle_timeout_triggered )); then
      break
    fi

    if (( iteration_processed_any )); then
      processed_any_total=1
    else
      if (( processed_any_total == 0 )); then
        info "No stories to process (already complete)."
      fi
      break
    fi

    if (( batch_limit_reached )); then
      remaining_tasks="$(gc_count_pending_tasks "$tasks_db" || echo 0)"
      [[ "$remaining_tasks" =~ ^[0-9]+$ ]] || remaining_tasks=0
      break
    fi

    if (( memory_cycle )); then
      pending_tasks="$(gc_count_pending_tasks "$tasks_db" || echo 0)"
      [[ "$pending_tasks" =~ ^[0-9]+$ ]] || pending_tasks=0
      remaining_tasks="$pending_tasks"
      if (( work_failed == 0 )); then
        if (( iteration_processed > 0 )) && (( pending_tasks > 0 )); then
          gc_trim_memory "memory-cycle"
          info "Memory-cycle paused after ${iteration_processed} task(s); ${pending_tasks} pending."
          memory_cycle_single=1
          continue_current_run=1
        elif (( pending_tasks == 0 )); then
          gc_trim_memory "memory-cycle-final"
        else
          gc_trim_memory "memory-cycle"
        fi
      else
        gc_trim_memory "memory-cycle-error"
      fi
    fi

    if (( continue_current_run == 0 )); then
      remaining_tasks="$(gc_count_pending_tasks "$tasks_db" || echo 0)"
      [[ "$remaining_tasks" =~ ^[0-9]+$ ]] || remaining_tasks=0
      local unstarted_tasks
      unstarted_tasks="$(gc_count_unstarted_tasks "$tasks_db" || echo 0)"
      [[ "$unstarted_tasks" =~ ^[0-9]+$ ]] || unstarted_tasks=0

      if (( manual_followups > 0 )); then
        if (( unstarted_tasks > 0 )); then
          info "Manual follow-ups recorded; ${unstarted_tasks} task(s) still pending — continuing backlog."
          continue_current_run=1
        elif (( remaining_tasks > 0 )); then
          warn "Manual follow-ups detected; backlog paused with ${remaining_tasks} review task(s)."
        fi
      elif (( work_failed == 0 && manual_followups == 0 && memory_cycle == 0 && batch_limit_reached == 0 && effective_batch_size == 0 && iteration_processed > 0 && remaining_tasks > 0 )); then
        if [[ -n "$story_filter" ]]; then
          info "Remaining tasks detected beyond filtered story; rerun with a broader filter to continue."
        else
          info "${remaining_tasks} task(s) remain; continuing work-on-tasks automatically."
          continue_current_run=1
        fi
      fi
    fi

    if (( continue_current_run )); then
      continue
    fi

    break
  done

  if (( idle_timeout_triggered )); then
    warn "work-on-tasks halted by idle timeout after ${idle_timeout}s without progress."
  fi

  if (( processed_any_total == 0 )); then
    gc_clear_active_task
    return 0
  fi

  gc_clear_active_task

  throughput_msg=""
  if throughput_msg="$(gc_update_throughput_metrics "$tasks_db" "flush")"; then
    if [[ -n "$throughput_msg" ]]; then
      info "$throughput_msg"
    fi
  else
    warn "Failed to finalise throughput metrics."
  fi

  if (( backlog_guard_enabled )); then
    local backlog_guard_snapshot_output_after=""
    backlog_snapshot_after_path="${run_dir}/backlog-after.json"
    if backlog_guard_snapshot_output_after="$(gc_backlog_guard_snapshot "$tasks_db" "" "$backlog_guard_window_value" "$backlog_guard_wip_limit" 2>/dev/null)"; then
      if [[ -n "$backlog_guard_snapshot_output_after" ]]; then
        printf '%s\n' "$backlog_guard_snapshot_output_after" >"$backlog_snapshot_after_path"
        local backlog_guard_messages=""
        if backlog_guard_messages="$(gc_backlog_guard_compare "$backlog_snapshot_before_path" "$backlog_snapshot_after_path" "$backlog_guard_wip_limit" 2>/dev/null)"; then
          if [[ -n "$backlog_guard_messages" ]]; then
            local backlog_alerts_path="${run_dir}/backlog-alerts.log"
            printf '%s\n' "$backlog_guard_messages" >"$backlog_alerts_path"
            while IFS=$'\t' read -r backlog_level backlog_message; do
              if [[ -z "$backlog_level" && -z "$backlog_message" ]]; then
                continue
              fi
              case "${backlog_level}" in
                WARN)
                  warn "$backlog_message"
                  ;;
                INFO)
                  info "$backlog_message"
                  ;;
                FREEZE)
                  warn "$backlog_message"
                  if [[ -n "$intake_lock_path" && ! -f "$intake_lock_path" ]]; then
                    {
                      printf 'frozen_at=%s\n' "$(date -u '+%Y-%m-%dT%H:%M:%SZ')"
                      printf 'frozen_by=work-on-tasks\n'
                      printf 'reason=%s\n' "$backlog_message"
                    } >"$intake_lock_path"
                    warn "Intake frozen to contain duplicate ingress → ${intake_lock_path#${PROJECT_ROOT:-$PWD}/}"
                  fi
                  ;;
                *)
                  info "$backlog_message"
                  ;;
              esac
            done <<<"$backlog_guard_messages"
          fi
        fi
      fi
    fi
  fi

  if (( work_failed == 0 && no_verify == 0 )); then
    if (( remaining_tasks == 0 )); then
      work_failed="$(gc_wot_run_verify_if_needed "completion" "$PROJECT_ROOT" "$any_changes" "$work_failed")"
      work_failed=$((work_failed))
    elif (( batch_limit_reached )); then
      work_failed="$(gc_wot_run_verify_if_needed "batch-limit" "$PROJECT_ROOT" "$any_changes" "$work_failed")"
      work_failed=$((work_failed))
    fi
  fi

  if (( ${GC_WOT_SOFT_VERIFY_TRIGGERED:-0} )); then
    warn "Verify step reported failures (soft mode); review verify artifacts before deployment."
    manual_followups=1
  fi

  if (( batch_limit_reached )); then
    info "Batch size limit hit after ${processed_total} task(s); rerun to continue from the next pending task."
  fi

  if (( usage_limit_triggered )); then
    warn "Codex usage limit confirmed by provider; halt further work until additional quota is available."
  fi

  if (( loop_guard_triggered )); then
    warn "LOOP_GUARD_TRIPPED: work-on-tasks halted to avoid infinite loop."
    return "$loop_guard_exit_code"
  fi

  if (( run_blocked_quota )); then
    warn "Run terminated because a prompt exceeded the configured token budget (status: blocked-quota)."
  fi

  if (( migration_transition_triggered )); then
    warn "Run paused because migration_epoch changed mid-execution; rerun work-on-tasks to continue with the updated task mapping."
  fi

  local budget_report_helper
  local budget_report_path="${LOG_DIR:-${PROJECT_ROOT:-$PWD}/.gpt-creator/logs}/budget-report.md"
  if budget_report_helper="$(gc_clone_python_tool "generate_budget_report.py" "$PROJECT_ROOT" 2>/dev/null)"; then
    local budget_tool_actions_json
    budget_tool_actions_json="$(gc_budget_collect_tool_actions_json)"
    "$python_bin" "$budget_report_helper" \
      --usage-file "${LOG_DIR:-${PROJECT_ROOT:-$PWD}/.gpt-creator/logs}/codex-usage.ndjson" \
      --run-id "$run_stamp" \
      --stage-limits "${GC_BUDGET_STAGE_LIMITS_JSON:-{}}" \
      --tool-actions "$budget_tool_actions_json" \
      --output "$budget_report_path" || warn "Failed to generate budget-report.md"
  fi

  if [[ $work_failed -eq 0 ]]; then
    if (( batch_limit_reached )); then
      ok "work-on-tasks paused → ${run_dir}"
    else
      ok "work-on-tasks complete → ${run_dir}"
    fi
    if (( manual_followups )); then
      warn "Manual review needed for some tasks — see notes above and preserved output artifacts."
    fi
  else
    warn "work-on-tasks completed with issues — inspect ${run_dir}"
    return 1
  fi
}


cmd_iterate() {
  warn "'iterate' is deprecated; prefer 'gpt-creator create-tasks' followed by 'gpt-creator work-on-tasks'."

  local root="" jira="" reverify=1
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --jira) jira="$(abs_path "$2")"; shift 2;;
      --no-verify) reverify=0; shift;;
      *) break;;
    esac
  done
  ensure_ctx "$root"
  [[ -n "$jira" ]] || jira="${INPUT_DIR}/jira.md"
  [[ -f "$jira" ]] || die "Jira tasks file not found: ${jira}"

  local tasks_json_local="${PLAN_DIR}/jira-tasks.local.json"
  gc_parse_jira_tasks "$jira" "$tasks_json_local"
  ok "Parsed Jira tasks → ${tasks_json_local}"
  local tasks_json="${tasks_json_local}"

  local codex_parse_prompt="${PLAN_DIR}/iterate-codex-parse.md"
  local codex_raw_json="${PLAN_DIR}/jira-tasks.codex.raw.txt"
  local codex_json="${PLAN_DIR}/jira-tasks.codex.json"
  {
    cat >"$codex_parse_prompt" <<'PROMPT'
# Instruction
You are a structured-data assistant. Convert the following Jira backlog markdown into strict JSON.

## Requirements
- Output **only** valid JSON (no prose, no code fences).
- Structure: { "tasks": [ { "epic_id": str, "epic_title": str, "story_id": str, "story_title": str, "id": str, "title": str, "assignees": [str], "tags": [str], "estimate": str, "description": str, "acceptance_criteria": [str], "dependencies": [str] } ] }.
- Each task begins with a bold identifier such as **T18.5.2**; treat every such block as a separate task and capture its parent story/epic when present.
- Preserve bullet details verbatim inside the description and acceptance criteria lists. Do not repeat metadata (assignee/tags/estimate) inside the description.
- Use empty strings/arrays when information is missing.
- Do not include explanatory text.

## Jira Markdown
PROMPT
    cat "$jira" >>"$codex_parse_prompt"
    cat >>"$codex_parse_prompt" <<'PROMPT'
## End Markdown
PROMPT
  }

  if codex_call "iterate-parse" --prompt "$codex_parse_prompt" --output "$codex_raw_json"; then
    local iterate_parse_helper
    iterate_parse_helper="$(gc_clone_python_tool "iterate_normalize_codex_json.py" "${PROJECT_ROOT:-$PWD}")" || return 1
    if python3 "$iterate_parse_helper" "$codex_raw_json" "$codex_json"; then
      ok "Codex parsed Jira tasks → ${codex_json}"
      tasks_json="$codex_json"
    else
      warn "Codex JSON output invalid; falling back to local parser results."
    fi
  else
    warn "Codex parsing step failed; using local parser output."
  fi

  local iterate_dir="${PLAN_DIR}/iterate"
  mkdir -p "$iterate_dir"
  local order_file="${iterate_dir}/tasks-order.txt"

  local iterate_prompts_helper
  iterate_prompts_helper="$(gc_clone_python_tool "iterate_prepare_task_prompts.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$iterate_prompts_helper" "$tasks_json" "$iterate_dir" "$PROJECT_ROOT"

  if [[ -s "$order_file" ]]; then
    while IFS= read -r prompt_path; do
      [[ -z "$prompt_path" ]] && continue
      local base_name
      base_name="$(basename "$prompt_path" .md)"
      local output_path
      output_path="${prompt_path%.md}.output.md"
      info "Running Codex for ${base_name}"
      codex_call "$base_name" --prompt "$prompt_path" --output "$output_path" || warn "Codex task ${base_name} returned non-zero"
    done < "$order_file"
  else
    warn "No Jira tasks to process after parsing."
  fi

  local summary_prompt="${iterate_dir}/summary.md"
  local summary_output="${iterate_dir}/summary.output.md"
  local iterate_summary_helper
  iterate_summary_helper="$(gc_clone_python_tool "iterate_build_summary_prompt.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$iterate_summary_helper" "$tasks_json" "$order_file" "$summary_prompt"

  codex_call "iterate-summary" --prompt "$summary_prompt" --output "$summary_output" || warn "Codex summary step returned non-zero"

  if [[ "$reverify" -eq 1 ]]; then
    info "Re-running verify after iteration"
    local iter_verify_start
    iter_verify_start="$(date +%s)"
    cmd_verify all --project "$PROJECT_ROOT"
    local iter_verify_duration=$(( ( $(date +%s) - iter_verify_start ) * 1000 ))
    gc_budget_log_stage "verify" 0 0 0 "$iter_verify_duration" "[]" "{}" "false"
  fi
}

cmd_binder() {
  local action="${1:-}"
  if [[ -z "$action" || "$action" == "-h" || "$action" == "--help" ]]; then
    cat <<'EOF'
Usage: gpt-creator binder clear --story SLUG [--project PATH] [--epic EPIC] [--task TASK]

Manage task binder cache entries. The 'clear' subcommand removes cached binders for a story
or a specific task.
EOF
    return 0
  fi
  shift || true
  case "$action" in
    clear)
      local project="${PROJECT_ROOT:-$PWD}"
      local epic=""
      local story=""
      local task=""
      while [[ $# -gt 0 ]]; do
        case "$1" in
          --project|-p)
            project="$(abs_path "${2:?project path required}")"
            shift 2
            ;;
          --story)
            story="${2:?story slug required}"
            shift 2
            ;;
          --epic)
            epic="${2:-}"
            shift 2
            ;;
          --task)
            task="${2:-}"
            shift 2
            ;;
          -h|--help)
            cat <<'EOF'
Usage: gpt-creator binder clear --story SLUG [--project PATH] [--epic EPIC] [--task TASK]
EOF
            return 0
            ;;
          --)
            shift
            break
            ;;
          *)
            die "Unknown option for binder clear: $1"
            ;;
        esac
      done
      [[ -n "$story" ]] || die "--story is required for binder clear"
      local helper_path
      helper_path="$(gc_clone_python_tool "task_binder.py" "$project")" || return 1
      if [[ -n "$task" ]]; then
        python3 "$helper_path" clear --project "$project" --epic "${epic:-$story}" --story "$story" --task "$task"
      else
        python3 "$helper_path" clear --project "$project" --epic "${epic:-$story}" --story "$story"
      fi
      ;;
    *)
      die "Unknown binder subcommand: ${action}"
      ;;
  esac
}

cmd_estimate() {
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project)
        root="$(abs_path "$2")"
        shift 2
        ;;
      -h|--help)
        cat <<'USAGE'
Usage: gpt-creator estimate [--project PATH]

Estimate how long it will take to finish the remaining tasks based on story points (default 15 SP/hour; refined by work-on-tasks throughput).
USAGE
        return 0
        ;;
      *)
        die "Unknown estimate option: $1"
        ;;
    esac
  done

  ensure_ctx "$root"
  local tasks_db="${PLAN_DIR}/tasks/tasks.db"
  if [[ ! -f "$tasks_db" ]]; then
    die "Tasks database not found at ${tasks_db}. Run 'gpt-creator create-tasks' first."
  fi

  local helper_path
  helper_path="$(gc_clone_python_tool "estimate_remaining_work.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$tasks_db"
}


cmd_tokens() {
  local root="" details=0 json_output=0
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project)
        root="$(abs_path "$2")"
        shift 2
        ;;
      --details)
        details=1
        shift
        ;;
      --json)
        json_output=1
        shift
        ;;
      -h|--help)
        cat <<'USAGE'
Usage: gpt-creator tokens [--project PATH] [--details] [--json]

Print aggregated Codex token usage captured under .gpt-creator/logs/codex-usage.ndjson.
USAGE
        return 0
        ;;
      *)
        die "Unknown tokens option: $1"
        ;;
    esac
  done

  local project_root=""
  if [[ -n "$root" ]]; then
    project_root="$root"
  elif [[ -n "${PROJECT_ROOT:-}" ]]; then
    project_root="$PROJECT_ROOT"
  else
    project_root="$PWD"
  fi

  local usage_file="${project_root}/.gpt-creator/logs/codex-usage.ndjson"
  if [[ ! -f "$usage_file" ]]; then
    warn "No Codex usage data found at ${usage_file}. Run a codex-enabled command first."
    return 1
  fi
  local helper_path
  helper_path="$(gc_clone_python_tool "tokens_report.py" "${PROJECT_ROOT:-$PWD}")" || return 1
  python3 "$helper_path" "$usage_file" "$details" "$json_output"
}


cmd_reports() {
  local root=""
  local mode="list"
  local slug=""
  local open_editor=0
  local work_branch=""
  local push_after=1
  local prompt_only=0
  local reporter_filter=""
  local close_invalid=0
  local close_comment="Authenticity failed (automated by gpt-creator reports audit)."
  local close_comment_set=0
  local include_closed=0
  local audit_limit=""
  local audit_limit_set=0
  local digests_path=""
  local digests_path_set=0
  local invalid_label=""
  local invalid_label_set=0
  local allow_pairs=()

  while [[ $# -gt 0 ]]; do
    case "$1" in
      list)
        mode="list"
        shift
        ;;
      backlog)
        mode="backlog"
        shift
        ;;
      auto)
        mode="auto"
        shift
        ;;
      audit)
        mode="audit"
        shift
        ;;
      work)
        mode="work"
        shift
        if [[ $# -eq 0 ]]; then
          die "reports work requires a slug identifier"
        fi
        slug="$1"
        shift
        ;;
      show)
        mode="show"
        shift
        if [[ $# -eq 0 ]]; then
          die "reports show requires a slug identifier"
        fi
        slug="$1"
        shift
        ;;
      --project)
        root="$(abs_path "$2")"
        shift 2
        ;;
      --open)
        open_editor=1
        shift
        ;;
      --branch)
        work_branch="$2"
        shift 2
        ;;
      --no-push)
        push_after=0
        shift
        ;;
      --push)
        push_after=1
        shift
        ;;
      --prompt-only)
        prompt_only=1
        shift
        ;;
      --reporter)
        reporter_filter="$2"
        shift 2
        ;;
      --close-invalid)
        close_invalid=1
        shift
        ;;
      --no-close-invalid)
        close_invalid=0
        shift
        ;;
      --comment)
        close_comment="${2:?--comment requires text}"
        close_comment_set=1
        shift 2
        ;;
      --include-closed)
        include_closed=1
        shift
        ;;
      --limit)
        audit_limit="${2:?--limit requires a positive integer}"
        audit_limit_set=1
        shift 2
        ;;
      --digests)
        digests_path="${2:?--digests requires a file path}"
        digests_path_set=1
        shift 2
        ;;
      --allow)
        allow_pairs+=("${2:?--allow requires VERSION=SHA256}")
        shift 2
        ;;
      --label-invalid)
        invalid_label="${2:?--label-invalid requires a value}"
        invalid_label_set=1
        shift 2
        ;;
      --no-label-invalid)
        invalid_label=""
        invalid_label_set=1
        shift
        ;;
      -h|--help)
        cat <<'USAGE'
Usage:
  gpt-creator reports [--project PATH]
  gpt-creator reports list [--project PATH]
  gpt-creator reports backlog [--project PATH]
  gpt-creator reports auto [--project PATH] [--reporter NAME] [--no-push] [--prompt-only]
  gpt-creator reports work [--project PATH] [--branch NAME] [--no-push] [--prompt-only] <slug>
  gpt-creator reports [--project PATH] [--open] <slug>
  gpt-creator reports show [--project PATH] [--open] <slug>
  gpt-creator reports audit [--close-invalid] [--include-closed] [--limit N]
                               [--digests FILE] [--allow VERSION=SHA256]
                               [--label-invalid NAME|--no-label-invalid]
                               [--comment TEXT]

List captured crash/stall reports, show the open backlog, automatically resolve matching issues, assign Codex to resolve a single report, display a specific entry, or audit GitHub auto-reports for authenticity.
USAGE
        return 0
        ;;
      *)
        if [[ -z "$slug" && "$mode" == "list" ]]; then
          mode="show"
          slug="$1"
          shift
        else
          die "Unknown reports argument: $1"
        fi
        ;;
    esac
  done

  if [[ "$mode" == "show" && -z "$slug" ]]; then
    die "reports show requires a slug identifier"
  fi

  if [[ "$mode" == "work" && -z "$slug" ]]; then
    die "reports work requires a slug identifier"
  fi

  if [[ "$mode" == "work" && "$open_editor" -ne 0 ]]; then
    die "--open cannot be combined with reports work"
  fi

  if [[ "$mode" == "auto" && "$open_editor" -ne 0 ]]; then
    die "--open cannot be combined with reports auto"
  fi

  if [[ "$mode" == "auto" && -n "$work_branch" ]]; then
    die "--branch is not supported for reports auto"
  fi

  if [[ "$mode" != "work" && "$mode" != "auto" ]]; then
    if [[ -n "$work_branch" || "$prompt_only" -ne 0 || "$push_after" -ne 1 ]]; then
      die "reports options --branch/--no-push/--prompt-only are only valid with 'work' or 'auto'"
    fi
  fi

  if [[ "$mode" != "auto" && -n "$reporter_filter" ]]; then
    die "--reporter is only supported with reports auto"
  fi

  if [[ -n "$audit_limit" ]]; then
    if [[ ! "$audit_limit" =~ ^[0-9]+$ ]]; then
      die "--limit expects a non-negative integer"
    fi
  fi

  if [[ "$mode" != "audit" ]]; then
    if (( close_invalid != 0 || include_closed != 0 || close_comment_set != 0 || audit_limit_set != 0 || digests_path_set != 0 || invalid_label_set != 0 || ${#allow_pairs[@]} != 0 )); then
      die "reports options --close-invalid/--include-closed/--limit/--digests/--allow/--label-invalid/--comment are only valid with 'audit'"
    fi
  fi

  local pair_entry
  for pair_entry in "${allow_pairs[@]}"; do
    if [[ ! "$pair_entry" =~ ^[^=]+=[0-9a-fA-F]{64}$ ]]; then
      die "--allow expects VERSION=SHA256 (got '${pair_entry}')"
    fi
  done

  if [[ "$mode" != "audit" ]]; then
    if [[ -n "$root" ]]; then
      ensure_ctx "$root"
    else
      ensure_ctx "${PROJECT_ROOT:-$PWD}"
    fi
  elif [[ -n "$root" ]]; then
    ensure_ctx "$root"
  fi

  local reports_dir=""
  if [[ "$mode" != "audit" ]]; then
    reports_dir="$(gc_reports_dir)" || die "Unable to access issue report directory"
  fi

  case "$mode" in
    audit)
      local repo="${GC_GITHUB_REPO:-}"
      local token="${GC_GITHUB_TOKEN:-}"
      if [[ -z "$repo" || -z "$token" ]]; then
        die "reports audit requires GC_GITHUB_REPO and GC_GITHUB_TOKEN to be set"
      fi
      local state="open"
      if (( include_closed )); then
        state="all"
      fi
      if [[ -z "$digests_path" ]]; then
        if [[ -n "${GC_REPORT_DIGESTS_PATH:-}" ]]; then
          digests_path="${GC_REPORT_DIGESTS_PATH}"
        elif [[ -n "${CLI_ROOT:-}" && -f "${CLI_ROOT}/config/release-digests.json" ]]; then
          digests_path="${CLI_ROOT}/config/release-digests.json"
        fi
      fi
      local allow_join=""
      if ((${#allow_pairs[@]} > 0)); then
        allow_join="$(printf '%s\n' "${allow_pairs[@]}")"
      fi
      local github_audit_helper
      github_audit_helper="$(gc_clone_python_tool "github_audit_auto_reports.py" "${PROJECT_ROOT:-$PWD}")" || github_audit_helper=""
      if [[ -n "$github_audit_helper" ]]; then
        "$python_bin" "$github_audit_helper" "$repo" "$token" "$state" "$close_invalid" "$close_comment" "$audit_limit" "$digests_path" "$invalid_label" "$allow_join"
      else
        warn "GitHub audit helper unavailable; skipping auto-report audit."
      fi
      return
      ;;
    list|backlog)
      if [[ -z "$(find "$reports_dir" -maxdepth 1 -type f \( -name '*.yml' -o -name '*.yaml' \) -print -quit 2>/dev/null)" ]]; then
        info "No issue reports recorded yet."
        return 0
      fi
      local reports_summary_helper
      reports_summary_helper="$(gc_clone_python_tool "reports_summarize.py" "${PROJECT_ROOT:-$PWD}")" || reports_summary_helper=""
      if [[ -n "$reports_summary_helper" ]]; then
        "$python_bin" "$reports_summary_helper" "$reports_dir" "$mode"
      else
        warn "Reports summary helper unavailable; skipping report summary."
      fi
      ;;
    auto)
      local reporter="${reporter_filter:-$(gc_reports_current_user)}"
      info "Auto-processing reports for reporter: ${reporter}"
      local auto_slugs=()
      local reports_list_helper
      reports_list_helper="$(gc_clone_python_tool "reports_list_by_reporter.py" "${PROJECT_ROOT:-$PWD}")" || reports_list_helper=""
      if [[ -n "$reports_list_helper" ]]; then
        while IFS= read -r slug_entry; do
          [[ -n "$slug_entry" ]] && auto_slugs+=("$slug_entry")
        done < <("$python_bin" "$reports_list_helper" "$reports_dir" "$reporter")
      fi
      if ((${#auto_slugs[@]} == 0)); then
        info "No matching reports for reporter '${reporter}'."
        return 0
      fi
      local slug_entry
      for slug_entry in "${auto_slugs[@]}"; do
        info "Resolving report ${slug_entry}"
        if ! gc_reports_run_work "$slug_entry" "" "$push_after" "$prompt_only" "$reporter"; then
          die "Codex failed while processing report ${slug_entry}"
        fi
      done
      ;;
    show)
      local report_path=""
      if ! report_path="$(gc_reports_resolve_slug "$slug")"; then
        die "No issue report found for slug: ${slug}"
      fi
      info "Report file: ${report_path}"
      printf '\n'
      cat "$report_path"
      printf '\n'
      if (( open_editor )); then
        local editor_cmd="${EDITOR_CMD:-${EDITOR:-vi}}"
        if [[ -z "$editor_cmd" ]]; then
          warn "EDITOR_CMD not set; skipping --open"
        else
          info "Opening report in ${editor_cmd}"
          if ! bash -lc "${editor_cmd} \"${report_path}\""; then
            warn "Failed to launch editor command: ${editor_cmd}"
          fi
        fi
      fi
      ;;
    work)
      if ! gc_reports_run_work "$slug" "$work_branch" "$push_after" "$prompt_only" ""; then
        die "Codex failed to resolve report ${slug}"
      fi
      ;;
  esac
}

cmd_create_project() {
  local template_request="auto"
  local path=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --template)
        template_request="${2:?--template requires a value (template name or 'auto')}"
        shift 2
        ;;
      --skip-template)
        template_request="skip"
        shift
        ;;
      -h|--help)
        cat <<'USAGE'
Usage: gpt-creator create-project [--template NAME|auto|skip] [--skip-template] <path>

Create a new project root, optionally scaffold it from a project template, then
run the full build pipeline (scan → normalize → plan → generate → db → run → verify).
USAGE
        return 0
        ;;
      *)
        if [[ -z "$path" ]]; then
          path="$1"
        else
          die "Unexpected argument: $1"
        fi
        shift
        ;;
    esac
  done

  [[ -n "$path" ]] || die "create-project requires a path"

  local project_root
  project_root="$(abs_path "$path")"
  mkdir -p "$project_root"

  if ! gc_apply_project_template "$project_root" "$template_request"; then
    warn "Project template application reported issues; continuing with base scaffolding."
  fi

  ensure_ctx "$project_root"
  info "Project root: ${PROJECT_ROOT}"

  cmd_scan --project "$PROJECT_ROOT"
  cmd_normalize --project "$PROJECT_ROOT"
  cmd_plan --project "$PROJECT_ROOT"
  cmd_generate all --project "$PROJECT_ROOT"
  cmd_db provision --project "$PROJECT_ROOT" || warn "Database provision step reported an error"
  cmd_run up --project "$PROJECT_ROOT" || warn "Stack start reported an error"
  cmd_verify acceptance --project "$PROJECT_ROOT" || warn "Acceptance checks failing — review stack health."
  ok "Project bootstrap complete"
}

cmd_bootstrap() {
  local template_request="auto"
  local path=""
  local fresh=0
  local rfp_path=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --template)
        template_request="${2:?--template requires a value (template name or 'auto')}"
        shift 2
        ;;
      --skip-template)
        template_request="skip"
        shift
        ;;
      --rfp)
        rfp_path="${2:?--rfp requires a file path}"
        shift 2
        ;;
      --fresh)
        fresh=1
        shift
        ;;
      -h|--help)
        cat <<'USAGE'
Usage: gpt-creator bootstrap [--template NAME|auto|skip] [--skip-template] [--rfp FILE] [--fresh] <path>

Generate a full project from an RFP in one step by running:
  • create-pdr → create-sds → create-db-dump → create-jira-tasks → scan/normalize/plan/generate/db/run/verify
Templates from project_templates/ are applied first (auto-selected by default).
USAGE
        return 0
        ;;
      *)
        if [[ -z "$path" ]]; then
          path="$1"
        else
          die "Unexpected argument: $1"
        fi
        shift
        ;;
    esac
  done

  [[ -n "$path" ]] || die "bootstrap requires a path"

  if [[ -n "$rfp_path" ]]; then
    [[ -f "$rfp_path" ]] || die "RFP file not found: ${rfp_path}"
  fi

  local project_root
  project_root="$(abs_path "$path")"
  mkdir -p "$project_root"

  ensure_ctx "$project_root"
  info "Project root: ${PROJECT_ROOT}"

  if (( fresh )); then
    gc_bootstrap_reset_state
  fi

  mkdir -p "$(gc_bootstrap_state_dir)"

  if gc_bootstrap_step_is_done template; then
    info "Step 'template' already completed; skipping."
  else
    if gc_apply_project_template "$PROJECT_ROOT" "$template_request"; then
      gc_bootstrap_mark_step template "done"
    else
      gc_bootstrap_mark_step template "failed"
      die "Project template application failed"
    fi
  fi

  if [[ -n "$rfp_path" ]]; then
    local staged_rfp="${INPUT_DIR}/rfp.md"
    local staged_docs_rfp="${STAGING_DIR}/docs/rfp.md"
    if gc_bootstrap_step_is_done stage-rfp; then
      if [[ ! -f "$staged_rfp" || ! -f "$staged_docs_rfp" ]]; then
        info "Restaging RFP (previous artifacts missing)."
        gc_bootstrap_mark_step stage-rfp "reset"
      fi
    fi
    if ! gc_bootstrap_step_is_done stage-rfp; then
      mkdir -p "${INPUT_DIR}"
      cp "$rfp_path" "$staged_rfp"
      mkdir -p "${STAGING_DIR}/docs"
      cp "$rfp_path" "${STAGING_DIR}/docs/rfp.md"
      gc_bootstrap_mark_step stage-rfp "done"
      info "Staged RFP → ${staged_rfp}"
    else
      info "Step 'stage-rfp' already completed; skipping."
    fi
  fi

  info "[1/10] Scanning documentation"
  if ! gc_bootstrap_run_step scan cmd_scan --project "$PROJECT_ROOT"; then
    die "Bootstrap halted during scan"
  fi

  info "[2/10] Normalizing documentation"
  if ! gc_bootstrap_run_step normalize cmd_normalize --project "$PROJECT_ROOT"; then
    die "Bootstrap halted during normalize"
  fi

  info "[3/10] Generating Product Requirements Document"
  if gc_bootstrap_step_is_done create-pdr; then
    info "Step 'create-pdr' already completed; skipping."
  else
    if ! gc_bootstrap_have_rfp; then
      warn "No RFP found in staging; skipping create-pdr. Provide --rfp or add .gpt-creator/staging/docs/rfp.md to enable this step."
      gc_bootstrap_mark_step create-pdr "done"
    elif bash "$CLI_ROOT/src/cli/create-pdr.sh" --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step create-pdr "done"
    else
      gc_bootstrap_mark_step create-pdr "failed"
      die "create-pdr failed"
    fi
  fi

  info "[4/10] Generating System Design Specification"
  if gc_bootstrap_step_is_done create-sds; then
    info "Step 'create-sds' already completed; skipping."
  else
    if bash "$CLI_ROOT/src/cli/create-sds.sh" --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step create-sds "done"
    else
      gc_bootstrap_mark_step create-sds "failed"
      die "create-sds failed"
    fi
  fi

  info "[5/11] Generating database schema & seed dumps"
  if gc_bootstrap_step_is_done create-db-dump; then
    info "Step 'create-db-dump' already completed; skipping."
  else
    if bash "$CLI_ROOT/src/cli/create-db-dump.sh" --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step create-db-dump "done"
    else
      gc_bootstrap_mark_step create-db-dump "failed"
      die "create-db-dump failed"
    fi
  fi

  info "[6/11] Mining Jira tasks"
  if ! gc_bootstrap_step_is_done create-jira-tasks; then
    if cmd_create_jira_tasks --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step create-jira-tasks "done"
    else
      gc_bootstrap_mark_step create-jira-tasks "failed"
      die "create-jira-tasks failed"
    fi
  else
    info "Step 'create-jira-tasks' already completed; skipping."
  fi

  info "[7/11] Planning build"
  if ! gc_bootstrap_step_is_done plan; then
    if cmd_plan --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step plan "done"
    else
      gc_bootstrap_mark_step plan "failed"
      die "plan step failed"
    fi
  else
    info "Step 'plan' already completed; skipping."
  fi

  if ! gc_bootstrap_step_is_done generate; then
    info "[8/11] Generating stack code"
    if cmd_generate all --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step generate "done"
    else
      gc_bootstrap_mark_step generate "failed"
      die "generate step failed"
    fi
  else
    info "Step 'generate' already completed; skipping."
  fi

  info "[9/11] Provisioning infrastructure"
  if ! gc_bootstrap_step_is_done db-provision; then
    if cmd_db provision --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step db-provision "done"
    else
      gc_bootstrap_mark_step db-provision "failed"
      die "Database provision failed"
    fi
  else
    info "Step 'db-provision' already completed; skipping."
  fi

  info "[10/11] Starting stack"
  if ! gc_bootstrap_step_is_done run-up; then
    if cmd_run up --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step run-up "done"
    else
      gc_bootstrap_mark_step run-up "failed"
      die "Stack start failed"
    fi
  else
    info "Step 'run-up' already completed; skipping."
  fi

  if ! gc_bootstrap_step_is_done verify; then
    if cmd_verify acceptance --project "$PROJECT_ROOT"; then
      gc_bootstrap_mark_step verify "done"
    else
      gc_bootstrap_mark_step verify "failed"
      die "Acceptance verification failed"
    fi
  else
    info "Step 'verify' already completed; skipping."
  fi

  info "[11/11] Verifying acceptance"
  gc_bootstrap_mark_complete
  ok "Bootstrap complete — code, docs, and backlog generated"
}

cmd_update() {
  local force=0
  local repo_url="${GC_UPDATE_REPO_URL:-https://github.com/bekirdag/gpt-creator.git}"
  local prefix="/usr/local"
  local tmpdir=""

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --force)
        force=1
        shift
        ;;
      -h|--help)
        cat <<EOF
Usage: ${APP_NAME} update [--force]

Fetches the latest gpt-creator sources and reinstalls the CLI.
EOF
        return 0
        ;;
      *)
        die "Unknown argument for update: $1"
        ;;
    esac
  done

  if ! command -v git >/dev/null 2>&1; then
    die "'git' is required for ${APP_NAME} update"
  fi

  tmpdir="$(mktemp -d "${TMPDIR:-/tmp}/gpt-creator-update.XXXXXX")" || die "Unable to create temporary directory"

  info "Cloning repository: $repo_url"
  if ! git clone "$repo_url" "$tmpdir"; then
    rm -rf "$tmpdir"
    die "Failed to clone repository from ${repo_url}"
  fi

  info "Fetching latest changes"
  if ! git -C "$tmpdir" pull --ff-only; then
    rm -rf "$tmpdir"
    die "Failed to update repository in $tmpdir"
  fi

  local install_script="$tmpdir/scripts/install.sh"
  if [[ ! -x "$install_script" ]]; then
    rm -rf "$tmpdir"
    die "Installer not found at $install_script"
  fi

  local -a install_cmd=("$install_script" --prefix "$prefix")
  if (( force )); then
    install_cmd+=("--force")
  fi

  info "Running installer: ${install_cmd[*]}"
  if ! "${install_cmd[@]}"; then
    rm -rf "$tmpdir"
    die "Install script failed"
  fi

  rm -rf "$tmpdir"
  ok "gpt-creator updated successfully"
}

cmd_keys() {
  local action="${1:-list}"
  case "$action" in
    list|status)
      shift || true
      if (($#)); then
        die "keys ${action} does not take additional arguments"
      fi
      gc_api_keys_list
      ;;
    set)
      shift || true
      local target="${1:-}"
      [[ -n "$target" ]] || die "keys set requires a service name or environment variable"
      gc_api_keys_set "$target"
      ;;
    help|-h|--help)
      cat <<EOF
Usage: ${APP_NAME} keys [list|status]
       ${APP_NAME} keys set <service>
       ${APP_NAME} keys <service>

Lists required third-party API keys and stores credentials under your user config directory.
EOF
      ;;
    *)
      if (($# > 1)); then
        die "Unknown keys action: $*"
      fi
      gc_api_keys_set "$action"
      ;;
  esac
}

cmd_tui() {
  ensure_go_runtime
  local go_bin="${GC_GO_BIN:-${GO_BIN:-go}}"
  if ! command -v "$go_bin" >/dev/null 2>&1; then
    die "Go 1.21+ is required to run the gpt-creator TUI. Automatic setup failed; install Go manually and set GO_BIN."
  fi
  local tui_dir="${CLI_ROOT}/tui"
  if [[ ! -d "$tui_dir" ]]; then
    die "TUI sources not found at ${tui_dir}"
  fi
  local skip_tidy="${GC_SKIP_TUI_TIDY:-}"
  info "Launching gpt-creator TUI (preview)"
  (
    cd "$tui_dir"
    local go_dir_readonly=0
    local readonly_path=""
    if [[ ! -w go.mod ]]; then
      go_dir_readonly=1
      readonly_path="${tui_dir}/go.mod"
    elif [[ -e go.sum && ! -w go.sum ]]; then
      go_dir_readonly=1
      readonly_path="${tui_dir}/go.sum"
    fi
    if [[ -z "$skip_tidy" ]]; then
      if (( go_dir_readonly )); then
        warn "Skipping 'go mod tidy' because ${readonly_path} is not writable"
      else
        info "Ensuring TUI Go modules are tidy"
        if ! "$go_bin" mod tidy >/dev/null 2>&1; then
          warn "'go mod tidy' reported issues; retrying with output"
          if ! "$go_bin" mod tidy; then
            die "Failed to tidy Go modules required for the TUI"
          fi
        fi
      fi
    fi
    if (( go_dir_readonly )) && [[ "${GOFLAGS:-}" != *"-mod="* ]]; then
      export GOFLAGS="${GOFLAGS:+${GOFLAGS} }-mod=readonly"
    fi
    "$go_bin" run . "$@"
  )
}

usage() {
cat <<EOF
${APP_NAME} v${VERSION}

Usage:
  ${APP_NAME} [--reports-on|--reports-off] [--reports-idle-timeout SECONDS] <command> [args]
  ${APP_NAME} create-project [--template NAME|auto|skip] [--skip-template] <path>
  ${APP_NAME} bootstrap [--template NAME|auto|skip] [--skip-template] [--rfp FILE] [--fresh] <path>
  ${APP_NAME} scan [--project <path>]
  ${APP_NAME} normalize [--project <path>]
  ${APP_NAME} plan [--project <path>]
  ${APP_NAME} generate <api|web|admin|db|docker|all> [--project <path>]
  ${APP_NAME} db <provision|import|seed> [--project <path>]
  ${APP_NAME} run <up|down|logs|open> [--project <path>]
  ${APP_NAME} refresh-stack [options]
  ${APP_NAME} verify <acceptance|nfr|all> [--project <path>] [--api-url API_BASE] [--api-health URL] [--web-url URL] [--admin-url URL]
  ${APP_NAME} create-sds [--project <path>] [--model NAME] [--dry-run] [--force]
  ${APP_NAME} create-pdr [--project <path>] [--model NAME] [--dry-run] [--force]
  ${APP_NAME} create-jira-tasks [--project <path>] [--model NAME] [--force] [--dry-run]
  ${APP_NAME} create-db-dump [--project <path>] [--model NAME] [--dry-run] [--force]
  ${APP_NAME} migrate-tasks [--project <path>] [--force]
  ${APP_NAME} refine-tasks [--project <path>] [--story SLUG] [--model NAME] [--dry-run]
  ${APP_NAME} create-tasks [--project <path>] [--jira <file>] [--force]
  ${APP_NAME} backlog [--project <path>] [--type epics|stories] [--item-children ID] [--progress] [--task-details ID]
  ${APP_NAME} dag validate [--project <path>] [--story SLUG]
  ${APP_NAME} estimate [--project <path>]
  ${APP_NAME} tokens [--project <path>] [--details] [--json]
  ${APP_NAME} reports [--project <path>] [list|backlog|show|work] [--branch NAME] [--no-push] [--prompt-only] [--open] [slug]
  ${APP_NAME} show-file [--range A:B|--head N|--tail N|--diff|--refresh] <path>
  ${APP_NAME} task-convert [options] (deprecated; runs create-tasks)
  ${APP_NAME} sweep-artifacts [--project <path>] [path...]
  ${APP_NAME} tidy-progress [--project <path>] [path...] (deprecated alias of sweep-artifacts)
  ${APP_NAME} next [--project <path>] [--story SLUG]
  ${APP_NAME} why --task TASK_ID [--project <path>] [--story SLUG]
  ${APP_NAME} work-on-tasks [--project <path>] [--story ID|SLUG] [--fresh] [--no-verify] [--keep-artifacts]
  ${APP_NAME} iterate [--project <path>] [--jira <file>] [--no-verify] (deprecated)
  ${APP_NAME} keys [list|set <service>]
  ${APP_NAME} tui
  ${APP_NAME} update [--force]
  ${APP_NAME} version
  ${APP_NAME} help

Global flags:
  --reports-on               Enable automatic crash/stall reports for the current invocation
  --reports-off              Disable automatic crash/stall reports (overrides GC_REPORTS_ON=1)
  --reports-idle-timeout <s> Override idle detection threshold in seconds (default: ${GC_REPORTS_IDLE_TIMEOUT})

Environment overrides:
  CODEX_BIN, CODEX_MODEL, DOCKER_BIN, MYSQL_BIN, EDITOR_CMD, GC_API_HEALTH_URL, GC_WEB_URL, GC_ADMIN_URL
EOF
}

main() {
  local cmd="${1:-help}"; shift || true
  local shell_bin="${BASH:-bash}"
  case "$cmd" in
    help|-h|--help) usage ;;
    version|-v|--version) echo "${APP_NAME} ${VERSION}" ;;
    create-project) cmd_create_project "$@" ;;
    bootstrap)      cmd_bootstrap "$@" ;;
    scan)           cmd_scan "$@" ;;
    normalize)      cmd_normalize "$@" ;;
    plan)           cmd_plan "$@" ;;
    generate)       cmd_generate "$@" ;;
    db)             cmd_db "$@" ;;
    run)            cmd_run "$@" ;;
    refresh-stack)  cmd_refresh_stack "$@" ;;
    verify)         cmd_verify "$@" ;;
    migrate-tasks)  cmd_migrate_tasks_json "$@" ;;
    refine-tasks)   cmd_refine_tasks "$@" ;;
    create-sds)     "$shell_bin" "$CLI_ROOT/src/cli/create-sds.sh" "$@" ;;
    create-pdr)     "$shell_bin" "$CLI_ROOT/src/cli/create-pdr.sh" "$@" ;;
    create-jira-tasks) "$CLI_ROOT/src/cli/create-jira-tasks.sh" "$@" ;;
    create-db-dump) "$shell_bin" "$CLI_ROOT/src/cli/create-db-dump.sh" "$@" ;;
    create-tasks)   cmd_create_tasks "$@" ;;
    backlog)        cmd_backlog "$@" ;;
    dag)            cmd_dag "$@" ;;
    estimate)       cmd_estimate "$@" ;;
    tokens)         cmd_tokens "$@" ;;
    reports)        cmd_reports "$@" ;;
    task-convert)   cmd_task_convert "$@" ;;
    show-file)      cmd_show_file "$@" ;;
    sweep-artifacts)  cmd_sweep_artifacts "$@" ;;
    tidy-progress)  cmd_tidy_progress "$@" ;;
    next)           cmd_next "$@" ;;
    why)            cmd_why "$@" ;;
    work-on-tasks)  cmd_work_on_tasks "$@" ;;
    iterate)        cmd_iterate "$@" ;;
    keys)           cmd_keys "$@" ;;
    binder)         cmd_binder "$@" ;;
    tui)            cmd_tui "$@" ;;
    update)         cmd_update "$@" ;;
    *) die "Unknown command: ${cmd}. See '${APP_NAME} help'" ;;
  esac
}

gc_load_api_keys

GC_ORIGINAL_ARGS=("$@")
gc_reports_extract_global_flags "$@"
set -- "${GC_FILTERED_ARGS[@]}"

GC_INVOCATION="$0"
if ((${#GC_ORIGINAL_ARGS[@]})); then
  GC_INVOCATION+=" $(printf '%q ' "${GC_ORIGINAL_ARGS[@]}")"
  GC_INVOCATION="${GC_INVOCATION% }"
fi

if gc_reports_enabled; then
  trap 'gc_reports_activity_trap "$BASH_COMMAND"' DEBUG
fi

trap 'gc_interrupt_handler INT' INT
trap 'gc_interrupt_handler TERM' TERM
trap 'gc_interrupt_handler TSTP' TSTP
trap 'gc_interrupt_handler QUIT' QUIT

trap 'gc_capture_error_context $? "$BASH_COMMAND"' ERR
trap 'gc_exit_handler $?' EXIT

main "$@"
