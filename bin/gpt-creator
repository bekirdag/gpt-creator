#!/usr/bin/env bash
# gpt-creator — scaffolding & orchestration CLI
# Aligns with Product Definition & Requirements (PDR v0.2)
# Usage: gpt-creator <command> [args]

set -Eeuo pipefail

resolve_cli_root() {
  local source="${BASH_SOURCE[0]}"
  while [[ -L "$source" ]]; do
    local dir
    dir="$(cd "$(dirname "$source")" && pwd)"
    source="$(readlink "$source")"
    [[ "$source" != /* ]] && source="$dir/$source"
  done
  cd "$(dirname "$source")/.." && pwd
}

CLI_ROOT="$(resolve_cli_root)"
unset -f resolve_cli_root

gc_env_file() { echo "${PROJECT_ROOT:-$PWD}/.env"; }

gc_random_string() {
  python3 - <<'PY'
import secrets, string
alphabet = string.ascii_letters + string.digits
print(''.join(secrets.choice(alphabet) for _ in range(32)))
PY
}

gc_set_env_var() {
  local key="$1" value="$2"
  local env_file="$(gc_env_file)"
  python3 - <<'PY' "$env_file" "$key" "$value"
import pathlib, sys
path = pathlib.Path(sys.argv[1])
key = sys.argv[2]
value = sys.argv[3]
if path.exists():
    lines = path.read_text().splitlines()
else:
    lines = []
for idx, line in enumerate(lines):
    if line.startswith(f"{key}="):
        lines[idx] = f"{key}={value}"
        break
else:
    lines.append(f"{key}={value}")
path.write_text('\n'.join(lines) + '\n')
PY
}

gc_load_env() {
  local env_file="$(gc_env_file)"
  if [[ -f "$env_file" ]]; then
    set -a
    # shellcheck disable=SC1090
    source "$env_file"
    set +a
  fi
  GC_DB_NAME="${GC_DB_NAME:-${DB_NAME:-app}}"
  GC_DB_USER="${GC_DB_USER:-${DB_USER:-app}}"
  GC_DB_PASSWORD="${GC_DB_PASSWORD:-${DB_PASSWORD:-app_pass}}"
  GC_DB_ROOT_PASSWORD="${GC_DB_ROOT_PASSWORD:-${DB_ROOT_PASSWORD:-root}}"
  GC_DB_HOST_PORT="${GC_DB_HOST_PORT:-${DB_HOST_PORT:-${DB_PORT:-3306}}}"
  DB_NAME="${DB_NAME:-$GC_DB_NAME}"
  DB_USER="${DB_USER:-$GC_DB_USER}"
  DB_PASSWORD="${DB_PASSWORD:-$GC_DB_PASSWORD}"
  DB_ROOT_PASSWORD="${DB_ROOT_PASSWORD:-$GC_DB_ROOT_PASSWORD}"
  DB_HOST_PORT="${DB_HOST_PORT:-$GC_DB_HOST_PORT}"
}

gc_create_env_if_missing() {
  local env_file="$(gc_env_file)"
  if [[ -f "$env_file" ]]; then
    return
  fi
  local slug
  slug="$(basename "${PROJECT_ROOT:-$PWD}")"
  slug=$(printf '%s' "$slug" | tr -c '[:alnum:]' '_')
  slug=$(printf '%s' "$slug" | tr '[:upper:]' '[:lower:]')
  slug=$(printf '%.12s' "$slug")
  [[ -n "$slug" ]] || slug="app"
  local db_name="${slug}_db"
  local db_user="gc_${slug}_user"
  local db_password="$(gc_random_string)"
  local db_root_password="$(gc_random_string)"
  cat > "$env_file" <<EOF
# gpt-creator environment
DB_NAME=${db_name}
DB_USER=${db_user}
DB_PASSWORD=${db_password}
DB_ROOT_USER=root
DB_ROOT_PASSWORD=${db_root_password}
DB_HOST=127.0.0.1
DB_PORT=3306
DB_HOST_PORT=3306
DATABASE_URL=mysql://${db_user}:${db_password}@127.0.0.1:3306/${db_name}
VITE_API_BASE=http://localhost:3000/api/v1
EOF
  chmod 600 "$env_file" || true
}

VERSION="0.2.0"
APP_NAME="gpt-creator"

# Defaults (override via env)
CODEX_BIN="${CODEX_BIN:-codex}"
CODEX_MODEL="${CODEX_MODEL:-gpt-5-high}"
EDITOR_CMD="${EDITOR_CMD:-code}"
DOCKER_BIN="${DOCKER_BIN:-docker}"
MYSQL_BIN="${MYSQL_BIN:-mysql}"

# Colors (TTY-only)
if [[ -t 1 ]]; then
  c_reset=$'\033[0m'; c_dim=$'\033[2m'; c_bold=$'\033[1m'
  c_red=$'\033[31m'; c_yellow=$'\033[33m'; c_cyan=$'\033[36m'; c_green=$'\033[32m'
else
  c_reset=; c_dim=; c_bold=; c_red=; c_yellow=; c_cyan=; c_green=
fi

ts() { date +"%Y-%m-%dT%H:%M:%S"; }
die() { echo "${c_red}✖${c_reset} $*" >&2; exit 1; }
info(){ echo "${c_cyan}➜${c_reset} $*"; }
ok()  { echo "${c_green}✔${c_reset} $*"; }
warn(){ echo "${c_yellow}!${c_reset} $*"; }

abs_path() {
  python3 - "$1" <<'PY' 2>/dev/null || perl -MCwd=abs_path -e 'print abs_path(shift)."\n"' "$1" || echo "$1"
import os,sys; print(os.path.abspath(sys.argv[1]))
PY
}

to_lower() {
  printf '%s' "$1" | tr '[:upper:]' '[:lower:]'
}

# Context directories inside project
ensure_ctx() {
  local root="${1:-}"
  if [[ -z "${root}" ]]; then root="${PROJECT_ROOT:-$PWD}"; fi
  PROJECT_ROOT="$(abs_path "$root")"
  GC_DIR="${PROJECT_ROOT}/.gpt-creator"
  STAGING_DIR="${GC_DIR}/staging"
  INPUT_DIR="${STAGING_DIR}/inputs"
  PLAN_DIR="${STAGING_DIR}/plan"
  LOG_DIR="${GC_DIR}/logs"
  ART_DIR="${GC_DIR}/artifacts"
  mkdir -p "$GC_DIR" "$STAGING_DIR" "$INPUT_DIR" "$PLAN_DIR" "$LOG_DIR" "$ART_DIR"
  gc_create_env_if_missing
  gc_load_env
}

# docker compose helper (prefers "docker compose" then docker-compose)
docker_compose() {
  if command -v docker >/dev/null 2>&1 && docker compose version >/dev/null 2>&1; then
    docker compose "$@"
  elif command -v docker-compose >/dev/null 2>&1; then
    docker-compose "$@"
  else
    die "docker compose is not available. Install Docker Desktop or docker-compose."
  fi
}

port_in_use() {
  local port="$1"
  if command -v lsof >/dev/null 2>&1; then
    lsof -nP -iTCP:"$port" -sTCP:LISTEN >/dev/null 2>&1 && return 0
  elif command -v netstat >/dev/null 2>&1; then
    netstat -an 2>/dev/null | grep -E "\.${port} .*LISTEN" >/dev/null && return 0
  fi
  return 1
}

find_free_port() {
  local start="${1:-3306}"
  local port="$start"; local limit=$((start+100))
  while (( port <= limit )); do
    if ! port_in_use "$port"; then
      echo "$port"
      return 0
    fi
    ((port++)) || true
  done
  echo "$start"  # fallback
}

wait_for_endpoint() {
  local url="$1" label="$2"
  local attempts="${3:-30}" delay="${4:-2}"
  local i=1
  while (( i <= attempts )); do
    if curl -fsS --max-time 2 "$url" >/dev/null 2>&1; then
      ok "${label} ready → ${url}"
      return 0
    fi
    sleep "$delay"
    ((i++)) || true
  done
  warn "${label} not ready after $((attempts * delay))s → ${url}"
  return 1
}

render_template_file() {
  local src="$1" dest="$2"
  local db_name="${GC_DB_NAME:-${DB_NAME:-app}}"
  local db_user="${GC_DB_USER:-${DB_USER:-app}}"
  local db_pass="${GC_DB_PASSWORD:-${DB_PASSWORD:-app_pass}}"
  local db_host_port="${GC_DB_HOST_PORT:-${DB_HOST_PORT:-3306}}"
  local db_root_pass="${GC_DB_ROOT_PASSWORD:-${DB_ROOT_PASSWORD:-root}}"
  python3 - <<'PY' "$src" "$dest" "$db_name" "$db_user" "$db_pass" "$db_host_port" "$db_root_pass"
import pathlib, sys
args = sys.argv[1:]
src, dest, db_name, db_user, db_pass, db_host_port = args[:6]
db_root_pass = args[6] if len(args) > 6 else ''
text = pathlib.Path(src).read_text()
text = text.replace('{{DB_NAME}}', db_name)
text = text.replace('{{DB_USER}}', db_user)
text = text.replace('{{DB_PASSWORD}}', db_pass)
text = text.replace('{{DB_HOST_PORT}}', db_host_port)
text = text.replace('{{DB_ROOT_PASSWORD}}', db_root_pass)
pathlib.Path(dest).write_text(text)
PY
}

gc_render_sql() {
  local src="$1" dest="$2" database="$3" app_user="$4" app_pass="$5"
  python3 - <<'PY' "$src" "$dest" "$database" "$app_user" "$app_pass"
import pathlib, re, sys
src, dest, db_name, app_user, app_pass = sys.argv[1:6]
text = pathlib.Path(src).read_text()

text = text.replace('{{DB_NAME}}', db_name)
text = text.replace('{{DB_USER}}', app_user)
text = text.replace('{{DB_PASSWORD}}', app_pass)

def rewrite_add_column_if_not_exists(sql):
    alter_pattern = re.compile(r'ALTER\s+TABLE\s+(`?)([A-Za-z_][A-Za-z0-9_]*)\1\s+(.*?);', re.IGNORECASE | re.DOTALL)
    parts = []
    last_idx = 0

    def find_clause_end(body, start_idx):
        depth = 0
        i = start_idx
        while i < len(body):
            ch = body[i]
            if ch == '(':
                depth += 1
            elif ch == ')':
                if depth > 0:
                    depth -= 1
            elif ch == ',' and depth == 0:
                return i
            elif ch == ';' and depth == 0:
                return i
            i += 1
        return len(body)

    def split_clauses(body):
        clauses = []
        current = []
        depth = 0
        for ch in body:
            if ch == '(':
                depth += 1
            elif ch == ')':
                if depth > 0:
                    depth -= 1
            if ch == ',' and depth == 0:
                clauses.append(''.join(current))
                current = []
                continue
            current.append(ch)
        tail = ''.join(current)
        if tail.strip():
            clauses.append(tail)
        return clauses

    add_col_pattern = re.compile(r'ADD\s+COLUMN\s+IF\s+NOT\s+EXISTS\s+(`?)([A-Za-z_][A-Za-z0-9_]*)\1\s+', re.IGNORECASE)
    add_key_pattern = re.compile(r'ADD\s+(UNIQUE\s+)?KEY\s+(`?)([A-Za-z_][A-Za-z0-9_]*)\2', re.IGNORECASE)
    add_constraint_pattern = re.compile(r'ADD\s+CONSTRAINT\s+(`?)([A-Za-z_][A-Za-z0-9_]*)\1\s+FOREIGN\s+KEY', re.IGNORECASE)

    for match in alter_pattern.finditer(sql):
        table_name = match.group(2)
        body = match.group(3)
        clauses = split_clauses(body)

        column_additions = []
        index_additions = []
        constraint_additions = []
        leftover_clauses = []

        for clause in clauses:
            clause_stripped = clause.strip()
            if not clause_stripped:
                continue
            col_match = add_col_pattern.match(clause_stripped)
            if col_match:
                definition = clause_stripped[col_match.end():].strip().rstrip(',')
                col_name = col_match.group(2)
                quote = col_match.group(1) or ''
                column_additions.append((col_name, quote, definition))
                continue
            key_match = add_key_pattern.match(clause_stripped)
            if key_match:
                index_name = key_match.group(3)
                index_additions.append((clause_stripped, index_name))
                continue
            constraint_match = add_constraint_pattern.match(clause_stripped)
            if constraint_match:
                constraint_name = constraint_match.group(2)
                constraint_additions.append((clause_stripped, constraint_name))
                continue
            leftover_clauses.append(clause.rstrip())

        if not (column_additions or index_additions or constraint_additions):
            continue

        parts.append(sql[last_idx:match.start()])

        dynamic_sql = []
        for col_name, quote, definition in column_additions:
            column_token = f"{quote}{col_name}{quote}"
            ddl = f"ALTER TABLE `{table_name}` ADD COLUMN {column_token} {definition}".strip()
            ddl_escaped = ddl.replace("'", "''")
            dynamic_sql.append(
                "SET @ddl := (\n"
                "  SELECT IF(\n"
                "    EXISTS(SELECT 1 FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = DATABASE() AND TABLE_NAME = '"
                + table_name + "' AND COLUMN_NAME = '" + col_name + "'),\n"
                "    'DO 0',\n"
                "    '" + ddl_escaped + "'\n"
                "  )\n"
                ");\nPREPARE stmt FROM @ddl;\nEXECUTE stmt;\nDEALLOCATE PREPARE stmt;\n"
            )

        for clause_text, index_name in index_additions:
            ddl = f"ALTER TABLE `{table_name}` {clause_text}".strip()
            ddl_escaped = ddl.replace("'", "''")
            dynamic_sql.append(
                "SET @ddl := (\n"
                "  SELECT IF(\n"
                "    EXISTS(SELECT 1 FROM INFORMATION_SCHEMA.STATISTICS WHERE TABLE_SCHEMA = DATABASE() AND TABLE_NAME = '"
                + table_name + "' AND INDEX_NAME = '" + index_name + "'),\n"
                "    'DO 0',\n"
                "    '" + ddl_escaped + "'\n"
                "  )\n"
                ");\nPREPARE stmt FROM @ddl;\nEXECUTE stmt;\nDEALLOCATE PREPARE stmt;\n"
            )

        for clause_text, constraint_name in constraint_additions:
            ddl = f"ALTER TABLE `{table_name}` {clause_text}".strip()
            ddl_escaped = ddl.replace("'", "''")
            dynamic_sql.append(
                "SET @ddl := (\n"
                "  SELECT IF(\n"
                "    EXISTS(SELECT 1 FROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS WHERE TABLE_SCHEMA = DATABASE() AND TABLE_NAME = '"
                + table_name + "' AND CONSTRAINT_NAME = '" + constraint_name + "'),\n"
                "    'DO 0',\n"
                "    '" + ddl_escaped + "'\n"
                "  )\n"
                ");\nPREPARE stmt FROM @ddl;\nEXECUTE stmt;\nDEALLOCATE PREPARE stmt;\n"
            )

        if leftover_clauses:
            remaining_body = ',\n'.join(leftover_clauses)
            dynamic_sql.append(f"ALTER TABLE `{table_name}`\n{remaining_body}\n;")

        parts.append('\n'.join(dynamic_sql))
        last_idx = match.end()

    parts.append(sql[last_idx:])
    return ''.join(parts)

text = rewrite_add_column_if_not_exists(text)

old_slug_update = """UPDATE instructors
SET slug = LOWER(
  REPLACE(
    REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(
      TRIM(COALESCE(NULLIF(display_name,''), CONCAT(TRIM(first_name),' ',TRIM(last_name)))),
      'ç','c'),'ğ','g'),'ı','i'),'ö','o'),'ş','s'),'ü','u'
    )
  , ' ', '-')
)
WHERE slug IS NULL;"""

new_slug_update = """UPDATE instructors
SET slug = LOWER(
  REPLACE(
    REPLACE(
      REPLACE(
        REPLACE(
          REPLACE(
            REPLACE(
              REPLACE(
                TRIM(COALESCE(NULLIF(display_name,''), CONCAT(TRIM(first_name),' ',TRIM(last_name)))),
                'ç','c'),
              'ğ','g'),
            'ı','i'),
          'ö','o'),
        'ş','s'),
      'ü','u'),
    ' ', '-')
)
WHERE slug IS NULL;"""

text = text.replace(old_slug_update, new_slug_update)

old_unique_block = """ALTER TABLE instructors
  MODIFY slug VARCHAR(120) NOT NULL,
  ADD UNIQUE KEY uq_instructors_slug (slug);"""

new_unique_block = """ALTER TABLE instructors
  MODIFY slug VARCHAR(120) NOT NULL;

SET @ddl := (
  SELECT IF(
    EXISTS(SELECT 1 FROM INFORMATION_SCHEMA.STATISTICS WHERE TABLE_SCHEMA = DATABASE() AND TABLE_NAME = 'instructors' AND INDEX_NAME = 'uq_instructors_slug'),
    'DO 0',
    'ALTER TABLE `instructors` ADD UNIQUE KEY `uq_instructors_slug` (`slug`)' 
  )
);
PREPARE stmt FROM @ddl;
EXECUTE stmt;
DEALLOCATE PREPARE stmt;"""

text = text.replace(old_unique_block, new_unique_block)

text = re.sub(r'^\s*USE\s+`?[^`]+`?;', lambda _m: f"USE `{db_name}`;", text, flags=re.IGNORECASE | re.MULTILINE)
text = re.sub(r'CREATE\s+DATABASE\s+IF\s+NOT\s+EXISTS\s+`[^`]+`', lambda _m: f"CREATE DATABASE IF NOT EXISTS `{db_name}`", text, flags=re.IGNORECASE)
text = re.sub(r'ON\s+`[^`]+`\.\*\s+TO', lambda _m: f"ON `{db_name}`.* TO", text, flags=re.IGNORECASE)
text = re.sub(r"(CREATE\s+USER[^']*')([^']+)(')", lambda m: f"{m.group(1)}{app_user}{m.group(3)}", text, flags=re.IGNORECASE)
text = re.sub(r"(IDENTIFIED\s+BY\s+')([^']+)(')", lambda m: f"{m.group(1)}{app_pass}{m.group(3)}", text, flags=re.IGNORECASE)
text = re.sub(r"(TO\s+')([^']+)('@)", lambda m: f"{m.group(1)}{app_user}{m.group(3)}", text, flags=re.IGNORECASE)

def wrap_add_column(match):
    prefix, name = match.group(1), match.group(2)
    if name.startswith('`') and name.endswith('`'):
        return match.group(0)
    return f"{prefix}`{name}`"

text = re.sub(r"(?i)(ADD\s+COLUMN\s+)([A-Za-z_][A-Za-z0-9_]*)", wrap_add_column, text)

for ident in ("row_number", "field_name", "error_code"):
    pattern = re.compile(rf"(?<![`'])\b{re.escape(ident)}\b(?![`'])", re.IGNORECASE)
    text = pattern.sub(lambda m: f"`{m.group(0)}`", text)

def drop_check_constraint(src_text, name):
    pattern = re.compile(rf"CONSTRAINT\s+{re.escape(name)}\s+CHECK\s*\(", re.IGNORECASE)
    while True:
        match = pattern.search(src_text)
        if not match:
            return src_text
        start = match.start()
        comma_idx = src_text.rfind(',', 0, start)
        if comma_idx == -1:
            comma_idx = start
        depth = 1
        i = match.end()
        while i < len(src_text):
            ch = src_text[i]
            if ch == '(':
                depth += 1
            elif ch == ')':
                depth -= 1
                if depth == 0:
                    i += 1
                    break
            i += 1
        else:
            return src_text
        src_text = src_text[:comma_idx] + src_text[i:]

for constraint in ("ck_seo_target", "ck_legal_rev_publish"):
    text = drop_check_constraint(text, constraint)

pathlib.Path(dest).write_text(text)
PY
}

gc_temp_file() {
  local dir="$1" prefix="$2" suffix="$3"
  python3 - <<'PY' "$dir" "$prefix" "$suffix"
import os, sys, tempfile, pathlib
dir_path, prefix, suffix = sys.argv[1:4]
path = pathlib.Path(dir_path)
path.mkdir(parents=True, exist_ok=True)
fd, temp_path = tempfile.mkstemp(prefix=prefix, suffix=suffix, dir=str(path))
os.close(fd)
print(temp_path)
PY
}

gc_execute_sql() {
  local compose_file="$1" sql_file="$2" database="$3"
  local root_user="$4" root_pass="$5" app_user="$6" app_pass="$7" fallback_init="$8" label="$9"
  local import_ok=0
  [[ -n "$label" ]] || label="operation"
  local container_host="127.0.0.1"

  if [[ -f "$compose_file" ]]; then
    if docker_compose -f "$compose_file" ps >/dev/null 2>&1; then
      info "Using docker-compose db service for ${label}"
      docker_compose -f "$compose_file" up -d db >/dev/null 2>&1 || true
      if [[ -n "$root_pass" ]]; then
        if docker_compose -f "$compose_file" exec -T db mysql -h"${container_host}" -u"${root_user}" -p"${root_pass}" "${database}" < "${sql_file}"; then
          import_ok=1
        else
          warn "Root ${label} failed; retrying as ${app_user}"
        fi
      else
        if docker_compose -f "$compose_file" exec -T db mysql -h"${container_host}" -u"${root_user}" "${database}" < "${sql_file}"; then
          import_ok=1
        fi
      fi
      if [[ "$import_ok" -ne 1 ]]; then
        if docker_compose -f "$compose_file" exec -T db mysql -h"${container_host}" -u"${app_user}" ${app_pass:+-p"${app_pass}"} "${database}" < "${sql_file}"; then
          import_ok=1
        fi
      fi
      if [[ "$import_ok" -ne 1 && -n "$fallback_init" && -f "$fallback_init" ]]; then
        local fallback_output fallback_user fallback_pass
        fallback_output="$(python3 - "$fallback_init" <<'PY'
import re, sys
text = open(sys.argv[1]).read()
user = re.search(r"CREATE USER IF NOT EXISTS '([^']+)'", text)
password = re.search(r"IDENTIFIED BY '([^']+)'", text)
if user and password:
    print(user.group(1))
    print(password.group(1))
PY
)"
        if [[ -n "$fallback_output" ]]; then
          IFS=$'\n' read -r fallback_user fallback_pass _ <<<"$fallback_output"
          unset IFS
          if [[ -n "$fallback_user" && -n "$fallback_pass" ]]; then
            if docker_compose -f "$compose_file" exec -T db mysql -h"${container_host}" -u"${fallback_user}" ${fallback_pass:+-p"${fallback_pass}"} "${database}" < "${sql_file}"; then
              import_ok=1
            fi
          fi
        fi
      fi
      if [[ "$import_ok" -eq 1 ]]; then
        return 0
      fi
    fi
  fi

  local host="${DB_HOST:-127.0.0.1}"
  local port="${DB_HOST_PORT:-${GC_DB_HOST_PORT:-${DB_PORT:-3306}}}"
  if ${MYSQL_BIN} -h "$host" -P "$port" -u "$root_user" ${root_pass:+-p"${root_pass}"} "$database" < "$sql_file"; then
    return 0
  fi

  if ${MYSQL_BIN} -h "$host" -P "$port" -u "$app_user" ${app_pass:+-p"${app_pass}"} "$database" < "$sql_file"; then
    return 0
  fi

  return 1
}

# ---------- Scan helpers ----------
has_pattern() { LC_ALL=C grep -E -i -m 1 -q -- "$1" "$2" 2>/dev/null; }
classify_file() {
  local path="$1"
  local name="${path##*/}"
  local lower="$(to_lower "$name")"
  local path_norm="$(to_lower "$path")"
  local ext="${lower##*.}"
  local type="" conf=0

  case "$ext" in
    md)
      if [[ "$lower" == *pdr* ]]; then type="pdr"; conf=0.95
      elif [[ "$lower" == *sds* ]]; then type="sds"; conf=0.92
      elif [[ "$lower" == *rfp* ]]; then type="rfp"; conf=0.9
      elif [[ "$lower" == *jira* ]]; then type="jira"; conf=0.88
      elif [[ "$lower" == *ui*pages* || "$lower" == *website*ui*pages* ]]; then type="ui_pages"; conf=0.85
      elif has_pattern '\bJIRA\b|Issue Key' "$path"; then type="jira"; conf=0.6
      fi
      ;;
    yml|yaml|json)
      if has_pattern '^[[:space:]]*openapi[[:space:]]*:[[:space:]]*3' "$path" || has_pattern '"openapi"[[:space:]]*:' "$path" || has_pattern '"swagger"[[:space:]]*:' "$path"; then
        type="openapi"; conf=0.94
      fi
      ;;
    sql)
      type="sql"; conf=0.65
      if has_pattern 'CREATE[[:space:]]+TABLE' "$path"; then conf=0.8; fi
      ;;
    mmd)
      type="mermaid"; conf=0.7
      ;;
    html)
      local is_html=0
      if [[ "$path_norm" == *"page_samples"* || "$path_norm" == *"page-samples"* ]]; then
        is_html=1
      elif echo "$lower" | grep -Eq '(abo|auth|prg|evt|ctn)[0-9]+\.html'; then
        is_html=1
      fi
      if [[ $is_html -eq 1 ]]; then
        type="page_sample_html"; conf=0.7
      fi
      ;;
    css)
      local is_css=0
      if [[ "$path_norm" == *"page_samples"* || "$path_norm" == *"samples"* ]]; then
        is_css=1
      elif [[ "$lower" == *style.css ]]; then
        is_css=1
      fi
      if [[ $is_css -eq 1 ]]; then
        type="page_sample_css"; conf=0.6
      fi
      ;;
  esac

  if [[ -n "$type" ]]; then
    printf '%s|%.2f\n' "$type" "$conf"
  fi
}

cmd_scan() {
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"

  info "Scanning ${PROJECT_ROOT} for project artifacts…"
  local manifest="${GC_DIR}/scan.tsv"
  local tmp="${manifest}.tmp"
  printf "type\tconfidence\tpath\n" > "$tmp"

  while IFS= read -r -d '' f; do
    local hit
    hit="$(classify_file "$f")" || true
    if [[ -n "$hit" ]]; then
      local type conf
      IFS='|' read -r type conf <<<"$hit"
      printf "%s\t%.2f\t%s\n" "$type" "$conf" "$f" >> "$tmp"
    fi
  done < <(find "$PROJECT_ROOT" \
      \( -name '.git' -o -name 'node_modules' -o -name 'dist' -o -name 'build' -o -name '.venv' -o -name '.gpt-creator' \) -prune -o -type f -print0)

  mv "$tmp" "$manifest"

  local scan_json="${STAGING_DIR}/scan.json"
  python3 - <<'PY' "$manifest" "$PROJECT_ROOT" "$scan_json"
import csv, json, sys, time, pathlib
manifest, root, out = sys.argv[1:4]
rows = []
with open(manifest, newline='') as fh:
    reader = csv.DictReader(fh, delimiter='\t')
    for row in reader:
        if not row['type']:
            continue
        rows.append({
            "type": row['type'],
            "confidence": float(row['confidence'] or 0),
            "path": str(pathlib.Path(row['path']).resolve())
        })
scan = {
    "project_root": str(pathlib.Path(root).resolve()),
    "generated_at": time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
    "artifacts": rows
}
pathlib.Path(out).parent.mkdir(parents=True, exist_ok=True)
with open(out, 'w') as fh:
    json.dump(scan, fh, indent=2)
print(out)
PY
  ok "Scan manifest → ${scan_json}"
}

cmd_normalize() {
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"

  local scan_json="${STAGING_DIR}/scan.json"
  if [[ ! -f "$scan_json" ]]; then
    warn "No scan.json found, running scan first."
    cmd_scan --project "$PROJECT_ROOT"
  fi

  scan_json="${STAGING_DIR}/scan.json"
  python3 - <<'PY' "$scan_json" "$INPUT_DIR" "$PLAN_DIR"
import json, sys, shutil, pathlib, time
scan_path, input_dir, plan_dir = sys.argv[1:4]
input_dir = pathlib.Path(input_dir)
plan_dir = pathlib.Path(plan_dir)
input_dir.mkdir(parents=True, exist_ok=True)
plan_dir.mkdir(parents=True, exist_ok=True)

data = json.load(open(scan_path))
project_root = pathlib.Path(data.get('project_root', '.')).resolve()
artifacts = data.get('artifacts', [])

unique_types = {"pdr", "sds", "rfp", "jira", "ui_pages", "openapi"}
unique = {}
for entry in artifacts:
    t = entry.get('type')
    if t in unique_types:
        if t not in unique or entry['confidence'] > unique[t]['confidence']:
            unique[t] = entry

multi_map = {
    "sql": pathlib.Path('sql'),
    "mermaid": pathlib.Path('mermaid'),
    "page_sample_html": pathlib.Path('page_samples'),
    "page_sample_css": pathlib.Path('page_samples')
}
collected = {key: [] for key in multi_map}
for entry in artifacts:
    t = entry.get('type')
    if t in multi_map:
        collected[t].append(entry)

provenance = []

def copy_file(src_path, rel_dest, entry):
    src = pathlib.Path(src_path)
    dest = input_dir / rel_dest
    dest.parent.mkdir(parents=True, exist_ok=True)
    shutil.copy2(src, dest)
    provenance.append({
        "type": entry.get('type'),
        "source": str(src),
        "destination": str(dest.relative_to(input_dir)),
        "confidence": entry.get('confidence', 0)
    })

name_map = {
    "pdr": pathlib.Path('pdr.md'),
    "sds": pathlib.Path('sds.md'),
    "rfp": pathlib.Path('rfp.md'),
    "jira": pathlib.Path('jira.md'),
    "ui_pages": pathlib.Path('ui-pages.md')
}

for t, entry in unique.items():
    src = entry['path']
    if t == 'openapi':
        suffix = pathlib.Path(src).suffix.lower()
        if suffix in {'.yaml', '.yml'}:
            rel = pathlib.Path('openapi.yaml')
        elif suffix == '.json':
            rel = pathlib.Path('openapi.json')
        else:
            rel = pathlib.Path('openapi.src')
    else:
        rel = name_map[t]
    copy_file(src, rel, entry)

for t, entries in collected.items():
    dest_root = multi_map[t]
    for entry in entries:
        src = pathlib.Path(entry['path'])
        try:
            rel = src.resolve().relative_to(project_root)
        except ValueError:
            rel = pathlib.Path(src.name)
        rel = dest_root / rel
        copy_file(src, rel, entry)

# discovery.yaml (summary)
import io
from textwrap import indent
summary = io.StringIO()
summary.write('generated_at: %s\n' % time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()))
summary.write('project_root: %s\n' % project_root)
summary.write('artifacts:\n')
for entry in artifacts:
    summary.write('  - type: %s\n' % entry.get('type'))
    summary.write('    confidence: %.2f\n' % entry.get('confidence', 0))
    summary.write('    path: %s\n' % entry.get('path'))
(input_dir / '..' / 'discovery.yaml').resolve().write_text(summary.getvalue())

prov = {
    'generated_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
    'entries': provenance
}
(plan_dir / 'provenance.json').write_text(json.dumps(prov, indent=2))
PY

  ok "Normalized inputs → ${INPUT_DIR}"
}

cmd_plan() {
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"

  local openapi=""
  for cand in "$INPUT_DIR/openapi.yaml" "$INPUT_DIR/openapi.yml" "$INPUT_DIR/openapi.json" "$INPUT_DIR/openapi.src"; do
    [[ -f "$cand" ]] && { openapi="$cand"; break; }
  done
  local sql_dir="$INPUT_DIR/sql"

  python3 - <<'PY' "$openapi" "$sql_dir" "$PLAN_DIR"
import json, os, re, sys, time, pathlib
from collections import OrderedDict
openapi_path, sql_dir, plan_dir = sys.argv[1:4]
plan_dir = pathlib.Path(plan_dir)
plan_dir.mkdir(parents=True, exist_ok=True)

routes = []
schemas = []
openapi_loaded = False
if openapi_path:
    try:
        text = pathlib.Path(openapi_path).read_text()
        if openapi_path.endswith('.json'):
            data = json.loads(text)
            openapi_loaded = True
        else:
            try:
                import yaml  # type: ignore
                data = yaml.safe_load(text)  # type: ignore
                openapi_loaded = True
            except Exception:
                data = None
        if openapi_loaded and isinstance(data, dict):
            for path, methods in (data.get('paths') or {}).items():
                if isinstance(methods, dict):
                    for method, body in methods.items():
                        if not isinstance(body, dict):
                            continue
                        routes.append({
                            'method': method.upper(),
                            'path': path,
                            'summary': body.get('summary') or ''
                        })
            schemas = list((data.get('components') or {}).get('schemas') or {})
        else:
            raise ValueError('fallback parser')
    except Exception:
        routes = []
        schemas = []
        text = pathlib.Path(openapi_path).read_text() if openapi_path else ''
        current_path = None
        for line in text.splitlines():
            if re.match(r'^\s*/[^\s]+:\s*$', line):
                current_path = line.strip().rstrip(':')
                continue
            if current_path:
                m = re.match(r'^\s{2,}(get|post|put|patch|delete|options|head):\s*$', line, re.I)
                if m:
                    routes.append({'method': m.group(1).upper(), 'path': current_path, 'summary': ''})
                    continue
                if re.match(r'^\S', line):
                    current_path = None

        in_components = False
        in_schemas = False
        for line in text.splitlines():
            stripped = line.strip()
            if not stripped:
                continue
            if re.match(r'^components:\s*$', stripped):
                in_components = True
                in_schemas = False
                continue
            if in_components and re.match(r'^schemas:\s*$', stripped):
                in_schemas = True
                continue
            indent = len(line) - len(line.lstrip(' '))
            if in_schemas:
                if indent <= 2 and not stripped.startswith('#') and not stripped.startswith('schemas:'):
                    in_schemas = False
                    continue
                if indent == 4 and re.match(r'^[A-Za-z0-9_.-]+:\s*$', stripped):
                    name = stripped.split(':', 1)[0]
                    schemas.append(name)

sql_tables = []
sql_dir_path = pathlib.Path(sql_dir)
if sql_dir and sql_dir_path.is_dir():
    for sql_file in sql_dir_path.rglob('*.sql'):
        try:
            text = sql_file.read_text()
        except Exception:
            continue
        for m in re.finditer(r'CREATE\s+TABLE\s+`?([A-Za-z0-9_]+)`?', text, flags=re.IGNORECASE):
            sql_tables.append(m.group(1))

schema_set = {s.lower() for s in schemas}
table_set = {t.lower() for t in sql_tables}
only_in_openapi = sorted(schema_set - table_set)
only_in_sql = sorted(table_set - schema_set)

routes_path = plan_dir / 'routes.md'
entities_path = plan_dir / 'entities.md'
tasks_path = plan_dir / 'tasks.json'
plan_todo = plan_dir / 'PLAN_TODO.md'

def write_routes():
    lines = ['# Routes', '']
    if routes:
        for item in sorted(routes, key=lambda r: (r['path'], r['method'])):
            summary = f" — {item['summary']}" if item.get('summary') else ''
            lines.append(f"- `{item['method']} {item['path']}`{summary}")
    else:
        lines.append('No routes detected — ensure openapi.yaml is present in staging/inputs.')
    routes_path.write_text('\n'.join(lines) + '\n')


def write_entities():
    lines = ['# Entities', '']
    lines.append('## OpenAPI Schemas')
    if schemas:
        for name in sorted(schemas):
            lines.append(f'- {name}')
    else:
        lines.append('- (none found)')
    lines.append('')
    lines.append('## SQL Tables')
    if sql_tables:
        for name in sorted(sql_tables):
            lines.append(f'- {name}')
    else:
        lines.append('- (none found)')
    lines.append('')
    lines.append('## Detected deltas')
    if only_in_openapi:
        lines.append('- Only in OpenAPI: ' + ', '.join(only_in_openapi))
    if only_in_sql:
        lines.append('- Only in SQL: ' + ', '.join(only_in_sql))
    if not only_in_openapi and not only_in_sql:
        lines.append('- None (schemas and tables aligned on name)')
    entities_path.write_text('\n'.join(lines) + '\n')


def write_tasks():
    tasks = []
    if only_in_openapi:
        tasks.append({
            'id': 'align-openapi-sql',
            'title': 'Align OpenAPI schemas with SQL tables',
            'details': f"Create tables or update schemas for: {', '.join(only_in_openapi)}"
        })
    if only_in_sql:
        tasks.append({
            'id': 'document-sql-gap',
            'title': 'Document SQL tables missing from OpenAPI',
            'details': f"Expose or document SQL tables not covered by API: {', '.join(only_in_sql)}"
        })
    tasks_path.write_text(json.dumps({'generated_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()), 'tasks': tasks}, indent=2))


def write_plan_todo():
    lines = [
        '# Build Plan',
        '',
        '- Validate discovery outputs under `staging/inputs`.',
        '- Review `routes.md` & `entities.md` for coverage and deltas.',
        '- Implement generation steps for API, DB, Web, Admin, Docker.',
        '- Run `gpt-creator generate all --project <path>` if not already executed.',
        '- Bring the stack up with `gpt-creator run up` and smoke test.',
        '- Execute `gpt-creator verify all` to satisfy acceptance & NFR gates.',
        '- Iterate on Jira tasks using `gpt-creator iterate` until checks pass.'
    ]
    plan_todo.write_text('\n'.join(lines) + '\n')

write_routes()
write_entities()
write_tasks()
write_plan_todo()
PY

  ok "Plan artifacts created under ${PLAN_DIR}"
}

copy_template_tree() {
  local src="$1" dest="$2"
  [[ -d "$src" ]] || die "Template directory not found: $src"
  find "$src" -type d ! -name '.DS_Store' | while IFS= read -r dir; do
    local rel="${dir#$src}"
    mkdir -p "$dest/$rel"
  done
  find "$src" -type f | while IFS= read -r file; do
    local base="$(basename "$file")"
    [[ "$base" == '.DS_Store' ]] && continue
    local rel="${file#$src/}"
    local target="$dest/$rel"
    if [[ "$target" == *.tmpl ]]; then
      target="${target%.tmpl}"
      mkdir -p "$(dirname "$target")"
      render_template_file "$file" "$target"
    else
      mkdir -p "$(dirname "$target")"
      cp "$file" "$target"
    fi
  done
}

cmd_generate() {
  local facet="${1:-}"; shift || true
  [[ -n "$facet" ]] || die "generate requires a facet: api|web|admin|db|docker|all"
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"
  local templates="$CLI_ROOT/templates"

  case "$facet" in
    api)
      local out="$PROJECT_ROOT/apps/api"
      mkdir -p "$out"
      copy_template_tree "$templates/api/nestjs" "$out"
      ok "API scaffolded → ${out}"
      ;;
    web)
      local out="$PROJECT_ROOT/apps/web"
      mkdir -p "$out"
      copy_template_tree "$templates/web/vue3" "$out"
      ok "Web scaffolded → ${out}"
      ;;
    admin)
      local out="$PROJECT_ROOT/apps/admin"
      mkdir -p "$out"
      copy_template_tree "$templates/admin/vue3" "$out"
      ok "Admin scaffolded → ${out}"
      ;;
    db)
      local out="$PROJECT_ROOT/db"
      mkdir -p "$out"
      copy_template_tree "$templates/db/mysql" "$out"
      ok "DB artifacts scaffolded → ${out}"
      ;;
    docker)
      local out="$PROJECT_ROOT/docker"
      mkdir -p "$out"
      local preferred="${GC_DB_HOST_PORT:-${DB_HOST_PORT:-${MYSQL_HOST_PORT:-3306}}}"
      if port_in_use "$preferred"; then
        local next; next="$(find_free_port "$preferred")"
        if [[ "$next" != "$preferred" ]]; then
          info "Port $preferred in use; remapping MySQL to $next"
          preferred="$next"
        fi
      fi
      GC_DB_HOST_PORT="$preferred"
      DB_HOST_PORT="$GC_DB_HOST_PORT"
      MYSQL_HOST_PORT="$GC_DB_HOST_PORT"
      gc_set_env_var DB_HOST_PORT "$GC_DB_HOST_PORT"
      gc_set_env_var MYSQL_HOST_PORT "$GC_DB_HOST_PORT"
      gc_set_env_var GC_DB_HOST_PORT "$GC_DB_HOST_PORT"
      local local_url="mysql://${GC_DB_USER}:${GC_DB_PASSWORD}@127.0.0.1:${GC_DB_HOST_PORT}/${GC_DB_NAME}"
      gc_set_env_var DATABASE_URL "$local_url"
      gc_load_env
      copy_template_tree "$templates/docker" "$out"
      ok "Docker assets scaffolded → ${out}"
      ;;
    all)
      for f in api db web admin docker; do
        cmd_generate "$f" --project "$PROJECT_ROOT"
      done
      return 0
      ;;
    *) die "Unknown facet: ${facet}";;
  esac
}

cmd_db() {
  local action="${1:-}"; shift || true
  [[ -n "$action" ]] || die "db requires: provision|import|seed"
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"
  local compose_file="$PROJECT_ROOT/docker/docker-compose.yml"

  case "$action" in
    provision)
      [[ -f "$compose_file" ]] || die "Compose file not found at ${compose_file}; run 'gpt-creator generate docker'."
      info "Starting database service via docker compose"
      docker_compose -f "$compose_file" up -d db
      ok "MySQL container provisioned"
      ;;
    import)
      local sql_file
      sql_file="$(find "$INPUT_DIR/sql" -maxdepth 2 -type f -name '*.sql' | head -n1 || true)"
      [[ -n "$sql_file" ]] || die "No staged SQL found under ${INPUT_DIR}/sql"
      info "Importing SQL from ${sql_file}"
      local database="${DB_NAME:-$GC_DB_NAME}"
      local root_user="${DB_ROOT_USER:-root}"
      local root_pass="${DB_ROOT_PASSWORD:-${GC_DB_ROOT_PASSWORD:-}}"
      local app_user="${DB_USER:-$GC_DB_USER}"
      local app_pass="${DB_PASSWORD:-$GC_DB_PASSWORD}"
      local cleanup_files=()
      trap 'for f in "${cleanup_files[@]}"; do [[ -n "$f" && -f "$f" ]] && rm -f "$f"; done; trap - RETURN' RETURN
      local rendered_sql
      rendered_sql="$(gc_temp_file "$STAGING_DIR" "import-" ".sql")"
      cleanup_files+=("$rendered_sql")
      gc_render_sql "$sql_file" "$rendered_sql" "$database" "$app_user" "$app_pass"
      local init_sql="${INPUT_DIR}/sql/db/init.sql"
      if gc_execute_sql "$compose_file" "$rendered_sql" "$database" "$root_user" "$root_pass" "$app_user" "$app_pass" "$init_sql" "import"; then
        ok "Database import finished"
      else
        die "Database import failed"
      fi
      ;;
    seed)
      local seed_file="${PROJECT_ROOT}/db/seed.sql"
      [[ -f "$seed_file" ]] || die "Seed file not found: ${seed_file}"
      info "Seeding database from ${seed_file}"
      local database="${DB_NAME:-$GC_DB_NAME}"
      local root_user="${DB_ROOT_USER:-root}"
      local root_pass="${DB_ROOT_PASSWORD:-${GC_DB_ROOT_PASSWORD:-}}"
      local app_user="${DB_USER:-$GC_DB_USER}"
      local app_pass="${DB_PASSWORD:-$GC_DB_PASSWORD}"
      local cleanup_files=()
      trap 'for f in "${cleanup_files[@]}"; do [[ -n "$f" && -f "$f" ]] && rm -f "$f"; done; trap - RETURN' RETURN
      local rendered_seed
      rendered_seed="$(gc_temp_file "$STAGING_DIR" "seed-" ".sql")"
      cleanup_files+=("$rendered_seed")
      gc_render_sql "$seed_file" "$rendered_seed" "$database" "$app_user" "$app_pass"
      local fallback_init="${PROJECT_ROOT}/db/init.sql"
      if [[ ! -f "$fallback_init" ]]; then
        fallback_init="${INPUT_DIR}/sql/db/init.sql"
      fi
      if gc_execute_sql "$compose_file" "$rendered_seed" "$database" "$root_user" "$root_pass" "$app_user" "$app_pass" "$fallback_init" "seed"; then
        ok "Database seed applied"
      else
        die "Database seed failed"
      fi
      ;;
    *) die "Unknown db action: ${action}";;
  esac
}

cmd_run() {
  local action="${1:-}"; shift || true
  [[ -n "$action" ]] || die "run requires: up|down|logs|open"
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"
  local compose_file="$PROJECT_ROOT/docker/docker-compose.yml"

  case "$action" in
    up)
      [[ -f "$compose_file" ]] || die "Compose file not found at ${compose_file}; generate docker assets first."
      docker_compose -f "$compose_file" up -d
      ok "Stack is starting (check docker compose ps)"
      local api_base="${GC_API_BASE_URL:-http://localhost:3000/api/v1}"
      local web_url="${GC_WEB_URL:-http://localhost:8080/}"
      local admin_url="${GC_ADMIN_URL:-http://localhost:8080/admin/}"
      wait_for_endpoint "${api_base%/}/health" "API /health" 40 3 || true
      local web_ping="${web_url%/}/__vite_ping"
      if ! wait_for_endpoint "$web_ping" "Web (vite ping)" 40 3; then
        wait_for_endpoint "${web_url%/}/" "Web" 20 3 || true
      fi
      local admin_ping="${admin_url%/}/__vite_ping"
      if ! wait_for_endpoint "$admin_ping" "Admin (vite ping)" 40 3; then
        wait_for_endpoint "${admin_url%/}/" "Admin" 20 3 || true
      fi
      ;;
    down)
      [[ -f "$compose_file" ]] || die "Compose file not found at ${compose_file}"
      docker_compose -f "$compose_file" down
      ok "Stack shut down"
      ;;
    logs)
      [[ -f "$compose_file" ]] || die "Compose file not found at ${compose_file}"
      docker_compose -f "$compose_file" logs -f
      ;;
    open)
      if command -v open >/dev/null 2>&1; then
        open "http://localhost:8080" || open "http://localhost:5173" || true
      else
        ${EDITOR_CMD} "$PROJECT_ROOT" || true
      fi
      ;;
    *) die "Unknown run action: ${action}";;
  esac
}

cmd_verify() {
  local kind="${1:-all}"; shift || true
  local root=""
  local api_base="${GC_API_BASE_URL:-http://localhost:3000/api/v1}"
  local api_health="${GC_API_HEALTH_URL:-}"
  local web_url="${GC_WEB_URL:-http://localhost:8080/}"
  local admin_url="${GC_ADMIN_URL:-http://localhost:8080/admin/}"

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --api-url) api_base="$2"; shift 2;;
      --api-health) api_health="$2"; shift 2;;
      --web-url) web_url="$2"; shift 2;;
      --admin-url) admin_url="$2"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"

  if [[ -z "$api_health" ]]; then
    local trimmed_base="${api_base%/}"
    api_health="${trimmed_base}/health"
  fi

  local verify_root="$CLI_ROOT/verify"
  [[ -d "$verify_root" ]] || die "verify scripts directory missing at ${verify_root}"

  local -a check_names
  case "$kind" in
    acceptance) check_names=(acceptance) ;;
    nfr) check_names=(openapi a11y lighthouse consent program_filters) ;;
    all) check_names=(acceptance openapi a11y lighthouse consent program_filters) ;;
    *) die "Unknown verify target: ${kind}";;
  esac

  local pass=0 fail=0 skip=0
  run_check() {
    local label="$1"; shift
    if "$@"; then
      ((pass++))
    else
      local status=$?
      if [[ $status -eq 3 ]]; then
        ((skip++))
        warn "${label} check skipped (missing dependency)"
      else
        ((fail++))
        warn "${label} check failed (exit ${status})"
      fi
    fi
  }

  for name in "${check_names[@]}"; do
    case "$name" in
      acceptance)
        run_check "acceptance" bash "$verify_root/acceptance.sh" "${api_base}" "${web_url}" "${admin_url}"
        ;;
      openapi)
        local spec=""
        for cand in "$INPUT_DIR/openapi.yaml" "$INPUT_DIR/openapi.yml" "$INPUT_DIR/openapi.json"; do
          [[ -f "$cand" ]] && { spec="$cand"; break; }
        done
        run_check "openapi" bash "$verify_root/check-openapi.sh" "${spec}" ;;
      a11y)
        run_check "a11y" bash "$verify_root/check-a11y.sh" "${web_url}" "${admin_url}"
        ;;
      lighthouse)
        run_check "lighthouse" bash "$verify_root/check-lighthouse.sh" "${web_url}" "${admin_url}"
        ;;
      consent)
        run_check "consent" bash "$verify_root/check-consent.sh" "${web_url}"
        ;;
      program_filters)
        run_check "program-filters" bash "$verify_root/check-program-filters.sh" "${api_base}"
        ;;
    esac
  done

  if (( fail > 0 )); then
    die "Verify failed — pass=${pass} fail=${fail} skip=${skip}"
  fi
  ok "Verify complete — pass=${pass} skip=${skip}"
}

codex_call() {
  local task="${1:?task}"; shift || true
  local prompt_dir="${GC_DIR}/prompts"
  mkdir -p "$prompt_dir"
  local prompt_file="${prompt_dir}/${task}.md"
  if [[ ! -f "$prompt_file" ]]; then
    cat >"$prompt_file" <<'PROMPT'
# Instruction
You are Codex (gpt-5-high) assisting the gpt-creator pipeline. Apply requested changes deterministically.
PROMPT
  fi
  if command -v "$CODEX_BIN" >/dev/null 2>&1; then
    info "Codex ${task} → model=${CODEX_MODEL}"
    "$CODEX_BIN" chat --model "$CODEX_MODEL" --prompt-file "$prompt_file" --stdin < /dev/null || warn "Codex invocation returned non-zero."
  else
    warn "Codex binary (${CODEX_BIN}) not found — skipping ${task}."
  fi
}

cmd_iterate() {
  local root="" jira="" reverify=1
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --jira) jira="$(abs_path "$2")"; shift 2;;
      --no-verify) reverify=0; shift;;
      *) break;;
    esac
  done
  ensure_ctx "$root"
  [[ -n "$jira" ]] || jira="${INPUT_DIR}/jira.md"
  [[ -f "$jira" ]] || die "Jira tasks file not found: ${jira}"

  local tasks_json="${PLAN_DIR}/jira-tasks.json"
  python3 - <<'PY' "$jira" "$tasks_json"
import json, re, sys, time
jira_path, out_path = sys.argv[1:3]
lines = open(jira_path).read().splitlines()
tasks = []
current = None
for line in lines:
    if re.match(r'^#+\s+', line):
        if current:
            tasks.append(current)
        current = {'title': line.lstrip('# ').strip(), 'details': ''}
    elif re.match(r'^-\s+\[ \]\s+', line):
        if current:
            tasks.append(current)
        current = {'title': line.split('[ ]', 1)[1].strip(), 'details': ''}
    else:
        if current is None:
            continue
        current['details'] = (current.get('details', '') + '\n' + line).strip()
if current:
    tasks.append(current)
json.dump({'generated_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()), 'tasks': tasks}, open(out_path, 'w'), indent=2)
PY

  ok "Parsed Jira tasks → ${tasks_json}"
  codex_call "iterate" "$jira" "$PLAN_DIR/tasks.json"
  if [[ "$reverify" -eq 1 ]]; then
    info "Re-running verify after iteration"
    cmd_verify all --project "$PROJECT_ROOT"
  fi
}

cmd_create_project() {
  local path="${1:-}"; [[ -n "$path" ]] || die "create-project requires a path"
  ensure_ctx "$path"
  mkdir -p "$PROJECT_ROOT"
  info "Project root: ${PROJECT_ROOT}"

  cmd_scan --project "$PROJECT_ROOT"
  cmd_normalize --project "$PROJECT_ROOT"
  cmd_plan --project "$PROJECT_ROOT"
  cmd_generate all --project "$PROJECT_ROOT"
  cmd_db provision --project "$PROJECT_ROOT" || warn "Database provision step reported an error"
  cmd_run up --project "$PROJECT_ROOT" || warn "Stack start reported an error"
  cmd_verify acceptance --project "$PROJECT_ROOT" || warn "Acceptance checks failing — review stack health."
  ok "Project bootstrap complete"
}

usage() {
cat <<EOF
${APP_NAME} v${VERSION}

Usage:
  ${APP_NAME} create-project <path>
  ${APP_NAME} scan [--project <path>]
  ${APP_NAME} normalize [--project <path>]
  ${APP_NAME} plan [--project <path>]
  ${APP_NAME} generate <api|web|admin|db|docker|all> [--project <path>]
  ${APP_NAME} db <provision|import|seed> [--project <path>]
  ${APP_NAME} run <up|down|logs|open> [--project <path>]
  ${APP_NAME} verify <acceptance|nfr|all> [--project <path>] [--api-url API_BASE] [--api-health URL] [--web-url URL] [--admin-url URL]
  ${APP_NAME} iterate [--project <path>] [--jira <file>] [--no-verify]
  ${APP_NAME} version
  ${APP_NAME} help

Environment overrides:
  CODEX_BIN, CODEX_MODEL, DOCKER_BIN, MYSQL_BIN, EDITOR_CMD, GC_API_HEALTH_URL, GC_WEB_URL, GC_ADMIN_URL
EOF
}

main() {
  local cmd="${1:-help}"; shift || true
  case "$cmd" in
    help|-h|--help) usage ;;
    version|-v|--version) echo "${APP_NAME} ${VERSION}" ;;
    create-project) cmd_create_project "$@" ;;
    scan)           cmd_scan "$@" ;;
    normalize)      cmd_normalize "$@" ;;
    plan)           cmd_plan "$@" ;;
    generate)       cmd_generate "$@" ;;
    db)             cmd_db "$@" ;;
    run)            cmd_run "$@" ;;
    verify)         cmd_verify "$@" ;;
    iterate)        cmd_iterate "$@" ;;
    *) die "Unknown command: ${cmd}. See '${APP_NAME} help'" ;;
  esac
}

main "$@"
