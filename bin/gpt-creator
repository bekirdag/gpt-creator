#!/usr/bin/env bash
# gpt-creator — scaffolding & orchestration CLI
# Aligns with Product Definition & Requirements (PDR v0.2)
# Usage: gpt-creator <command> [args]

set -Eeuo pipefail

# Ensure docker compose commands have adequate timeouts unless caller overrides
if [[ -n "${GC_DOCKER_COMPOSE_TIMEOUT:-}" ]]; then
  COMPOSE_HTTP_TIMEOUT="$GC_DOCKER_COMPOSE_TIMEOUT"
  DOCKER_CLIENT_TIMEOUT="$GC_DOCKER_COMPOSE_TIMEOUT"
fi
: "${COMPOSE_HTTP_TIMEOUT:=600}"
: "${DOCKER_CLIENT_TIMEOUT:=$COMPOSE_HTTP_TIMEOUT}"
export COMPOSE_HTTP_TIMEOUT DOCKER_CLIENT_TIMEOUT

: "${GC_DOCKER_HEALTH_TIMEOUT:=10}"
: "${GC_DOCKER_HEALTH_INTERVAL:=1}"
: "${GC_PNPM_VERSION:=10.17.1}"

resolve_cli_root() {
  local source="${BASH_SOURCE[0]}"
  while [[ -L "$source" ]]; do
    local dir
    dir="$(cd "$(dirname "$source")" && pwd)"
    source="$(readlink "$source")"
    [[ "$source" != /* ]] && source="$dir/$source"
  done
  cd "$(dirname "$source")/.." && pwd
}

cmd_task_convert() {
  warn "'task-convert' is deprecated; use 'create-tasks' instead. Running create-tasks now."
  cmd_create_tasks "$@"
}

CLI_ROOT="$(resolve_cli_root)"
unset -f resolve_cli_root

gc_env_file() { echo "${PROJECT_ROOT:-$PWD}/.env"; }

gc_random_string() {
  python3 - <<'PY'
import secrets, string
alphabet = string.ascii_letters + string.digits
print(''.join(secrets.choice(alphabet) for _ in range(32)))
PY
}

gc_set_env_var() {
  local key="$1" value="$2"
  local env_file="$(gc_env_file)"
  python3 - <<'PY' "$env_file" "$key" "$value"
import pathlib, sys
path = pathlib.Path(sys.argv[1])
key = sys.argv[2]
value = sys.argv[3]
if path.exists():
    raw_lines = path.read_text().splitlines()
else:
    raw_lines = []
lines = []
for line in raw_lines:
    stripped = line.strip()
    if not stripped:
        lines.append('')
        continue
    if stripped.startswith('#') or '=' in line:
        lines.append(line)
# Non key/value lines are dropped
for idx, line in enumerate(lines):
    if line.startswith(f"{key}="):
        lines[idx] = f"{key}={value}"
        break
else:
    lines.append(f"{key}={value}")
path.write_text('\n'.join(lines) + '\n')
PY
}

gc_env_sync_ports() {
  GC_PORT_RESERVATIONS=""
  GC_DB_HOST_PORT="${GC_DB_HOST_PORT:-${DB_HOST_PORT:-${DB_PORT:-3306}}}"
  DB_NAME="${DB_NAME:-$GC_DB_NAME}"
  DB_USER="${DB_USER:-$GC_DB_USER}"
  DB_PASSWORD="${DB_PASSWORD:-$GC_DB_PASSWORD}"
  DB_ROOT_PASSWORD="${DB_ROOT_PASSWORD:-$GC_DB_ROOT_PASSWORD}"
  DB_HOST_PORT="${DB_HOST_PORT:-$GC_DB_HOST_PORT}"
  GC_API_HOST_PORT="${GC_API_HOST_PORT:-${API_HOST_PORT:-3000}}"
  GC_WEB_HOST_PORT="${GC_WEB_HOST_PORT:-${WEB_HOST_PORT:-5173}}"
  GC_ADMIN_HOST_PORT="${GC_ADMIN_HOST_PORT:-${ADMIN_HOST_PORT:-5174}}"
  GC_PROXY_HOST_PORT="${GC_PROXY_HOST_PORT:-${PROXY_HOST_PORT:-8080}}"
  API_HOST_PORT="${API_HOST_PORT:-$GC_API_HOST_PORT}"
  WEB_HOST_PORT="${WEB_HOST_PORT:-$GC_WEB_HOST_PORT}"
  ADMIN_HOST_PORT="${ADMIN_HOST_PORT:-$GC_ADMIN_HOST_PORT}"
  PROXY_HOST_PORT="${PROXY_HOST_PORT:-$GC_PROXY_HOST_PORT}"
  local api_base_default="http://localhost:${GC_API_HOST_PORT}/api/v1"
  GC_API_BASE_URL="${GC_API_BASE_URL:-$api_base_default}"
  VITE_API_BASE="${VITE_API_BASE:-$GC_API_BASE_URL}"
  GC_API_HEALTH_URL="${GC_API_HEALTH_URL:-http://localhost:${GC_API_HOST_PORT}/health}"
  local proxy_origin="http://localhost:${GC_PROXY_HOST_PORT}"
  GC_WEB_URL="${GC_WEB_URL:-${proxy_origin}/}"
  GC_ADMIN_URL="${GC_ADMIN_URL:-${proxy_origin}/admin/}"
  gc_reserve_port db "$GC_DB_HOST_PORT"
  gc_reserve_port api "$GC_API_HOST_PORT"
  gc_reserve_port web "$GC_WEB_HOST_PORT"
  gc_reserve_port admin "$GC_ADMIN_HOST_PORT"
  gc_reserve_port proxy "$GC_PROXY_HOST_PORT"
}

gc_sanitize_env_file() {
  local env_file="$1"
  python3 - <<'PY' "$env_file"
import pathlib, re, sys
path = pathlib.Path(sys.argv[1])
if not path.exists():
    raise SystemExit(0)
pattern = re.compile(r"^(?:export\s+)?([A-Za-z_][A-Za-z0-9_]*)=(.*)$")
ansi_re = re.compile(r"\x1b\[[0-9;]*m")
whitespace_re = re.compile(r"\s")
lines = path.read_text().splitlines()
cleaned = []
for line in lines:
    stripped = line.strip()
    if not stripped:
        continue
    if stripped.startswith('#'):
        cleaned.append(line)
        continue
    match = pattern.match(line)
    if not match:
        # drop invalid line
        continue
    key, value = match.groups()
    value = ansi_re.sub('', value).strip()
    if '➜' in value or 'remapping' in value or value.startswith('Port '):
        continue
    if key.endswith('_HOST_PORT') or key in {'DB_HOST_PORT', 'DB_PORT', 'MYSQL_HOST_PORT'}:
        if not value.isdigit():
            continue
    if whitespace_re.search(value) and not (value.startswith('"') and value.endswith('"')) and not (value.startswith("'") and value.endswith("'")):
        # unquoted whitespace breaks sourcing; drop
        continue
    cleaned.append(f"{key}={value}")
path.write_text('\n'.join(cleaned) + ('\n' if cleaned else ''))
PY
}

gc_load_env() {
  local env_file="$(gc_env_file)"
  if [[ -f "$env_file" ]]; then
    gc_sanitize_env_file "$env_file"
    set -a
    # shellcheck disable=SC1090
    source "$env_file"
    set +a
  fi
  GC_DB_NAME="${GC_DB_NAME:-${DB_NAME:-app}}"
  GC_DB_USER="${GC_DB_USER:-${DB_USER:-app}}"
  GC_DB_PASSWORD="${GC_DB_PASSWORD:-${DB_PASSWORD:-app_pass}}"
  GC_DB_ROOT_PASSWORD="${GC_DB_ROOT_PASSWORD:-${DB_ROOT_PASSWORD:-root}}"
  gc_env_sync_ports
}

gc_create_env_if_missing() {
  local env_file="$(gc_env_file)"
  if [[ -f "$env_file" ]]; then
    return
  fi
  local slug
  slug="$(basename "${PROJECT_ROOT:-$PWD}")"
  slug=$(printf '%s' "$slug" | tr -c '[:alnum:]' '_')
  slug=$(printf '%s' "$slug" | tr '[:upper:]' '[:lower:]')
  slug=$(printf '%.12s' "$slug")
  [[ -n "$slug" ]] || slug="app"
  local db_name="${slug}_db"
  local db_user="gc_${slug}_user"
  local db_password="$(gc_random_string)"
  local db_root_password="$(gc_random_string)"
  cat > "$env_file" <<EOF
# gpt-creator environment
DB_NAME=${db_name}
DB_USER=${db_user}
DB_PASSWORD=${db_password}
DB_ROOT_USER=root
DB_ROOT_PASSWORD=${db_root_password}
DB_HOST=127.0.0.1
DB_PORT=3306
DB_HOST_PORT=3306
API_HOST_PORT=3000
WEB_HOST_PORT=5173
ADMIN_HOST_PORT=5174
PROXY_HOST_PORT=8080
DATABASE_URL=mysql://${db_user}:${db_password}@127.0.0.1:3306/${db_name}
VITE_API_BASE=http://localhost:3000/api/v1
EOF
  chmod 600 "$env_file" || true
}

VERSION="0.2.0"
APP_NAME="gpt-creator"

# Defaults (override via env)
CODEX_BIN="${CODEX_BIN:-codex}"
CODEX_MODEL="${CODEX_MODEL:-gpt-5-codex}"
CODEX_FALLBACK_MODEL="${CODEX_FALLBACK_MODEL:-gpt-5-codex}"
CODEX_REASONING_EFFORT="${CODEX_REASONING_EFFORT:-high}"
EDITOR_CMD="${EDITOR_CMD:-code}"
DOCKER_BIN="${DOCKER_BIN:-docker}"
MYSQL_BIN="${MYSQL_BIN:-mysql}"

# Colors (TTY-only)
if [[ -t 1 ]]; then
  c_reset=$'\033[0m'; c_dim=$'\033[2m'; c_bold=$'\033[1m'
  c_red=$'\033[31m'; c_yellow=$'\033[33m'; c_cyan=$'\033[36m'; c_green=$'\033[32m'
else
  c_reset=; c_dim=; c_bold=; c_red=; c_yellow=; c_cyan=; c_green=
fi

ts() { date +"%Y-%m-%dT%H:%M:%S"; }
die() { echo "${c_red}✖${c_reset} $*" >&2; exit 1; }
info(){ echo "${c_cyan}➜${c_reset} $*"; }
ok()  { echo "${c_green}✔${c_reset} $*"; }
warn(){ echo "${c_yellow}!${c_reset} $*"; }

abs_path() {
  python3 - "$1" <<'PY' 2>/dev/null || perl -MCwd=abs_path -e 'print abs_path(shift)."\n"' "$1" || echo "$1"
import os,sys; print(os.path.abspath(sys.argv[1]))
PY
}

to_lower() {
  printf '%s' "$1" | tr '[:upper:]' '[:lower:]'
}

slugify_name() {
  local s="${1:-}"
  s="$(to_lower "$s")"
  s="$(printf '%s' "$s" | tr -cs 'a-z0-9' '-')"
  s="$(printf '%s' "$s" | sed -E 's/-+/-/g; s/^-+//; s/-+$//')"
  printf '%s\n' "${s:-gptcreator}"
}

GC_PORT_RESERVATIONS=""

gc_port_for_service() {
  local service="$1"
  local entry
  for entry in $GC_PORT_RESERVATIONS; do
    local svc="${entry%%:*}"
    if [[ "$svc" == "$service" ]]; then
      printf '%s\n' "${entry#*:}"
      return 0
    fi
  done
  return 1
}

gc_unreserve_port() {
  local service="$1"
  [[ -n "$service" ]] || return 0
  local entry new_list=""
  for entry in $GC_PORT_RESERVATIONS; do
    local svc="${entry%%:*}"
    if [[ "$svc" == "$service" ]]; then
      continue
    fi
    if [[ -z "$new_list" ]]; then
      new_list="$entry"
    else
      new_list+=" $entry"
    fi
  done
  GC_PORT_RESERVATIONS="$new_list"
}

gc_reserve_port() {
  local service="$1" port="$2"
  [[ -n "$service" && -n "$port" ]] || return 0
  gc_unreserve_port "$service"
  if [[ -z "${GC_PORT_RESERVATIONS:-}" ]]; then
    GC_PORT_RESERVATIONS="${service}:${port}"
  else
    GC_PORT_RESERVATIONS+=" ${service}:${port}"
  fi
}

gc_port_is_reserved() {
  local port="$1"
  local entry
  for entry in $GC_PORT_RESERVATIONS; do
    if [[ "${entry#*:}" == "$port" ]]; then
      return 0
    fi
  done
  return 1
}

gc_port_reserved_by_other() {
  local port="$1" service="$2"
  local entry
  for entry in $GC_PORT_RESERVATIONS; do
    local svc="${entry%%:*}"
    local val="${entry#*:}"
    if [[ "$val" == "$port" && "$svc" != "$service" ]]; then
      return 0
    fi
  done
  return 1
}

# Context directories inside project
ensure_ctx() {
  local root="${1:-}"
  if [[ -z "${root}" ]]; then root="${PROJECT_ROOT:-$PWD}"; fi
  PROJECT_ROOT="$(abs_path "$root")"
  GC_DIR="${PROJECT_ROOT}/.gpt-creator"
  STAGING_DIR="${GC_DIR}/staging"
  INPUT_DIR="${STAGING_DIR}/inputs"
  PLAN_DIR="${STAGING_DIR}/plan"
  LOG_DIR="${GC_DIR}/logs"
  ART_DIR="${GC_DIR}/artifacts"
  mkdir -p "$GC_DIR" "$STAGING_DIR" "$INPUT_DIR" "$PLAN_DIR" "$LOG_DIR" "$ART_DIR"
  gc_create_env_if_missing
  gc_load_env
  local base_name
  base_name="$(basename "$PROJECT_ROOT")"
  PROJECT_SLUG="$(slugify_name "${GC_DOCKER_PROJECT_NAME:-$base_name}")"
  GC_DOCKER_PROJECT_NAME="${GC_DOCKER_PROJECT_NAME:-$PROJECT_SLUG}"
  COMPOSE_PROJECT_NAME="${COMPOSE_PROJECT_NAME:-$GC_DOCKER_PROJECT_NAME}"
  export GC_DOCKER_PROJECT_NAME COMPOSE_PROJECT_NAME PROJECT_SLUG
}

gc_parse_jira_tasks() {
  local jira_file="${1:?jira markdown path required}"
  local out_json="${2:?output json path required}"
  python3 - <<'PY' "$jira_file" "$out_json"
import json
import re
import sys
import time
from pathlib import Path

jira_path, out_path = sys.argv[1:3]
lines = Path(jira_path).read_text(encoding='utf-8').splitlines()

epic_id = ""
epic_title = ""
story_id = ""
story_title = ""
tasks = []
current = None
section = None

dashes = r'[\-\u2012\u2013\u2014\u2015]'


def normalise_list(value: str):
    cleaned = value.replace('+', ',').replace('/', ',').replace('&', ',')
    parts = [part.strip() for part in re.split(r',|;|\\band\\b', cleaned, flags=re.I) if part.strip()]
    seen = set()
    ordered = []
    for item in parts:
        key = item.lower()
        if key not in seen:
            seen.add(key)
            ordered.append(item)
    return ordered


def flush_current():
    global current, tasks, section
    if current is None:
        return
    description_lines = [line.rstrip() for line in current['description_lines'] if line.strip()]
    description = "\n".join(description_lines).strip()
    current['description'] = description
    del current['description_lines']
    tasks.append(current)
    current = None
    section = None


for raw in lines:
    stripped = raw.strip()
    if not stripped:
        if current is not None and section == 'description':
            current['description_lines'].append('')
        continue

    if stripped.lower() in {'**epic**', '**story**', '### **story**'}:
        continue

    epic_heading = re.match(r'^##\s+Epic\s+([A-Za-z0-9_.-]+)\s+' + dashes + r'\s+(.*)$', stripped)
    epic_bold = re.match(r'^\*\*([Ee][A-Za-z0-9_.:-]+)\s*' + dashes + r'\s*(.+?)\*\*$', stripped)
    if epic_heading or epic_bold:
        flush_current()
        if epic_heading:
            epic_id, epic_title = epic_heading.group(1).strip(), epic_heading.group(2).strip()
        else:
            epic_id, epic_title = epic_bold.group(1).strip(), epic_bold.group(2).strip()
        story_id = ""
        story_title = ""
        continue

    story_heading = re.match(r'^###\s+Story\s+([A-Za-z0-9_.-]+)\s+' + dashes + r'\s+(.*)$', stripped)
    story_bold = re.match(r'^\*\*([Ss][A-Za-z0-9_.:-]+)\s*' + dashes + r'\s*(.+?)\*\*$', stripped)
    if story_heading or story_bold:
        flush_current()
        if story_heading:
            story_id, story_title = story_heading.group(1).strip(), story_heading.group(2).strip()
        else:
            story_id, story_title = story_bold.group(1).strip(), story_bold.group(2).strip()
        continue

    task_match = re.match(r'^\*\*([Tt][A-Za-z0-9_.:-]+)\s*' + dashes + r'\s*(.+?)\*\*$', stripped)
    if task_match:
        flush_current()
        task_id, task_title = task_match.groups()
        current = {
            'epic_id': epic_id,
            'epic_title': epic_title,
            'story_id': story_id,
            'story_title': story_title,
            'id': task_id.strip(),
            'title': task_title.strip(),
            'assignees': [],
            'tags': [],
            'estimate': '',
            'description_lines': [],
            'acceptance_criteria': [],
            'dependencies': [],
        }
        section = None
        continue

    if current is None:
        continue

    if '**Description:**' in stripped:
        section = 'description'
        after = stripped.split('**Description:**', 1)[1].strip()
        if after:
            current['description_lines'].append(after)
        continue

    if '**Acceptance Criteria:**' in stripped:
        section = 'ac'
        after = stripped.split('**Acceptance Criteria:**', 1)[1].strip()
        if after:
            current['acceptance_criteria'].append(after)
        continue

    if '**Dependencies:**' in stripped:
        section = 'dependencies'
        after = stripped.split('**Dependencies:**', 1)[1].strip()
        if after:
            current['dependencies'] = normalise_list(after)
        continue

    if section == 'ac':
        if stripped.startswith('*') or stripped.startswith('-'):
            current['acceptance_criteria'].append(stripped.lstrip('*- ').rstrip())
            continue
        else:
            section = None

    if section == 'dependencies':
        if stripped.startswith('*') or stripped.startswith('-'):
            current['dependencies'].extend(normalise_list(stripped.lstrip('*- ')))
            continue
        else:
            section = None

    segments = [seg.strip() for seg in re.split(r'[·•]', stripped) if seg.strip()]
    meta_consumed = False
    for seg in segments:
        plain = seg.replace('**', '')
        lower = plain.lower()
        if lower.startswith('assignee:'):
            value = plain.split(':', 1)[1].strip()
            if value:
                current['assignees'] = normalise_list(value)
                meta_consumed = True
        elif lower.startswith('tags:'):
            value = plain.split(':', 1)[1].strip()
            if value:
                current['tags'] = normalise_list(value)
                meta_consumed = True
        elif lower.startswith('estimate:'):
            value = plain.split(':', 1)[1].strip()
            if value and not current['estimate']:
                current['estimate'] = value
                meta_consumed = True

    if section == 'description' or (not meta_consumed and section is None):
        current['description_lines'].append(raw.rstrip())

flush_current()

payload = {
    'generated_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
    'tasks': tasks
}
Path(out_path).write_text(json.dumps(payload, indent=2) + '\n', encoding='utf-8')
PY
}

gc_build_tasks_db() {
  local tasks_json="${1:?tasks json path required}"
  local db_path="${2:?sqlite db path required}"
  local force_flag="${3:-0}"
  python3 - <<'PY' "$tasks_json" "$db_path" "$force_flag"
import json
import re
import sqlite3
import sys
import time
from collections import OrderedDict
from pathlib import Path

tasks_json_path = Path(sys.argv[1])
db_path = Path(sys.argv[2])
force = sys.argv[3] == '1'

payload = json.loads(tasks_json_path.read_text(encoding='utf-8'))
all_tasks = payload.get('tasks') or []
generated_at = time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())

def slugify(text: str) -> str:
    slug = re.sub(r'[^a-z0-9]+', '-', (text or '').lower()).strip('-')
    return slug or 'item'

def story_key_for(task: dict) -> str:
    return '|'.join([
        (task.get('story_id') or '').strip(),
        (task.get('story_title') or '').strip(),
        (task.get('epic_id') or '').strip(),
        (task.get('epic_title') or '').strip(),
    ])

grouped = OrderedDict()
for task in all_tasks:
    key = story_key_for(task)
    grouped.setdefault(key, {
        'story_id': (task.get('story_id') or '').strip(),
        'story_title': (task.get('story_title') or '').strip(),
        'epic_id': (task.get('epic_id') or '').strip(),
        'epic_title': (task.get('epic_title') or '').strip(),
        'tasks': []
    })
    grouped[key]['tasks'].append(task)

db_path.parent.mkdir(parents=True, exist_ok=True)
conn = sqlite3.connect(str(db_path))
conn.row_factory = sqlite3.Row
cur = conn.cursor()
cur.execute('PRAGMA foreign_keys = ON')
cur.execute('PRAGMA journal_mode = WAL')

def list_to_text(values):
    return ', '.join(str(item).strip() for item in values if str(item).strip()) if values else None

def as_text(value):
    if isinstance(value, list):
        return list_to_text(value)
    if isinstance(value, dict):
        return json.dumps(value, ensure_ascii=False)
    if value is None:
        return None
    text = str(value).strip()
    return text if text else None

def ensure_table():
    cur.execute('''
        CREATE TABLE IF NOT EXISTS metadata (
          key TEXT PRIMARY KEY,
          value TEXT NOT NULL
        )
    ''')
    cur.execute('''
        CREATE TABLE IF NOT EXISTS epics (
          epic_key TEXT PRIMARY KEY,
          epic_id TEXT,
          title TEXT,
          slug TEXT,
          created_at TEXT NOT NULL,
          updated_at TEXT NOT NULL
        )
    ''')
    cur.execute('''
        CREATE TABLE IF NOT EXISTS stories (
          story_slug TEXT PRIMARY KEY,
          story_key TEXT UNIQUE,
          story_id TEXT,
          story_title TEXT,
          epic_key TEXT,
          epic_title TEXT,
          sequence INTEGER,
          status TEXT,
          completed_tasks INTEGER,
          total_tasks INTEGER,
          last_run TEXT,
          updated_at TEXT NOT NULL,
          created_at TEXT NOT NULL,
          FOREIGN KEY(epic_key) REFERENCES epics(epic_key)
        )
    ''')
    cur.execute('''
        CREATE TABLE IF NOT EXISTS tasks (
          id INTEGER PRIMARY KEY AUTOINCREMENT,
          story_slug TEXT NOT NULL,
          position INTEGER NOT NULL,
          task_id TEXT,
          title TEXT,
          description TEXT,
          estimate TEXT,
          assignees_json TEXT,
          tags_json TEXT,
          acceptance_json TEXT,
          dependencies_json TEXT,
          tags_text TEXT,
          story_points TEXT,
          dependencies_text TEXT,
          assignee_text TEXT,
          document_reference TEXT,
          idempotency TEXT,
          rate_limits TEXT,
          rbac TEXT,
          messaging_workflows TEXT,
          performance_targets TEXT,
          observability TEXT,
          acceptance_text TEXT,
          endpoints TEXT,
          sample_create_request TEXT,
          sample_create_response TEXT,
          user_story_ref_id TEXT,
          epic_ref_id TEXT,
          status TEXT NOT NULL DEFAULT 'pending',
          started_at TEXT,
          completed_at TEXT,
          last_run TEXT,
          story_id TEXT,
          story_title TEXT,
          epic_key TEXT,
          epic_title TEXT,
          updated_at TEXT NOT NULL,
          created_at TEXT NOT NULL,
          UNIQUE(story_slug, position),
          FOREIGN KEY(story_slug) REFERENCES stories(story_slug)
        )
    ''')

ensure_table()

def column_exists(table: str, column: str) -> bool:
    cur.execute(f"PRAGMA table_info({table})")
    return any(row['name'] == column for row in cur.fetchall())

def ensure_column(table: str, column: str, definition: str):
    if not column_exists(table, column):
        cur.execute(f"ALTER TABLE {table} ADD COLUMN {column} {definition}")

ensure_column('stories', 'completed_tasks', 'INTEGER')
ensure_column('stories', 'total_tasks', 'INTEGER')
ensure_column('stories', 'status', "TEXT DEFAULT 'pending'")
ensure_column('stories', 'last_run', 'TEXT')
ensure_column('stories', 'epic_title', 'TEXT')

ensure_column('tasks', 'story_id', 'TEXT')
ensure_column('tasks', 'story_title', 'TEXT')
ensure_column('tasks', 'epic_key', 'TEXT')
ensure_column('tasks', 'epic_title', 'TEXT')
ensure_column('tasks', 'status', "TEXT DEFAULT 'pending'")
ensure_column('tasks', 'started_at', 'TEXT')
ensure_column('tasks', 'completed_at', 'TEXT')
ensure_column('tasks', 'last_run', 'TEXT')
ensure_column('tasks', 'tags_text', 'TEXT')
ensure_column('tasks', 'story_points', 'TEXT')
ensure_column('tasks', 'dependencies_text', 'TEXT')
ensure_column('tasks', 'assignee_text', 'TEXT')
ensure_column('tasks', 'document_reference', 'TEXT')
ensure_column('tasks', 'idempotency', 'TEXT')
ensure_column('tasks', 'rate_limits', 'TEXT')
ensure_column('tasks', 'rbac', 'TEXT')
ensure_column('tasks', 'messaging_workflows', 'TEXT')
ensure_column('tasks', 'performance_targets', 'TEXT')
ensure_column('tasks', 'observability', 'TEXT')
ensure_column('tasks', 'acceptance_text', 'TEXT')
ensure_column('tasks', 'endpoints', 'TEXT')
ensure_column('tasks', 'sample_create_request', 'TEXT')
ensure_column('tasks', 'sample_create_response', 'TEXT')
ensure_column('tasks', 'user_story_ref_id', 'TEXT')
ensure_column('tasks', 'epic_ref_id', 'TEXT')

prior_story_slugs = {}
prior_story_state = {}
prior_task_state = {}

if not force:
    try:
        for row in cur.execute('SELECT story_slug, story_key, status, completed_tasks, total_tasks, last_run, updated_at, created_at FROM stories'):
            story_slug = row['story_slug']
            story_key = row['story_key']
            if story_key:
                prior_story_slugs[story_key] = story_slug
            prior_story_state[story_slug] = {
                'status': row['status'] or 'pending',
                'completed_tasks': int(row['completed_tasks'] or 0),
                'total_tasks': int(row['total_tasks'] or 0),
                'last_run': row['last_run'],
                'updated_at': row['updated_at'],
                'created_at': row['created_at'],
            }
    except sqlite3.OperationalError:
        pass

    try:
        for row in cur.execute('SELECT story_slug, position, task_id, status, started_at, completed_at, last_run FROM tasks'):
            base = {
                'status': row['status'] or 'pending',
                'started_at': row['started_at'],
                'completed_at': row['completed_at'],
                'last_run': row['last_run'],
            }
            prior_task_state[('pos', row['story_slug'], row['position'])] = base
            task_id = (row['task_id'] or '').strip().lower()
            if task_id:
                prior_task_state[('id', row['story_slug'], task_id)] = base
    except sqlite3.OperationalError:
        pass

cur.execute('DELETE FROM tasks')
cur.execute('DELETE FROM stories')
cur.execute('DELETE FROM epics')

used_story_slugs = set(prior_story_slugs.values())
used_story_slugs.discard('')

def assign_story_slug(preferred: str, story_key: str) -> str:
    if not preferred:
        preferred = 'story'
    slug = slugify(preferred)
    if story_key in prior_story_slugs:
        return prior_story_slugs[story_key]
    base = slug or 'story'
    slug = base
    i = 2
    while slug in used_story_slugs:
        slug = f"{base}-{i}"
        i += 1
    used_story_slugs.add(slug)
    return slug

epics_inserted = {}
story_count = 0
task_count = 0
restored_stories = 0
restored_tasks = 0

for sequence, (story_key, info) in enumerate(grouped.items(), start=1):
    tasks = info['tasks']
    if not tasks:
        continue
    story_id = info['story_id']
    story_title = info['story_title']
    epic_id = info['epic_id']
    epic_title = info['epic_title']

    preferred_slug_source = story_id or story_title or epic_id or f'story-{sequence}'
    story_slug = assign_story_slug(preferred_slug_source, story_key)
    restored = story_slug in prior_story_state
    if restored:
        restored_stories += 1

    epic_key = (epic_id or epic_title or '').strip()
    if not epic_key:
        epic_key = None
    else:
        epic_slug = slugify(epic_key)
        if epic_key not in epics_inserted:
            cur.execute('''
                INSERT OR REPLACE INTO epics(epic_key, epic_id, title, slug, created_at, updated_at)
                VALUES(?, ?, ?, ?, ?, ?)
            ''', (
                epic_key,
                epic_id or None,
                epic_title or None,
                epic_slug,
                generated_at,
                generated_at,
            ))
            epics_inserted[epic_key] = True

    completed_tasks = 0
    story_status = 'pending'
    story_total = len(tasks)

    for position, task in enumerate(tasks):
        task_id = (task.get('id') or '').strip()
        description = (task.get('description') or '').strip()
        estimate = (task.get('estimate') or '').strip()
        assignees = task.get('assignees') or []
        tags = task.get('tags') or []
        acceptance = task.get('acceptance_criteria') or []
        dependencies = task.get('dependencies') or []

        key_id = ('id', story_slug, task_id.lower()) if task_id else None
        key_pos = ('pos', story_slug, position)
        restore = None
        if key_id and key_id in prior_task_state and not force:
            restore = prior_task_state[key_id]
        elif key_pos in prior_task_state and not force:
            restore = prior_task_state[key_pos]

        status = (restore or {}).get('status') or 'pending'
        started_at = (restore or {}).get('started_at')
        completed_at = (restore or {}).get('completed_at')
        last_run = (restore or {}).get('last_run')

        if status == 'complete':
            completed_tasks += 1

        if restore:
            restored_tasks += 1

        tags_text = list_to_text(tags)
        dependencies_text = list_to_text(dependencies)
        assignee_text = list_to_text(assignees)
        story_points = as_text(task.get('story_points')) or (estimate if estimate else None)
        document_reference = as_text(task.get('document_reference') or task.get('document_ref'))
        idempotency_text = as_text(task.get('idempotency'))
        rate_limits = as_text(task.get('rate_limits'))
        rbac_text = as_text(task.get('rbac') or task.get('rbac_requirements'))
        messaging_workflows = as_text(task.get('messaging_workflows') or task.get('messaging_and_workflows'))
        performance_targets = as_text(task.get('performance_targets'))
        observability = as_text(task.get('observability'))
        acceptance_text = '\n'.join(item.strip() for item in acceptance if item.strip()) if acceptance else None
        if not acceptance_text:
            acceptance_text = as_text(task.get('acceptance_text'))
        endpoints = as_text(task.get('endpoints'))
        sample_create_request = as_text(task.get('sample_create_request') or task.get('sample_request'))
        sample_create_response = as_text(task.get('sample_create_response') or task.get('sample_response'))
        user_story_ref_id = as_text(task.get('user_story_ref_id') or task.get('user_story_reference_id') or story_id)
        epic_ref_id = as_text(task.get('epic_ref_id') or task.get('epic_reference_id') or epic_id)

        cur.execute('''
            INSERT INTO tasks (
              story_slug, position, task_id, title, description, estimate,
              assignees_json, tags_json, acceptance_json, dependencies_json,
              tags_text, story_points, dependencies_text, assignee_text,
              document_reference, idempotency, rate_limits, rbac,
              messaging_workflows, performance_targets, observability,
              acceptance_text, endpoints, sample_create_request, sample_create_response,
              user_story_ref_id, epic_ref_id,
              status, started_at, completed_at, last_run,
              story_id, story_title, epic_key, epic_title,
              updated_at, created_at
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            story_slug,
            position,
            task_id or None,
            (task.get('title') or '').strip() or None,
            description or None,
            estimate or None,
            json.dumps(assignees, ensure_ascii=False),
            json.dumps(tags, ensure_ascii=False),
            json.dumps(acceptance, ensure_ascii=False),
            json.dumps(dependencies, ensure_ascii=False),
            tags_text,
            story_points,
            dependencies_text,
            assignee_text,
            document_reference,
            idempotency_text,
            rate_limits,
            rbac_text,
            messaging_workflows,
            performance_targets,
            observability,
            acceptance_text,
            endpoints,
            sample_create_request,
            sample_create_response,
            user_story_ref_id,
            epic_ref_id,
            status,
            started_at,
            completed_at,
            last_run,
            story_id or None,
            story_title or None,
            epic_key,
            epic_title or None,
            generated_at,
            generated_at,
        ))

    if completed_tasks >= story_total and story_total > 0:
        story_status = 'complete'
    elif completed_tasks > 0:
        story_status = 'in-progress'
    else:
        story_status = 'pending'

    if restored and not force:
        state = prior_story_state.get(story_slug, {})
        if state:
            story_status = state.get('status') or story_status
            restored_completed = int(state.get('completed_tasks') or completed_tasks)
            completed_tasks = max(completed_tasks, restored_completed)
            story_total = state.get('total_tasks') or story_total

    cur.execute('''
        INSERT OR REPLACE INTO stories (
          story_slug, story_key, story_id, story_title,
          epic_key, epic_title, sequence, status,
          completed_tasks, total_tasks, last_run,
          updated_at, created_at
        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    ''', (
        story_slug,
        story_key,
        story_id or None,
        story_title or None,
        epic_key,
        epic_title or None,
        sequence,
        story_status,
        completed_tasks,
        story_total,
        (prior_story_state.get(story_slug) or {}).get('last_run') if restored and not force else None,
        generated_at,
        (prior_story_state.get(story_slug) or {}).get('created_at', generated_at) if restored and not force else generated_at,
    ))

    story_count += 1
    task_count += story_total

cur.execute('INSERT OR REPLACE INTO metadata(key, value) VALUES(?, ?)', ('generated_at', generated_at))
cur.execute('INSERT OR REPLACE INTO metadata(key, value) VALUES(?, ?)', ('source', str(tasks_json_path)))

conn.commit()
conn.close()

print(f'STORIES {story_count}')
print(f'TASKS {task_count}')
print(f'RESTORED_STORIES {restored_stories}')
print(f'RESTORED_TASKS {restored_tasks}')
PY
}

gc_build_context_file() {
  local dest_file="${1:?context destination required}"
  local staging_dir="${2:-$GC_STAGING_DIR}"
  mkdir -p "$(dirname "$dest_file")"
  {
    echo "# Project Context (auto-generated)"
    echo
    shopt -s nullglob
    shopt -s globstar 2>/dev/null || true
    local f
    for f in \
      "$staging_dir"/pdr.* \
      "$staging_dir"/sds.* \
      "$staging_dir"/openapi.* \
      "$staging_dir"/*.sql \
      "$staging_dir"/*.mmd \
      "$staging_dir"/*ui*pages*.* \
      "$staging_dir"/*rfp*.* \
      "$staging_dir"/*style*.* \
      "$staging_dir"/*css*; do
      [[ -f "$f" ]] || continue
      echo ""
      echo "----- FILE: $(basename "$f") -----"
      if command -v file >/dev/null 2>&1 && file -b --mime-type "$f" | grep -q 'text'; then
        sed -e 's/\t/  /g' "$f" | sed -e $'s/\r$//'
      else
        echo "(binary or non-text file; path: $f)"
      fi
    done
    shopt -u globstar 2>/dev/null || true
    shopt -u nullglob || true
  } >"$dest_file"
}

gc_update_work_state() {
  local db_path="${1:?tasks database path required}"
  local story_slug="${2:?story slug required}"
  local status="${3:?status required}"
  local completed="${4:-0}"
  local total="${5:-0}"
  local run_stamp="${6:-manual}"
  python3 - <<'PY' "$db_path" "$story_slug" "$status" "$completed" "$total" "$run_stamp"
import sqlite3
import sys
import time

db_path, story_slug, status, completed, total, run_stamp = sys.argv[1:7]
completed = int(completed or 0)
total = int(total or 0)
run_stamp = run_stamp or 'manual'
timestamp = time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())

conn = sqlite3.connect(db_path)
cur = conn.cursor()
cur.execute(
    """
    UPDATE stories
       SET status = ?,
           completed_tasks = ?,
           total_tasks = ?,
           last_run = ?,
           updated_at = ?
     WHERE story_slug = ?
    """,
    (status, completed, total, run_stamp, timestamp, story_slug)
)
if cur.rowcount == 0:
    cur.execute(
        """
        INSERT INTO stories (story_slug, story_key, status, completed_tasks, total_tasks, last_run, created_at, updated_at)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """,
        (story_slug, story_slug, status, completed, total, run_stamp, timestamp, timestamp)
    )
conn.commit()
conn.close()
PY
}

gc_update_task_state() {
  local db_path="${1:?tasks database path required}"
  local story_slug="${2:?story slug required}"
  local position="${3:?task position required}"
  local status="${4:?status required}"
  local run_stamp="${5:-manual}"
  python3 - <<'PY' "$db_path" "$story_slug" "$position" "$status" "$run_stamp"
import sqlite3
import sys
import time

db_path, story_slug, position, status, run_stamp = sys.argv[1:6]
position = int(position)
run_stamp = run_stamp or 'manual'
timestamp = time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime())

conn = sqlite3.connect(db_path)
conn.row_factory = sqlite3.Row
cur = conn.cursor()
row = cur.execute(
    'SELECT id, status, started_at, completed_at FROM tasks WHERE story_slug = ? AND position = ?',
    (story_slug, position)
).fetchone()

if row is not None:
    fields = [
        ('status', status),
        ('last_run', run_stamp),
        ('updated_at', timestamp),
    ]

    started_at = row['started_at']
    completed_at = row['completed_at']

    if status == 'in-progress' and not started_at:
        fields.append(('started_at', timestamp))
    elif status == 'complete':
        if not started_at:
            fields.append(('started_at', timestamp))
        fields.append(('completed_at', timestamp))
    elif status == 'pending':
        fields.append(('started_at', None))
        fields.append(('completed_at', None))
    elif status == 'blocked':
        if not started_at:
            fields.append(('started_at', timestamp))

    set_clause = ', '.join(f"{col} = ?" for col, _ in fields)
    params = [value for _, value in fields] + [story_slug, position]
    cur.execute(f'UPDATE tasks SET {set_clause} WHERE story_slug = ? AND position = ?', params)

conn.commit()
conn.close()
PY
}

gc_trim_memory() {
  local phase="${1:-memory-cycle}"
  info "[memory] Reclaiming resources (${phase})"

  if command -v python3 >/dev/null 2>&1; then
    python3 - <<'PY' >/dev/null 2>&1 || true
import gc
gc.collect()
PY
  fi

  if command -v purge >/dev/null 2>&1; then
    purge >/dev/null 2>&1 || true
  elif command -v sync >/dev/null 2>&1; then
    sync || true
  fi

  if command -v docker >/dev/null 2>&1; then
    docker container prune -f >/dev/null 2>&1 || true
    docker image prune -f >/dev/null 2>&1 || true
  fi
}

gc_count_pending_tasks() {
  local db_path="${1:?tasks database path required}"
  python3 - <<'PY' "$db_path"
import sqlite3
import sys

db_path = sys.argv[1]
conn = sqlite3.connect(db_path)
cur = conn.cursor()
row = cur.execute("SELECT COUNT(*) FROM tasks WHERE LOWER(COALESCE(status, 'pending')) != 'complete'").fetchone()
pending = row[0] if row else 0
conn.close()
print(pending)
PY
}

gc_tasks_db_has_rows() {
  local db_path="${1:?tasks database path required}"
  [[ -f "$db_path" ]] || return 1
  python3 - <<'PY' "$db_path"
import sqlite3
import sys

db_path = sys.argv[1]
try:
    conn = sqlite3.connect(db_path)
except sqlite3.DatabaseError:
    sys.exit(1)

stories = 0
tasks = 0
try:
    cur = conn.cursor()
    cur.execute("SELECT COUNT(*) FROM stories")
    row = cur.fetchone()
    stories = int(row[0]) if row and row[0] is not None else 0
    cur.execute("SELECT COUNT(*) FROM tasks")
    row = cur.fetchone()
    tasks = int(row[0]) if row and row[0] is not None else 0
except sqlite3.DatabaseError:
    conn.close()
    sys.exit(1)
finally:
    try:
        conn.close()
    except Exception:
        pass

sys.exit(0 if stories > 0 and tasks > 0 else 1)
PY
}

gc_has_legacy_tasks_json() {
  local json_dir="${PLAN_DIR}/create-jira-tasks/json"
  [[ -f "${json_dir}/epics.json" ]] || return 1
  [[ -d "${json_dir}/stories" ]] || return 1
  [[ -d "${json_dir}/tasks" ]] || return 1
  return 0
}

gc_rebuild_tasks_db_from_json() {
  local force_flag="${1:-0}"
  local json_dir="${PLAN_DIR}/create-jira-tasks/json"
  local epics_json="${json_dir}/epics.json"
  local stories_dir="${json_dir}/stories"
  local tasks_dir="${json_dir}/tasks"
  local refined_dir="${json_dir}/refined"
  [[ -f "$epics_json" ]] || return 1
  [[ -d "$stories_dir" ]] || return 1
  [[ -d "$tasks_dir" ]] || return 1

  local payload="${json_dir}/tasks_payload.json"
  python3 "${CLI_ROOT}/src/lib/create-jira-tasks/to_payload.py" \
    "$epics_json" "$stories_dir" "$tasks_dir" "$refined_dir" "$payload" || return 1

  local tasks_workspace="${PLAN_DIR}/tasks"
  mkdir -p "$tasks_workspace"
  local db_path="${tasks_workspace}/tasks.db"
  python3 "${CLI_ROOT}/src/lib/create-jira-tasks/to_sqlite.py" \
    "$payload" "$db_path" "$force_flag" || return 1

  return 0
}

# docker compose helper (prefers "docker compose" then docker-compose)
docker_compose() {
  local project_name="${GC_DOCKER_PROJECT_NAME:-${COMPOSE_PROJECT_NAME:-}}"
  if [[ -z "$project_name" ]]; then
    local base="$(basename "${PROJECT_ROOT:-$PWD}")"
    project_name="$(slugify_name "$base")"
  fi
  local compose_verbose_flag=""
  if [[ -n "${GC_DOCKER_VERBOSE:-}" ]]; then
    compose_verbose_flag="--verbose"
  fi
  if command -v docker >/dev/null 2>&1 && docker compose version >/dev/null 2>&1; then
    if [[ -n "$compose_verbose_flag" ]]; then
      COMPOSE_PROJECT_NAME="$project_name" docker compose "$compose_verbose_flag" "$@"
    else
      COMPOSE_PROJECT_NAME="$project_name" docker compose "$@"
    fi
  elif command -v docker-compose >/dev/null 2>&1; then
    if [[ -n "$compose_verbose_flag" ]]; then
      docker-compose -p "$project_name" "$compose_verbose_flag" "$@"
    else
      docker-compose -p "$project_name" "$@"
    fi
  else
    die "docker compose is not available. Install Docker Desktop or docker-compose."
  fi
}

gc_compose_port() {
  local compose_file="$1" service="$2" container_port="${3:-}"
  [[ -f "$compose_file" ]] || return 1
  local output=""
  if command -v docker >/dev/null 2>&1 && docker compose version >/dev/null 2>&1; then
    output="$(COMPOSE_PROJECT_NAME="$GC_DOCKER_PROJECT_NAME" docker compose -f "$compose_file" port "$service" "$container_port" 2>/dev/null | head -n1)"
  fi
  if [[ -z "$output" ]] && command -v docker-compose >/dev/null 2>&1; then
    output="$(docker-compose -p "$GC_DOCKER_PROJECT_NAME" -f "$compose_file" port "$service" "$container_port" 2>/dev/null | head -n1)"
  fi
  [[ -n "$output" ]] || return 1
  output="${output##*:}"
  [[ "$output" =~ ^[0-9]+$ ]] || return 1
  printf '%s\n' "$output"
}

gc_container_name() {
  local service="$1"
  printf '%s-%s\n' "${GC_DOCKER_PROJECT_NAME}" "$service"
}

container_exists() {
  local name="$1"
  docker ps -a --format '{{.Names}}' | grep -Fxq "$name"
}

container_state() {
  local name="$1"
  docker inspect -f '{{.State.Status}}' "$name" 2>/dev/null || echo "absent"
}

gc_start_created_containers() {
  local compose_file="$1"
  shift || true
  local -a services=("$@")
  local service container state started any_started=0

  for service in "${services[@]}"; do
    container="$(gc_container_name "$service")"
    if ! container_exists "$container"; then
      continue
    fi
    state="$(container_state "$container")"
    if [[ "$state" == "created" ]]; then
      info "Container ${container} stuck in 'created'; attempting manual start"
      if docker start "$container" >/dev/null 2>&1; then
        started=1
        any_started=1
      else
        warn "Failed to start ${container}; attempting compose start fallback"
        docker_compose -f "$compose_file" start "$service" >/dev/null 2>&1 || true
      fi
    fi
  done

  if (( any_started == 1 )); then
    local wait_seconds=0
    local timeout="${GC_DOCKER_HEALTH_TIMEOUT:-10}"
    local poll_interval="${GC_DOCKER_HEALTH_INTERVAL:-1}"
    (( poll_interval <= 0 )) && poll_interval=1
    while (( wait_seconds < timeout )); do
      local all_ready=1
      for service in "${services[@]}"; do
        container="$(gc_container_name "$service")"
        if ! container_exists "$container"; then
          continue
        fi
        state="$(container_state "$container")"
        case "$state" in
          running|healthy) continue ;;
          exited|dead)
            warn "Container ${container} exited unexpectedly (state=${state}). Check logs."
            all_ready=0
            ;;
          *)
            all_ready=0
            ;;
        esac
      done
      if (( all_ready == 1 )); then
        break
      fi
      sleep "$poll_interval"
      (( wait_seconds += poll_interval )) || true
    done
  fi
}

port_in_use() {
  local port="$1"
  if command -v lsof >/dev/null 2>&1; then
    lsof -nP -iTCP:"$port" -sTCP:LISTEN >/dev/null 2>&1 && return 0
  elif command -v netstat >/dev/null 2>&1; then
    netstat -an 2>/dev/null | grep -E "\.${port} .*LISTEN" >/dev/null && return 0
  fi
  return 1
}

find_free_port() {
  local start="${1:-3306}"
  local port="$start"; local limit=$((start+100))
  while (( port <= limit )); do
    if ! port_in_use "$port" && ! gc_port_is_reserved "$port"; then
      echo "$port"
      return 0
    fi
    ((port++)) || true
  done
  echo "$start"  # fallback
}

gc_pick_port() {
  local label="$1"
  local default="$2"
  shift 2
  local service_key="$(slugify_name "$label")"
  [[ -n "$service_key" ]] || service_key="$label"
  local existing_port=""
  existing_port="$(gc_port_for_service "$service_key" 2>/dev/null || true)"
  gc_unreserve_port "$service_key"
  local port=""
  local env_name value
  for env_name in "$@"; do
    value="${!env_name:-}"
    if [[ -n "$value" ]] && [[ "$value" =~ ^[0-9]+$ ]]; then
      port="$value"
      break
    fi
  done
  [[ -n "$port" ]] || port="$default"
  if [[ ! "$port" =~ ^[0-9]+$ ]] || (( port < 1 || port > 65535 )); then
    port="$default"
  fi
  local original="$port"
  local attempts=0
  local limit=200
  while (( attempts < limit )); do
    if (( port < 1 || port > 65535 )); then
      port="$default"
    fi
    if ! port_in_use "$port" && ! gc_port_reserved_by_other "$port" "$service_key"; then
      break
    fi
    ((port++))
    ((attempts++))
  done
  while gc_port_reserved_by_other "$port" "$service_key"; do
    ((port++))
  done
  if (( attempts >= limit )); then
    warn "Unable to find free port for ${label}; using ${port}" >&2
  elif [[ -n "$original" && "$port" != "$original" ]]; then
    info "Port ${original} in use; remapping ${label} to ${port}" >&2
  elif [[ -z "$existing_port" && "$port" != "$default" ]]; then
    info "Port ${default} in use; remapping ${label} to ${port}" >&2
  fi
  gc_reserve_port "$service_key" "$port"
  echo "$port"
}

wait_for_endpoint() {
  local url="$1" label="$2"
  local max_time="${3:-${GC_DOCKER_HEALTH_TIMEOUT:-10}}"
  local delay="${4:-${GC_DOCKER_HEALTH_INTERVAL:-1}}"
  (( delay <= 0 )) && delay=1
  (( max_time <= 0 )) && max_time=1
  local attempts=$(( (max_time + delay - 1) / delay ))
  (( attempts < 1 )) && attempts=1
  local i=1
  while (( i <= attempts )); do
    if curl -fsS --max-time 2 "$url" >/dev/null 2>&1; then
      ok "${label} ready → ${url}"
      return 0
    fi
    sleep "$delay"
    ((i++)) || true
  done
  warn "${label} not ready after ${max_time}s → ${url}"
  return 1
}

gc_sha256_file() {
  local path="$1"
  [[ -f "$path" ]] || return 1
  python3 - <<'PY' "$path"
import hashlib, pathlib, sys
path = pathlib.Path(sys.argv[1])
data = path.read_bytes()
print(hashlib.sha256(data).hexdigest())
PY
}

gc_host_prepare_pnpm() {
  if command -v pnpm >/dev/null 2>&1; then
    return 0
  fi
  if command -v corepack >/dev/null 2>&1; then
    corepack enable pnpm >/dev/null 2>&1 || true
    local version="${GC_PNPM_VERSION:-10.17.1}"
    corepack prepare "pnpm@${version}" --activate >/dev/null 2>&1 || \
      corepack use "pnpm@${version}" >/dev/null 2>&1 || true
  fi
  command -v pnpm >/dev/null 2>&1
}

gc_refresh_stack_prepare_node_modules() {
  [[ "${GC_SKIP_HOST_PNPM_INSTALL:-0}" == "1" ]] && return 0
  local root="${PROJECT_ROOT:-$PWD}"
  local lock_file="${root}/pnpm-lock.yaml"
  local has_manifest=0
  if [[ -f "${root}/pnpm-workspace.yaml" || -f "${root}/package.json" ]]; then
    has_manifest=1
  fi
  (( has_manifest )) || return 0

  local modules_dir="${root}/node_modules/.pnpm"
  local stamp_file="${root}/node_modules/.pnpm-lock.hash"
  local need_install=0
  local lock_hash=""

  if [[ -f "$lock_file" ]]; then
    lock_hash="$(gc_sha256_file "$lock_file" 2>/dev/null || true)"
  fi

  if [[ ! -d "$modules_dir" ]]; then
    need_install=1
  else
    if [[ -z "$lock_hash" ]]; then
      need_install=1
    else
      local stamp_hash=""
      [[ -f "$stamp_file" ]] && stamp_hash="$(cat "$stamp_file" 2>/dev/null || true)"
      if [[ "$lock_hash" != "$stamp_hash" ]]; then
        need_install=1
      fi
    fi
  fi

  if (( need_install )); then
    if [[ ! -f "$lock_file" ]]; then
      info "pnpm lockfile missing; generating via install"
    fi
    info "Installing workspace dependencies via pnpm (host)"
    if ! gc_host_prepare_pnpm; then
      warn "pnpm is not available on the host; skipping host install (containers may retry)."
      return 0
    fi
    local install_rc=0
    if (cd "$root" && CI=1 PNPM_IGNORE_NODE_VERSION=1 pnpm install --frozen-lockfile --unsafe-perm --prefer-offline --engine-strict=false --reporter=append-only); then
      install_rc=0
    else
      warn "pnpm install --frozen-lockfile failed; retrying without frozen lockfile"
      if (cd "$root" && CI=1 PNPM_IGNORE_NODE_VERSION=1 pnpm install --unsafe-perm --prefer-offline --engine-strict=false --no-frozen-lockfile --reporter=append-only); then
        install_rc=0
      else
        install_rc=1
      fi
    fi
    if (( install_rc == 0 )); then
      lock_hash="$(gc_sha256_file "$lock_file" 2>/dev/null || true)"
      if [[ -n "$lock_hash" ]]; then
        mkdir -p "${root}/node_modules"
        printf '%s' "$lock_hash" > "$stamp_file"
      else
        rm -f "$stamp_file"
      fi
      ok "Host dependencies installed"
    else
      warn "Host pnpm install failed; containers will attempt dependency installation."
    fi
  fi
}

render_template_file() {
  local src="$1" dest="$2"
  local db_name="${GC_DB_NAME:-${DB_NAME:-app}}"
  local db_user="${GC_DB_USER:-${DB_USER:-app}}"
  local db_pass="${GC_DB_PASSWORD:-${DB_PASSWORD:-app_pass}}"
  local db_host_port="${GC_DB_HOST_PORT:-${DB_HOST_PORT:-3306}}"
  local db_root_pass="${GC_DB_ROOT_PASSWORD:-${DB_ROOT_PASSWORD:-root}}"
  local project_slug="${GC_DOCKER_PROJECT_NAME:-${PROJECT_SLUG:-$(slugify_name "$(basename "${PROJECT_ROOT:-$PWD}")")}}"
  local api_host_port="${GC_API_HOST_PORT:-${API_HOST_PORT:-3000}}"
  local web_host_port="${GC_WEB_HOST_PORT:-${WEB_HOST_PORT:-5173}}"
  local admin_host_port="${GC_ADMIN_HOST_PORT:-${ADMIN_HOST_PORT:-5174}}"
  local proxy_host_port="${GC_PROXY_HOST_PORT:-${PROXY_HOST_PORT:-8080}}"
  python3 - <<'PY' "$src" "$dest" "$db_name" "$db_user" "$db_pass" "$db_host_port" "$db_root_pass" "$project_slug" "$api_host_port" "$web_host_port" "$admin_host_port" "$proxy_host_port"
import pathlib, sys
args = sys.argv[1:]
src, dest, db_name, db_user, db_pass, db_host_port = args[:6]
db_root_pass = args[6] if len(args) > 6 else ''
project_slug = args[7] if len(args) > 7 else 'gptcreator'
api_host_port = args[8] if len(args) > 8 else '3000'
web_host_port = args[9] if len(args) > 9 else '5173'
admin_host_port = args[10] if len(args) > 10 else '5174'
proxy_host_port = args[11] if len(args) > 11 else '8080'
text = pathlib.Path(src).read_text()
text = text.replace('{{DB_NAME}}', db_name)
text = text.replace('{{DB_USER}}', db_user)
text = text.replace('{{DB_PASSWORD}}', db_pass)
text = text.replace('{{DB_HOST_PORT}}', db_host_port)
text = text.replace('{{DB_ROOT_PASSWORD}}', db_root_pass)
text = text.replace('{{PROJECT_SLUG}}', project_slug)
text = text.replace('{{API_HOST_PORT}}', api_host_port)
text = text.replace('{{WEB_HOST_PORT}}', web_host_port)
text = text.replace('{{ADMIN_HOST_PORT}}', admin_host_port)
text = text.replace('{{PROXY_HOST_PORT}}', proxy_host_port)
pathlib.Path(dest).write_text(text)
PY
}

gc_render_sql() {
  local src="$1" dest="$2" database="$3" app_user="$4" app_pass="$5"
  python3 - <<'PY' "$src" "$dest" "$database" "$app_user" "$app_pass"
import pathlib, re, sys
src, dest, db_name, app_user, app_pass = sys.argv[1:6]
text = pathlib.Path(src).read_text()

text = text.replace('{{DB_NAME}}', db_name)
text = text.replace('{{DB_USER}}', app_user)
text = text.replace('{{DB_PASSWORD}}', app_pass)

def rewrite_add_column_if_not_exists(sql):
    alter_pattern = re.compile(r'ALTER\s+TABLE\s+(`?)([A-Za-z_][A-Za-z0-9_]*)\1\s+(.*?);', re.IGNORECASE | re.DOTALL)
    parts = []
    last_idx = 0

    def find_clause_end(body, start_idx):
        depth = 0
        i = start_idx
        while i < len(body):
            ch = body[i]
            if ch == '(':
                depth += 1
            elif ch == ')':
                if depth > 0:
                    depth -= 1
            elif ch == ',' and depth == 0:
                return i
            elif ch == ';' and depth == 0:
                return i
            i += 1
        return len(body)

    def split_clauses(body):
        clauses = []
        current = []
        depth = 0
        for ch in body:
            if ch == '(':
                depth += 1
            elif ch == ')':
                if depth > 0:
                    depth -= 1
            if ch == ',' and depth == 0:
                clauses.append(''.join(current))
                current = []
                continue
            current.append(ch)
        tail = ''.join(current)
        if tail.strip():
            clauses.append(tail)
        return clauses

    add_col_pattern = re.compile(r'ADD\s+COLUMN\s+IF\s+NOT\s+EXISTS\s+(`?)([A-Za-z_][A-Za-z0-9_]*)\1\s+', re.IGNORECASE)
    add_key_pattern = re.compile(r'ADD\s+(UNIQUE\s+)?KEY\s+(`?)([A-Za-z_][A-Za-z0-9_]*)\2', re.IGNORECASE)
    add_constraint_pattern = re.compile(r'ADD\s+CONSTRAINT\s+(`?)([A-Za-z_][A-Za-z0-9_]*)\1\s+FOREIGN\s+KEY', re.IGNORECASE)

    for match in alter_pattern.finditer(sql):
        table_name = match.group(2)
        body = match.group(3)
        clauses = split_clauses(body)

        column_additions = []
        index_additions = []
        constraint_additions = []
        leftover_clauses = []

        for clause in clauses:
            clause_stripped = clause.strip()
            if not clause_stripped:
                continue
            col_match = add_col_pattern.match(clause_stripped)
            if col_match:
                definition = clause_stripped[col_match.end():].strip().rstrip(',')
                col_name = col_match.group(2)
                quote = col_match.group(1) or ''
                column_additions.append((col_name, quote, definition))
                continue
            key_match = add_key_pattern.match(clause_stripped)
            if key_match:
                index_name = key_match.group(3)
                index_additions.append((clause_stripped, index_name))
                continue
            constraint_match = add_constraint_pattern.match(clause_stripped)
            if constraint_match:
                constraint_name = constraint_match.group(2)
                constraint_additions.append((clause_stripped, constraint_name))
                continue
            leftover_clauses.append(clause.rstrip())

        if not (column_additions or index_additions or constraint_additions):
            continue

        parts.append(sql[last_idx:match.start()])

        dynamic_sql = []
        for col_name, quote, definition in column_additions:
            column_token = f"{quote}{col_name}{quote}"
            ddl = f"ALTER TABLE `{table_name}` ADD COLUMN {column_token} {definition}".strip()
            ddl_escaped = ddl.replace("'", "''")
            dynamic_sql.append(
                "SET @ddl := (\n"
                "  SELECT IF(\n"
                "    EXISTS(SELECT 1 FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_SCHEMA = DATABASE() AND TABLE_NAME = '"
                + table_name + "' AND COLUMN_NAME = '" + col_name + "'),\n"
                "    'DO 0',\n"
                "    '" + ddl_escaped + "'\n"
                "  )\n"
                ");\nPREPARE stmt FROM @ddl;\nEXECUTE stmt;\nDEALLOCATE PREPARE stmt;\n"
            )

        for clause_text, index_name in index_additions:
            ddl = f"ALTER TABLE `{table_name}` {clause_text}".strip()
            ddl_escaped = ddl.replace("'", "''")
            dynamic_sql.append(
                "SET @ddl := (\n"
                "  SELECT IF(\n"
                "    EXISTS(SELECT 1 FROM INFORMATION_SCHEMA.STATISTICS WHERE TABLE_SCHEMA = DATABASE() AND TABLE_NAME = '"
                + table_name + "' AND INDEX_NAME = '" + index_name + "'),\n"
                "    'DO 0',\n"
                "    '" + ddl_escaped + "'\n"
                "  )\n"
                ");\nPREPARE stmt FROM @ddl;\nEXECUTE stmt;\nDEALLOCATE PREPARE stmt;\n"
            )

        for clause_text, constraint_name in constraint_additions:
            ddl = f"ALTER TABLE `{table_name}` {clause_text}".strip()
            ddl_escaped = ddl.replace("'", "''")
            dynamic_sql.append(
                "SET @ddl := (\n"
                "  SELECT IF(\n"
                "    EXISTS(SELECT 1 FROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS WHERE TABLE_SCHEMA = DATABASE() AND TABLE_NAME = '"
                + table_name + "' AND CONSTRAINT_NAME = '" + constraint_name + "'),\n"
                "    'DO 0',\n"
                "    '" + ddl_escaped + "'\n"
                "  )\n"
                ");\nPREPARE stmt FROM @ddl;\nEXECUTE stmt;\nDEALLOCATE PREPARE stmt;\n"
            )

        if leftover_clauses:
            remaining_body = ',\n'.join(leftover_clauses)
            dynamic_sql.append(f"ALTER TABLE `{table_name}`\n{remaining_body}\n;")

        parts.append('\n'.join(dynamic_sql))
        last_idx = match.end()

    parts.append(sql[last_idx:])
    return ''.join(parts)

text = rewrite_add_column_if_not_exists(text)

old_slug_update = """UPDATE instructors
SET slug = LOWER(
  REPLACE(
    REPLACE(REPLACE(REPLACE(REPLACE(REPLACE(
      TRIM(COALESCE(NULLIF(display_name,''), CONCAT(TRIM(first_name),' ',TRIM(last_name)))),
      'ç','c'),'ğ','g'),'ı','i'),'ö','o'),'ş','s'),'ü','u'
    )
  , ' ', '-')
)
WHERE slug IS NULL;"""

new_slug_update = """UPDATE instructors
SET slug = LOWER(
  REPLACE(
    REPLACE(
      REPLACE(
        REPLACE(
          REPLACE(
            REPLACE(
              REPLACE(
                TRIM(COALESCE(NULLIF(display_name,''), CONCAT(TRIM(first_name),' ',TRIM(last_name)))),
                'ç','c'),
              'ğ','g'),
            'ı','i'),
          'ö','o'),
        'ş','s'),
      'ü','u'),
    ' ', '-')
)
WHERE slug IS NULL;"""

text = text.replace(old_slug_update, new_slug_update)

old_unique_block = """ALTER TABLE instructors
  MODIFY slug VARCHAR(120) NOT NULL,
  ADD UNIQUE KEY uq_instructors_slug (slug);"""

new_unique_block = """ALTER TABLE instructors
  MODIFY slug VARCHAR(120) NOT NULL;

SET @ddl := (
  SELECT IF(
    EXISTS(SELECT 1 FROM INFORMATION_SCHEMA.STATISTICS WHERE TABLE_SCHEMA = DATABASE() AND TABLE_NAME = 'instructors' AND INDEX_NAME = 'uq_instructors_slug'),
    'DO 0',
    'ALTER TABLE `instructors` ADD UNIQUE KEY `uq_instructors_slug` (`slug`)' 
  )
);
PREPARE stmt FROM @ddl;
EXECUTE stmt;
DEALLOCATE PREPARE stmt;"""

text = text.replace(old_unique_block, new_unique_block)

text = re.sub(r'^\s*USE\s+`?[^`]+`?;', lambda _m: f"USE `{db_name}`;", text, flags=re.IGNORECASE | re.MULTILINE)
text = re.sub(r'CREATE\s+DATABASE\s+IF\s+NOT\s+EXISTS\s+`[^`]+`', lambda _m: f"CREATE DATABASE IF NOT EXISTS `{db_name}`", text, flags=re.IGNORECASE)
text = re.sub(r'ON\s+`[^`]+`\.\*\s+TO', lambda _m: f"ON `{db_name}`.* TO", text, flags=re.IGNORECASE)
text = re.sub(r"(CREATE\s+USER[^']*')([^']+)(')", lambda m: f"{m.group(1)}{app_user}{m.group(3)}", text, flags=re.IGNORECASE)
text = re.sub(r"(IDENTIFIED\s+BY\s+')([^']+)(')", lambda m: f"{m.group(1)}{app_pass}{m.group(3)}", text, flags=re.IGNORECASE)
text = re.sub(r"(TO\s+')([^']+)('@)", lambda m: f"{m.group(1)}{app_user}{m.group(3)}", text, flags=re.IGNORECASE)

def wrap_add_column(match):
    prefix, name = match.group(1), match.group(2)
    if name.startswith('`') and name.endswith('`'):
        return match.group(0)
    return f"{prefix}`{name}`"

text = re.sub(r"(?i)(ADD\s+COLUMN\s+)([A-Za-z_][A-Za-z0-9_]*)", wrap_add_column, text)

for ident in ("row_number", "field_name", "error_code"):
    pattern = re.compile(rf"(?<![`'])\b{re.escape(ident)}\b(?![`'])", re.IGNORECASE)
    text = pattern.sub(lambda m: f"`{m.group(0)}`", text)

def drop_check_constraint(src_text, name):
    pattern = re.compile(rf"CONSTRAINT\s+{re.escape(name)}\s+CHECK\s*\(", re.IGNORECASE)
    while True:
        match = pattern.search(src_text)
        if not match:
            return src_text
        start = match.start()
        comma_idx = src_text.rfind(',', 0, start)
        if comma_idx == -1:
            comma_idx = start
        depth = 1
        i = match.end()
        while i < len(src_text):
            ch = src_text[i]
            if ch == '(':
                depth += 1
            elif ch == ')':
                depth -= 1
                if depth == 0:
                    i += 1
                    break
            i += 1
        else:
            return src_text
        src_text = src_text[:comma_idx] + src_text[i:]

for constraint in ("ck_seo_target", "ck_legal_rev_publish"):
    text = drop_check_constraint(text, constraint)

pathlib.Path(dest).write_text(text)
PY
}

gc_temp_file() {
  local dir="$1" prefix="$2" suffix="$3"
  python3 - <<'PY' "$dir" "$prefix" "$suffix"
import os, sys, tempfile, pathlib
dir_path, prefix, suffix = sys.argv[1:4]
path = pathlib.Path(dir_path)
path.mkdir(parents=True, exist_ok=True)
fd, temp_path = tempfile.mkstemp(prefix=prefix, suffix=suffix, dir=str(path))
os.close(fd)
print(temp_path)
PY
}

gc_execute_sql() {
  local compose_file="$1" sql_file="$2" database="$3"
  local root_user="$4" root_pass="$5" app_user="$6" app_pass="$7" fallback_init="$8" label="$9"
  local import_ok=0
  [[ -n "$label" ]] || label="operation"
  local container_host="127.0.0.1"

  if [[ -f "$compose_file" ]]; then
    if docker_compose -f "$compose_file" ps >/dev/null 2>&1; then
      info "Using docker-compose db service for ${label}"
      docker_compose -f "$compose_file" up -d db >/dev/null 2>&1 || true
      if [[ -n "$root_pass" ]]; then
        if docker_compose -f "$compose_file" exec -T db mysql -h"${container_host}" -u"${root_user}" -p"${root_pass}" "${database}" < "${sql_file}"; then
          import_ok=1
        else
          warn "Root ${label} failed; retrying as ${app_user}"
        fi
      else
        if docker_compose -f "$compose_file" exec -T db mysql -h"${container_host}" -u"${root_user}" "${database}" < "${sql_file}"; then
          import_ok=1
        fi
      fi
      if [[ "$import_ok" -ne 1 ]]; then
        if docker_compose -f "$compose_file" exec -T db mysql -h"${container_host}" -u"${app_user}" ${app_pass:+-p"${app_pass}"} "${database}" < "${sql_file}"; then
          import_ok=1
        fi
      fi
      if [[ "$import_ok" -ne 1 && -n "$fallback_init" && -f "$fallback_init" ]]; then
        local fallback_output fallback_user fallback_pass
        fallback_output="$(python3 - "$fallback_init" <<'PY'
import re, sys
text = open(sys.argv[1]).read()
user = re.search(r"CREATE USER IF NOT EXISTS '([^']+)'", text)
password = re.search(r"IDENTIFIED BY '([^']+)'", text)
if user and password:
    print(user.group(1))
    print(password.group(1))
PY
)"
        if [[ -n "$fallback_output" ]]; then
          IFS=$'\n' read -r fallback_user fallback_pass _ <<<"$fallback_output"
          unset IFS
          if [[ -n "$fallback_user" && -n "$fallback_pass" ]]; then
            if docker_compose -f "$compose_file" exec -T db mysql -h"${container_host}" -u"${fallback_user}" ${fallback_pass:+-p"${fallback_pass}"} "${database}" < "${sql_file}"; then
              import_ok=1
            fi
          fi
        fi
      fi
      if [[ "$import_ok" -eq 1 ]]; then
        return 0
      fi
    fi
  fi

  local host="${DB_HOST:-127.0.0.1}"
  local port="${DB_HOST_PORT:-${GC_DB_HOST_PORT:-${DB_PORT:-3306}}}"
  if ${MYSQL_BIN} -h "$host" -P "$port" -u "$root_user" ${root_pass:+-p"${root_pass}"} "$database" < "$sql_file"; then
    return 0
  fi

  if ${MYSQL_BIN} -h "$host" -P "$port" -u "$app_user" ${app_pass:+-p"${app_pass}"} "$database" < "$sql_file"; then
    return 0
  fi

  return 1
}

gc_refresh_stack_collect_sql() {
  local root="$1"
  python3 - <<'PY' "$root"
import os
import re
import shlex
import sys
from pathlib import Path

root = os.path.abspath(sys.argv[1])

candidate_dirs = []
seen_dirs = set()
for rel in [
    os.path.join('.gpt-creator', 'staging', 'sql'),
    os.path.join('.gpt-creator', 'staging'),
    os.path.join('staging', 'sql'),
    os.path.join('staging'),
    os.path.join('db'),
    os.path.join('database'),
    os.path.join('sql'),
    os.path.join('data', 'sql'),
    os.path.join('data'),
    '.',
]:
    path = os.path.abspath(os.path.join(root, rel))
    if os.path.isdir(path) and path not in seen_dirs:
        candidate_dirs.append(path)
        seen_dirs.add(path)

ignore_dirs = {
    '.git', '.hg', '.svn', '.tox', '.pytest_cache', '.idea', '.vscode',
    '__pycache__', 'node_modules', 'vendor', 'dist', 'build', 'tmp', 'temp'
}

entries = []
seen_files = set()

for base in candidate_dirs:
    for dirpath, dirnames, filenames in os.walk(base):
        dirnames[:] = [d for d in dirnames if d not in ignore_dirs]
        for fname in filenames:
            if not fname.lower().endswith('.sql'):
                continue
            full = os.path.abspath(os.path.join(dirpath, fname))
            if full in seen_files:
                continue
            seen_files.add(full)
            rel_path = os.path.relpath(full, root)
            base_name = os.path.basename(full)
            rel_norm = rel_path.replace('\\', '/')
            if rel_norm.startswith('.gpt-creator/staging/') and (base_name.startswith('import-') or base_name.startswith('seed-')):
                continue
            lower = fname.lower()
            dir_lower = dirpath.lower()
            label = 'schema'
            if 'init' in lower or 'init' in dir_lower:
                label = 'init'
            elif any(token in lower or token in dir_lower for token in ('seed', 'fixture', 'sample', 'data-seed', 'seed-data')):
                label = 'seed'
            elif any(token in lower for token in ('dump', 'schema', 'structure', 'backup', 'snapshot')):
                label = 'schema'
            try:
                mtime = os.path.getmtime(full)
            except OSError:
                mtime = 0
            entries.append((label, mtime, full))

order_map = {'init': 0, 'schema': 1, 'seed': 2}
entries.sort(key=lambda item: (order_map.get(item[0], 3), item[1], item[2]))

init_list = [path for label, _, path in entries if label == 'init']
schema_list = [path for label, _, path in entries if label == 'schema']
seed_list = [path for label, _, path in entries if label == 'seed']
all_list = [path for _, _, path in entries]

db_create_re = re.compile(r"CREATE\s+DATABASE\s+(?:IF\s+NOT\s+EXISTS\s+)?(?P<name>`[^`]+`|\"[^\"]+\"|'[^']+'|[A-Za-z0-9_]+)", re.IGNORECASE)
use_re = re.compile(r"\bUSE\s+(?P<name>`[^`]+`|\"[^\"]+\"|'[^']+'|[A-Za-z0-9_]+)", re.IGNORECASE)
create_user_re = re.compile(r"CREATE\s+USER\s+(?:IF\s+NOT\s+EXISTS\s+)?'(?P<user>[^']+)'(?:\s*@\s*'(?P<host>[^']*)')?\s+IDENTIFIED(?:\s+WITH\s+[A-Za-z0-9_]+)?\s+BY\s+'(?P<pw>[^']+)'", re.IGNORECASE)
alter_user_re = re.compile(r"ALTER\s+USER\s+'(?P<user>[^']+)'(?:\s*@\s*'(?P<host>[^']*)')?\s+IDENTIFIED(?:\s+WITH\s+[A-Za-z0-9_]+)?\s+BY\s+'(?P<pw>[^']+)'", re.IGNORECASE)
grant_re = re.compile(r"GRANT\s+.+?\s+ON\s+(?P<db>`[^`]+`|\"[^\"]+\"|'[^']+'|[A-Za-z0-9_]+(?:\\.[^\s;]+)?)\s+TO\s+'(?P<user>[^']+)'", re.IGNORECASE)

def normalise_identifier(token: str) -> str:
    token = token.strip().rstrip(';').strip()
    if token.endswith('.*'):
        token = token[:-2]
    if '.' in token:
        token = token.split('.', 1)[0]
    if token and token[0] in "`\"'" and token[-1] == token[0]:
        token = token[1:-1]
    return token.strip()

db_name = ''
app_user = ''
app_password = ''
user_host = ''

for label, _, path in entries:
    if db_name and app_user and app_password:
        break
    try:
        text = Path(path).read_text(encoding='utf-8', errors='ignore')
    except Exception:
        continue
    if not db_name:
        match = db_create_re.search(text)
        if match:
            db_name = normalise_identifier(match.group('name'))
    if not db_name:
        match = use_re.search(text)
        if match:
            db_name = normalise_identifier(match.group('name'))
    if not app_user or not app_password:
        match = create_user_re.search(text) or alter_user_re.search(text)
        if match:
            app_user = match.group('user')
            app_password = match.group('pw')
            host = match.group('host') if match.group('host') is not None else ''
            user_host = host or '%'
    if app_user and not db_name:
        for m in grant_re.finditer(text):
            if m.group('user') == app_user:
                candidate = normalise_identifier(m.group('db'))
                if candidate and candidate != '*':
                    db_name = candidate
                    break

def emit_array(name, values):
    if values:
        joined = ' '.join(shlex.quote(v) for v in values)
        print(f"{name}=({joined})")
    else:
        print(f"{name}=()")

def emit_value(name, value):
    quoted = shlex.quote(value) if value else "''"
    print(f"{name}={quoted}")

emit_array('refresh_sql_init_files', init_list)
emit_array('refresh_sql_schema_files', schema_list)
emit_array('refresh_sql_seed_files', seed_list)
emit_array('refresh_sql_all_files', all_list)
emit_value('refresh_sql_default_db_name', db_name)
emit_value('refresh_sql_default_db_user', app_user)
emit_value('refresh_sql_default_db_password', app_password)
emit_value('refresh_sql_default_user_host', user_host or '%')
PY
}

gc_refresh_stack_exec_mysql() {
  local container_id="$1" sql_file="$2" user="$3" password="$4" database="$5"
  local port="${6:-3306}"

  [[ -n "$container_id" ]] || return 1
  [[ -f "$sql_file" ]] || return 1
  local -a cmd=(docker exec -i "$container_id" mysql --protocol=TCP -h 127.0.0.1 -P "$port" "-u${user}")
  if [[ -n "$password" ]]; then
    cmd+=("-p${password}")
  fi
  if [[ -n "$database" ]]; then
    cmd+=("$database")
  fi
  if ! "${cmd[@]}" <"$sql_file"; then
    return 1
  fi
  return 0
}

gc_refresh_stack_exec_inline_sql() {
  local container_id="$1" user="$2" password="$3" database="$4"
  local port="${5:-3306}"
  local sql_content
  sql_content="$(cat)"
  local -a cmd=(docker exec -i "$container_id" mysql --protocol=TCP -h 127.0.0.1 -P "$port" "-u${user}")
  if [[ -n "$password" ]]; then
    cmd+=("-p${password}")
  fi
  if [[ -n "$database" ]]; then
    cmd+=("$database")
  fi
  if ! printf "%s" "$sql_content" | "${cmd[@]}"; then
    return 1
  fi
  return 0
}

gc_refresh_stack_inspect_containers() {
  local compose_file="${1:?compose file required}"
  local -a container_ids=()
  mapfile -t container_ids < <(docker_compose -f "$compose_file" ps --all -q 2>/dev/null | awk 'NF')
  if (( ${#container_ids[@]} == 0 )); then
    mapfile -t container_ids < <(docker_compose -f "$compose_file" ps -q 2>/dev/null | awk 'NF')
  fi
  if (( ${#container_ids[@]} == 0 )); then
    printf '%s\n' "No containers found for project ${GC_DOCKER_PROJECT_NAME}."
    return 1
  fi

  local inspect_json=""
  if ! inspect_json="$(docker inspect "${container_ids[@]}" 2>/dev/null)"; then
    printf '%s\n' "Failed to inspect Docker containers for project ${GC_DOCKER_PROJECT_NAME}."
    return 1
  fi

  python3 - <<'PY' <<<"${inspect_json}"
import json
import sys

try:
    data = json.load(sys.stdin)
except Exception as exc:
    print(f"Unable to parse docker inspect output: {exc}")
    sys.exit(1)

if not isinstance(data, list):
    data = [data]

if not data:
    print("No container state data returned by docker inspect.")
    sys.exit(1)

pending = []
failures = []
healthy = []

for entry in data:
    name = (entry.get("Name") or "").lstrip("/")
    labels = entry.get("Config", {}).get("Labels", {}) or {}
    service = labels.get("com.docker.compose.service") or name
    state = entry.get("State") or {}
    status = (state.get("Status") or "").lower()
    health = (state.get("Health", {}).get("Status") or "").lower()
    exit_code = state.get("ExitCode")

    detail = f"{service}: status={status or 'unknown'}"
    if health:
        detail += f", health={health}"
    if exit_code not in (None, 0):
        detail += f", exit_code={exit_code}"

    if status == "running":
        if health in ("", "healthy"):
            healthy.append(detail)
        elif health == "starting":
            pending.append(detail)
        else:
            failures.append(detail)
    elif status in ("created", "starting"):
        pending.append(detail)
    else:
        failures.append(detail)

if failures:
    print("Container failures detected:")
    for line in failures:
        print(f"  - {line}")
    sys.exit(1)

if pending:
    print("Containers still starting:")
    for line in pending:
        print(f"  - {line}")
    sys.exit(2)

print("All containers running and healthy:")
for line in healthy:
    print(f"  - {line}")
sys.exit(0)
PY
}

gc_refresh_stack_wait_for_containers() {
  local compose_file="${1:?compose file required}"
  local timeout="${2:-${GC_DOCKER_HEALTH_TIMEOUT:-10}}"
  local interval="${3:-${GC_DOCKER_HEALTH_INTERVAL:-1}}"
  (( interval <= 0 )) && interval=1
  local elapsed=0
  local output rc

  while (( elapsed <= timeout )); do
    output="$(gc_refresh_stack_inspect_containers "$compose_file")"
    rc=$?
    if (( rc == 0 )); then
      while IFS= read -r line; do
        [[ -z "$line" ]] && continue
        info "$line"
      done <<<"$output"
      return 0
    elif (( rc == 2 )); then
      while IFS= read -r line; do
        [[ -z "$line" ]] && continue
        info "$line"
      done <<<"$output"
      sleep "$interval"
      (( elapsed += interval ))
      continue
    else
      while IFS= read -r line; do
        [[ -z "$line" ]] && continue
        warn "$line"
      done <<<"$output"
      return 1
    fi
  done

  warn "Timed out after ${timeout}s waiting for containers to report healthy state."
  output="$(gc_refresh_stack_inspect_containers "$compose_file")"
  rc=$?
  local log_fn=warn
  if (( rc == 0 )); then
    log_fn=info
  fi
  while IFS= read -r line; do
    [[ -z "$line" ]] && continue
    "$log_fn" "$line"
  done <<<"$output"
  (( rc == 0 )) || return 1
  return 0
}

# ---------- Scan helpers ----------
has_pattern() { LC_ALL=C grep -E -i -m 1 -q -- "$1" "$2" 2>/dev/null; }
classify_file() {
  local path="$1"
  local name="${path##*/}"
  local lower="$(to_lower "$name")"
  local path_norm="$(to_lower "$path")"
  local ext="${lower##*.}"
  local type="" conf=0

  case "$ext" in
    md)
      if [[ "$lower" == *pdr* ]]; then type="pdr"; conf=0.95
      elif [[ "$lower" == *sds* ]]; then type="sds"; conf=0.92
      elif [[ "$lower" == *rfp* ]]; then type="rfp"; conf=0.9
      elif [[ "$lower" == *jira* ]]; then type="jira"; conf=0.88
      elif [[ "$lower" == *ui*pages* || "$lower" == *website*ui*pages* ]]; then type="ui_pages"; conf=0.85
      elif has_pattern '\bJIRA\b|Issue Key' "$path"; then type="jira"; conf=0.6
      fi
      ;;
    yml|yaml|json)
      if has_pattern '^[[:space:]]*openapi[[:space:]]*:[[:space:]]*3' "$path" || has_pattern '"openapi"[[:space:]]*:' "$path" || has_pattern '"swagger"[[:space:]]*:' "$path"; then
        type="openapi"; conf=0.94
      fi
      ;;
    sql)
      type="sql"; conf=0.65
      if has_pattern 'CREATE[[:space:]]+TABLE' "$path"; then conf=0.8; fi
      ;;
    mmd)
      type="mermaid"; conf=0.7
      ;;
    html)
      local is_html=0
      if [[ "$path_norm" == *"page_samples"* || "$path_norm" == *"page-samples"* ]]; then
        is_html=1
      elif echo "$lower" | grep -Eq '(abo|auth|prg|evt|ctn)[0-9]+\.html'; then
        is_html=1
      fi
      if [[ $is_html -eq 1 ]]; then
        type="page_sample_html"; conf=0.7
      fi
      ;;
    css)
      local is_css=0
      if [[ "$path_norm" == *"page_samples"* || "$path_norm" == *"samples"* ]]; then
        is_css=1
      elif [[ "$lower" == *style.css ]]; then
        is_css=1
      fi
      if [[ $is_css -eq 1 ]]; then
        type="page_sample_css"; conf=0.6
      fi
      ;;
  esac

  if [[ -n "$type" ]]; then
    printf '%s|%.2f\n' "$type" "$conf"
  fi
}

cmd_scan() {
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"

  info "Scanning ${PROJECT_ROOT} for project artifacts…"
  local manifest="${GC_DIR}/scan.tsv"
  local tmp="${manifest}.tmp"
  printf "type\tconfidence\tpath\n" > "$tmp"

  while IFS= read -r -d '' f; do
    local hit
    hit="$(classify_file "$f")" || true
    if [[ -n "$hit" ]]; then
      local type conf
      IFS='|' read -r type conf <<<"$hit"
      printf "%s\t%.2f\t%s\n" "$type" "$conf" "$f" >> "$tmp"
    fi
  done < <(find "$PROJECT_ROOT" \
      \( -name '.git' -o -name 'node_modules' -o -name 'dist' -o -name 'build' -o -name '.venv' -o -name '.gpt-creator' \) -prune -o -type f -print0)

  mv "$tmp" "$manifest"

  local scan_json="${STAGING_DIR}/scan.json"
  python3 - <<'PY' "$manifest" "$PROJECT_ROOT" "$scan_json"
import csv, json, sys, time, pathlib
manifest, root, out = sys.argv[1:4]
rows = []
with open(manifest, newline='') as fh:
    reader = csv.DictReader(fh, delimiter='\t')
    for row in reader:
        if not row['type']:
            continue
        rows.append({
            "type": row['type'],
            "confidence": float(row['confidence'] or 0),
            "path": str(pathlib.Path(row['path']).resolve())
        })
scan = {
    "project_root": str(pathlib.Path(root).resolve()),
    "generated_at": time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
    "artifacts": rows
}
pathlib.Path(out).parent.mkdir(parents=True, exist_ok=True)
with open(out, 'w') as fh:
    json.dump(scan, fh, indent=2)
print(out)
PY
  ok "Scan manifest → ${scan_json}"
}

cmd_normalize() {
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"

  local scan_json="${STAGING_DIR}/scan.json"
  if [[ ! -f "$scan_json" ]]; then
    warn "No scan.json found, running scan first."
    cmd_scan --project "$PROJECT_ROOT"
  fi

  scan_json="${STAGING_DIR}/scan.json"
  python3 - <<'PY' "$scan_json" "$INPUT_DIR" "$PLAN_DIR"
import json, sys, shutil, pathlib, time
scan_path, input_dir, plan_dir = sys.argv[1:4]
input_dir = pathlib.Path(input_dir)
plan_dir = pathlib.Path(plan_dir)
input_dir.mkdir(parents=True, exist_ok=True)
plan_dir.mkdir(parents=True, exist_ok=True)

data = json.load(open(scan_path))
project_root = pathlib.Path(data.get('project_root', '.')).resolve()
artifacts = data.get('artifacts', [])

unique_types = {"pdr", "sds", "rfp", "jira", "ui_pages", "openapi"}
unique = {}
for entry in artifacts:
    t = entry.get('type')
    if t in unique_types:
        if t not in unique or entry['confidence'] > unique[t]['confidence']:
            unique[t] = entry

multi_map = {
    "sql": pathlib.Path('sql'),
    "mermaid": pathlib.Path('mermaid'),
    "page_sample_html": pathlib.Path('page_samples'),
    "page_sample_css": pathlib.Path('page_samples')
}
collected = {key: [] for key in multi_map}
for entry in artifacts:
    t = entry.get('type')
    if t in multi_map:
        collected[t].append(entry)

provenance = []

def copy_file(src_path, rel_dest, entry):
    src = pathlib.Path(src_path)
    dest = input_dir / rel_dest
    dest.parent.mkdir(parents=True, exist_ok=True)
    shutil.copy2(src, dest)
    provenance.append({
        "type": entry.get('type'),
        "source": str(src),
        "destination": str(dest.relative_to(input_dir)),
        "confidence": entry.get('confidence', 0)
    })

name_map = {
    "pdr": pathlib.Path('pdr.md'),
    "sds": pathlib.Path('sds.md'),
    "rfp": pathlib.Path('rfp.md'),
    "jira": pathlib.Path('jira.md'),
    "ui_pages": pathlib.Path('ui-pages.md')
}

for t, entry in unique.items():
    src = entry['path']
    if t == 'openapi':
        suffix = pathlib.Path(src).suffix.lower()
        if suffix in {'.yaml', '.yml'}:
            rel = pathlib.Path('openapi.yaml')
        elif suffix == '.json':
            rel = pathlib.Path('openapi.json')
        else:
            rel = pathlib.Path('openapi.src')
    else:
        rel = name_map[t]
    copy_file(src, rel, entry)

for t, entries in collected.items():
    dest_root = multi_map[t]
    for entry in entries:
        src = pathlib.Path(entry['path'])
        try:
            rel = src.resolve().relative_to(project_root)
        except ValueError:
            rel = pathlib.Path(src.name)
        rel = dest_root / rel
        copy_file(src, rel, entry)

# discovery.yaml (summary)
import io
from textwrap import indent
summary = io.StringIO()
summary.write('generated_at: %s\n' % time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()))
summary.write('project_root: %s\n' % project_root)
summary.write('artifacts:\n')
for entry in artifacts:
    summary.write('  - type: %s\n' % entry.get('type'))
    summary.write('    confidence: %.2f\n' % entry.get('confidence', 0))
    summary.write('    path: %s\n' % entry.get('path'))
(input_dir / '..' / 'discovery.yaml').resolve().write_text(summary.getvalue())

prov = {
    'generated_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()),
    'entries': provenance
}
(plan_dir / 'provenance.json').write_text(json.dumps(prov, indent=2))
PY

  ok "Normalized inputs → ${INPUT_DIR}"
}

cmd_plan() {
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"

  local openapi=""
  for cand in "$INPUT_DIR/openapi.yaml" "$INPUT_DIR/openapi.yml" "$INPUT_DIR/openapi.json" "$INPUT_DIR/openapi.src"; do
    [[ -f "$cand" ]] && { openapi="$cand"; break; }
  done
  local sql_dir="$INPUT_DIR/sql"

  python3 - <<'PY' "$openapi" "$sql_dir" "$PLAN_DIR"
import json, os, re, sys, time, pathlib
from collections import OrderedDict
openapi_path, sql_dir, plan_dir = sys.argv[1:4]
plan_dir = pathlib.Path(plan_dir)
plan_dir.mkdir(parents=True, exist_ok=True)

routes = []
schemas = []
openapi_loaded = False
if openapi_path:
    try:
        text = pathlib.Path(openapi_path).read_text()
        if openapi_path.endswith('.json'):
            data = json.loads(text)
            openapi_loaded = True
        else:
            try:
                import yaml  # type: ignore
                data = yaml.safe_load(text)  # type: ignore
                openapi_loaded = True
            except Exception:
                data = None
        if openapi_loaded and isinstance(data, dict):
            for path, methods in (data.get('paths') or {}).items():
                if isinstance(methods, dict):
                    for method, body in methods.items():
                        if not isinstance(body, dict):
                            continue
                        routes.append({
                            'method': method.upper(),
                            'path': path,
                            'summary': body.get('summary') or ''
                        })
            schemas = list((data.get('components') or {}).get('schemas') or {})
        else:
            raise ValueError('fallback parser')
    except Exception:
        routes = []
        schemas = []
        text = pathlib.Path(openapi_path).read_text() if openapi_path else ''
        current_path = None
        for line in text.splitlines():
            if re.match(r'^\s*/[^\s]+:\s*$', line):
                current_path = line.strip().rstrip(':')
                continue
            if current_path:
                m = re.match(r'^\s{2,}(get|post|put|patch|delete|options|head):\s*$', line, re.I)
                if m:
                    routes.append({'method': m.group(1).upper(), 'path': current_path, 'summary': ''})
                    continue
                if re.match(r'^\S', line):
                    current_path = None

        in_components = False
        in_schemas = False
        for line in text.splitlines():
            stripped = line.strip()
            if not stripped:
                continue
            if re.match(r'^components:\s*$', stripped):
                in_components = True
                in_schemas = False
                continue
            if in_components and re.match(r'^schemas:\s*$', stripped):
                in_schemas = True
                continue
            indent = len(line) - len(line.lstrip(' '))
            if in_schemas:
                if indent <= 2 and not stripped.startswith('#') and not stripped.startswith('schemas:'):
                    in_schemas = False
                    continue
                if indent == 4 and re.match(r'^[A-Za-z0-9_.-]+:\s*$', stripped):
                    name = stripped.split(':', 1)[0]
                    schemas.append(name)

sql_tables = []
sql_dir_path = pathlib.Path(sql_dir)
if sql_dir and sql_dir_path.is_dir():
    for sql_file in sql_dir_path.rglob('*.sql'):
        try:
            text = sql_file.read_text()
        except Exception:
            continue
        for m in re.finditer(r'CREATE\s+TABLE\s+`?([A-Za-z0-9_]+)`?', text, flags=re.IGNORECASE):
            sql_tables.append(m.group(1))

schema_set = {s.lower() for s in schemas}
table_set = {t.lower() for t in sql_tables}
only_in_openapi = sorted(schema_set - table_set)
only_in_sql = sorted(table_set - schema_set)

routes_path = plan_dir / 'routes.md'
entities_path = plan_dir / 'entities.md'
tasks_path = plan_dir / 'tasks.json'
plan_todo = plan_dir / 'PLAN_TODO.md'

def write_routes():
    lines = ['# Routes', '']
    if routes:
        for item in sorted(routes, key=lambda r: (r['path'], r['method'])):
            summary = f" — {item['summary']}" if item.get('summary') else ''
            lines.append(f"- `{item['method']} {item['path']}`{summary}")
    else:
        lines.append('No routes detected — ensure openapi.yaml is present in staging/inputs.')
    routes_path.write_text('\n'.join(lines) + '\n')


def write_entities():
    lines = ['# Entities', '']
    lines.append('## OpenAPI Schemas')
    if schemas:
        for name in sorted(schemas):
            lines.append(f'- {name}')
    else:
        lines.append('- (none found)')
    lines.append('')
    lines.append('## SQL Tables')
    if sql_tables:
        for name in sorted(sql_tables):
            lines.append(f'- {name}')
    else:
        lines.append('- (none found)')
    lines.append('')
    lines.append('## Detected deltas')
    if only_in_openapi:
        lines.append('- Only in OpenAPI: ' + ', '.join(only_in_openapi))
    if only_in_sql:
        lines.append('- Only in SQL: ' + ', '.join(only_in_sql))
    if not only_in_openapi and not only_in_sql:
        lines.append('- None (schemas and tables aligned on name)')
    entities_path.write_text('\n'.join(lines) + '\n')


def write_tasks():
    tasks = []
    if only_in_openapi:
        tasks.append({
            'id': 'align-openapi-sql',
            'title': 'Align OpenAPI schemas with SQL tables',
            'details': f"Create tables or update schemas for: {', '.join(only_in_openapi)}"
        })
    if only_in_sql:
        tasks.append({
            'id': 'document-sql-gap',
            'title': 'Document SQL tables missing from OpenAPI',
            'details': f"Expose or document SQL tables not covered by API: {', '.join(only_in_sql)}"
        })
    tasks_path.write_text(json.dumps({'generated_at': time.strftime('%Y-%m-%dT%H:%M:%SZ', time.gmtime()), 'tasks': tasks}, indent=2))


def write_plan_todo():
    lines = [
        '# Build Plan',
        '',
        '- Validate discovery outputs under `staging/inputs`.',
        '- Review `routes.md` & `entities.md` for coverage and deltas.',
        '- Implement generation steps for API, DB, Web, Admin, Docker.',
        '- Run `gpt-creator generate all --project <path>` if not already executed.',
        '- Bring the stack up with `gpt-creator run up` and smoke test.',
        '- Execute `gpt-creator verify all` to satisfy acceptance & NFR gates.',
        '- Iterate on Jira tasks using `gpt-creator iterate` until checks pass.'
    ]
    plan_todo.write_text('\n'.join(lines) + '\n')

write_routes()
write_entities()
write_tasks()
write_plan_todo()
PY

  ok "Plan artifacts created under ${PLAN_DIR}"
}

copy_template_tree() {
  local src="$1" dest="$2"
  [[ -d "$src" ]] || die "Template directory not found: $src"
  find "$src" -type d ! -name '.DS_Store' | while IFS= read -r dir; do
    local rel="${dir#$src}"
    mkdir -p "$dest/$rel"
  done
  find "$src" -type f | while IFS= read -r file; do
    local base="$(basename "$file")"
    [[ "$base" == '.DS_Store' ]] && continue
    local rel="${file#$src/}"
    local target="$dest/$rel"
    if [[ "$target" == *.tmpl ]]; then
      target="${target%.tmpl}"
      mkdir -p "$(dirname "$target")"
      render_template_file "$file" "$target"
    else
      mkdir -p "$(dirname "$target")"
      cp "$file" "$target"
    fi
  done
}

cmd_generate() {
  local facet="${1:-}"; shift || true
  [[ -n "$facet" ]] || die "generate requires a facet: api|web|admin|db|docker|all"
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"
  local templates="$CLI_ROOT/templates"

  case "$facet" in
    api)
      local out="$PROJECT_ROOT/apps/api"
      mkdir -p "$out"
      copy_template_tree "$templates/api/nestjs" "$out"
      ok "API scaffolded → ${out}"
      ;;
    web)
      local out="$PROJECT_ROOT/apps/web"
      mkdir -p "$out"
      copy_template_tree "$templates/web/vue3" "$out"
      ok "Web scaffolded → ${out}"
      ;;
    admin)
      local out="$PROJECT_ROOT/apps/admin"
      mkdir -p "$out"
      copy_template_tree "$templates/admin/vue3" "$out"
      ok "Admin scaffolded → ${out}"
      ;;
    db)
      local out="$PROJECT_ROOT/db"
      mkdir -p "$out"
      copy_template_tree "$templates/db/mysql" "$out"
      ok "DB artifacts scaffolded → ${out}"
      ;;
    docker)
      local out="$PROJECT_ROOT/docker"
      mkdir -p "$out"
      local preferred="${GC_DB_HOST_PORT:-${DB_HOST_PORT:-${MYSQL_HOST_PORT:-3306}}}"
      gc_unreserve_port db
      if port_in_use "$preferred"; then
        local next; next="$(find_free_port "$preferred")"
        if [[ "$next" != "$preferred" ]]; then
          info "Port $preferred in use; remapping MySQL to $next"
          preferred="$next"
        fi
      fi
      GC_DB_HOST_PORT="$preferred"
      DB_HOST_PORT="$GC_DB_HOST_PORT"
      MYSQL_HOST_PORT="$GC_DB_HOST_PORT"
      gc_reserve_port db "$GC_DB_HOST_PORT"
      gc_set_env_var DB_HOST_PORT "$GC_DB_HOST_PORT"
      gc_set_env_var MYSQL_HOST_PORT "$GC_DB_HOST_PORT"
      gc_set_env_var GC_DB_HOST_PORT "$GC_DB_HOST_PORT"
      local api_host_port="$(gc_pick_port "API" 3000 GC_API_HOST_PORT API_HOST_PORT)"
      GC_API_HOST_PORT="$api_host_port"
      API_HOST_PORT="$GC_API_HOST_PORT"
      gc_set_env_var API_HOST_PORT "$API_HOST_PORT"
      gc_set_env_var GC_API_HOST_PORT "$GC_API_HOST_PORT"
      gc_reserve_port api "$GC_API_HOST_PORT"
      local web_host_port="$(gc_pick_port "Web" 5173 GC_WEB_HOST_PORT WEB_HOST_PORT)"
      GC_WEB_HOST_PORT="$web_host_port"
      WEB_HOST_PORT="$GC_WEB_HOST_PORT"
      gc_set_env_var WEB_HOST_PORT "$WEB_HOST_PORT"
      gc_set_env_var GC_WEB_HOST_PORT "$GC_WEB_HOST_PORT"
      gc_reserve_port web "$GC_WEB_HOST_PORT"
      local admin_host_port="$(gc_pick_port "Admin" 5174 GC_ADMIN_HOST_PORT ADMIN_HOST_PORT)"
      GC_ADMIN_HOST_PORT="$admin_host_port"
      ADMIN_HOST_PORT="$GC_ADMIN_HOST_PORT"
      gc_set_env_var ADMIN_HOST_PORT "$ADMIN_HOST_PORT"
      gc_set_env_var GC_ADMIN_HOST_PORT "$GC_ADMIN_HOST_PORT"
      gc_reserve_port admin "$GC_ADMIN_HOST_PORT"
      local proxy_host_port="$(gc_pick_port "Proxy" 8080 GC_PROXY_HOST_PORT PROXY_HOST_PORT)"
      GC_PROXY_HOST_PORT="$proxy_host_port"
      PROXY_HOST_PORT="$GC_PROXY_HOST_PORT"
      gc_set_env_var PROXY_HOST_PORT "$PROXY_HOST_PORT"
      gc_set_env_var GC_PROXY_HOST_PORT "$GC_PROXY_HOST_PORT"
      gc_reserve_port proxy "$GC_PROXY_HOST_PORT"
      local local_url="mysql://${GC_DB_USER}:${GC_DB_PASSWORD}@127.0.0.1:${GC_DB_HOST_PORT}/${GC_DB_NAME}"
      gc_set_env_var DATABASE_URL "$local_url"
      local api_base_url="http://localhost:${GC_API_HOST_PORT}/api/v1"
      gc_set_env_var GC_API_BASE_URL "$api_base_url"
      gc_set_env_var VITE_API_BASE "$api_base_url"
      local api_health_url="http://localhost:${GC_API_HOST_PORT}/health"
      gc_set_env_var GC_API_HEALTH_URL "$api_health_url"
      local proxy_base="http://localhost:${GC_PROXY_HOST_PORT}"
      gc_set_env_var GC_WEB_URL "${proxy_base}/"
      gc_set_env_var GC_ADMIN_URL "${proxy_base}/admin/"
      gc_load_env
      copy_template_tree "$templates/docker" "$out"
      if [[ -f "$out/pnpm-entry.sh" ]]; then
        chmod +x "$out/pnpm-entry.sh" || true
      fi
      ok "Docker assets scaffolded → ${out}"
      ;;
    all)
      for f in api db web admin docker; do
        cmd_generate "$f" --project "$PROJECT_ROOT"
      done
      return 0
      ;;
    *) die "Unknown facet: ${facet}";;
  esac
}

cmd_db() {
  local action="${1:-}"; shift || true
  [[ -n "$action" ]] || die "db requires: provision|import|seed"
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"
  local compose_file="$PROJECT_ROOT/docker/docker-compose.yml"

  case "$action" in
    provision)
      [[ -f "$compose_file" ]] || die "Compose file not found at ${compose_file}; run 'gpt-creator generate docker'."
      info "Starting database service via docker compose"
      docker_compose -f "$compose_file" up -d db
      ok "MySQL container provisioned"
      ;;
    import)
      local sql_file
      sql_file="$(find "$INPUT_DIR/sql" -maxdepth 2 -type f -name '*.sql' | head -n1 || true)"
      [[ -n "$sql_file" ]] || die "No staged SQL found under ${INPUT_DIR}/sql"
      info "Importing SQL from ${sql_file}"
      local database="${DB_NAME:-$GC_DB_NAME}"
      local root_user="${DB_ROOT_USER:-root}"
      local root_pass="${DB_ROOT_PASSWORD:-${GC_DB_ROOT_PASSWORD:-}}"
      local app_user="${DB_USER:-$GC_DB_USER}"
      local app_pass="${DB_PASSWORD:-$GC_DB_PASSWORD}"
      local cleanup_files=()
      trap 'for f in "${cleanup_files[@]}"; do [[ -n "$f" && -f "$f" ]] && rm -f "$f"; done; trap - RETURN' RETURN
      local rendered_sql
      rendered_sql="$(gc_temp_file "$STAGING_DIR" "import-" ".sql")"
      cleanup_files+=("$rendered_sql")
      gc_render_sql "$sql_file" "$rendered_sql" "$database" "$app_user" "$app_pass"
      local init_sql="${INPUT_DIR}/sql/db/init.sql"
      if gc_execute_sql "$compose_file" "$rendered_sql" "$database" "$root_user" "$root_pass" "$app_user" "$app_pass" "$init_sql" "import"; then
        ok "Database import finished"
      else
        die "Database import failed"
      fi
      ;;
    seed)
      local seed_file="${PROJECT_ROOT}/db/seed.sql"
      [[ -f "$seed_file" ]] || die "Seed file not found: ${seed_file}"
      info "Seeding database from ${seed_file}"
      local database="${DB_NAME:-$GC_DB_NAME}"
      local root_user="${DB_ROOT_USER:-root}"
      local root_pass="${DB_ROOT_PASSWORD:-${GC_DB_ROOT_PASSWORD:-}}"
      local app_user="${DB_USER:-$GC_DB_USER}"
      local app_pass="${DB_PASSWORD:-$GC_DB_PASSWORD}"
      local cleanup_files=()
      trap 'for f in "${cleanup_files[@]}"; do [[ -n "$f" && -f "$f" ]] && rm -f "$f"; done; trap - RETURN' RETURN
      local rendered_seed
      rendered_seed="$(gc_temp_file "$STAGING_DIR" "seed-" ".sql")"
      cleanup_files+=("$rendered_seed")
      gc_render_sql "$seed_file" "$rendered_seed" "$database" "$app_user" "$app_pass"
      local fallback_init="${PROJECT_ROOT}/db/init.sql"
      if [[ ! -f "$fallback_init" ]]; then
        fallback_init="${INPUT_DIR}/sql/db/init.sql"
      fi
      if gc_execute_sql "$compose_file" "$rendered_seed" "$database" "$root_user" "$root_pass" "$app_user" "$app_pass" "$fallback_init" "seed"; then
        ok "Database seed applied"
      else
        die "Database seed failed"
      fi
      ;;
    *) die "Unknown db action: ${action}";;
  esac
}

cmd_run() {
  local action="${1:-}"; shift || true
  [[ -n "$action" ]] || die "run requires: up|down|logs|open"
  local root=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"
  local compose_file="$PROJECT_ROOT/docker/docker-compose.yml"

  case "$action" in
    up)
      [[ -f "$compose_file" ]] || die "Compose file not found at ${compose_file}; generate docker assets first."
      gc_refresh_stack_prepare_node_modules
      docker_compose -f "$compose_file" up -d
      ok "Stack is starting (check docker compose ps)"
      local api_base="${GC_API_BASE_URL:-http://localhost:3000/api/v1}"
      local web_url="${GC_WEB_URL:-http://localhost:8080/}"
      local admin_url="${GC_ADMIN_URL:-http://localhost:8080/admin/}"
      local health_timeout="${GC_DOCKER_HEALTH_TIMEOUT:-10}"
      local health_interval="${GC_DOCKER_HEALTH_INTERVAL:-1}"
      wait_for_endpoint "${api_base%/}/health" "API /health" "$health_timeout" "$health_interval" || true
      local web_ping="${web_url%/}/__vite_ping"
      if ! wait_for_endpoint "$web_ping" "Web (vite ping)" "$health_timeout" "$health_interval"; then
        wait_for_endpoint "${web_url%/}/" "Web" "$health_timeout" "$health_interval" || true
      fi
      local admin_ping="${admin_url%/}/__vite_ping"
      if ! wait_for_endpoint "$admin_ping" "Admin (vite ping)" "$health_timeout" "$health_interval"; then
        wait_for_endpoint "${admin_url%/}/" "Admin" "$health_timeout" "$health_interval" || true
      fi
      ;;
    down)
      [[ -f "$compose_file" ]] || die "Compose file not found at ${compose_file}"
      docker_compose -f "$compose_file" down
      ok "Stack shut down"
      ;;
    logs)
      [[ -f "$compose_file" ]] || die "Compose file not found at ${compose_file}"
      docker_compose -f "$compose_file" logs -f
      ;;
    open)
      if command -v open >/dev/null 2>&1; then
        open "http://localhost:8080" || open "http://localhost:5173" || true
      else
        ${EDITOR_CMD} "$PROJECT_ROOT" || true
      fi
      ;;
    *) die "Unknown run action: ${action}";;
  esac
}

cmd_refresh_stack() {
  local root="" compose_override="" sql_override="" seed_override=""
  local skip_import=0 skip_seed=0
  local only_services="" skip_services=""
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --compose) compose_override="$(abs_path "$2")"; shift 2;;
      --sql) sql_override="$(abs_path "$2")"; shift 2;;
      --seed) seed_override="$(abs_path "$2")"; shift 2;;
      --no-import) skip_import=1; shift;;
      --no-seed) skip_seed=1; shift;;
      --only-services) only_services="${2:-}"; shift 2;;
      --skip-services) skip_services="${2:-}"; shift 2;;
      -h|--help)
        cat <<'EOHELP'
Usage: gpt-creator refresh-stack [options]

Tear down, rebuild, and prime the local Docker stack (schema + seeds).

Options:
  --project PATH   Project root (defaults to current directory)
  --compose FILE   Override docker-compose file
  --sql FILE       Explicit SQL dump to import (auto-discovered if omitted)
  --seed FILE      Seed SQL file to apply after import
  --only-services LIST  Comma/space separated subset of services to start (e.g. "web,api")
  --skip-services LIST  Comma/space separated services to skip when starting (e.g. "db,admin")
  --no-import      Skip schema import step
  --no-seed        Skip seeding step
  -h, --help       Show this help
EOHELP
        return 0
        ;;
      *) break;;
    esac
  done

  ensure_ctx "$root"

  local -a refresh_sql_init_files=() refresh_sql_schema_files=() refresh_sql_seed_files=() refresh_sql_all_files=()
  local refresh_sql_default_db_name="" refresh_sql_default_db_user="" refresh_sql_default_db_password="" refresh_sql_default_user_host=""
  eval "$(gc_refresh_stack_collect_sql "$PROJECT_ROOT")"

  if [[ -n "$sql_override" ]]; then
    refresh_sql_schema_files=("$sql_override")
  fi
  if [[ -n "$seed_override" ]]; then
    refresh_sql_seed_files=("$seed_override")
  fi

  local env_updated=0
  if [[ -n "$refresh_sql_default_db_name" && "$refresh_sql_default_db_name" != "$GC_DB_NAME" ]]; then
    gc_set_env_var DB_NAME "$refresh_sql_default_db_name"
    gc_set_env_var GC_DB_NAME "$refresh_sql_default_db_name"
    env_updated=1
  fi
  if [[ -n "$refresh_sql_default_db_user" && "$refresh_sql_default_db_user" != "$GC_DB_USER" ]]; then
    gc_set_env_var DB_USER "$refresh_sql_default_db_user"
    gc_set_env_var GC_DB_USER "$refresh_sql_default_db_user"
    env_updated=1
  fi
  if [[ -n "$refresh_sql_default_db_password" && "$refresh_sql_default_db_password" != "$GC_DB_PASSWORD" ]]; then
    gc_set_env_var DB_PASSWORD "$refresh_sql_default_db_password"
    gc_set_env_var GC_DB_PASSWORD "$refresh_sql_default_db_password"
    env_updated=1
  fi
  if (( env_updated )); then
    gc_load_env
    local host_port="${GC_DB_HOST_PORT:-${DB_HOST_PORT:-3306}}"
    local database_url="mysql://${GC_DB_USER}:${GC_DB_PASSWORD}@127.0.0.1:${host_port}/${GC_DB_NAME}"
    gc_set_env_var DATABASE_URL "$database_url"
  fi

  info "Using database '${GC_DB_NAME}' with user '${GC_DB_USER}'"

  local compose_file="$compose_override"
  if [[ -n "$compose_file" ]]; then
    compose_file="$(abs_path "$compose_file")"
  else
    info "Rendering docker assets from templates"
    if ! cmd_generate docker --project "$PROJECT_ROOT"; then
      die "Failed to generate docker assets"
    fi
    if [[ -f "${PROJECT_ROOT}/docker/compose.yaml" ]]; then
      compose_file="${PROJECT_ROOT}/docker/compose.yaml"
    elif [[ -f "${PROJECT_ROOT}/docker/docker-compose.yml" ]]; then
      compose_file="${PROJECT_ROOT}/docker/docker-compose.yml"
    elif [[ -f "${PROJECT_ROOT}/docker-compose.yml" ]]; then
      compose_file="${PROJECT_ROOT}/docker-compose.yml"
    else
      die "Compose file not found after generation. Expected docker/compose.yaml or docker-compose.yml"
    fi
  fi

  info "Refreshing Docker stack for ${GC_DOCKER_PROJECT_NAME}"

  info "Stopping existing containers (removing volumes)"
  docker_compose -f "$compose_file" down -v --remove-orphans || true

  local slug="$GC_DOCKER_PROJECT_NAME"
  local -a stale_containers=(
    "${slug}-db"
    "${slug}-api"
    "${slug}-web"
    "${slug}-admin"
    "${slug}-proxy"
    "${slug}_db"
    "${slug}_api"
    "${slug}_web"
    "${slug}_admin"
    "${slug}_proxy"
  )
  local container
  for container in "${stale_containers[@]}"; do
    if docker ps -a --format '{{.Names}}' | grep -Fxq "$container"; then
      info "Removing leftover container ${container}"
      docker rm -f "$container" >/dev/null 2>&1 || true
    fi
  done

  if (( ${#refresh_sql_all_files[@]} > 0 )); then
    info "Discovered SQL assets:"
    local listed
    for listed in "${refresh_sql_all_files[@]}"; do
      if [[ "$listed" == "$PROJECT_ROOT/"* ]]; then
        info "  - ${listed#$PROJECT_ROOT/}"
      else
        info "  - ${listed}"
      fi
    done
  else
    info "No SQL assets discovered automatically."
  fi

  local -a all_services=(db api web admin proxy)
  local -a services_to_start=()
  if [[ -n "$only_services" ]]; then
    local normalized_only="${only_services//,/ }"
    read -r -a services_to_start <<< "$normalized_only"
  else
    services_to_start=("${all_services[@]}")
  fi
  if [[ -n "$skip_services" ]]; then
    local -a skip_list=()
    local normalized_skip="${skip_services//,/ }"
    read -r -a skip_list <<< "$normalized_skip"
    if (( ${#skip_list[@]} > 0 )); then
      local -a filtered=()
      local svc skip_flag skip_item
      for svc in "${services_to_start[@]}"; do
        skip_flag=0
        for skip_item in "${skip_list[@]}"; do
          [[ -z "$skip_item" ]] && continue
          if [[ "$svc" == "$skip_item" ]]; then
            skip_flag=1
            break
          fi
        done
        if (( skip_flag == 0 )); then
          filtered+=("$svc")
        fi
      done
      services_to_start=("${filtered[@]}")
    fi
  fi
  if (( ${#services_to_start[@]} > 0 )); then
    # Deduplicate and drop empties
    local -a deduped=()
    local svc seen_services=""
    for svc in "${services_to_start[@]}"; do
      [[ -z "$svc" ]] && continue
      case " $seen_services " in
        *" $svc "*) continue ;;
      esac
      deduped+=("$svc")
      seen_services+=" $svc"
    done
    services_to_start=("${deduped[@]}")
  fi

  if (( ${#services_to_start[@]} == 0 )); then
    warn "No services selected to start; skipping docker compose up."
  else
    info "Building and starting containers (${services_to_start[*]})"
    GC_DOCKER_VERBOSE="${GC_DOCKER_VERBOSE:-1}"
    gc_refresh_stack_prepare_node_modules
    docker_compose -f "$compose_file" up -d --build "${services_to_start[@]}"
    gc_start_created_containers "$compose_file" "${services_to_start[@]}"
  fi

  local db_requested=0
  local svc
  for svc in "${services_to_start[@]}"; do
    if [[ "$svc" == "db" ]]; then
      db_requested=1
      break
    fi
  done

  local db_container=""
  if (( db_requested )); then
    db_container="$(docker_compose -f "$compose_file" ps -q db || true)"
    if [[ -n "$db_container" ]]; then
      info "Waiting for MySQL to be ready…"
      local waited=0
      local mysql_timeout="${GC_DOCKER_HEALTH_TIMEOUT:-10}"
      local sleep_interval="${GC_DOCKER_HEALTH_INTERVAL:-1}"
      (( sleep_interval <= 0 )) && sleep_interval=1
      while (( waited < mysql_timeout )); do
        if docker exec -i "$db_container" sh -lc 'mysqladmin ping -h 127.0.0.1 --silent' >/dev/null 2>&1; then
          info "MySQL is ready."
          break
        fi
        sleep "$sleep_interval"
        ((waited += sleep_interval)) || true
      done
      if (( waited >= mysql_timeout )); then
        warn "MySQL readiness timeout after ${mysql_timeout}s (continuing)."
      fi
    else
      warn "Database container did not start; SQL import will be skipped."
    fi
  else
    info "Database service excluded from start; skipping readiness wait."
  fi

  docker_compose -f "$compose_file" ps

  local db_port="3306"
  local root_user="${DB_ROOT_USER:-root}"
  local root_pass="${DB_ROOT_PASSWORD:-${GC_DB_ROOT_PASSWORD:-}}"
  local app_user="${DB_USER:-$GC_DB_USER}"
  local app_pass="${DB_PASSWORD:-$GC_DB_PASSWORD}"
  local db_name="${DB_NAME:-$GC_DB_NAME}"
  local app_host="${refresh_sql_default_user_host:-%}"

  local import_rc=0 seed_rc=0
  local schema_attempted=0 seed_attempted=0

  if [[ -z "$db_container" ]]; then
    if (( skip_import == 0 )); then
      import_rc=1
    fi
    if (( skip_seed == 0 )); then
      seed_rc=1
    fi
  else
    if (( skip_import == 0 || skip_seed == 0 )); then
      local ensure_sql
      ensure_sql="$(python3 - <<'PY' "$db_name" "$app_user" "$app_pass" "$app_host"
import sys

db, user, password, host = sys.argv[1:5]
if not host:
    host = '%'
if not db:
    db = 'app'
if not user:
    user = 'app'

def quote_identifier(name: str) -> str:
    return '`' + name.replace('`', '``') + '`'

def quote_string(value: str) -> str:
    return "'" + value.replace("'", "''") + "'"

statements = [
    f"CREATE DATABASE IF NOT EXISTS {quote_identifier(db)} CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;",
    f"CREATE USER IF NOT EXISTS {quote_string(user)}@{quote_string(host)} IDENTIFIED BY {quote_string(password)};",
    f"GRANT ALL PRIVILEGES ON {quote_identifier(db)}.* TO {quote_string(user)}@{quote_string(host)};",
    "FLUSH PRIVILEGES;",
]
print("\n".join(statements))
PY
)"
      if ! gc_refresh_stack_exec_inline_sql "$db_container" "$root_user" "$root_pass" "" "$db_port" <<<"$ensure_sql"; then
        warn "Failed to ensure database or user; continuing with imports."
      else
        info "Ensured database ${db_name} and user ${app_user}"
      fi
    fi

    if (( skip_import == 0 )) && (( ${#refresh_sql_init_files[@]} + ${#refresh_sql_schema_files[@]} == 0 )); then
      info "No schema SQL files found; skipping import."
      skip_import=1
    fi
    if (( skip_seed == 0 )) && (( ${#refresh_sql_seed_files[@]} == 0 )); then
      info "No seed SQL files found; skipping seeding."
      skip_seed=1
    fi

    if (( skip_import == 0 )); then
      local file display
      for file in "${refresh_sql_init_files[@]}"; do
        [[ -f "$file" ]] || { warn "Init SQL not found: $file"; import_rc=1; continue; }
        display="$file"
        [[ "$display" == "$PROJECT_ROOT/"* ]] && display="${display#$PROJECT_ROOT/}"
        info "Applying init SQL: ${display}"
        ((schema_attempted++))
        if ! gc_refresh_stack_exec_mysql "$db_container" "$file" "$root_user" "$root_pass" "" "$db_port"; then
          warn "Init SQL failed as ${root_user}; retrying as ${app_user}"
          if ! gc_refresh_stack_exec_mysql "$db_container" "$file" "$app_user" "$app_pass" "" "$db_port"; then
            warn "Init SQL failed: ${display}"
            import_rc=1
            continue
          fi
        fi
      done

      for file in "${refresh_sql_schema_files[@]}"; do
        [[ -f "$file" ]] || { warn "Schema SQL not found: $file"; import_rc=1; continue; }
        display="$file"
        [[ "$display" == "$PROJECT_ROOT/"* ]] && display="${display#$PROJECT_ROOT/}"
        info "Importing schema SQL: ${display}"
        ((schema_attempted++))
        if ! gc_refresh_stack_exec_mysql "$db_container" "$file" "$root_user" "$root_pass" "$db_name" "$db_port"; then
          warn "Schema import failed as ${root_user}; retrying as ${app_user}"
          if ! gc_refresh_stack_exec_mysql "$db_container" "$file" "$app_user" "$app_pass" "$db_name" "$db_port"; then
            warn "Schema SQL failed: ${display}"
            import_rc=1
            continue
          fi
        fi
      done
    else
      info "Skipping schema import (--no-import)"
    fi

    if (( skip_seed == 0 )); then
      local seed_file seed_display
      for seed_file in "${refresh_sql_seed_files[@]}"; do
        [[ -f "$seed_file" ]] || { warn "Seed SQL not found: $seed_file"; seed_rc=1; continue; }
        seed_display="$seed_file"
        [[ "$seed_display" == "$PROJECT_ROOT/"* ]] && seed_display="${seed_display#$PROJECT_ROOT/}"
        info "Applying seed SQL: ${seed_display}"
        ((seed_attempted++))
        if ! gc_refresh_stack_exec_mysql "$db_container" "$seed_file" "$root_user" "$root_pass" "$db_name" "$db_port"; then
          warn "Seed import failed as ${root_user}; retrying as ${app_user}"
          if ! gc_refresh_stack_exec_mysql "$db_container" "$seed_file" "$app_user" "$app_pass" "$db_name" "$db_port"; then
            warn "Seed SQL failed: ${seed_display}"
            seed_rc=1
            continue
          fi
        fi
      done
    else
      info "Skipping seeding (--no-seed)"
    fi
  fi

  info "Verifying Docker service health"
  local stack_health_rc=0
  if gc_refresh_stack_wait_for_containers "$compose_file" "${GC_DOCKER_HEALTH_TIMEOUT:-10}" "${GC_DOCKER_HEALTH_INTERVAL:-1}"; then
    ok "Docker services healthy"
  else
    stack_health_rc=1
    warn "Docker services reported issues; inspect compose logs for details."
  fi

  local status=0
  if (( import_rc != 0 )); then
    status=1
  elif (( skip_import == 0 && schema_attempted > 0 )); then
    ok "Database schema imported"
  fi
  if (( seed_rc != 0 )); then
    status=1
  elif (( skip_seed == 0 && seed_attempted > 0 )); then
    ok "Database seeds applied"
  fi
  if (( stack_health_rc != 0 )); then
    status=1
  fi

  if (( status == 0 )); then
    ok "Stack refreshed successfully"
  else
    warn "Stack refresh completed with issues; inspect logs above."
  fi
  return $status
}


cmd_verify() {
  local kind="${1:-all}"; shift || true
  local root=""
  local api_base="${GC_API_BASE_URL:-http://localhost:3000/api/v1}"
  local api_health="${GC_API_HEALTH_URL:-}"
  local web_url="${GC_WEB_URL:-http://localhost:8080/}"
  local admin_url="${GC_ADMIN_URL:-http://localhost:8080/admin/}"
  local api_base_override=0 api_health_override=0 web_url_override=0 admin_url_override=0

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --api-url) api_base="$2"; api_base_override=1; shift 2;;
      --api-health) api_health="$2"; api_health_override=1; shift 2;;
      --web-url) web_url="$2"; web_url_override=1; shift 2;;
      --admin-url) admin_url="$2"; admin_url_override=1; shift 2;;
      *) break;;
    esac
  done
  ensure_ctx "$root"

  local compose_file="${PROJECT_ROOT}/docker/docker-compose.yml"
  local ports_updated=0
  if [[ -f "$compose_file" ]]; then
    local detected
    if detected="$(gc_compose_port "$compose_file" api 3000)"; then
      if [[ -n "$detected" && "$detected" != "$GC_API_HOST_PORT" ]]; then
        GC_API_HOST_PORT="$detected"; API_HOST_PORT="$detected"; ports_updated=1
      fi
    fi
    if detected="$(gc_compose_port "$compose_file" web 5173)"; then
      if [[ -n "$detected" && "$detected" != "$GC_WEB_HOST_PORT" ]]; then
        GC_WEB_HOST_PORT="$detected"; WEB_HOST_PORT="$detected"; ports_updated=1
      fi
    fi
    if detected="$(gc_compose_port "$compose_file" admin 5173)"; then
      if [[ -n "$detected" && "$detected" != "$GC_ADMIN_HOST_PORT" ]]; then
        GC_ADMIN_HOST_PORT="$detected"; ADMIN_HOST_PORT="$detected"; ports_updated=1
      fi
    fi
    if detected="$(gc_compose_port "$compose_file" proxy 80)"; then
      if [[ -n "$detected" && "$detected" != "$GC_PROXY_HOST_PORT" ]]; then
        GC_PROXY_HOST_PORT="$detected"; PROXY_HOST_PORT="$detected"; ports_updated=1
      fi
    fi
  fi
  (( ports_updated )) && gc_env_sync_ports

  if (( api_base_override == 0 )); then
    api_base="${GC_API_BASE_URL:-$api_base}"
  fi
  if (( web_url_override == 0 )); then
    web_url="${GC_WEB_URL:-$web_url}"
  fi
  if (( admin_url_override == 0 )); then
    admin_url="${GC_ADMIN_URL:-$admin_url}"
  fi
  if (( api_health_override == 0 )); then
    api_health="${GC_API_HEALTH_URL:-$api_health}"
  fi

  local trimmed_base="${api_base%/}"
  api_health="${api_health:-${trimmed_base}/health}"

  local verify_root="$CLI_ROOT/verify"
  [[ -d "$verify_root" ]] || die "verify scripts directory missing at ${verify_root}"

  local -a check_names
  case "$kind" in
    acceptance) check_names=(acceptance) ;;
    nfr) check_names=(openapi a11y lighthouse consent program_filters) ;;
    all) check_names=(acceptance openapi a11y lighthouse consent program_filters) ;;
    *) die "Unknown verify target: ${kind}";;
  esac

  local pass=0 fail=0 skip=0
  compose_file="${PROJECT_ROOT}/docker/docker-compose.yml"
  run_check() {
    local label="$1"; shift
    if "$@"; then
      ((pass++))
    else
      local status=$?
      if [[ $status -eq 3 ]]; then
        ((skip++))
        warn "${label} check skipped (missing dependency)"
      else
        ((fail++))
        warn "${label} check failed (exit ${status})"
      fi
    fi
  }

  for name in "${check_names[@]}"; do
    case "$name" in
      acceptance)
        run_check "acceptance" \
          env PROJECT_ROOT="$PROJECT_ROOT" GC_COMPOSE_FILE="$compose_file" \
          bash "$verify_root/acceptance.sh" "${api_base}" "${web_url}" "${admin_url}"
        ;;
      openapi)
        local spec=""
        for cand in "$INPUT_DIR/openapi.yaml" "$INPUT_DIR/openapi.yml" "$INPUT_DIR/openapi.json"; do
          [[ -f "$cand" ]] && { spec="$cand"; break; }
        done
        run_check "openapi" bash "$verify_root/check-openapi.sh" "${spec}" ;;
      a11y)
        run_check "a11y" bash "$verify_root/check-a11y.sh" "${web_url}" "${admin_url}"
        ;;
      lighthouse)
        run_check "lighthouse" bash "$verify_root/check-lighthouse.sh" "${web_url}" "${admin_url}"
        ;;
      consent)
        run_check "consent" bash "$verify_root/check-consent.sh" "${web_url}"
        ;;
      program_filters)
        run_check "program-filters" bash "$verify_root/check-program-filters.sh" "${api_base}"
        ;;
    esac
  done

  if (( fail > 0 )); then
    die "Verify failed — pass=${pass} fail=${fail} skip=${skip}"
  fi
  ok "Verify complete — pass=${pass} skip=${skip}"
}

codex_call() {
  local task="${1:?task}"; shift || true
  local prompt_dir="${GC_DIR}/prompts"
  mkdir -p "$prompt_dir"

  local prompt_file=""
  local output_file=""

  if [[ $# -gt 0 && -f "$1" ]]; then
    prompt_file="$1"
    shift || true
  fi

  while [[ $# -gt 0 ]]; do
    case "$1" in
      --prompt) prompt_file="$2"; shift 2;;
      --output) output_file="$2"; shift 2;;
      *) break;;
    esac
  done

  if [[ -z "$prompt_file" ]]; then
    prompt_file="${prompt_dir}/${task}.md"
    if [[ ! -f "$prompt_file" ]]; then
      cat >"$prompt_file" <<'PROMPT'
# Instruction
You are Codex (gpt-5-codex) assisting the gpt-creator pipeline. Apply requested changes deterministically.
PROMPT
    fi
  fi

  if command -v "$CODEX_BIN" >/dev/null 2>&1; then
    info "Codex ${task} → model=${CODEX_MODEL}"
    local fallback_model="${CODEX_FALLBACK_MODEL:-gpt-5-codex}"
    local run_codex_model
    run_codex_model() {
      local model="$1"
      shift || true
      local args=(exec --model "$model")
      if [[ -n "${CODEX_PROFILE:-}" ]]; then
        args+=(--profile "$CODEX_PROFILE")
      fi
      if [[ -n "${PROJECT_ROOT:-}" ]]; then
        args+=(--cd "$PROJECT_ROOT")
      fi
      if [[ -n "${CODEX_REASONING_EFFORT:-}" ]]; then
        args+=(-c "model_reasoning_effort=\"${CODEX_REASONING_EFFORT}\"")
      fi
      args+=(--full-auto --sandbox workspace-write --skip-git-repo-check)
      if [[ -n "$output_file" ]]; then
        mkdir -p "$(dirname "$output_file")"
        args+=(--output-last-message "$output_file")
      fi
      "$CODEX_BIN" "${args[@]}" < "$prompt_file"
      return $?
    }
    if run_codex_model "$CODEX_MODEL"; then
      :
    elif [[ "$CODEX_MODEL" != "$fallback_model" ]]; then
      warn "Codex model ${CODEX_MODEL} failed; retrying with ${fallback_model}."
      run_codex_model "$fallback_model" || warn "Codex invocation returned non-zero."
    else
      warn "Codex invocation returned non-zero."
    fi
  else
    warn "Codex binary (${CODEX_BIN}) not found — skipping ${task}."
  fi
}

ensure_node_runtime() {
  local root_dir="${1:-$PROJECT_ROOT}"
  local target_version="20.10.0"
  local runtime_root="${PLAN_DIR}/.runtime"
  local archive_dir="node-v${target_version}"

  # If current node already satisfies v20.x with sufficient minor, skip.
  if command -v node >/dev/null 2>&1; then
    local current
    current="$(node -v 2>/dev/null | sed 's/^v//')"
    if [[ "$current" == 20.* ]]; then
      local minor="${current#20.}"
      minor="${minor%%.*}"
      local patch="${current#20.${minor}.}"
      [[ -z "$patch" ]] && patch=0
      if (( 10#${minor:-0} > 10 )) || { (( 10#${minor:-0} == 10 )) && (( 10#${patch:-0} >= 0 )); }; then
        return
      fi
    fi
  fi

  mkdir -p "$runtime_root"

  local os_name="$(uname -s)"
  local arch_name="$(uname -m)"
  local os_tag="" arch_tag="" ext="tar.xz"

  case "$os_name" in
    Darwin)
      os_tag="darwin"
      ext="tar.gz"
      ;;
    Linux)
      os_tag="linux"
      ;;
    *)
      warn "Unsupported OS (${os_name}) for automatic Node runtime; continuing with system Node."
      return
      ;;
  esac

  case "$arch_name" in
    x86_64|amd64)
      arch_tag="x64"
      ;;
    arm64|aarch64)
      arch_tag="arm64"
      ;;
    *)
      warn "Unsupported CPU architecture (${arch_name}) for automatic Node runtime; continuing with system Node."
      return
      ;;
  esac

  local archive_name="node-v${target_version}-${os_tag}-${arch_tag}"
  local target_dir="${runtime_root}/${archive_name}"

  if [[ ! -d "$target_dir" ]]; then
    local url="https://nodejs.org/dist/v${target_version}/${archive_name}.${ext}"
    info "Downloading Node.js ${target_version} (${os_tag}-${arch_tag})"
    local tmp_archive
    tmp_archive="$(mktemp "${runtime_root}/node-${target_version}.XXXXXX")"
    if ! curl -fsSL "$url" -o "$tmp_archive"; then
      warn "Failed to download Node.js runtime from ${url}; continuing with system Node."
      rm -f "$tmp_archive"
      return
    fi
    mkdir -p "$runtime_root"
    if [[ "$ext" == "tar.gz" ]]; then
      tar -xzf "$tmp_archive" -C "$runtime_root"
    else
      tar -xJf "$tmp_archive" -C "$runtime_root"
    fi
    rm -f "$tmp_archive"
  fi

  if [[ ! -d "$target_dir" ]]; then
    warn "Node runtime directory ${target_dir} missing after download; continuing with system Node."
    return
  fi

  export PATH="${target_dir}/bin:${PATH}"
  export NODE_HOME="$target_dir"
  export npm_config_cache="${runtime_root}/npm-cache"
  export PNPM_HOME="${runtime_root}/pnpm-home"
  mkdir -p "$npm_config_cache" "$PNPM_HOME"
  export PATH="${PNPM_HOME}:${PATH}"
  export GC_NODE_RUNTIME="${target_dir}"
  ok "Node.js runtime pinned to ${target_version}"
}

ensure_node_dependencies() {
  local root_dir="${1:-$PROJECT_ROOT}"
  local sentinel="${PLAN_DIR}/.deps-installed"

  ensure_node_runtime "$root_dir"

  if [[ -f "$sentinel" ]]; then
    return
  fi

  local installer_desc=""
  local -a installer_cmd=()

  if [[ -f "$root_dir/pnpm-lock.yaml" || -f "$root_dir/pnpm-workspace.yaml" ]]; then
    if command -v pnpm >/dev/null 2>&1; then
      installer_desc="pnpm workspace"
      installer_cmd=(pnpm install --frozen-lockfile)
    fi
  fi

  if [[ ${#installer_cmd[@]} -eq 0 && -f "$root_dir/package.json" ]]; then
    if command -v pnpm >/dev/null 2>&1; then
      installer_desc="pnpm"
      installer_cmd=(pnpm install)
    elif command -v npm >/dev/null 2>&1; then
      installer_desc="npm"
      installer_cmd=(npm install)
    fi
  fi

  if [[ ${#installer_cmd[@]} -eq 0 ]]; then
    return
  fi

  local log_file="/tmp/gc_deps_install.log"

  info "Ensuring Node.js dependencies via ${installer_desc}"
  local install_status=1
  if [[ ${installer_cmd[0]} == pnpm ]]; then
    if (cd "$root_dir" && CI=1 PNPM_IGNORE_NODE_VERSION=1 "${installer_cmd[@]}" >"$log_file" 2>&1); then
      install_status=0
    fi
  else
    if (cd "$root_dir" && "${installer_cmd[@]}" >"$log_file" 2>&1); then
      install_status=0
    fi
  fi

  if (( install_status == 0 )); then
    mkdir -p "$(dirname "$sentinel")"
    touch "$sentinel"
    ok "Dependencies installed"
  else
    warn "Dependency installation via ${installer_desc} failed; inspect ${log_file}. Continuing anyway."
  fi
}

gc_apply_codex_changes() {
  local output_file="${1:?output file required}"
  local project_root="${2:?project root required}"
  python3 - <<'PY' "$output_file" "$project_root"
import json
import subprocess
import sys
from pathlib import Path

output_path = Path(sys.argv[1])
project_root = Path(sys.argv[2])

if not output_path.exists():
    print("no-output", flush=True)
    sys.exit(0)

raw = output_path.read_text(encoding='utf-8').strip()
if not raw:
    print("empty-output", flush=True)
    sys.exit(0)

# Remove code fences if present
if '```' in raw:
    cleaned = []
    fenced = False
    for line in raw.splitlines():
        marker = line.strip()
        if marker.startswith('```'):
            fenced = not fenced
            continue
        if not fenced:
            cleaned.append(line)
    raw = '\n'.join(cleaned).strip()

start = raw.find('{')
end = raw.rfind('}')
if start == -1 or end == -1 or end <= start:
    raise SystemExit("JSON not found in Codex output")

fragment = raw[start:end+1]

import re
fragment = re.sub(r'\\"(?=[}\]\n])', r'\\""', fragment)

while True:
    try:
        payload = json.loads(fragment)
        break
    except json.JSONDecodeError as exc:
        if 'Invalid \\escape' in exc.msg:
            fragment = fragment[:exc.pos] + '\\' + fragment[exc.pos:]
            continue
        decoder = json.JSONDecoder(strict=False)
        try:
            payload = decoder.decode(fragment)
            break
        except json.JSONDecodeError:
            raw_dump = output_path.with_suffix(output_path.suffix + '.raw.txt')
            raw_dump.parent.mkdir(parents=True, exist_ok=True)
            raw_dump.write_text(raw, encoding='utf-8')
            rel_dump = raw_dump
            try:
                rel_dump = raw_dump.relative_to(project_root)
            except ValueError:
                rel_dump = raw_dump
            payload = {
                'plan': [],
                'changes': [],
                'commands': [],
                'notes': [
                    f"Codex output could not be parsed as JSON; review {rel_dump}."
                ],
            }
            print('STATUS parse-error')
            print(f"RAW {rel_dump}")
            break

changes = payload.get('changes') or []
written = []
patched = []
manual_notes = []

def rewrite_patch_paths(diff_text: str) -> str:
    mapping = {
        'api/': 'apps/api/',
        'web/': 'apps/web/',
        'admin/': 'apps/admin/',
        'site/': 'apps/web/',
    }

    def rewrite_path(path: str) -> str:
        for old, new in mapping.items():
            if path.startswith(old) and not path.startswith(new):
                return new + path[len(old):]
        return path

    lines = diff_text.splitlines()
    rewritten = []
    for line in lines:
        if line.startswith('diff --git a/'):
            parts = line.split()
            if len(parts) >= 4:
                a_path = parts[2][2:]
                b_path = parts[3][2:]
                new_a = rewrite_path(a_path)
                new_b = rewrite_path(b_path)
                if new_a != a_path or new_b != b_path:
                    line = f"diff --git a/{new_a} b/{new_b}"
        elif line.startswith('--- a/'):
            path = line[6:]
            new_path = rewrite_path(path)
            if new_path != path:
                line = f"--- a/{new_path}"
        elif line.startswith('+++ b/'):
            path = line[6:]
            new_path = rewrite_path(path)
            if new_path != path:
                line = f"+++ b/{new_path}"
        rewritten.append(line)
    return '\n'.join(rewritten)

def ensure_diff_headers(diff_text: str, path: str) -> str:
    if 'diff --git ' in diff_text:
        return diff_text

    lines = diff_text.splitlines()
    header = [
        f'diff --git a/{path} b/{path}',
        f'--- a/{path}',
        f'+++ b/{path}',
    ]
    return '\n'.join(header + lines)

def ensure_within_root(path: Path) -> Path:
    try:
        full = (project_root / path).resolve(strict=False)
        project = project_root.resolve(strict=True)
    except FileNotFoundError:
        project = project_root.resolve()
        full = (project_root / path).resolve(strict=False)
    if not str(full).startswith(str(project)):
        raise ValueError(f"Path {path} escapes project root")
    return full

for change in changes:
    ctype = change.get('type')
    path = change.get('path')
    if not path:
        raise ValueError('Change entry missing path')
    if ctype == 'file':
        content = change.get('content', '')
        dest = ensure_within_root(Path(path))
        dest.parent.mkdir(parents=True, exist_ok=True)
        dest.write_text(content, encoding='utf-8')
        written.append(str(dest.relative_to(project_root)))
    elif ctype == 'patch':
        diff = change.get('diff')
        if not diff:
            raise ValueError(f"Patch change for {path} missing diff")
        diff = rewrite_patch_paths(diff)
        diff = ensure_diff_headers(diff, path)
        if not diff.endswith('\n'):
            diff += '\n'
        proc = subprocess.run(
            ['git', 'apply', '--whitespace=nowarn', '-'],
            input=diff.encode('utf-8'),
            cwd=str(project_root),
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            check=False,
        )
        if proc.returncode != 0:
            git_err = proc.stderr.decode('utf-8')

            three_way = subprocess.run(
                ['git', 'apply', '--3way', '--whitespace=nowarn', '-'],
                input=diff.encode('utf-8'),
                cwd=str(project_root),
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                check=False,
            )

            if three_way.returncode == 0:
                patched.append(path + ' (3way)')
                continue

            git_err += three_way.stderr.decode('utf-8')

            fallback = subprocess.run(
                ['patch', '-p1', '--forward', '--silent'],
                input=diff.encode('utf-8'),
                cwd=str(project_root),
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                check=False,
            )
            if fallback.returncode != 0:
                # check if patch already applied
                already = subprocess.run(
                    ['git', 'apply', '--reverse', '--check', '-'],
                    input=diff.encode('utf-8'),
                    cwd=str(project_root),
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    check=False,
                )
                if already.returncode == 0:
                    patched.append(path + ' (already applied)')
                    continue

                new_content = None
                diff_lines = diff.splitlines()
                multi_file = sum(1 for line in diff_lines if line.startswith('diff --git ')) > 1
                if not multi_file and any(line.startswith('--- /dev/null') for line in diff_lines):
                    capture = False
                    content_lines = []
                    for line in diff_lines:
                        if line.startswith('@@'):
                            capture = True
                            continue
                        if not capture:
                            continue
                        if not line or line.startswith('diff --git'):
                            continue
                        if line.startswith('+'):
                            content_lines.append(line[1:])
                        elif line.startswith('-') or line.startswith('---') or line.startswith('+++'):
                            continue
                        elif line.startswith('\\'):
                            continue
                        else:
                            content_lines.append(line)
                    if content_lines:
                        new_content = '\n'.join(content_lines)
                        if not new_content.endswith('\n'):
                            new_content += '\n'
                if new_content is not None:
                    dest = ensure_within_root(Path(path))
                    if dest.exists() and dest.is_dir():
                        new_content = None
                    else:
                        dest.parent.mkdir(parents=True, exist_ok=True)
                        if dest.exists():
                            existing = dest.read_text(encoding='utf-8')
                            if existing == new_content:
                                patched.append(path + ' (already exists)')
                                continue
                        dest.write_text(new_content, encoding='utf-8')
                        patched.append(path + ' (reconstructed)')
                        continue

                if new_content is None:
                    try:
                        proc_noctx = subprocess.run(
                            ['git', 'apply', '--reject', '--whitespace=nowarn', '-'],
                            input=diff.encode('utf-8'),
                            cwd=str(project_root),
                            stdout=subprocess.PIPE,
                            stderr=subprocess.PIPE,
                            check=False,
                        )
                        if proc_noctx.returncode == 0:
                            patched.append(path + ' (partial apply)')
                            continue
                        else:
                            git_err += proc_noctx.stderr.decode('utf-8')
                    except Exception:
                        pass

                manual_patch = output_path.with_suffix(output_path.suffix + f".{len(manual_notes)+1}.patch")
                manual_patch.write_text(diff, encoding='utf-8')
                relative_manual = manual_patch
                try:
                    relative_manual = manual_patch.relative_to(project_root)
                except ValueError:
                    pass
                manual_notes.append(f"Patch for {path} could not be applied automatically. Review and apply {relative_manual} manually.")
                patched.append(path + ' (manual)')
                sys.stderr.write(git_err)
                sys.stderr.write(fallback.stderr.decode('utf-8'))
                continue
            patched.append(path + ' (patch)')
        else:
            patched.append(path)
    else:
        raise ValueError(f"Unknown change type: {ctype}")

summary = {
    'written': written,
    'patched': patched,
    'commands': payload.get('commands') or [],
    'notes': (payload.get('notes') or []) + manual_notes,
}
print('APPLIED')
for path in written:
    print(f"WRITE {path}")
for path in patched:
    print(f"PATCH {path}")
for cmd in summary['commands']:
    print(f"CMD {cmd}")
for note in summary['notes']:
    print(f"NOTE {note}")
PY
}

cmd_create_tasks() {
  local root="" jira="" force=0
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --jira) jira="$(abs_path "$2")"; shift 2;;
      --force) force=1; shift;;
      -h|--help)
        cat <<'EOUSAGE'
Usage: gpt-creator create-tasks [--project PATH] [--jira FILE] [--force]

Convert Jira markdown tasks into a project-scoped SQLite database stored under .gpt-creator/staging/plan/tasks.

Options:
  --project PATH  Project root (defaults to current directory)
  --jira FILE     Jira markdown source (defaults to staging/inputs/jira.md)
  --force         Rebuild the tasks database without restoring prior progress metadata
EOUSAGE
        return 0
        ;;
      *) break;;
    esac
  done

  ensure_ctx "$root"
  [[ -n "$jira" ]] || jira="${INPUT_DIR}/jira.md"
  [[ -f "$jira" ]] || die "Jira tasks file not found: ${jira}"

  local tasks_dir="${PLAN_DIR}/tasks"
  local parsed_local="${tasks_dir}/parsed.local.json"
  local tasks_db="${tasks_dir}/tasks.db"
  mkdir -p "$tasks_dir"

  info "Parsing Jira backlog → ${parsed_local}"
  gc_parse_jira_tasks "$jira" "$parsed_local"

  info "Building tasks database → ${tasks_db}"
  local db_stats
  if ! db_stats="$(gc_build_tasks_db "$parsed_local" "$tasks_db" "$force")"; then
    die "Failed to build Jira tasks SQLite database"
  fi

  local story_count=0 task_count=0 restored_stories=0 restored_tasks=0
  while IFS= read -r line; do
    case "$line" in
      STORIES\ *) story_count="${line#STORIES }" ;;
      TASKS\ *) task_count="${line#TASKS }" ;;
      RESTORED_STORIES\ *) restored_stories="${line#RESTORED_STORIES }" ;;
      RESTORED_TASKS\ *) restored_tasks="${line#RESTORED_TASKS }" ;;
    esac
  done <<<"$db_stats"

  info "Stories: ${story_count:-0} (restored: ${restored_stories:-0})"
  info "Tasks: ${task_count:-0} (restored statuses: ${restored_tasks:-0})"
  ok "Tasks database updated → ${tasks_db}"

  local legacy_manifest="${tasks_dir}/manifest.json"
  local legacy_stories="${tasks_dir}/stories"
  if [[ -f "$legacy_manifest" || -d "$legacy_stories" ]]; then
    warn "Legacy task manifest/JSON artifacts detected under ${tasks_dir}; they are no longer used now that tasks live in SQLite. Remove them when convenient to avoid confusion."
  fi
}

cmd_migrate_tasks_json() {
  local root="" force=0
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --force) force=1; shift;;
      -h|--help)
        cat <<'EOUSAGE'
Usage: gpt-creator migrate-tasks [--project PATH] [--force]

Populate the tasks SQLite database from the JSON outputs produced by
`gpt-creator create-jira-tasks` (under plan/create-jira-tasks/json).

Options:
  --project PATH  Project root (defaults to current directory)
  --force         Rebuild the database without restoring prior task status metadata
EOUSAGE
        return 0
        ;;
      *) break;;
    esac
  done

  ensure_ctx "$root"

  local pipeline_dir="${PLAN_DIR}/create-jira-tasks"
  local json_dir="${pipeline_dir}/json"
  local epics_json="${json_dir}/epics.json"
  local stories_dir="${json_dir}/stories"
  local tasks_dir="${json_dir}/tasks"

  [[ -f "$epics_json" ]] || die "Epics JSON not found: ${epics_json}"
  [[ -d "$stories_dir" ]] || die "Stories JSON directory not found: ${stories_dir}"
  [[ -d "$tasks_dir" ]] || die "Tasks JSON directory not found: ${tasks_dir}"

  local payload="${json_dir}/tasks_payload.json"
  local tasks_workspace="${PLAN_DIR}/tasks"
  mkdir -p "$tasks_workspace"
  local db_path="${tasks_workspace}/tasks.db"

  info "Building tasks payload from JSON → ${payload}"
  info "Updating tasks database → ${db_path}"

  if ! gc_rebuild_tasks_db_from_json "$force"; then
    die "Failed to rebuild tasks database from JSON payload"
  fi

  ok "Tasks database updated → ${db_path}"
}

cmd_refine_tasks() {
  local root="" story_filter="" model_override="" dry_run=0 force=0
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --story) story_filter="$2"; shift 2;;
      --model) model_override="$2"; shift 2;;
      --dry-run) dry_run=1; shift;;
      --force) force=1; shift;;
      -h|--help)
        cat <<'EOUSAGE'
Usage: gpt-creator refine-tasks [--project PATH] [--story SLUG] [--model NAME] [--dry-run] [--force]

Refine tasks stored in the SQLite backlog using Codex, updating each task
record and the refined JSON artifacts as soon as a task is enriched.

Options:
  --project PATH  Project root (defaults to current directory)
  --story SLUG    Limit refinement to a single story slug (optional)
  --model NAME    Override Codex model (defaults to CODEX_MODEL/GC defaults)
  --dry-run       Build prompts without invoking Codex
  --force         Reset refinement progress and reprocess every task
EOUSAGE
        return 0
        ;;
      *) break;;
    esac
  done

  ensure_ctx "$root"

  local tasks_db="${PLAN_DIR}/tasks/tasks.db"
  [[ -f "$tasks_db" ]] || die "Tasks database not found: ${tasks_db}"

  local pipeline_dir="${PLAN_DIR}/create-jira-tasks"
  local json_tasks_dir="${pipeline_dir}/json/tasks"
  [[ -d "$json_tasks_dir" ]] || die "Tasks JSON directory not found: ${json_tasks_dir}"

  local have_refined
  have_refined=$(python3 - <<'PY' "$tasks_db"
import sqlite3, sys

path = sys.argv[1]
conn = sqlite3.connect(path)
cur = conn.cursor()
cur.execute("PRAGMA table_info(tasks)")
cols = {row[1] for row in cur.fetchall()}
added = False
if "refined" not in cols:
    cur.execute("ALTER TABLE tasks ADD COLUMN refined INTEGER DEFAULT 0")
    added = True
if "refined_at" not in cols:
    cur.execute("ALTER TABLE tasks ADD COLUMN refined_at TEXT")
    added = True
if added:
    conn.commit()
cur.execute("PRAGMA table_info(tasks)")
cols = {row[1] for row in cur.fetchall()}
conn.close()
print("1" if "refined" in cols else "0")
PY
  )

  local summary
  summary=$(python3 - <<'PY' "$tasks_db" "$story_filter"
import sqlite3, sys

db_path = sys.argv[1]
filters_raw = sys.argv[2] if len(sys.argv) > 2 else ''
filters = {value.strip().lower() for value in filters_raw.split(',') if value.strip()}

conn = sqlite3.connect(db_path)
cur = conn.cursor()

cur.execute("PRAGMA table_info(tasks)")
cols = {row[1] for row in cur.fetchall()}
have_refined = 'refined' in cols

story_map = {
    (row[0] or '').strip(): (row[1] or '').strip().lower()
    for row in cur.execute("SELECT story_slug, story_id FROM stories")
}

def story_in_scope(slug: str) -> bool:
    lower = slug.lower()
    if not filters:
        return True
    if lower in filters:
        return True
    story_id_lower = story_map.get(slug, '')
    if story_id_lower in filters:
        return True
    return False

if have_refined:
    rows = cur.execute("SELECT story_slug, COALESCE(refined, 0) FROM tasks").fetchall()
else:
    rows = [(slug, 0) for (slug,) in cur.execute("SELECT story_slug FROM tasks").fetchall()]
conn.close()

total_tasks = 0
refined_tasks = 0
stories_total = set()
stories_pending = set()

for slug, refined in rows:
    slug = (slug or '').strip()
    if not slug or not story_in_scope(slug):
        continue
    stories_total.add(slug)
    total_tasks += 1
    try:
        refined_value = int(refined)
    except Exception:
        refined_value = 0
    if refined_value:
        refined_tasks += 1
    else:
        stories_pending.add(slug)

pending_tasks = total_tasks - refined_tasks
print(total_tasks, refined_tasks, pending_tasks, len(stories_total), len(stories_pending))
PY
  ) || die "Failed to summarise tasks backlog"

  local total_tasks refined_tasks pending_tasks total_stories pending_stories
  read -r total_tasks refined_tasks pending_tasks total_stories pending_stories <<<"$summary"

  if (( force )); then
    python3 - <<'PY' "$tasks_db"
import sqlite3, sys
conn = sqlite3.connect(sys.argv[1])
cur = conn.cursor()
cur.execute("UPDATE tasks SET refined = 0, refined_at = NULL")
conn.commit()
conn.close()
PY
    refined_tasks=0
    pending_tasks=$total_tasks
  fi

  info "Backlog summary → tasks: total=${total_tasks}, refined=${refined_tasks}, pending=${pending_tasks}; stories: total=${total_stories}, pending=${pending_stories}${story_filter:+ (filter='${story_filter}')}."

  local codex_cmd="${CODEX_BIN:-${CODEX_CMD:-codex}}"
  if (( dry_run == 0 )) && ! command -v "$codex_cmd" >/dev/null 2>&1; then
    warn "Codex CLI '$codex_cmd' not found; switching to --dry-run."
    dry_run=1
  fi

  local model_name="${model_override:-${CODEX_MODEL:-$GC_DEFAULT_MODEL}}"

  # shellcheck source=src/lib/create-jira-tasks/pipeline.sh
  source "${CLI_ROOT}/src/lib/create-jira-tasks/pipeline.sh"

  local force_flag=0
  local skip_refine=0
  local dry_flag="$dry_run"
  cjt::init "$PROJECT_ROOT" "$model_name" "$force_flag" "$skip_refine" "$dry_flag"
  CJT_DOC_FILES=()
  cjt::build_context_files

  CJT_SYNC_DB=1
  CJT_TASKS_DB_PATH="$tasks_db"
  CJT_IGNORE_REFINE_STATE=1
  CJT_REFINE_FORCE=$force
  CJT_HAVE_REFINED_COLUMN="$have_refined"
  CJT_REFINE_TOTAL_TASKS="$total_tasks"
  CJT_REFINE_REFINED_TASKS="$refined_tasks"
  CJT_REFINE_PENDING_TASKS="$pending_tasks"
  CJT_REFINE_TOTAL_STORIES="$total_stories"
  CJT_REFINE_PENDING_STORIES="$pending_stories"
  if [[ -n "$story_filter" ]]; then
    CJT_ONLY_STORY_SLUG="$story_filter"
  fi

  cjt::refine_tasks
  ok "Task refinement complete"
}

gc_write_task_prompt() {
  local db_path="${1:?tasks db path required}"
  local story_slug="${2:?story slug required}"
  local task_index="${3:?task index required}"
  local prompt_path="${4:?prompt path required}"
  local context_tail="${5:-}"
  local model_name="${6:-$CODEX_MODEL}"
  local project_root="${7:-$PROJECT_ROOT}"
  python3 - "$db_path" "$story_slug" "$task_index" "$prompt_path" "$context_tail" "$model_name" "$project_root" <<'PY'
import json
import re
import sqlite3
import sys
from pathlib import Path

DB_PATH, STORY_SLUG, TASK_INDEX, PROMPT_PATH, CONTEXT_TAIL_PATH, MODEL_NAME, PROJECT_ROOT = sys.argv[1:8]
TASK_INDEX = int(TASK_INDEX)

conn = sqlite3.connect(DB_PATH)
conn.row_factory = sqlite3.Row
cur = conn.cursor()

story_row = cur.execute(
    'SELECT story_id, story_title, epic_key, epic_title, sequence FROM stories WHERE story_slug = ?',
    (STORY_SLUG,)
).fetchone()
if story_row is None:
    raise SystemExit(f"Story slug not found: {STORY_SLUG}")

task_rows = cur.execute(
    'SELECT task_id, title, description, estimate, assignees_json, tags_json, acceptance_json, dependencies_json, '
    'tags_text, story_points, dependencies_text, assignee_text, document_reference, idempotency, rate_limits, rbac, '
    'messaging_workflows, performance_targets, observability, acceptance_text, endpoints, sample_create_request, '
    'sample_create_response, user_story_ref_id, epic_ref_id '
    'FROM tasks WHERE story_slug = ? ORDER BY position ASC',
    (STORY_SLUG,)
).fetchall()
conn.close()

if TASK_INDEX < 0 or TASK_INDEX >= len(task_rows):
    raise SystemExit(2)

task = task_rows[TASK_INDEX]

def parse_json_list(value):
    if not value:
        return []
    try:
        parsed = json.loads(value)
        if isinstance(parsed, list):
            return [str(item).strip() for item in parsed if str(item).strip()]
    except Exception:
        pass
    return []

def clean(value: str) -> str:
    return (value or '').strip()


def project_display_name(root: str) -> str:
    if not root:
        return "Project"
    try:
        name = Path(root).name.strip()
    except Exception:
        name = ""
    if not name:
        return "Project"
    tokens = [token for token in re.split(r'[^A-Za-z0-9]+', name) if token]
    if not tokens:
        return "Project"
    words = []
    for token in tokens:
        if len(token) <= 3:
            words.append(token.upper())
        elif token.isupper():
            words.append(token)
        else:
            words.append(token.capitalize())
    return ' '.join(words) or "Project"

assignees = parse_json_list(task['assignees_json'])
tags = parse_json_list(task['tags_json'])
acceptance = parse_json_list(task['acceptance_json'])
dependencies = parse_json_list(task['dependencies_json'])

description = clean(task['description'])
if description:
    description_lines = description.splitlines()
else:
    description_lines = []

tags_text = clean(task['tags_text'])
story_points = clean(task['story_points'])
dependencies_text = clean(task['dependencies_text'])
assignee_text = clean(task['assignee_text'])
document_reference = clean(task['document_reference'])
idempotency_text = clean(task['idempotency'])
rate_limits = clean(task['rate_limits'])
rbac_text = clean(task['rbac'])
messaging_workflows = clean(task['messaging_workflows'])
performance_targets = clean(task['performance_targets'])
observability_text = clean(task['observability'])
acceptance_text_extra = (task['acceptance_text'] or '').strip() if task['acceptance_text'] else ''
endpoints_text = (task['endpoints'] or '').strip() if task['endpoints'] else ''
sample_create_request = (task['sample_create_request'] or '').strip() if task['sample_create_request'] else ''
sample_create_response = (task['sample_create_response'] or '').strip() if task['sample_create_response'] else ''
user_story_ref_id = clean(task['user_story_ref_id'])
epic_ref_id = clean(task['epic_ref_id'])

project_display = project_display_name(PROJECT_ROOT)
repo_path = PROJECT_ROOT or '.'

lines = []
lines.append(f"# You are Codex (model: {MODEL_NAME})")
lines.append("")
lines.append(f"You are assisting the {project_display} delivery team. Implement the task precisely using the repository at: {repo_path}")
lines.append("")
lines.append("## Story")

epic_id = clean(story_row['epic_key'])
epic_title = clean(story_row['epic_title'])
if epic_id or epic_title:
    parts = [part for part in [epic_id, epic_title] if part]
    lines.append("- Epic: " + " — ".join(parts))

story_id = clean(story_row['story_id'])
story_title = clean(story_row['story_title'])
if story_id or story_title:
    parts = [part for part in [story_id, story_title] if part]
    lines.append("- Story: " + " — ".join(parts))
sequence = story_row['sequence']
if sequence:
    lines.append(f"- Story order: {sequence}")

lines.append("")
lines.append("## Task")
task_id = clean(task['task_id'])
task_title = clean(task['title'])
if task_id:
    lines.append(f"- Task ID: {task_id}")
if task_title:
    lines.append(f"- Title: {task_title}")
estimate = clean(task['estimate'])
if estimate:
    lines.append(f"- Estimate: {estimate}")
if assignees:
    lines.append("- Assignees: " + ", ".join(assignees))
elif assignee_text:
    lines.append(f"- Assignee: {assignee_text}")
if tags:
    lines.append("- Tags: " + ", ".join(tags))
elif tags_text:
    lines.append(f"- Tags: {tags_text}")
if story_points and story_points != estimate:
    lines.append(f"- Story points: {story_points}")
elif story_points and not estimate:
    lines.append(f"- Story points: {story_points}")
if document_reference:
    lines.append(f"- Document reference: {document_reference}")
if idempotency_text:
    lines.append(f"- Idempotency: {idempotency_text}")
if rate_limits:
    lines.append(f"- Rate limits: {rate_limits}")
if rbac_text:
    lines.append(f"- RBAC: {rbac_text}")
if messaging_workflows:
    lines.append(f"- Messaging & workflows: {messaging_workflows}")
if performance_targets:
    lines.append(f"- Performance targets: {performance_targets}")
if observability_text:
    lines.append(f"- Observability: {observability_text}")
if user_story_ref_id and user_story_ref_id.lower() != story_id.lower():
    lines.append(f"- User story reference ID: {user_story_ref_id}")
if epic_ref_id and epic_ref_id.lower() != epic_id.lower():
    lines.append(f"- Epic reference ID: {epic_ref_id}")

lines.append("")
lines.append("### Description")
if description_lines:
    lines.extend(description_lines)
else:
    lines.append("(No additional description provided.)")

if acceptance:
    lines.append("")
    lines.append("### Acceptance Criteria")
    for item in acceptance:
        lines.append(f"- {item}")
elif acceptance_text_extra:
    lines.append("")
    lines.append("### Acceptance Criteria")
    lines.extend(acceptance_text_extra.splitlines())

if dependencies:
    lines.append("")
    lines.append("### Dependencies")
    for dep in dependencies:
        lines.append(f"- {dep}")
elif dependencies_text:
    lines.append("")
    lines.append("### Dependencies")
    lines.extend(dependencies_text.splitlines())

if endpoints_text:
    lines.append("")
    lines.append("### Endpoints")
    lines.extend(endpoints_text.splitlines())

if sample_create_request:
    lines.append("")
    lines.append("### Sample Create Request")
    lines.extend(sample_create_request.splitlines())

if sample_create_response:
    lines.append("")
    lines.append("### Sample Create Response")
    lines.extend(sample_create_response.splitlines())

lines.append("")
lines.append("## Instructions")
lines.append("- Draft a short plan before modifying files.")
lines.append("- Apply changes directly in the repository; commits are not required.")
lines.append("- Respond with structured JSON described below — no markdown fencing or prose outside the JSON.")
lines.append("- Verify acceptance criteria before finishing.")
lines.append("- If blocked, explain why and suggest next steps inside the JSON response.")
lines.append("- Prefer pnpm for install/build scripts; avoid npm/yarn unless explicitly required.")
lines.append("- Assume limited network access; note any commands that cannot run for that reason instead of failing silently.")

lines.append("")
lines.append("## Output JSON schema")
lines.append("Return a single JSON object with keys exactly as follows (omit null/empty collections when not needed):")
lines.append("{")
lines.append("  \"plan\": [\"short step-by-step plan items...\"],")
lines.append("  \"changes\": [")
lines.append("    { \"type\": \"patch\", \"path\": \"relative/file/path\", \"diff\": \"UNIFIED_DIFF\" },")
lines.append("    { \"type\": \"file\", \"path\": \"relative/file/path\", \"content\": \"entire file content\" }")
lines.append("  ],")
lines.append("  \"commands\": [\"optional shell commands to run (e.g., pnpm install)\"],")
lines.append("  \"notes\": [\"follow-up items or blockers\"]")
lines.append("}")
lines.append("- Use UTF-8, escape newlines as \\n inside JSON strings.")
lines.append("- Diff entries must be valid unified diffs (git apply compatible) against the current workspace.")
lines.append("- File entries provide the complete desired file content (for new or fully rewritten files).")
lines.append("- Do not emit markdown fences, commentary, or additional text outside the JSON object.")

if CONTEXT_TAIL_PATH:
    context_path = Path(CONTEXT_TAIL_PATH)
    if context_path.exists():
        tail_text = context_path.read_text(encoding='utf-8').splitlines()
        lines.append("")
        lines.append("## Shared Context (truncated)")
        lines.append("")
        lines.extend(tail_text)

prompt_path = Path(PROMPT_PATH)
prompt_path.parent.mkdir(parents=True, exist_ok=True)
prompt_path.write_text("\n".join(lines) + "\n", encoding='utf-8')

print(f"{task_id}\t{task_title}")
PY
}

cmd_work_on_tasks() {
  local root="" resume=1 story_filter="" no_verify=0 keep_artifacts=0 memory_cycle=0
  local batch_size=0 sleep_between=0
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --story|--from-story) story_filter="$2"; shift 2;;
      --fresh) resume=0; shift;;
      --no-verify) no_verify=1; shift;;
      --keep-artifacts) keep_artifacts=1; shift;;
      --memory-cycle) memory_cycle=1; shift;;
      --batch-size) batch_size="${2:-0}"; shift 2;;
      --sleep-between) sleep_between="${2:-0}"; shift 2;;
      -h|--help)
        cat <<'EOHELP'
Usage: gpt-creator work-on-tasks [options]

Execute tasks from the project SQLite backlog using Codex, with resumable progress.

Options:
  --project PATH       Project root (defaults to current directory)
  --story ID|SLUG      Start from the matching story id or slug (inclusive)
  --fresh              Ignore saved progress and start from the first story
  --no-verify          Skip running gpt-creator verify after tasks complete
  --keep-artifacts     Retain Codex prompt/output files for each task (default cleans up)
  --memory-cycle       Process one task at a time, prune caches, and auto-resume to limit memory usage
  --batch-size NUM     Process at most NUM tasks this run, then pause (default: unlimited)
  --sleep-between SEC  Sleep SEC seconds between tasks (default: 0)
EOHELP
        return 0
        ;;
      *) break;;
    esac
  done

  [[ "$batch_size" =~ ^[0-9]+$ ]] || die "Invalid --batch-size value: ${batch_size}"
  [[ "$sleep_between" =~ ^[0-9]+$ ]] || die "Invalid --sleep-between value: ${sleep_between}"
  batch_size=$((batch_size))
  sleep_between=$((sleep_between))

  if (( memory_cycle )); then
    if (( batch_size == 0 || batch_size > 1 )); then
      info "Memory-cycle enabled; forcing --batch-size 1 for iterative runs."
    fi
    batch_size=1
  fi

  ensure_ctx "$root"
  local tasks_dir="${PLAN_DIR}/tasks"
  local tasks_db="${tasks_dir}/tasks.db"
  mkdir -p "$tasks_dir"

  if ! gc_tasks_db_has_rows "$tasks_db"; then
    if gc_has_legacy_tasks_json; then
      info "Tasks database missing or empty; importing legacy JSON backlog into ${tasks_db}."
      if gc_rebuild_tasks_db_from_json 0; then
        ok "Legacy backlog migrated to tasks.db"
      else
        warn "Failed to migrate legacy JSON backlog into tasks.db"
      fi
    fi
  fi

  if ! gc_tasks_db_has_rows "$tasks_db"; then
    die "Task database missing or empty. Run 'gpt-creator create-tasks' or 'gpt-creator migrate-tasks' first."
  fi

  ensure_node_dependencies "$PROJECT_ROOT"

  local state_dir="${PLAN_DIR}/work"
  local runs_dir="${state_dir}/runs"
  mkdir -p "$runs_dir"

  local run_stamp="$(date +%Y%m%d_%H%M%S)"
  local run_dir="${runs_dir}/${run_stamp}"
  mkdir -p "$run_dir"
  local ctx_file="${run_dir}/context.md"
  gc_build_context_file "$ctx_file" "$STAGING_DIR"
  local context_tail="${run_dir}/context_tail.md"
  if ! tail -n 400 "$ctx_file" >"$context_tail" 2>/dev/null; then
    cp "$ctx_file" "$context_tail"
  fi

  info "Work run directory → ${run_dir}"

  local resume_flag=1
  [[ $resume -eq 1 ]] || resume_flag=0

  local work_failed=0
  local processed_any=0
  local processed_total=0
  local batch_limit_reached=0
  local any_changes=0
  local manual_followups=0

  while IFS=$'\t' read -r sequence slug story_id story_title epic_id epic_title total_tasks next_task completed status; do
    if [[ -z "${sequence}${slug}${story_id}${story_title}${epic_id}${epic_title}" ]]; then
      continue
    fi

    processed_any=1

    local total_tasks_int=0
    [[ -n "$total_tasks" ]] && total_tasks_int=$((total_tasks))
    local next_task_int=0
    [[ -n "$next_task" ]] && next_task_int=$((next_task))

    printf -v story_prefix "%03d" "${sequence:-0}"
    [[ -n "$slug" ]] || slug="story-${story_prefix}"
    local story_run_dir="${run_dir}/story_${story_prefix}_${slug}"
    mkdir -p "${story_run_dir}/prompts" "${story_run_dir}/out"

    info "Story ${story_prefix} (${story_id:-$slug}) — ${story_title:-Unnamed}"

    if (( total_tasks_int == 0 )); then
      info "  No tasks for this story; marking complete."
      gc_update_work_state "$tasks_db" "$slug" "complete" 0 0 "$run_stamp"
      if (( keep_artifacts == 0 )); then
        rmdir "${story_run_dir}/prompts" 2>/dev/null || true
        rmdir "${story_run_dir}/out" 2>/dev/null || true
      fi
      continue
    fi

    gc_update_work_state "$tasks_db" "$slug" "in-progress" "$next_task_int" "$total_tasks_int" "$run_stamp"

    local task_index
    local story_failed=0
    for (( task_index = next_task_int; task_index < total_tasks_int; task_index++ )); do
      if (( batch_size > 0 && processed_total >= batch_size )); then
        batch_limit_reached=1
        break
      fi
      local task_number
      printf -v task_number "%03d" $((task_index + 1))
      local prompt_path="${story_run_dir}/prompts/task_${task_number}.prompt.md"
      local output_path="${story_run_dir}/out/task_${task_number}.out.md"

      local prompt_meta
      if ! prompt_meta="$(gc_write_task_prompt "$tasks_db" "$slug" "$task_index" "$prompt_path" "$context_tail" "$CODEX_MODEL" "$PROJECT_ROOT")"; then
        warn "  Failed to build prompt for task index ${task_index}"
        work_failed=1
        break
      fi

      local task_id="" task_title=""
      IFS=$'\t' read -r task_id task_title <<<"$prompt_meta"
      info "  Task ${task_number}: ${task_id:-no-id} — ${task_title:-(untitled)}"

      local call_name="story-${slug}-task-${task_number}"
      local codex_ok=0
      local attempt=0
      local max_attempts=2
      local prompt_augmented=0
      local keep_output=$keep_artifacts
      local parse_error_final=0
      local break_after_update=0
      local updated_state_for_task=0
      gc_update_task_state "$tasks_db" "$slug" "$task_index" "in-progress" "$run_stamp"
      while (( attempt < max_attempts )); do
        (( ++attempt ))
        if codex_call "$call_name" --prompt "$prompt_path" --output "$output_path"; then
          if [[ ! -s "$output_path" ]]; then
            warn "  Codex produced no output for ${call_name}; manual review required."
            manual_followups=1
            keep_output=1
            gc_update_work_state "$tasks_db" "$slug" "in-progress" $((task_index + 1)) "$total_tasks_int" "$run_stamp"
            updated_state_for_task=1
            codex_ok=1
            break
          fi
          local apply_output
          if ! apply_output="$(gc_apply_codex_changes "$output_path" "$PROJECT_ROOT")"; then
            warn "  Failed to apply changes for ${call_name}; manual review required (see ${output_path})."
            manual_followups=1
            keep_output=1
            gc_update_work_state "$tasks_db" "$slug" "in-progress" $((task_index + 1)) "$total_tasks_int" "$run_stamp"
            updated_state_for_task=1
            codex_ok=1
            break
          fi
          if [[ "$apply_output" == "no-output" || "$apply_output" == "empty-output" ]]; then
            warn "  Codex produced no actionable JSON for ${call_name}; manual review required."
            manual_followups=1
            keep_output=1
            gc_update_work_state "$tasks_db" "$slug" "in-progress" $((task_index + 1)) "$total_tasks_int" "$run_stamp"
            updated_state_for_task=1
            codex_ok=1
            break
          fi
          local apply_status="ok"
          while IFS= read -r change_line; do
            case "$change_line" in
              STATUS\ *) apply_status="${change_line#STATUS }" ;;
              APPLIED) info "    Changes applied." ;;
              WRITE\ *) info "    Wrote ${change_line#WRITE }"; any_changes=1 ;;
              PATCH\ *) info "    Patched ${change_line#PATCH }"; any_changes=1 ;;
              CMD\ *) info "    Suggested command: ${change_line#CMD }" ;;
              NOTE\ *) warn "    Note: ${change_line#NOTE }" ;;
            esac
          done <<<"$apply_output"

          if [[ "$apply_status" == "parse-error" ]]; then
            if (( attempt < max_attempts )); then
              warn "  Codex returned invalid JSON; retrying (attempt $((attempt + 1)) of ${max_attempts})."
              if (( prompt_augmented == 0 )); then
                cat >>"$prompt_path" <<'REM'

## Reminder
- Output a single JSON object exactly as described above; do not include any explanatory text outside the JSON.
- If no changes are required, return the JSON with an empty `changes` array and clear notes explaining why.
- Diff entries must remain valid unified diffs.
REM
                prompt_augmented=1
              fi
              continue
            else
              warn "  Codex output was invalid JSON after retry; manual review required (see ${output_path})."
              manual_followups=1
              keep_output=1
              gc_update_work_state "$tasks_db" "$slug" "in-progress" $((task_index + 1)) "$total_tasks_int" "$run_stamp"
              updated_state_for_task=1
              codex_ok=1
              parse_error_final=1
              break_after_update=1
            fi
          fi

          if (( updated_state_for_task == 0 )); then
            gc_update_work_state "$tasks_db" "$slug" "in-progress" $((task_index + 1)) "$total_tasks_int" "$run_stamp"
            updated_state_for_task=1
          fi
          if (( keep_output == 0 )); then
            rm -f "$prompt_path" "$output_path"
          fi
          codex_ok=1
        else
          warn "  Codex execution failed for ${call_name}; progress saved."
          manual_followups=1
          keep_output=1
          gc_update_work_state "$tasks_db" "$slug" "in-progress" $((task_index + 1)) "$total_tasks_int" "$run_stamp"
          updated_state_for_task=1
          codex_ok=1
          break
        fi
        break
      done

      if (( codex_ok == 0 )); then
        gc_update_task_state "$tasks_db" "$slug" "$task_index" "blocked" "$run_stamp"
        story_failed=1
        break
      fi

      gc_update_task_state "$tasks_db" "$slug" "$task_index" "complete" "$run_stamp"
      (( ++processed_total ))

      if (( break_after_update )); then
        continue
      fi

      if (( sleep_between > 0 )); then
        sleep "$sleep_between"
      fi

    done

    if (( batch_limit_reached )); then
      break
    fi

    if (( story_failed )); then
      warn "Stopping at story ${slug} due to previous error."
      break
    fi

    gc_update_work_state "$tasks_db" "$slug" "complete" "$total_tasks_int" "$total_tasks_int" "$run_stamp"
    if (( keep_artifacts == 0 )); then
      rmdir "${story_run_dir}/prompts" 2>/dev/null || true
      rmdir "${story_run_dir}/out" 2>/dev/null || true
    fi
  done < <(
    python3 - "$tasks_db" "${story_filter}" "$resume_flag" <<'PY'
import sqlite3
import sys

DB_PATH = sys.argv[1]
story_filter = (sys.argv[2] or '').strip().lower()
resume_flag = sys.argv[3] == "1"

conn = sqlite3.connect(DB_PATH)
conn.row_factory = sqlite3.Row
cur = conn.cursor()

stories = cur.execute('SELECT story_slug, story_id, story_title, epic_key, epic_title, sequence, status FROM stories ORDER BY sequence ASC, story_slug ASC').fetchall()

def norm(value):
    return (value or "").strip().lower()

start_allowed = not story_filter

for story in stories:
    slug = story["story_slug"] or ""
    sequence = story["sequence"] or 0
    story_id = (story["story_id"] or "").strip()
    epic_key = (story["epic_key"] or "").strip()
    epic_title = (story["epic_title"] or "").strip()
    story_title = (story["story_title"] or "").strip()

    story_title_clean = story_title.replace('\t', ' ').replace('\n', ' ')
    epic_title_clean = epic_title.replace('\t', ' ').replace('\n', ' ')
    epic_key_clean = epic_key.replace('\t', ' ').replace('\n', ' ')
    story_id_clean = story_id.replace('\t', ' ').replace('\n', ' ')

    if story_filter and not start_allowed:
        keys = {norm(story_id), norm(slug), norm(epic_key), norm(str(sequence))}
        if story_filter in keys:
            start_allowed = True
        else:
            continue

    task_rows = cur.execute('SELECT position, status FROM tasks WHERE story_slug = ? ORDER BY position ASC', (slug,)).fetchall()
    total = len(task_rows)
    completed = 0
    next_index = 0
    for row in task_rows:
        status = (row["status"] or "").strip().lower()
        if status == "complete":
            completed += 1
            continue
        next_index = row["position"] or 0
        break
    else:
        next_index = total

    current_status = (story["status"] or "").strip()

    if resume_flag and not story_filter and current_status.lower() == "complete":
        continue

    if resume_flag:
        if total == 0:
            next_index = 0
        elif completed >= total:
            if story_filter:
                next_index = total
            else:
                continue
    else:
        next_index = 0 if total > 0 else 0

    print("	".join([
        str(sequence),
        slug,
        story_id_clean,
        story_title_clean,
        epic_key_clean,
        epic_title_clean,
        str(total),
        str(next_index),
        str(completed),
        current_status,
    ]))

conn.close()
PY
  )

  if (( processed_any == 0 )); then
    info "No stories to process (already complete)."
    return 0
  fi

  local pending_tasks=0
  if (( memory_cycle )); then
    pending_tasks="$(gc_count_pending_tasks "$tasks_db" || echo 0)"
    [[ "$pending_tasks" =~ ^[0-9]+$ ]] || pending_tasks=0

    if (( work_failed == 0 )); then
      if (( processed_total > 0 )) && (( pending_tasks > 0 )); then
        gc_trim_memory "memory-cycle"
        info "Memory-cycle paused after ${processed_total} task(s); ${pending_tasks} pending."
        local -a restart_cmd=("$0" work-on-tasks)
        if [[ -n "$PROJECT_ROOT" ]]; then
          restart_cmd+=(--project "$PROJECT_ROOT")
        fi
        if [[ -n "$story_filter" ]]; then
          restart_cmd+=(--story "$story_filter")
        fi
        if (( no_verify )); then
          restart_cmd+=(--no-verify)
        fi
        if (( keep_artifacts )); then
          restart_cmd+=(--keep-artifacts)
        fi
        if (( sleep_between > 0 )); then
          restart_cmd+=(--sleep-between "$sleep_between")
        fi
        restart_cmd+=(--batch-size 1 --memory-cycle)
        info "Restarting work-on-tasks to continue processing."
        exec "${restart_cmd[@]}"
      elif (( pending_tasks == 0 )); then
        gc_trim_memory "memory-cycle-final"
        batch_limit_reached=0
      else
        gc_trim_memory "memory-cycle"
      fi
    else
      gc_trim_memory "memory-cycle-error"
    fi
  fi

  local remaining_tasks=0
  if (( work_failed == 0 )); then
    remaining_tasks="$(gc_count_pending_tasks "$tasks_db" || echo 0)"
    [[ "$remaining_tasks" =~ ^[0-9]+$ ]] || remaining_tasks=0
  fi

  if (( work_failed == 0 && memory_cycle == 0 && batch_limit_reached == 0 && batch_size == 0 && processed_total > 0 && remaining_tasks > 0 )); then
    if [[ -n "$story_filter" ]]; then
      info "Remaining tasks detected beyond filtered story; rerun with a broader filter to continue."
    else
      info "${remaining_tasks} task(s) remain; continuing work-on-tasks automatically."
      local -a restart_cmd=("$0" work-on-tasks)
      if [[ -n "$PROJECT_ROOT" ]]; then
        restart_cmd+=(--project "$PROJECT_ROOT")
      fi
      if (( no_verify )); then
        restart_cmd+=(--no-verify)
      fi
      if (( keep_artifacts )); then
        restart_cmd+=(--keep-artifacts)
      fi
      if (( sleep_between > 0 )); then
        restart_cmd+=(--sleep-between "$sleep_between")
      fi
      info "Restarting work-on-tasks to process remaining backlog."
      exec "${restart_cmd[@]}"
    fi
  fi

  if [[ $work_failed -eq 0 && $batch_limit_reached -eq 0 && $no_verify -eq 0 && $remaining_tasks -eq 0 ]]; then
    if (( any_changes == 0 )); then
      info "No repository changes detected; skipping verify."
    else
      info "Re-running verify after work run"
      if ! cmd_verify all --project "$PROJECT_ROOT"; then
        warn "Verify command reported failures."
        work_failed=1
      fi
    fi
  fi

  if (( batch_limit_reached )); then
    info "Batch size limit hit after ${processed_total} task(s); rerun to continue from the next pending task."
  fi

  if [[ $work_failed -eq 0 ]]; then
    if (( batch_limit_reached )); then
      ok "work-on-tasks paused → ${run_dir}"
    else
      ok "work-on-tasks complete → ${run_dir}"
    fi
    if (( manual_followups )); then
      warn "Manual review needed for some tasks — see notes above and preserved output artifacts."
    fi
  else
    warn "work-on-tasks completed with issues — inspect ${run_dir}"
    return 1
  fi
}


cmd_iterate() {
  warn "'iterate' is deprecated; prefer 'gpt-creator create-tasks' followed by 'gpt-creator work-on-tasks'."

  local root="" jira="" reverify=1
  while [[ $# -gt 0 ]]; do
    case "$1" in
      --project) root="$(abs_path "$2")"; shift 2;;
      --jira) jira="$(abs_path "$2")"; shift 2;;
      --no-verify) reverify=0; shift;;
      *) break;;
    esac
  done
  ensure_ctx "$root"
  [[ -n "$jira" ]] || jira="${INPUT_DIR}/jira.md"
  [[ -f "$jira" ]] || die "Jira tasks file not found: ${jira}"

  local tasks_json_local="${PLAN_DIR}/jira-tasks.local.json"
  gc_parse_jira_tasks "$jira" "$tasks_json_local"
  ok "Parsed Jira tasks → ${tasks_json_local}"
  local tasks_json="${tasks_json_local}"

  local codex_parse_prompt="${PLAN_DIR}/iterate-codex-parse.md"
  local codex_raw_json="${PLAN_DIR}/jira-tasks.codex.raw.txt"
  local codex_json="${PLAN_DIR}/jira-tasks.codex.json"
  {
    cat >"$codex_parse_prompt" <<'PROMPT'
# Instruction
You are a structured-data assistant. Convert the following Jira backlog markdown into strict JSON.

## Requirements
- Output **only** valid JSON (no prose, no code fences).
- Structure: { "tasks": [ { "epic_id": str, "epic_title": str, "story_id": str, "story_title": str, "id": str, "title": str, "assignees": [str], "tags": [str], "estimate": str, "description": str, "acceptance_criteria": [str], "dependencies": [str] } ] }.
- Each task begins with a bold identifier such as **T18.5.2**; treat every such block as a separate task and capture its parent story/epic when present.
- Preserve bullet details verbatim inside the description and acceptance criteria lists. Do not repeat metadata (assignee/tags/estimate) inside the description.
- Use empty strings/arrays when information is missing.
- Do not include explanatory text.

## Jira Markdown
PROMPT
    cat "$jira" >>"$codex_parse_prompt"
    cat >>"$codex_parse_prompt" <<'PROMPT'
## End Markdown
PROMPT
  }

  if codex_call "iterate-parse" --prompt "$codex_parse_prompt" --output "$codex_raw_json"; then
    if python3 - <<'PY' "$codex_raw_json" "$codex_json"
import json, pathlib, re, sys
raw_path, out_path = sys.argv[1:3]
text = pathlib.Path(raw_path).read_text().strip()
if not text:
    raise SystemExit(1)
if text.startswith('```'):
    text = re.sub(r'^```[a-zA-Z0-9_-]*\s*', '', text)
    text = re.sub(r'```\s*$', '', text)
data = json.loads(text)
if isinstance(data, list):
    data = {'tasks': data}
elif 'tasks' not in data:
    data = {'tasks': [data]}
pathlib.Path(out_path).write_text(json.dumps(data, indent=2) + '\n')
PY
      then
      ok "Codex parsed Jira tasks → ${codex_json}"
      tasks_json="$codex_json"
    else
      warn "Codex JSON output invalid; falling back to local parser results."
    fi
  else
    warn "Codex parsing step failed; using local parser output."
  fi

  local iterate_dir="${PLAN_DIR}/iterate"
  mkdir -p "$iterate_dir"
  local order_file="${iterate_dir}/tasks-order.txt"

  python3 - <<'PY' "$tasks_json" "$iterate_dir" "$PROJECT_ROOT"
import json, pathlib, sys
source, out_dir, project_root = sys.argv[1:4]
tasks = json.load(open(source)).get('tasks', [])
out = pathlib.Path(out_dir)
out.mkdir(parents=True, exist_ok=True)
index_path = out / 'tasks-order.txt'
with index_path.open('w') as idx:
    for i, task in enumerate(tasks, 1):
        title = (task.get('title') or '').strip() or f'Task {i}'
        task_id = (task.get('id') or '').strip()
        description = (task.get('description') or '').strip() or '(No additional details provided.)'
        estimate = (task.get('estimate') or '').strip()
        tags = ', '.join(task.get('tags') or [])
        assignees = ', '.join(task.get('assignees') or [])
        story_bits = [part for part in [(task.get('story_id') or '').strip(), (task.get('story_title') or '').strip()] if part]
        prompt_path = out / f'task-{i:02d}.md'
        idx.write(str(prompt_path) + '\n')
        lines = []
        if task_id and not title.startswith(task_id):
            lines.append(f"# Task {i}: {task_id} — {title}")
        else:
            lines.append(f"# Task {i}: {title}")
        lines.append('')
        lines.append('## Context')
        lines.append(f'- Working directory: {project_root}')
        if task_id:
            lines.append(f'- Task ID: {task_id}')
        if story_bits:
            lines.append(f"- Story: {' — '.join(story_bits)}")
        if assignees:
            lines.append(f'- Assignees: {assignees}')
        if estimate:
            lines.append(f'- Estimate: {estimate}')
        if tags:
            lines.append(f'- Tags: {tags}')
        lines.append('')
        lines.append('## Description')
        lines.append(description or '(No additional details provided.)')
        lines.append('')
        if task.get('acceptance_criteria'):
            lines.append('## Acceptance Criteria')
            for ac in task['acceptance_criteria']:
                lines.append(f'- {ac}')
            lines.append('')
        if task.get('dependencies'):
            lines.append('## Dependencies')
            for dep in task['dependencies']:
                lines.append(f'- {dep}')
            lines.append('')
        lines.append('')
        lines.append('## Instructions')
        lines.append('- Outline your plan before modifying files.')
        lines.append('- Implement the task in the repository; commits are not required.')
        lines.append('- Show relevant diffs (git snippets) and command results.')
        lines.append('- Verify acceptance criteria for this task.')
        lines.append('- If blocked, explain why and propose next steps.')
        lines.append('')
        lines.append('## Output Format')
        lines.append('- Begin with a heading `Task {i}`.')
        lines.append('- Summarise changes, tests, and outstanding follow-ups.')
        prompt_path.write_text('\n'.join(lines) + '\n')
PY

  if [[ -s "$order_file" ]]; then
    while IFS= read -r prompt_path; do
      [[ -z "$prompt_path" ]] && continue
      local base_name="$(basename "$prompt_path" .md)"
      local output_path="${prompt_path%.md}.output.md"
      info "Running Codex for ${base_name}"
      codex_call "$base_name" --prompt "$prompt_path" --output "$output_path" || warn "Codex task ${base_name} returned non-zero"
    done < "$order_file"
  else
    warn "No Jira tasks to process after parsing."
  fi

  local summary_prompt="${iterate_dir}/summary.md"
  local summary_output="${iterate_dir}/summary.output.md"
  python3 - <<'PY' "$tasks_json" "$order_file" "$summary_prompt"
import json, pathlib, sys
tasks = json.load(open(sys.argv[1])).get('tasks', [])
order_file = pathlib.Path(sys.argv[2])
prompt_path = pathlib.Path(sys.argv[3])
lines = ['# Summary Request', '', 'Summarise the completed Jira work and list follow-up actions.']
lines.append('')
lines.append('## Task Reports')
if order_file.exists():
    for i, prompt in enumerate(order_file.read_text().splitlines(), 1):
        if not prompt:
            continue
        title = tasks[i-1].get('title') if i-1 < len(tasks) else f'Task {i}'
        out_path = pathlib.Path(prompt).with_suffix('.output.md')
        lines.append(f'- Task {i}: {title}')
        if out_path.exists():
            content = out_path.read_text().strip()
            if content:
                snippet = content[:2000]
                lines.append('  ```')
                lines.append(snippet)
                lines.append('  ```')
        else:
            lines.append('  (No output captured)')
else:
    lines.append('- No outputs available.')
lines.append('')
lines.append('## Output Requirements')
lines.append('- Provide an overall summary of work completed.')
lines.append('- List follow-up items or blockers.')
lines.append('- Use markdown headings and bullet lists.')
prompt_path.write_text('\n'.join(lines) + '\n')
PY

  codex_call "iterate-summary" --prompt "$summary_prompt" --output "$summary_output" || warn "Codex summary step returned non-zero"

  if [[ "$reverify" -eq 1 ]]; then
    info "Re-running verify after iteration"
    cmd_verify all --project "$PROJECT_ROOT"
  fi
}



cmd_create_project() {
  local path="${1:-}"; [[ -n "$path" ]] || die "create-project requires a path"
  ensure_ctx "$path"
  mkdir -p "$PROJECT_ROOT"
  info "Project root: ${PROJECT_ROOT}"

  cmd_scan --project "$PROJECT_ROOT"
  cmd_normalize --project "$PROJECT_ROOT"
  cmd_plan --project "$PROJECT_ROOT"
  cmd_generate all --project "$PROJECT_ROOT"
  cmd_db provision --project "$PROJECT_ROOT" || warn "Database provision step reported an error"
  cmd_run up --project "$PROJECT_ROOT" || warn "Stack start reported an error"
  cmd_verify acceptance --project "$PROJECT_ROOT" || warn "Acceptance checks failing — review stack health."
  ok "Project bootstrap complete"
}

usage() {
cat <<EOF
${APP_NAME} v${VERSION}

Usage:
  ${APP_NAME} create-project <path>
  ${APP_NAME} scan [--project <path>]
  ${APP_NAME} normalize [--project <path>]
  ${APP_NAME} plan [--project <path>]
  ${APP_NAME} generate <api|web|admin|db|docker|all> [--project <path>]
  ${APP_NAME} db <provision|import|seed> [--project <path>]
  ${APP_NAME} run <up|down|logs|open> [--project <path>]
  ${APP_NAME} refresh-stack [options]
  ${APP_NAME} verify <acceptance|nfr|all> [--project <path>] [--api-url API_BASE] [--api-health URL] [--web-url URL] [--admin-url URL]
  ${APP_NAME} create-sds [--project <path>] [--model NAME] [--dry-run] [--force]
  ${APP_NAME} create-pdr [--project <path>] [--model NAME] [--dry-run] [--force]
  ${APP_NAME} create-jira-tasks [--project <path>] [--model NAME] [--force] [--dry-run]
  ${APP_NAME} migrate-tasks [--project <path>] [--force]
  ${APP_NAME} refine-tasks [--project <path>] [--story SLUG] [--model NAME] [--dry-run]
  ${APP_NAME} create-tasks [--project <path>] [--jira <file>] [--force]
  ${APP_NAME} work-on-tasks [--project <path>] [--story ID|SLUG] [--fresh] [--no-verify] [--keep-artifacts]
  ${APP_NAME} iterate [--project <path>] [--jira <file>] [--no-verify] (deprecated)
  ${APP_NAME} version
  ${APP_NAME} help

Environment overrides:
  CODEX_BIN, CODEX_MODEL, DOCKER_BIN, MYSQL_BIN, EDITOR_CMD, GC_API_HEALTH_URL, GC_WEB_URL, GC_ADMIN_URL
EOF
}

main() {
  local cmd="${1:-help}"; shift || true
  case "$cmd" in
    help|-h|--help) usage ;;
    version|-v|--version) echo "${APP_NAME} ${VERSION}" ;;
    create-project) cmd_create_project "$@" ;;
    scan)           cmd_scan "$@" ;;
    normalize)      cmd_normalize "$@" ;;
    plan)           cmd_plan "$@" ;;
    generate)       cmd_generate "$@" ;;
    db)             cmd_db "$@" ;;
    run)            cmd_run "$@" ;;
    refresh-stack)  cmd_refresh_stack "$@" ;;
    verify)         cmd_verify "$@" ;;
    migrate-tasks)  cmd_migrate_tasks_json "$@" ;;
    refine-tasks)   cmd_refine_tasks "$@" ;;
    create-sds)       "$CLI_ROOT/src/cli/create-sds.sh" "$@" ;;
    create-pdr)       "$CLI_ROOT/src/cli/create-pdr.sh" "$@" ;;
    create-jira-tasks) "$CLI_ROOT/src/cli/create-jira-tasks.sh" "$@" ;;
    create-tasks)   cmd_create_tasks "$@" ;;
    task-convert)   cmd_task_convert "$@" ;;
    work-on-tasks)  cmd_work_on_tasks "$@" ;;
    iterate)        cmd_iterate "$@" ;;
    *) die "Unknown command: ${cmd}. See '${APP_NAME} help'" ;;
  esac
}

main "$@"
